paperId,title,url,publicationTypes,abstract,year,citationCount,authorIds,corresponding_author,venue,number_reviewers,reviewers,approved,comments,proceedings,edition,keywords,Citations
29ddc1f43f28af7c846515e32cc167bc66886d0c,Parameter-Efficient Transfer Learning for NLP,https://www.semanticscholar.org/paper/29ddc1f43f28af7c846515e32cc167bc66886d0c,Conference,"Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.",2019,2453,"2815290, 1911881, 40569328, 68973833, 51985388, 2813347, 2809991, 1802148",2815290,Nicosia,2,"50052368, 115300694",Y,"In the videos, it seems like the robot could get a slightly better view if it took another couple of steps.;Algorithm 1 does not make clear the relationship between the model learned in step 2 and the algorithms in steps 4 to 6.",International Conference on Machine Learning,19,"task,parameters,transfer,nlp,nlp","5c45a5d05ac564adb67811eeb9d41d6460c70135, 63adc1e5086481e36b19b62707a96b799da51e59, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 239bf45c13b3f6d38c74026b535f785febf9cd08, ef25b02f3be31c699255ee05aa90a4a17461d95d, a0a79dad89857a96f8f71b14238e5237cbfc4787, 2e965b5d97c2d6fb4af284307735be39283792ba, 8b417c2be7a7707f372049fb1193f0d42f799562, c9f320789e98d2c7a798a9705e26dbe317677966, bdb68c5e2369633b20e733774ac66eb4600c34d1, 322d91190acd8ac8c64598f5126947b0485ba249, febe776e285dc5e72c7e3ee697a87a794e1c00ff, 0456ed94f6c455f99cb67abe8a28aeb3ef9f489b, 92930ed3560ea6c86d53cf52158bc793b089054d, 013eb12ce5468f79d58bf859653f4929c5a2bd14"
d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea,Energy and Policy Considerations for Deep Learning in NLP,https://www.semanticscholar.org/paper/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea,Conference,"Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",2019,2113,"2268272, 47079359, 143753639",2268272,Bratislava,3,"38608914, 2285816475, 2109716565",Y,"1. The authors claimed that this is the first end-to-end trainable hierarchical attention model, but there is a previous work that also addressed the similar task: Seo et al, Progressive Attention Networks for Visual Attribute Prediction, in Arxiv preprint:1606.02393, 2016;on the positive side: The paper is well written, quality and clarity of the work are good.;There are many typos (e.g., ""This advantage is also its difficulty"", ""Much previous work on language modeling has evaluated "", ""We focus in on the task"", and others) so the writing needs to be significantly improved for it to be a conference paper, preferably with some help from a native English speaker.",Annual Meeting of the Association for Computational Linguistics,19,"nlp,hardware,networks,models,nlp","9d6aa5247b9919a86f174e918107c234c548274d, 13c4e5a6122f3fa2663f63e49537091da6532f35, 34ca47eed139a7f0694611528f75debc43385518, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 657fbf29ea0b4904a3e98d1556f9acf38dddae5f, f9253a3e2627f1c2a0a7e2cea320a4ec4e4d2ff9, 0ecf3f089e6dc7944b440227069b9d0143e18d78, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, 4954fa180728932959997a4768411ff9136aac81, 1c2efb418f79b5d29913e014a1dfd78865221c39, 2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c, fd0496ab020acf366375615ab40235e6dd3c5897, be2b0396de9431bae931642516a1d3e4906329f5, db6ad6ded1cfa26fdc7437f27fb823ec533e96fe, 77a096d80eb4dd4ccd103d1660c5a5498f7d026b, 82a09f72d7b9587e3b519b1cd9640a5a611f3480, 099043827df60225cf33c820052716cce64d49e9, f208b3fb28c556ab62f9d202b7beae89700a338a"
06d7cb8c8816360feb33c3367073e0ef66d7d0b0,Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks,https://www.semanticscholar.org/paper/06d7cb8c8816360feb33c3367073e0ef66d7d0b0,Conference,"How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions—training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.",2022,432,"1705260, 1817207, 1805993128, 2156538832, 2162779320, 1667817604, 2063938464, 2162779517, 2064353087, 1689466093, 2074097046, 8458211, 2162779839, 93841942, 9377739, 2110871234, 2158994272, 2162779709, 81331041, 50494955, 40879549, 1423660254, 1576655836, 2067056655, 2162781785, 39765564, 2159207824, 72254820, 51479145, 1380075136, 2112127876, 2150098899, 19255781, 2126503480, 2144058688, 2064619864, 1699545, 144365875, 2548384, 1783281",1705260,Vienna,2,"153475895, 1753223",Y,"Good contribution, paper needs to be made clearer This paper thoroughly analyzes an algorithmic task (determining if two points in a maze are connected, which requires BFS to solve) by constructing an explicit ConvNet solution and analytically deriving properties of the loss surface around this analytical solution.;Considering representative synthetic problems is a good idea, but it is not clear to me why this particular choice is useful for the purpose.",Conference on Empirical Methods in Natural Language Processing,22,"instructions,nlp,models,tasks,nlp","90aca7b4cdd728b28b2fb5b4dc3ae3e37daa5b94, ed9e7821b3e51c7e59183300d6c8cf90c8de0f26, a8d76d84408c1fe6b1543084e6cec3dfc4ede429, 017010b941d902a467f6d329ae5e74fd67e67912, 3db1841fd5f2561a11dfbd8173616b3e695c84a1, 43e624ddeed82df944a6cae0dedec3372438e243, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, 453fdfeefd6498a65be339d7e8722f6f3288907e, d9b5194f3f959eda2e95df6a340254f52ced46f4, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, 518a7c79968a56d63a691d42f8378be6c776167e, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, 8f99f5cb3950fcd1628c6a563be4b4fc4966fa66, 91b2b47cabd800ef658b65bfe1f52b7293a740c3, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, da3f33d858586d24cb265e79eb54f3746e998f57, fa7b8acd47631bada5b66049824bfd335ac6bf8f, 3dd7f7118ee174265889d00d100cfe2a02871be8, 56266342b01a4f2ddc28a1e8401dbbad105736a5, b795c74a0150ec091003ffbaa5bd7d74487c137b, ed935c6b359a7a486c28240d796e84897d095125, 156d8e2aa90b5ccc9be10477ca70deaad0151387, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968"
5471114e37448bea2457b74894b1ecb92bbcfdf6,From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models,https://www.semanticscholar.org/paper/5471114e37448bea2457b74894b1ecb92bbcfdf6,Conference,"Language models (LMs) are pretrained on diverse data sources—news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness.",2023,72,"2114887261, 50487261, 2169159066, 2073587169",2114887261,Andorra,3,"47540245, 2113511266, 2152050780",Y,The particular structural elements also induce certain inductive biases which make learning or generalization easier in many cases.;The manuscript mainly presents a cheap pruning algorithm for dense layers of DNNs.;The main novelty in this paper is that it uses the label as a third view of a multi-view model and make use of cross moments.,Annual Meeting of the Association for Computational Linguistics,23,"lms,biases,data,nlp,nlp","ca0e479ba2327f71e842d033b6b48b082962cc6a, dca4d9abbc82e57dfa52f932e893d467a63e0682, 4e58100b319d74f97ed550a4e5fa32dea8c06fe1, d88c1255876b62fb5f5a8b292098ca430710a540, 4b9184937da308914b9e13c43bfd75845eaf910b, fd3259ecd1c94731f5d9bd9c0bf7417f2e147c71, e30d9b8ce108d982169621b88a5e3fb69fec70e1, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, df7336844a31165db0ae08f1cd0f560c9e3faeea, ae026f29c2d571871f426ff4873d43b4ff90b9ad, be383c607d4d357c763d2329ab71799c6e1393b4, c30c0092bf4eb8a44faec3fc60cdd5006276bcdc, a830083704284c8c5ddaf04f676c6ce23d583942, 5030702fea15d66a73fc997325431f1d7945ad9a, 598231eb906b183f7a2a408ef4536127e11e3de9, 0090023afc66cd2741568599057f4e82b566137c, 8ec5896b4490c6e127d1718ffc36a3439d84cb81, ce54e3b89a2570035b70885e6901ad4c92ae41c9, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 4a7eea3ec3080ecb277bfe466afce4822a1071d7, 8fd8cb4951ae9634a378383a880fbf16ac5d6926, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, e8b7a9be9f2d0578a95319ed5841978e10429967, 447884e7da189102189a156966623335c72199b0, 08764019e9762da527253b37b0ff39c46a4206b7, 4267178106cef2e77284bde309dfaaf9fd46a91b, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, 78e80b6f3fa46e26ea4e8542815305c3ba88c5df, ba9b6f805feb62c978d384211f910790643a023e, 60119658af638693f6de23d8466968e60c428ac7, 11342d45911ee8a7c9e3a94117ce774ad7036172, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 933baeec555352784848a93284c9dd0e79477759, d7fddafbbc372da4fa884f67bdc32db71b888806"
13c4e5a6122f3fa2663f63e49537091da6532f35,Are NLP Models really able to Solve Simple Math Word Problems?,https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35,Conference,"The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered “solved” with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.",2021,385,"1443788809, 92954142, 144260125",1443788809,Dublin,3,"2805081, 2146671969, 2328103",Y,"If this is not the case for some reason, more detailed explanation is needed.;6. Figure 4 seems a bit misleading.;This seems at odds with models, such as DRAW, evaluate the likelihood -- once at the end of the generative drawing process.",North American Chapter of the Association for Computational Linguistics,21,"mwps,solvers,mwp,nlp,nlp","950850e22e42201f152d90dc6f53d53e39d37657, 5c39e37022661f81f79e481240ed9b175dec6513, eeac4411ae119c6c7ac33a11f762f2495b4dd960, 453fdfeefd6498a65be339d7e8722f6f3288907e, 72a6a5d5864eb915231c7128b90977f1d2acf6e9, 00d1f3423a33f73ca6aee884a58834547475d2f0, bf69c98fca9a9f6c1cde871beddbcdc668b77771, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, cf41991d89301c3c12420d150792cb1163999962, 2097ff87df3cb9427c7388bc7b997ed56907d45b, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, 7637ed79d30d0139901175ae4abedd822c217ab4, 643da4c4de1954daeac571a82367241db012a8bf, 538fbdb8013ab43a9b5d725461b294ad29fcced7, f7b69b8babfa607f2e8b0371f24cb720a7a827d6, 742747c7a453b293352b772d0d99541c96a351c3, fa26a6d434450b185e669170e79fd3e1d29716bf, cb3968152f7d93f53d24b00279a90d5071ddc85a, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, 92afbbe41174a545f9da9992e33c9a9592e529aa, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 364128bcce9836d60e685bb717b80f30e25092e0, b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8, b651d67502790e1d6d41c589e1d93e996ba7b935, 2478f43de2f1fcd9301a81e9b42d86b1d46db02d, e4788ee4f5e90c6f42cedc5116acd2d6475c3180, 73cbaf5f2441ef3478266b5438c0e90d1ae71652, 09797949fc70fbad8f3fed4f6cf4a91a9c709652, 2019cf49b51021a376f9833a53565513f0d8107b, 41c93960a066876d5e4f1dacaef75cd8daa2791f, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, d916776e0c6a04b0def4c22257c188776c2edab2, 9a60d3c9d3a5b7f165325fca45bd418d651682d3, 6ba00c2386f2edc0b43eec442cd1923b5d964633, b473e91cbe80c8b46451b49153cd5f93030480ab, 4e58100b319d74f97ed550a4e5fa32dea8c06fe1"
33ec7eb2168e37e3007d1059aa96b9a63254b4da,Beyond Accuracy: Behavioral Testing of NLP Models with CheckList,https://www.semanticscholar.org/paper/33ec7eb2168e37e3007d1059aa96b9a63254b4da,Conference,"Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",2020,866,"78846919, 35232494, 1730156, 34650964",78846919,San Marino,3,"2868254, 89843190, 1707355",Y,"The paper is based on the cyclical learning rates proposed by Smith (2015, 2017).;In Tables 7 and 8, the human beings agree with the LeNet in >= 58% of cases.;However, batch normalization only sees the variation in the activations given to it by a SPECIFIC set of weights.",Annual Meeting of the Association for Computational Linguistics,20,"nlp,models,checklist,test,nlp","0fe0f08e71e188cb2a663f36acb296d9eb6fe694, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, b2114228411d367cfa6ca091008291f250a2c490, d47a682723f710395454687319bb55635e653105, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, 12c9bcf710d30ba991fb765ace07f177f53ecfd9, 01bf0e83159712fbbbd12171a7e268547a4cfbc5, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, 6d2d6707a0674ee41d539d106e6cb359a0a6ca06, 0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4, 9db0247728950788a2b42097d81dc0e24eed6bb2, 819167ace2f0caae7745d2f25a803979be5fbfae, ccca203382e5dd198c089a0f1d7af7bef0f694e9, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 417326e51d78ba8bd2621f23e539b41bbdd336d6, e32a2519b59d62cff6cb8136ee242dc3754ed57b, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, a9c1566119695250f68a572a4260b03721cc8ba3, 1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0, 68ade27b68c05d1b60b8a9d8f8aadbe76900c60c, 2019cf49b51021a376f9833a53565513f0d8107b, 518a7c79968a56d63a691d42f8378be6c776167e, 0599f45e03ac2016321df0dd653ba4c0034c79d5, 8713452753fd01de5616121af93e173d4f74eaf6, 34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61, 4cd033a56b19f87f6adfefeef5fcc990306ecf40, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, eacf9284a39adcd56172665f31fd5a72560bba7a, bcc9245ebf57072c07c5a9d4eff3aaaac36cb7c6, 647d0e540627c5a903299a90d20530f8e48c18d9, 00bbc94806ca0821a9c82d8aedf16f0e6263b89f, 77e6c9917536949a82e5ca02c4882b69ee8a4fd6, 6dc4883228c95e8a332320fcc587a0ff33c84d59, b6593b5ad6b0e00232ee88cf2bc578645c1299a0, f3d594544126e202dbd81c186ca3ce448af5255c, b067177b1e17287185eb3b82ccc3d7c646b3ec40"
97906df07855b029b7aae7c2a1c6c5e8df1d531c,BERT Rediscovers the Classical NLP Pipeline,https://www.semanticscholar.org/paper/97906df07855b029b7aae7c2a1c6c5e8df1d531c,Conference,"Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",2019,1193,"6117577, 143790066, 2949185",6117577,Stockholm,3,"6413143, 51488437, 151213231",Y,"All components are trained with dense supervision (e.g. loop closure, ego motion with orientation-position, and the ground truth local overhead map).;Summary: This work is about model evaluation for molecule generation and design.;I found the formulation of the \alpha to be non-intuitive and confusing at times.",Annual Meeting of the Association for Computational Linguistics,19,"nlp,model,information,pipeline,nlp","ca0e479ba2327f71e842d033b6b48b082962cc6a, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, d3855b7351c11145e51301e6b686f748ca35c802, a620ea8654f86b282e0d2c1fbc6616b90ca45bd1, cb3968152f7d93f53d24b00279a90d5071ddc85a, 7bc9607c5cf3fc817675d46844f529097d579514, 98b9086750f08a21c8778ab986339321e9caf790, 742747c7a453b293352b772d0d99541c96a351c3, 9d6acac70b2d1fdb861a08b00766ef263109cd7f, 244e08e109f6f4fa0f846977e1aef1e7b6a9e816, 3994334c81478a4b17341eb1f494dbccbb73d999, f3d594544126e202dbd81c186ca3ce448af5255c, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, 3813b88a4ec3c63919df47e9694b577f4691f7e5, 2f44ab0e52eb98fdfc0086638887f175b6f2fe4b, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, e449b9b3fe04fe260731a3c74d2123bf6eaadf5b, 5f51d468ce730eeade7e9f419a1fe7152582be25, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, 332e0eab5fba8e6940f3e481f542a99ac17b9717, eed62d36d1b976ac3873c83645f1c25f5096f89c, c84389369720dcd2f004c48e58fbac2c45c8f092, 6001895c2fcf69528d01306e9d293d9d2a4cc67b, b9b639522465cc606df878eee62e7f9c4bf19e62, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, 551bbf493f55ea2c7b64ef8b91fc81d7bf6d68fe, 54ddb00fa691728944fd8becea90a373d21597cf, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, 6be56f559a74c0124526242e70cbdfd16cbc60a7, 6dcb1cd576b0e54b900f45a178efe271c383de04, be082d70534db088315f2cc5b42c2fdcd58c1b8c, 709af143f78bc62413c50ea1a7ee75b0702c4f59, 0601e9e434b30320c316c76228b97c093fa98ad6, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, 147c868b721c8d29df7c61db7f2360114c760614, dad8965c5a4c0a0ea1eb3837c6a9c3b42597c2ce"
d47a682723f710395454687319bb55635e653105,Language (Technology) is Power: A Critical Survey of “Bias” in NLP,https://www.semanticscholar.org/paper/d47a682723f710395454687319bb55635e653105,Conference,"We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process. We further find that these papers’ proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “bias” in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of “bias”---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements—and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",2020,838,"3422038, 2881033, 2065041692, 1831395",3422038,Monaco,3,"2108724347, 9162688, 31347453",Y,Their abstract also claims to utilize a convex programming formulation.;Hence inferring a structured latent space is a challenge.;Follow up experiments extend the basic setup significantly.,Annual Meeting of the Association for Computational Linguistics,20,"bias,nlp,systems,papers,nlp","f5ac3ec506a5a01807dd7196c0c52eef008b78cc, b795c74a0150ec091003ffbaa5bd7d74487c137b, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, e7f3478fd8aac6940a4bf4f5eb60ac38f6b0b85b, 8c33ca066e2ab615e24c65198c794114436053dd, f4207bcad24fe4eaf3849d6c1fe38c36545b5cb3, 08be4e23951a0def1c5d235befbb39c8d8d373a3, 256e95ca331cbd35b3a23cc306b6627e6771a963, 410fba9f03212257d0881811802e6620e59bc827, 8c7aee0bbc062d5b4bcd34951b1e002274288206, 75c364909914f17791837ec88090262aa6656d3e, ce2d5b5856bb6c9ab5c2390eb8b180c75a162055, cf5dfc4a9f7a82b32640128ca10832eace55880e, fd0496ab020acf366375615ab40235e6dd3c5897, 0af24df95f8a0b7e6fc257a98a5820314d1243f3, 8e259f940f007e08207ddb7c3a052f52036d7bf6"
c9b56cb026a38e39bb0228faac57accd6f65e6f7,"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP",https://www.semanticscholar.org/paper/c9b56cb026a38e39bb0228faac57accd6f65e6f7,Conference,"While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack’s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.",2020,522,"153769695, 1453652787, 1693182792, 1829303908, 2068347799, 121817403",153769695,Vilnius,3,"3010457, 1410115257, 48919600",Y,"It might be good to emphasize that you don’t train on the IWAE bound in any experiments.;But the current system is a great start.;When citing Shen et al., 2017, consider also mentioning the following: Controllable Invariance through Adversarial Feature Learning; Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, Graham Neubig; NIPS 2017; https://arxiv.org/abs/1705.11122 Response read -- thanks.",Conference on Empirical Methods in Natural Language Processing,20,"attacks,nlp,textattack,code,nlp","b977e8de38dc0d13817bca1ed20036badfe2a58c, 48265726215736f7dd7ceccacac488422032397c, 4dcbe71d261ba5caa0a06d5c265b5509edbbe1c4, 16753e0317730e8c1b297338300a8c6163dd06f2, 2c3eef2f17369912e330281d54b535675077e4ca, f2bc4057e696f49c326bf8e1588772a16f053754, f381c53aeb7742e4047d06d84f9e0c4f523231a3, a357f1ff27e184d9a5ef69e665e8ca891032bf71, 71854ff4306cf65c3c2161f7be2d0346275f72d5, bc44c0c64a473e035b11ae60a1993ad3db1acd2e, dc10a3f4dae2db48148ff6781454ddcc856ae8da, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, 733fc094e785724621c46e20db1be69f132ad9df, 0df5a4f9cc8a244715fe9968732497d2ac2a7cd1, 00d1f3423a33f73ca6aee884a58834547475d2f0, d2a609ffb814442d0728aef9f6616f9cd775face, 8b28792f8405b737229afb92c99c579b86d8aa98, dcbaf58b16ac7ef947879ea37c021466357b291a, 929305892d4ddae575a0fc23227a8139f7681632, 4be9368abc2474d6fd38639e523cf03af1873fd9, 5ea7bf772fecf95cbf53b2c7f719c9440322a115, 784141489258258b12979061d92c1a616da26525, ed9e7821b3e51c7e59183300d6c8cf90c8de0f26, febe776e285dc5e72c7e3ee697a87a794e1c00ff, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, c5c4142a01981787a71bf6ebcb791520c458ab5d, 39602922b04885047254444fd1a1586d797617ce, c84389369720dcd2f004c48e58fbac2c45c8f092, b09139c153bac8893e8faea2b3a59159234caadc, 6b3756d32ab5b0a5715a5cfc3672290d2d643017, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, 665b0c776ff7507c32793f10ce9edf90bc2f674a, 0863a5ce955e5193e535e1442086dc460dd295f0, b9518627db25f05930e931f56497602363a75491, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, b781fb7f3725a9d899d3d250b378d729a8a00442, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, f72d3f58ff73353978e224af348448b34d27cf7b"
0e141942fa265142f41a2a26eb17b6005d3af29e,The State and Fate of Linguistic Diversity and Inclusion in the NLP World,https://www.semanticscholar.org/paper/0e141942fa265142f41a2a26eb17b6005d3af29e,Conference,"Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the “language agnostic” status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.",2020,532,"47070483, 50074956, 2410839, 3086996, 143990839",47070483,Chisinau,2,"32830771, 2163313042",Y,Do you get comparable inception scores?;The main signal I lack from reading the paper is whether the proposed model actually does better than a reasonable baseline.,Annual Meeting of the Association for Computational Linguistics,20,"language,languages,technologies,nlp,nlp","696b388ee6221c6dbcfd647a06883b2bfee773d9, 9a0424bdd12cdcdf45b556b0b9dcc6fc5a55520b, 2c3eef2f17369912e330281d54b535675077e4ca, c30c0092bf4eb8a44faec3fc60cdd5006276bcdc, 20a3ed03888037e2802fa9abad02ffa0a8dcc228, ff74bfbd9ebf4c54809873aecb04be27e9402cb8, 9eacf62f1e546748428c7e4843731b1595294200, c997ed5ac5772bb0fce8bb11b8c86bc33310f39a, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, 05b8b67451fb105576c58af960e6e6d98f9103e7, 119a9e5b563cf1134897553ee49325b5a5bd9fb9, 0456ed94f6c455f99cb67abe8a28aeb3ef9f489b, 6f31f59f7b4bd751b5d8657d8a878bef571f9a4c, baafed5f8968118af04dbbb1cf172f1c10bede25, 9eea59c34f139f3d2153226c8cf026e975622074, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, a2ec47b9bcc95d2456a8a42199233e5d9129ef18, 82870bc488b57cdf5ea62877109a7278af2926b3, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, f497c1ece7b6f3560bb39958e2673f476d608f98, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, 7c217cc7524251f42887438834912e06129c3299, 799d5a8271887adede035644d878c7bd555576df, 5371896313ac227eb819038dd55f213cb42b99e2, e4788ee4f5e90c6f42cedc5116acd2d6475c3180, 695bdc6e24608364491b9418a220c65a7cd17413, cf5dfc4a9f7a82b32640128ca10832eace55880e, 62fe1c3a866a5e368e110d6d8ed2385c84072cac, 102ebe229df18c8733ea1b8def56cd79996e2178, 38179848e2d6a3ad373b1793848816111428ac36, e2f3aa4ecc487fa496d1a51e875e747d4b7e6001, f8e57fa370fe10147aa22714e08409fc1b7dae4b, 40416ac3bf78583eea37661b1b446e9939245b3e, 929305892d4ddae575a0fc23227a8139f7681632, f18be38578ee52aa7071c404d42e3d53ae003122, 6628f9ee35e36cdfdcac8a46cef4dba8d529a83b, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 6ff909c6fe089fc8ebfc64eca0f0c3cc34ba277f"
579476d19566efc842929ea6bdd18ab760c8cfa2,Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?,https://www.semanticscholar.org/paper/579476d19566efc842929ea6bdd18ab760c8cfa2,Conference,"With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is “defined” by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.",2020,408,"41016275, 79775260",41016275,Berlin,2,"145201124, 2309967",Y,"Interesting set of ideas and direction, but lack of quantitative analysis supporting the results.;In this paper, the authors propose deep architecture that preserves mutual information between the input and the hidden representation and show that the loss of information can only occur at the final layer.",Annual Meeting of the Association for Computational Linguistics,20,"faithfulness,interpretation,evaluation,nlp,nlp","80d9f0eb47b712988d19cbe29a7bfa63f2a175d0, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, 375125029b085e70a109491656b69aa01bc2a166, fc61c7221350806c25379f385c27b2102ff8eb57, d0ab11de3077490c80a08abd0fb8827bac84c454, 0e9a44ce661c3535d5ce747912540080324489f5, da9b8b4073e6ad44b3da66e1e117cb1ddbf8836d, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, 371a343457a4fbff00000bf4faa29b2b2f85744c, 8674494bd7a076286b905912d26d47f7501c4046, 7637ed79d30d0139901175ae4abedd822c217ab4, facd5f5deb152229ceb1803434d8690a09ab4129, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, 834fdec542153aae5fe725df801aac87ba5e8f56, 5030702fea15d66a73fc997325431f1d7945ad9a, 1b0aa15937fdf59103a5213bccf09cff83d0ee3e, 0fe0f08e71e188cb2a663f36acb296d9eb6fe694, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, 77a59de2e2b832321875cadcf9619dc313f02384, ec7f5dc077480df149bcd4358a3aa8441878ca59, a9cbbef8f4426329d0687025b34287c35bdd8b38, 08be4e23951a0def1c5d235befbb39c8d8d373a3, d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b, 239bf45c13b3f6d38c74026b535f785febf9cd08, 54a26f5ef4b0524c60249fe98d0fc3646b2791ad, 9b529fe170823f95509585d5aa39fa01a43558fd, 78e80b6f3fa46e26ea4e8542815305c3ba88c5df, 6aca07154c111f1c8738347d7112cad6b0bf974a, db4cf9f6a653d5c15973e836c800ea47743251ae, 980858461df7c4349f17b427686c5bcbcffbdc04, 68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a, 7b9b756ab509cb9f52dbac95e3e901d571f0784f"
77a096d80eb4dd4ccd103d1660c5a5498f7d026b,Dynabench: Rethinking Benchmarking in NLP,https://www.semanticscholar.org/paper/77a096d80eb4dd4ccd103d1660c5a5498f7d026b,Conference,"We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",2021,256,"1743722, 153408953, 40383658, 9264826, 80833908, 47039337, 2737827, 119869488, 1422035486, 1736107, 1500242049, 48662861, 2138053020, 1918552, 3422908, 143977268, 144922861, 2110032535",1743722,Dublin,2,"2729273, 82614785",Y,"I feel that the authors should give a more prominent disclaimer to potential users of the test.;If so, doesn't that correspond to evaluating the model under a different generative assumption?",North American Chapter of the Association for Computational Linguistics,21,"model,creation,platform,nlp,nlp","834fdec542153aae5fe725df801aac87ba5e8f56, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, 742747c7a453b293352b772d0d99541c96a351c3, 69a72ff5b30642d11c96635e99aadad3140d33a7, 733fc094e785724621c46e20db1be69f132ad9df, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, a13149a80855412d970d0de2b41c611f4cf7e1da, 7a1e71cb1310c4a873e7a4e54d1a6dab0553adce, 9b529fe170823f95509585d5aa39fa01a43558fd, cde985e542218aced8c4a627cda3dd12939805f1, 37b9478b792fb1e7ad5f2c322ed4445b93df6c9e, 598231eb906b183f7a2a408ef4536127e11e3de9, f406aceba4f29cc7cfbe7edb2f52f01374486589, 9f5b82d9915d0752957602224c5056be7e749c83, 11342d45911ee8a7c9e3a94117ce774ad7036172, fa26a6d434450b185e669170e79fd3e1d29716bf, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, 71854ff4306cf65c3c2161f7be2d0346275f72d5, 9e5d389fcdce4c0a6827ebc804b4e863aa28c14c, 787ae2c51cd82b904bb4fb9ccb15266381af5436, ba9b6f805feb62c978d384211f910790643a023e, 647c4a9331e01e31a4350361d3460f0397fe694f, 6001895c2fcf69528d01306e9d293d9d2a4cc67b, d88c1255876b62fb5f5a8b292098ca430710a540"
087dd95e13efd47aef2a6582e6801b39fc0f83d8,ERASER: A Benchmark to Evaluate Rationalized NLP Models,https://www.semanticscholar.org/paper/087dd95e13efd47aef2a6582e6801b39fc0f83d8,Conference,"State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",2019,473,"48727916, 49837811, 8937909, 51172373, 2228109, 2166511, 1912476",48727916,Lisbon,2,"41020222, 2066229431",Y,"I welcome and are grateful for any theory in the area.;Indeed, the authors have commented: ""AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures.""",Annual Meeting of the Association for Computational Linguistics,19,"models,benchmark,rationales,nlp,nlp","63316bb5b88d362051c048e864c3ae5d97a26d30, 537f5e8e4139392cd2d108f32495e5b2b80151ac, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, 6aca07154c111f1c8738347d7112cad6b0bf974a, 2c3eef2f17369912e330281d54b535675077e4ca, 4be7d1524edb0137599a5cc95f72844b85a52fe1, b27c98d8378848f2c23a067f2c5196f3b5a07572, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, 5107c83173e43f51d1bdebf6cafda525a7c26bf0, 1e1cf81a1113482be3f0c280db994a832cb9426a, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, f91dbd39d4c742ba675e447b04a0b0c70b33e836, 371a343457a4fbff00000bf4faa29b2b2f85744c, 453fdfeefd6498a65be339d7e8722f6f3288907e, 7bc9607c5cf3fc817675d46844f529097d579514, 9e66ae24a541255c2d931184498ee116ce81478a, d0238f2fa88b0316ab862f843a5521eff0fd9bf5, a5710a7a54b50c26f3c40c37a19b2ec63330e90b, fd0496ab020acf366375615ab40235e6dd3c5897, 14dd50979af27bd2574c8068db11d27028b56afd, eda6756ab2844c390584686dc5e6385f4a8369cd, b85f3a66245d483f3eb3447eaf9950bd55f2b21e, 8c33ca066e2ab615e24c65198c794114436053dd, 6a1b25f7a67395ad1e676027322913acbb0a0635, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 0599f45e03ac2016321df0dd653ba4c0034c79d5, 74bc39003e65119eaa6ba339a61b45b417a638b7, d280cf82a6144b3e168840802b1a8a14d4eaccb9, 6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, cb3968152f7d93f53d24b00279a90d5071ddc85a, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 6dcb1cd576b0e54b900f45a178efe271c383de04, 71854ff4306cf65c3c2161f7be2d0346275f72d5, b651d67502790e1d6d41c589e1d93e996ba7b935"
a747e8f2659df479c0092301b9658fc582423df1,"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia",https://www.semanticscholar.org/paper/a747e8f2659df479c0092301b9658fc582423df1,Conference,"NLP research is impeded by a lack of resources and awareness of the challenges presented by underrepresented languages and dialects. Focusing on the languages spoken in Indonesia, the second most linguistically diverse and the fourth most populous nation of the world, we provide an overview of the current state of NLP research for Indonesia’s 700+ languages. We highlight challenges in Indonesian NLP and how these affect the performance of current NLP systems. Finally, we provide general recommendations to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.",2022,56,"8129718, 9162688, 2789148, 66986482, 2279712392, 1935324, 46199596, 35722593, 2368148, 145465286, 1800564, 2124014463",8129718,Oslo,2,"5840536, 47224454",Y,"It would be nice to provide some sort of analysis of it, even an empirical one.;Moreover, the authors proposed updating the parameter \theta of the generator g_\theta.",Annual Meeting of the Association for Computational Linguistics,22,"nlp,languages,indonesia,research,nlp","1ab91d6ac7afc1a0121487a9089fa70edc1634d4, 8e259f940f007e08207ddb7c3a052f52036d7bf6, a1ef4052acb63356928bb440874c470ad48cb40c, 353c88c231ce156d604e074af276422422fc73f7, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, f7b69b8babfa607f2e8b0371f24cb720a7a827d6, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, fa75a55760e6ea49b39b83cb85c99a22e1088254, 0456ed94f6c455f99cb67abe8a28aeb3ef9f489b, ec7f5dc077480df149bcd4358a3aa8441878ca59, 62fe1c3a866a5e368e110d6d8ed2385c84072cac, f4c4e148546089123f8da5db4fb246ab4062bd40, 695bdc6e24608364491b9418a220c65a7cd17413, 11be2469ab1d1c508e7b6e14148990741ba87884, 6544259ff6b335b1dcec75e031b6d57e5b9509f4, 5406e153957dd7a165264da6e6e5d81251997404, 9c1265fd47df8d825f0ddd6d9ab834002592e38e, 2141334fad7248fc707607bc9453d44686ae07a7, 8d67b76222d84dcd337b8a2c78f13837070a79ce, 178571a5cde984c895493e2eb6c5487449d055cf, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, ce54e3b89a2570035b70885e6901ad4c92ae41c9, fac67bf55456b52ac6e4f280ad953d0250c74ebc, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, 3dfa820702b6181c9964931f0a4d47fd298bf429, ada0b87cd5c30d31186c38fb12e631d29426a3bf, 649c3497e3b34b15a5011259fcb837cf6c1ac04a, ad10ef93675513a68b93d54f3a461160b53318a3, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 9a51df7eaf2457b034c49ca8ca4ca3b81e6c08c0"
598231eb906b183f7a2a408ef4536127e11e3de9,Challenges and Strategies in Cross-Cultural NLP,https://www.semanticscholar.org/paper/598231eb906b183f7a2a408ef4536127e11e3de9,Conference,"Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.",2022,72,"2064295987, 37922370, 49568895, 3295381, 30671790, 6547490, 83574123, 2093582149, 2125376289, 1717462692, 50110151, 82259306, 1660797358, 1700187",2064295987,Podgorica,2,"1803520, 2043402",Y,Are the test environments sufficiently different from the training ones?;The model is based on both source/target syntax trees and performs an attentional encoder-decoder style network over the tree structure.,Annual Meeting of the Association for Computational Linguistics,22,"nlp,language,efforts,speakers,nlp","3c8a456509e6c0805354bd40a35e3f2dbf8069b1, 752604994a7ca548ff2954114fc61a501d857b1c, 8760bc7631c0cb04e7138254e9fd6451b7def8ca, b415e836d447ea9efb7629a1de67cd2a6f9e7ba8, df7336844a31165db0ae08f1cd0f560c9e3faeea, f18be38578ee52aa7071c404d42e3d53ae003122, 410fba9f03212257d0881811802e6620e59bc827, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, f86f1748d1b6d22870f4347fd5d65314ba800583, 3994334c81478a4b17341eb1f494dbccbb73d999, 3faeb21fe256b99391d69570053a8c2d91e9f348, 0311ace1d499cadd1cc0c515a625d1d045f60d25, 68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a, 9b54941de1e21826ecc28b32730ac3f69991ede4, fb00016c1e048b9373803add001c1ec7e877cb23, 68a3d32416977e88cf1bfa4ad548d403f5f089d6, 69e76e16740ed69f4dc55361a3d319ac2f1293dd, 0cec0c296efedb814342b4b841d4583efbfc6777, 742747c7a453b293352b772d0d99541c96a351c3, 57a3fc6d0aaad3c10c793b4e59390ca04c935282"
3a7bbc46795929f0eace82b64c44c92a48682fb5,FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP,https://www.semanticscholar.org/paper/3a7bbc46795929f0eace82b64c44c92a48682fb5,Conference,"We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to “mix and match” various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a “model zoo” of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",2019,679,"2403712, 2077245166, 2725328, 4565995, 134757625, 2742129",2403712,Warsaw,3,"143651788, 2243348413, 5486617",Y,"The fact that the proposed technique is simple yet yields such speedups is encouraging.;In light of this, I think it is reasonable to ignore the fact that the proposed initialization does not provide benefits on Penn Treebank and text8.;The motivation is difficult to grasp and the contributions do not seem compelling.",North American Chapter of the Association for Computational Linguistics,19,"framework,nlp,models,training,nlp","59c414c9efb77562f5d1aad8af14eaac968c69c0, 3924aa213ff891812c66a6909ab902684d3eb107, 63d8426ba1f51a8525dd19fd8ec92934ec71aea5, c292e473b3825eeb9db03c70b2e1c033aea190d5, 742747c7a453b293352b772d0d99541c96a351c3, c2528e88d5554e9df9f9d482ad46cb5331c4d794, 7669fca7fbc071b4ca78b4c326ff8f3a4b6ae616, ff7bcaa4556cb13fc7bf03e477172493546172cd, 84a36e19f9394f22b34f79756fa9628a795e02ea, 1661d0d8d47cac41e01c59c60aac3675b4396698, 152877c51df17cdd4a87d19e452c6daecfadf6c3, eff6546819d25df0bccdc89f02554a43a4f1c464, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, 263a58f4fd32caca1dad2351af4d711aec451fe6, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, 340f48901f72278f6bf78a04ee5b01df208cc508, 9712624bb61abb0da989514cae558cfab61bb9d2, c50a909e20bd07f4aea09dc6dae539b45b406a96, 0894585294c67193ff3190240554677b56fd79a0, 4b1280229ced73f6c86550f24ef01490fde52285, f5ac3ec506a5a01807dd7196c0c52eef008b78cc, 024006d4c2a89f7acacc6e4438d156525b60a98f, 62df84d6a4d26f95e4714796c2337c9848cc13b5, 2e965b5d97c2d6fb4af284307735be39283792ba, fdc57c18f3b636c3273542327ae540217972558f, 1c748f86182a62d44d5b44316db510f8d833e19f, 44786a2c2a8ba8cf5c74a8fb10098c220e924c56, 4aec7bcb292cb763341a22fc67a1cfaceb3a2c67, 91b9d3ab7532ea24ae70cd726355f25235b1fe8b, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a"
be7cb8f79bc018e57467168fc0c7f8ad59bba04f,Adaptive Testing and Debugging of NLP Models,https://www.semanticscholar.org/paper/be7cb8f79bc018e57467168fc0c7f8ad59bba04f,Conference,"Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor, or only work for a very restrictive class of bugs. We present AdaTest, a process which uses large scale language models (LMs) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model. Such bugs are then addressed through an iterative text-fix-retest loop, inspired by traditional software development. In experiments with expert and non-expert users and commercial / research models for 8 different tasks, AdaTest makes users 5-10x more effective at finding bugs than current approaches, and helps users effectively fix bugs without adding new bugs.",2022,53,"78846919, 23451726",78846919,Bratislava,3,"1505828520, 103131985, 1996394",Y,"After reading the paper, it’s not clear to me what the components of the model are, what each of them take as input and produce as output, what these modules do and how they are combined.;* There are many estimators for f-divergences (like the ones cited above and many others based e.g. on nearest-neighbors) that are sample-based and thus correspond to the ""implicit"" case that the authors discuss.;It is not clear how a softmax output of a CNN, which is trained in a supervised way, follow such assumptions.",Annual Meeting of the Association for Computational Linguistics,22,"bugs,models,users,nlp,nlp","155f27879f185f1ab04107c91c2ae7cf6a910a03, 624b2f14be4287d6a400cdf88a6f911b434b182e, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0, 6fb020754f6de564c3a0a07bb656c0a90be1f87d, ae38dc77a962161107361f213db9216ee1274037, 6d2d6707a0674ee41d539d106e6cb359a0a6ca06, bdb68c5e2369633b20e733774ac66eb4600c34d1, 1ed33896e82cc3810b349cdffc2039f0b8edd82e, cde39ce861e4c7514ee07fd91b6b8aac50cbf01b, cd29c25c489562b409a60f83365f93f33ee1a0a1, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, eef23d76e175c0cff8e81ffcb2721c10539c8cbd, da3f33d858586d24cb265e79eb54f3746e998f57, 0456ed94f6c455f99cb67abe8a28aeb3ef9f489b, a9cbbef8f4426329d0687025b34287c35bdd8b38, d57f11ed40c3cdcbb36cb758191db4f2c9372965, ca05ca5f70616456d0f2e05c4a51cbf8b8d273f7, dd2deed2ce6e110236a1280db765fa02c7488eb1, 29409efa04ac99ccf01d2a011d21d5d14e870000, 0599f45e03ac2016321df0dd653ba4c0034c79d5, 024006d4c2a89f7acacc6e4438d156525b60a98f, a3636512a48321baab95c94052de2a0a88460602, 557ee73ff4e87ca0cb1b6c78af0730a2d94b710f, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 8f2cf30f9c825d8ef7d622601dbd525ace95e025, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, 15370f51d666ab8ef17185679553c6a8647b2a15, cb12fe714dd4fa1d14b6a8023d494c14c89e90ea, a2a514ed839dafdd0fb76d6c2615f25f35bf8087, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, f18be38578ee52aa7071c404d42e3d53ae003122, 45de91a919780d5540872cf047986a370625e61c, 390bcf15a1b13cb0d5966859c35c69a31238e838, 9712624bb61abb0da989514cae558cfab61bb9d2, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, a0d18dddaa995b126ad373e33767b9b881d16b2f, 8babcaf89f8537dc628a029ebf932100f57289fd, ff7bcaa4556cb13fc7bf03e477172493546172cd, 74bc39003e65119eaa6ba339a61b45b417a638b7, 24ab4e99e582c9770281eee0a39cbeb70ddd891a, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, b9518627db25f05930e931f56497602363a75491, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, f3d594544126e202dbd81c186ca3ce448af5255c, 6001895c2fcf69528d01306e9d293d9d2a4cc67b, d2a5dcecd2ffdf03473df1688091f08fadb114a3, ff75865cde62592d068b2afd055c57c81d77158b, a56bf7ee9a56d8f84079684339a953c2df9ce76b, fc32074b37a6d9dda535a70f9689022e70508520"
3caf34532597683c980134579b156cd0d7db2f40,Universal Adversarial Triggers for Attacking and Analyzing NLP,https://www.semanticscholar.org/paper/3caf34532597683c980134579b156cd0d7db2f40,Conference,"Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94% to 0.55%, 72% of “why” questions in SQuAD to be answered “to kill american people”, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.",2019,625,"145217343, 2113511266, 1380266797, 40642935, 34650964",145217343,Vaduz,3,"1864353, 1830383266, 47149500",Y,"How much can change between the goal images and the environment before the system fails?;What is LSS in figure 4?;The WAGE parameters (i.e., the quantized no. of bits used) are 2-8-8-8, respectively.",Conference on Empirical Methods in Natural Language Processing,19,"model,triggers,models,nlp,nlp","8babcaf89f8537dc628a029ebf932100f57289fd, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, ac4350d3a2673a4c34fa949714ede8b45fd04f1d, 950850e22e42201f152d90dc6f53d53e39d37657, eadb1e7da375939e25083ae3936c4f4ef1f2a719, dc10a3f4dae2db48148ff6781454ddcc856ae8da, 11be2469ab1d1c508e7b6e14148990741ba87884, 6be56f559a74c0124526242e70cbdfd16cbc60a7, f51bc74814a3452009ea5ca262d9768d08149ee6, 14dd50979af27bd2574c8068db11d27028b56afd, d9b5194f3f959eda2e95df6a340254f52ced46f4, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, 2bc3644ce4de7fce5812c1455e056649a47c1bbf, bbed457fd04ba4972018382d1a01a0bdde399d3c, 19b93280f17696a4ddfa2c75490a50ab107addf2, 2ee03e28208a9310a9be4032c2b04ebdddb83cc7, 02fa2389b1b64b661192e224bed8af6df0ce80f6, da5d78b3e3a1544fde98fba86088e1215e97cbe8, db528269ef800727245c0fcb35b692d29c1ccdc9, 16753e0317730e8c1b297338300a8c6163dd06f2, adc180e1fe404b650fca3bb7970e43bdce34a611, be082d70534db088315f2cc5b42c2fdcd58c1b8c, facd5f5deb152229ceb1803434d8690a09ab4129, 6fb020754f6de564c3a0a07bb656c0a90be1f87d, 3c68025d95970a9b9aa1b742a678704cd09d2bf4, bad4c08f03587e38ee960e2aa76e16d722826e7c, 1b3675fc0f2b16743b1e1f0c2f84829cfdb3d34f, 0fa554d981809c5eb78956c779f75092c4f6c16b, 8ee45aeb7c97e3346cc62f216f673b91277ac718, 76b93abde3ec4e5bd84a38d488a8db209dc4ac98, 15d1450a8797e2feaa4c0ca4ebcd43c8cbd8b61d, 2c03df8b48bf3fa39054345bafabfeff15bfd11d, f4207bcad24fe4eaf3849d6c1fe38c36545b5cb3, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, ad10ef93675513a68b93d54f3a461160b53318a3, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, 5031790972d496547b6613d46a4a0134c824db6e, da3f33d858586d24cb265e79eb54f3746e998f57, 695bdc6e24608364491b9418a220c65a7cd17413, d1efcef213c433445be56d7479eb47d972b3ee79"
d235a9085e0543fcbe502fbc269f9a8ee01dcbab,AdaPrompt: Adaptive Model Training for Prompt-based NLP,https://www.semanticscholar.org/paper/d235a9085e0543fcbe502fbc269f9a8ee01dcbab,Conference,"Prompt-based learning, with its capability to tackle zero-shot and few-shot NLP tasks, has gained much attention in community. The main idea is to bridge the gap between NLP downstream tasks and language modeling (LM), by mapping these tasks into natural language prompts, which are then filled by pre-trained language models (PLMs). However, for prompt learning, there are still two salient gaps between NLP tasks and pretraining. First, prompt information is not necessarily sufficiently present during LM pretraining. Second, task-specific data are not necessarily well represented during pretraining. We address these two issues by proposing AdaPrompt, adaptively retrieving external data for continual pretraining of PLMs by making use of both task and prompt characteristics. In addition, we make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers. Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings. In addition, in zero-shot settings, our method outperforms standard prompt-based methods by up to 26.35\% relative error reduction.",2022,33,"2109404730, 39798499, 2152290059, 2992833, 8652308, 48262024, 2145913600",2109404730,Chisinau,2,"77790220, 2772242",Y,"The idea has some novelty and the results on several tasks attempting to prove its effectiveness against systems that handle named entities in a static way.;For the application of images, using text description to refine the representation is a natural and important research question.",Conference on Empirical Methods in Natural Language Processing,22,"tasks,language,nlp,plms,nlp","cb12fe714dd4fa1d14b6a8023d494c14c89e90ea, 2374106a32169c07703599ff3f6f4b31e8067b89, 3dd7f7118ee174265889d00d100cfe2a02871be8, c4cdf2e0d89aadb13bf9c61654ff9e17be2b01b9, c419ee7315b9edfd8fc55bab16534fc55a564fcd, f12930cd5f58990badc1a7c5d2749cad004cfb0e, 7637ed79d30d0139901175ae4abedd822c217ab4, 0f38b3d717e0fcc6eacc9c6e78b252227440e04e, 8d06f13ec042f0d8d845af5deadeb3dd9ad11f70, f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d, 37b9478b792fb1e7ad5f2c322ed4445b93df6c9e, 8ec5896b4490c6e127d1718ffc36a3439d84cb81, c9f320789e98d2c7a798a9705e26dbe317677966, 41d6b6bd4bf44bf13b9157b603e33360e5e77a01, 4895c430c7810b45840b58cc9182f12143013a43, 5dfde01d761d97c3a6c609007531973eb1229d09, 2a15e9ac5593cd1f8bd3a724e78a173037c02fea, aa9bf8b4c4ce8c28b9f47d7ce4f416aa9726bb0d, 0a71dd8bec060195e14eb9d0a7abbc08d960d4d5, 1ff76ab0fcf22110df62337d462e15d79a2a2593, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, d7fddafbbc372da4fa884f67bdc32db71b888806, 8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, ca0e479ba2327f71e842d033b6b48b082962cc6a, b52db9e41e15f76bdcfbe674abe0314af545c430, b22ed1ea1d174af48c655d48e284afc239ebfa6a, e3b94a5f28522e6825aff16ff07d56bd70d26c96, b473e91cbe80c8b46451b49153cd5f93030480ab, 2dafea864f74a477414c3b71b742f7997e216102, 2019cf49b51021a376f9833a53565513f0d8107b, 639bfab64e2f35917d450013e136cb24c7755fad, 102ebe229df18c8733ea1b8def56cd79996e2178, 1cd497e82bdc46a9d3d28b2316ea7cbe9aee5467"
011095a0082e5e301f9bf30267b193c1c9e7e370,Perturbation Augmentation for Fairer NLP,https://www.semanticscholar.org/paper/011095a0082e5e301f9bf30267b193c1c9e7e370,Conference,"Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and train a neural perturbation model, which we show outperforms heuristic alternatives. We find that (i) language models (LMs) pre-trained on demographically perturbed corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE datasets exhibit less demographic bias on downstream tasks, and (iii) fairness improvements do not come at the expense of performance on downstream tasks. Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models. We hope that this exploration of neural demographic perturbation will help drive more improvement towards fairer NLP.",2022,34,"2149798086, 51519704, 2166312768, 51324296, 2111313627, 2110032535",2149798086,Helsinki,3,"2118438836, 2282113562, 2113619066",Y,"It would be interesting to see the results of the new activation function on LSTM.;After all, reward seems to play a very important role for the proposed system.;To start, why not plot the empirical histogram of p(z|x) (for some fixed x's) to get a sense of how well-behaved it is as a distribution.",Conference on Empirical Methods in Natural Language Processing,22,"models,nlp,language,datasets,nlp","4d4d1ac9a9ad8592a6cc05082437d706ee176a38, 69a72ff5b30642d11c96635e99aadad3140d33a7, 0a829289a16ae48837cc2905635435db98bacc76, 7bb477077968d68aa7a6059d8d6d801fb28274da, d422df8bff4e677a3077635db116679d25142bfc, 3b230f14c46e7e177e9bebb2ebc9f46b346b646d, 7fa273f450251523e6b7fcc2eb3fdbdfd4a30493, d8a5474f450330ad25c1e22f27e88f3630cb840d, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, fa75a55760e6ea49b39b83cb85c99a22e1088254, 9954ec0a95fc0840f7712d7b0c67b242bb536a9e, 8ba8a0d18a06752f5a39996ccf1e914da0941443, cd5b4b113d5fa90b5dc5aa372111b89d18df88fb, b135e330cc1473c8c24fa63bb9a5b64f51993f9e, 73d4accea441aae2373828a8dc2175aa2759c38f, 0f74e7b650f346676b12c44d16d774fda9a45c9a, 957f5b1e7ca48891c2e279aefbfa0f04d989c21e, 8713452753fd01de5616121af93e173d4f74eaf6, e57aeb158a38ccf33d2f0f5a8f63f1209497e329, 7669fca7fbc071b4ca78b4c326ff8f3a4b6ae616, 31a61d009442436d04b9d4e1c5beee37172289ae, 42375e9a309fe5bf5c671c918cd8c9d5d85568cc, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, 9eea59c34f139f3d2153226c8cf026e975622074, 62df84d6a4d26f95e4714796c2337c9848cc13b5, be082d70534db088315f2cc5b42c2fdcd58c1b8c, 74bec51a66499fcfbced16ff3fce696acf98c9e1, f4cfc7cbad257f1688772d59f694c16189dba811, 24d21ecaeb2d2ecc20e26a5e3f5128247704ccfe"
e57aeb158a38ccf33d2f0f5a8f63f1209497e329,What Do NLP Researchers Believe? Results of the NLP Community Metasurvey,https://www.semanticscholar.org/paper/e57aeb158a38ccf33d2f0f5a8f63f1209497e329,Conference,"We present the results of the NLP Community Metasurvey. Run from May to June 2022, it elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split in half on the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In addition, the survey posed meta-questions, asking respondents to predict the distribution of survey responses. This allows us to uncover false sociological beliefs where the community’s predictions don’t match reality. Among other results, we find that the community greatly overestimates its own belief in the usefulness of benchmarks and the potential for scaling to solve real-world problems, while underestimating its belief in the importance of linguistic structure, inductive bias, and interdisciplinary science.",2022,26,"38614754, 14487640, 119389860, 49355602, 144906624, 13336152, 93811144, 10666396, 46230016, 80842917, 1799822",38614754,Valletta,3,"48738717, 1919541, 2157681212",Y,"This is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial NLL loss and a reconstruction loss at the last hidden layer.;The explanation of the cause of ""super-convergence"" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments.;This paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values (possibly different for positive and negative weights).",Annual Meeting of the Association for Computational Linguistics,22,"results,nlp,community,respondents,nlp","9c11d9e75c6136943d2fa7ff7cb1f1090d406658, e9a986c8ff6c2f381d026fe014f6aaa865f34da7, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, a1ef4052acb63356928bb440874c470ad48cb40c, c12b80b44d9acfe6cd92fdf965264c4b706c367c, b795c74a0150ec091003ffbaa5bd7d74487c137b, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 98ce7af921e7c52d81df64d632d34eb09522cd75, b52db9e41e15f76bdcfbe674abe0314af545c430, 1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7, 794b3ffd28d28606230efc975eeec9f0522fb139, 4e58100b319d74f97ed550a4e5fa32dea8c06fe1, c9f320789e98d2c7a798a9705e26dbe317677966, 57e6cca1479a4642f867e69b4dee93d14259dc3d, d1206ccabd1980848f14472d6548251c2fab7963, f62ab3fcc45eb787f4eb3213a3ffcae97799e9e5, 3924aa213ff891812c66a6909ab902684d3eb107, bf69c98fca9a9f6c1cde871beddbcdc668b77771, 0c4070272fb7d6b98971a107b022ff8abf0aa55e, f5ac3ec506a5a01807dd7196c0c52eef008b78cc, db0cc2f21b20cbc0ab8946090967399c25709614, bd6c027a3604d6c8fa23435bf382455b2bee436b, 5c39e37022661f81f79e481240ed9b175dec6513, 8b28792f8405b737229afb92c99c579b86d8aa98, ee6f23590783adec7cf6b2030c6a46f3117a708e, d280cf82a6144b3e168840802b1a8a14d4eaccb9, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, dd2deed2ce6e110236a1280db765fa02c7488eb1, 81c02f123b3ef09cf1a8e5a1332451f0d46663fa, 5f7f10f913ecc478ff7ba304c265fd3c700b47d7, 4a7477881b66d12e79c704805781d4683a6a6be1, f4c4e148546089123f8da5db4fb246ab4062bd40, 417326e51d78ba8bd2621f23e539b41bbdd336d6, 56ae2837b6cd4d143bbfa4a4b06811d70126106f, 780c725848aac1118d00c8bb306719ec803369cd, cd74acb268404cde24f5131a22d04d48776b283e, 8d0f755dea90f35f4b126a01fa3cce96b3bdd344, f72d3f58ff73353978e224af348448b34d27cf7b, 9a0965beef113cc37491004b1848149e00300561, 1e1cf81a1113482be3f0c280db994a832cb9426a, b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1, 8adb47deeef943c2c1bae41f9498a382fb818a16, 03532123ccffae8d411264320e8a5ae2b6eddea0, 32524aa3ae8522542753ed7e6f4cca3970e4acab, d916776e0c6a04b0def4c22257c188776c2edab2, 2141334fad7248fc707607bc9453d44686ae07a7"
7fa273f450251523e6b7fcc2eb3fdbdfd4a30493,CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP,https://www.semanticscholar.org/paper/7fa273f450251523e6b7fcc2eb3fdbdfd4a30493,Conference,"Humans can learn a new language task efficiently with only few examples, by leveraging their knowledge obtained when learning prior tasks. In this paper, we explore whether and how such cross-task generalization ability can be acquired, and further applied to build better few-shot learners across diverse NLP tasks. We introduce CrossFit, a problem setup for studying cross-task generalization ability, which standardizes seen/unseen task partitions, data access during different learning stages, and the evaluation protocols. To instantiate different seen/unseen task partitions in CrossFit and facilitate in-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse few-shot NLP tasks created from open-access NLP datasets and converted to a unified text-to-text format. Our analysis reveals that the few-shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks. We also observe that the selection of upstream learning tasks can significantly influence few-shot performance on unseen tasks, asking further analysis on task similarity and transferability.",2021,141,"1557391091, 51583409, 145201124",1557391091,Zagreb,3,"145993598, 9543395, 2805081",Y,"It is somewhat alarming how the analysis has little to do with the neural networks and how dropout works, let along RNNs, while the strength of the empirical results are all on RNNs.;In fact, linear PCA can be viewed as an autoencoder model with linear encoder and decoder (so that the squared error reconstruction loss between a given sample and the sample reconstructed by the autoencoder is minimal (Bishop, 2006)).;After introducing the idea and related work, the authors in Section 3 give details about how to perform the quantization.",Conference on Empirical Methods in Natural Language Processing,21,"tasks,nlp,task,ability,nlp","5030702fea15d66a73fc997325431f1d7945ad9a, 5c45a5d05ac564adb67811eeb9d41d6460c70135, db6084fdb3baceddacdc726474722debe1ef7e65, 72afe82af4c2ca100c36eb35292e85d806527f0a, 472644c5f4155635cf9e9e37540bfa53c20e7610, f1300d9be8254b028337d9757755ba906fe6955b, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632, 7171a0e9b07ebc98a32eb912262613efc20f283a, f208b3fb28c556ab62f9d202b7beae89700a338a, 5406e153957dd7a165264da6e6e5d81251997404, a13149a80855412d970d0de2b41c611f4cf7e1da, 0ac0025529c1f9056036be43c561ba67fb8d12a5, 7c24234042988e2f820a4350f43422ed2ad6fc52, 224c11bc51b4959bc787d6681c2b152468294b11, 661ccdb41fe977d47273e586389cacc1489f3286, 6a261e1e38506b0e4c113ba29a2d5e5d0709ed26, 5031790972d496547b6613d46a4a0134c824db6e, d008893e01fa7f6c5fb01dadf3f97ee96835c303, 42543dc42e65609bbbf2be470d54dd923532c36a, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, 37b0a7a6c8fb26e32b9206847e78d521a2cd5900, a84da48f5eb57cc3f74fd2885f4e5ffd9b5ba2fa, 5471114e37448bea2457b74894b1ecb92bbcfdf6, d81fc968196e06ccafd7ea4c008b13e1cad1be64, 5f7f10f913ecc478ff7ba304c265fd3c700b47d7, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, 94deb62af3054c49e7d80bd7eb3ed5efe990fc0b, 5e31fa4e69d1a3587230f5d134c0b7e2ed84a742, 3a083d843f891b3574494c385699c21766ce8b7a, 9b529fe170823f95509585d5aa39fa01a43558fd, cd8a9914d50b0ac63315872530274d158d6aff09, 84725855d10b531eb8cbe54935dda0440c2fc750, b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1, 63316bb5b88d362051c048e864c3ae5d97a26d30, 99f06e88e76f1af51d08d7adfb26d758ebc6acab, 57e6cca1479a4642f867e69b4dee93d14259dc3d, cd5b4b113d5fa90b5dc5aa372111b89d18df88fb, e359e8960b0b09e8685a32927b7818f4b06ef881, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, a56bf7ee9a56d8f84079684339a953c2df9ce76b, 649c3497e3b34b15a5011259fcb837cf6c1ac04a, 643da4c4de1954daeac571a82367241db012a8bf, 4be9368abc2474d6fd38639e523cf03af1873fd9, 139a0c7a60667979dcb57eae677f75ff3f0b0196, 8d06f13ec042f0d8d845af5deadeb3dd9ad11f70, 5107c83173e43f51d1bdebf6cafda525a7c26bf0"
185e7d2a761594451b02ace240356dadad2aef78,Dice Loss for Data-imbalanced NLP Tasks,https://www.semanticscholar.org/paper/185e7d2a761594451b02ace240356dadad2aef78,Conference,"Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sørensen--Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.",2019,366,"2845020, 2109329406, 65844131, 2115780454, 144894837, 49298465",2845020,Riga,3,"2063938464, 2051536212, 2073587169",Y,"However, the presentation of the method is not clear, the experiment is not sufficient, and the paper is not polished.;In other words, the evaluation is a bit lazy somewhat in the same sense as the writing and treatment of related work; the authors implemented the model and ran it on a collection of public data sets, but did not venture further into scientific reporting of the merits and limitations of the approach.;As such the paper would be a nice contribution to ICLR.",Annual Meeting of the Association for Computational Linguistics,19,"examples,training,tasks,nlp,nlp","0983883619a0ca597d055d0e58da2f514052913d, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, fa7b8acd47631bada5b66049824bfd335ac6bf8f, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, 2eda2921a8da4b325f9d05f556594a5884c398a7, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, 7c8bd41e9cebdb01f445a51ba43839c8d9b3ab91, 263a58f4fd32caca1dad2351af4d711aec451fe6, aa9bf8b4c4ce8c28b9f47d7ce4f416aa9726bb0d, b266510f5f9b40d42b51884ad13a1867fb3284fd, a281d563261c738f13b9e58a525e7e265a619c93, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, 340f48901f72278f6bf78a04ee5b01df208cc508, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, e2f3aa4ecc487fa496d1a51e875e747d4b7e6001, 8b28792f8405b737229afb92c99c579b86d8aa98, d6bc29a897fd85e7187dc33c3c974b8879462237, f14fc9e399d44463a17cc47a9b339b58f6ef7502, c15f30a3e84910a28cc560e7db097fd99339e8c1, 846883b7761cb5fe4468d42bf9d328b5d1030175, e23b2e47b0ac6f50000078828f27571804dcd6a2, 9619cde5c79d91ca5c432186668618312175f8dd, d2a5dcecd2ffdf03473df1688091f08fadb114a3, ec2f9076448ba25a225618603adde60caa76c4df"
d6f002d88638de71114dab083f0ea8ceea6b6a5a,Benchmarking Intersectional Biases in NLP,https://www.semanticscholar.org/paper/d6f002d88638de71114dab083f0ea8ceea6b6a5a,Conference,"There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.",2022,32,"9051130, 2143685866, 2152595932, 3047212, 144849629",9051130,Athens,2,"48469973, 2231158981",Y,"In the experiment, the proposed method shows no or slight degradation of accuracy with big savings in communication cost.;Concerning the lack of mathematical rigour in the statistical physics literature on which the authors comment, they might want to relate to a very recent work https://arxiv.org/pdf/1708.03395.pdf work that sets all the past statistical physics results on optimal generalization in single layer neural networks on fully rigorous basis by proving that the corresponding formulas stemming from the replica method are indeed correct.",North American Chapter of the Association for Computational Linguistics,22,"models,work,nlp,bias,nlp","99f06e88e76f1af51d08d7adfb26d758ebc6acab, 1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7, 8ba8a0d18a06752f5a39996ccf1e914da0941443, 3789eb72c32ecf5e33442570358dd786dd67c8a2, 9e540662619327a3056d9e40bb58058868f6f805, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, c96fc88631f2b8e2fe192027a8a237445635328c, 761020759f7e9f84c3ac77f59a42862cc6a6004e, b5904cd5dbf73b8d5ff13517de490c292d877ee0, cf5fddf6717e88e2bbed6b0bfe54dfeb311e6789, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, 573fd2ce97c70bb29097e8efb28a27af791225ca, 8ec5896b4490c6e127d1718ffc36a3439d84cb81, 9583ac53a19cdf0db81fef6eb0b63e66adbe2324, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, c665003881c3c35589d1e48da1ee7234b48f2ac8, cb03b665069dad5e895a2c244929ea427f1fb9d1, da3f33d858586d24cb265e79eb54f3746e998f57, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, cd2f4aaf98bb1e020cff310000c8049d3460c54e, 8babcaf89f8537dc628a029ebf932100f57289fd, c6879e43828b293567f5e2da039d23845189d6a7, 4b1280229ced73f6c86550f24ef01490fde52285, 80d9f0eb47b712988d19cbe29a7bfa63f2a175d0, 5bedddda2aab2c5a03017d96a6dd7acf517c6961, 1986318d8a565fbff8fde545b8d0c2012c6462d8, 5e4597eb21a393b23e473cf66cb5ae8b27cab03e, acc296f981cde8d8c205982fc4422ec35531b769, c84389369720dcd2f004c48e58fbac2c45c8f092, 3faeb21fe256b99391d69570053a8c2d91e9f348, a60a4e5f7f872b9825ddff5d379857c2091ca52b, 6628f9ee35e36cdfdcac8a46cef4dba8d529a83b, 794b3ffd28d28606230efc975eeec9f0522fb139, e23b2e47b0ac6f50000078828f27571804dcd6a2, 0311ace1d499cadd1cc0c515a625d1d045f60d25, 0599f45e03ac2016321df0dd653ba4c0034c79d5, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 94214d6d922ce095719d488642cbcc75dc52f273, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, b9f190339ce0b4cdad3464c45ec3266a7369fb7c, b067177b1e17287185eb3b82ccc3d7c646b3ec40, 102ebe229df18c8733ea1b8def56cd79996e2178, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, b34fc78de28be598e21118d7cb9d84d63374addc, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, e89c37b7c2ff465db43c4b9f674867ec4b98aa8b, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 85328b4a8132bf4299f8cd7f8e79e850d561c8fc, 8799ec4bdf98bc313aa8c41a706a385e026d1f88, df1a2539afbad27c4c80115a6f8f59a089024865"
f72053903270d9a7f41108461ad04d5aa075218d,SHAP-Based Explanation Methods: A Review for NLP Interpretability,https://www.semanticscholar.org/paper/f72053903270d9a7f41108461ad04d5aa075218d,Conference,"Model explanations are crucial for the transparent, safe, and trustworthy deployment of machine learning models. The SHapley Additive exPlanations (SHAP) framework is considered by many to be a gold standard for local explanations thanks to its solid theoretical background and general applicability. In the years following its publication, several variants appeared in the literature—presenting adaptations in the core assumptions and target applications. In this work, we review all relevant SHAP-based interpretability approaches available to date and provide instructive examples as well as recommendations regarding their applicability to NLP use cases.",2022,32,"1786389, 2178446, 2187454523, 2187454784, 146800020",1786389,Dublin,3,"2108691840, 144717855, 93421340",Y,"If the baselines are stronger and this point is more convincing, I am happy to raise my rating of the paper.;3) and extracting 2D slices from the compressed space and computing 2D histograms per slice.;In other words, the evaluation is a bit lazy somewhat in the same sense as the writing and treatment of related work; the authors implemented the model and ran it on a collection of public data sets, but did not venture further into scientific reporting of the merits and limitations of the approach.",International Conference on Computational Linguistics,22,"explanations,applicability,nlp,model,nlp","d1a6b3a5efde3783b53f822dc8dd00aaac934b95, c30c0092bf4eb8a44faec3fc60cdd5006276bcdc, aad2d03c17bc7d1e636d0e79944ad4588af989d5, 5b34752817bc0d6aa96466dabcbc24a83dd071fe, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 808e9ce4e86e79098edea7f00b5b91663b87a5e6, 5bc511aa30f72720260d792e57537379fb04c395, 72a6a5d5864eb915231c7128b90977f1d2acf6e9, 9e195234688778b2beb3528632e78dbabf816332, acc296f981cde8d8c205982fc4422ec35531b769, 18d87bff073687c025f9bd23ab2dfb20d5f72a66, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, 456c011594ecacdd24298a161787389ccbe4b88b, 9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a, b4c9c134ad5bd4a037115df65411b4c49abe1322, d3855b7351c11145e51301e6b686f748ca35c802, ae026f29c2d571871f426ff4873d43b4ff90b9ad, a281d563261c738f13b9e58a525e7e265a619c93, a620ea8654f86b282e0d2c1fbc6616b90ca45bd1, 0cec0c296efedb814342b4b841d4583efbfc6777, 3c8a456509e6c0805354bd40a35e3f2dbf8069b1, bad4c08f03587e38ee960e2aa76e16d722826e7c, da5d78b3e3a1544fde98fba86088e1215e97cbe8, c419ee7315b9edfd8fc55bab16534fc55a564fcd, f9c990b1b5724e50e5632b94fdb7484ece8a6ce7, a0367346bc355c36badec2d2c47ce55a320cd75e"
322d91190acd8ac8c64598f5126947b0485ba249,Quantified Reproducibility Assessment of NLP Results,https://www.semanticscholar.org/paper/322d91190acd8ac8c64598f5126947b0485ba249,Conference,"This paper describes and tests a method for carrying out quantified reproducibility assessment (QRA) that is based on concepts and definitions from metrology. QRA produces a single score estimating the degree of reproducibility of a given system and evaluation measure, on the basis of the scores from, and differences between, different reproductions. We test QRA on 18 different system and evaluation measure combinations (involving diverse NLP tasks and types of evaluation), for each of which we have the original results and one to seven reproduction results. The proposed QRA method produces degree-of-reproducibility scores that are comparable across multiple reproductions not only of the same, but also of different, original studies. We find that the proposed method facilitates insights into causes of variation between reproductions, and as a result, allows conclusions to be drawn about what aspects of system and/or evaluation design need to be changed in order to improve reproducibility.",2022,25,"41052836, 2162189986, 2738095",41052836,Podgorica,2,"2113909888, 2115457464",Y,"I could then identify the influential dimension(s) by taking the largest absolute values in the gradient vector.;The paper says Deep Voice 2 (Arik et al., 2017a) is only prior work for multi-speaker TTS.",Annual Meeting of the Association for Computational Linguistics,22,"qra,method,reproducibility,nlp,nlp","cd5b4b113d5fa90b5dc5aa372111b89d18df88fb, 0ecf3f089e6dc7944b440227069b9d0143e18d78, 7669fca7fbc071b4ca78b4c326ff8f3a4b6ae616, a18f02e5c24e1f924aea268dd343bbdea234f2bb, d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea, 8f2cf30f9c825d8ef7d622601dbd525ace95e025, eda6756ab2844c390584686dc5e6385f4a8369cd, f3b8a3ffa0bdfe5578e0f0f44ea3b66cad38c032, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 381c7853690a0fee6f00d2608a7779737f1365f9, 4f480bae3196dbbc27ab383bce33478ea963f9b3, e23b2e47b0ac6f50000078828f27571804dcd6a2, 0f74e7b650f346676b12c44d16d774fda9a45c9a, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, 8713452753fd01de5616121af93e173d4f74eaf6, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, 55fa5be5288f6097ae5bd2dfe58fc07b3b39bfb6, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, 20381972ee66e03ea218e5b3d39d6423b6e35f0f, 29279e52008848ee494f5af1b836313ab99c25ed, a81ba6a07bf7a2ecff871e3362a77404501d0927, 57af930b57886186695f279efa03adaf272c4de1, 45674df7143e43bc589cfabd26dd194c2a7f090d, 263a58f4fd32caca1dad2351af4d711aec451fe6, 084a93c8ac0230ea9fe64d445ad1d6af5ea0b3b3, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, c419ee7315b9edfd8fc55bab16534fc55a564fcd, e24b8a9531573d284647239affc6c855505b0de4, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, df7d26339adf4eb0c07160947b9d2973c24911ba"
472644c5f4155635cf9e9e37540bfa53c20e7610,Semantically Equivalent Adversarial Rules for Debugging NLP models,https://www.semanticscholar.org/paper/472644c5f4155635cf9e9e37540bfa53c20e7610,Conference,"Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) – semantic-preserving perturbations that induce changes in the model’s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) – simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.",2018,440,"78846919, 34650964, 1730156",78846919,San Marino,3,"1912476, 47627363, 2112425054",Y,"If the contribution of the paper is the ""output stochastic"" noise model, I think it is worth experimenting with the design options one has with such a model.;Inclusion of Assumption 3 would in particular require better justification.;The model is further extended for conditional generation and demonstrated on a range of image benchmark data sets.",Annual Meeting of the Association for Computational Linguistics,18,"instances,adversaries,sears,nlp,nlp","7a1e71cb1310c4a873e7a4e54d1a6dab0553adce, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, 31a61d009442436d04b9d4e1c5beee37172289ae, df1a2539afbad27c4c80115a6f8f59a089024865, 28cc044d5ba938472bc53d87240583982ad21663, a357f1ff27e184d9a5ef69e665e8ca891032bf71, f8e57fa370fe10147aa22714e08409fc1b7dae4b, d9b5194f3f959eda2e95df6a340254f52ced46f4, aea731e7cf33aa3d482b13f42cedbc1adb3271c6, c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b, 1986318d8a565fbff8fde545b8d0c2012c6462d8, eb9e0da8b7170e3ca4364f2f9010599c2d2556f1, c15f30a3e84910a28cc560e7db097fd99339e8c1, 375125029b085e70a109491656b69aa01bc2a166, 147c868b721c8d29df7c61db7f2360114c760614, 42375e9a309fe5bf5c671c918cd8c9d5d85568cc, c5d3fbc024ad9a0d28669b6030de23fc5c3f7888, f3b8a3ffa0bdfe5578e0f0f44ea3b66cad38c032, f295157f37cfb43cd8d8d2690ea124edc5ea59c2, 046eb47d56beb8069b0098e3d01608f81ebb6849, 676664ee7471738577f641e6159e7596625b7fdb, ea160adc0d78e54669281b8b145bcd832e648fee, 71a85e735a3686bef8cce3725ae5ba82e2cabb1b, e2e7d964c09e27d334fcb8761d69918630629387, bd6c027a3604d6c8fa23435bf382455b2bee436b, a2a514ed839dafdd0fb76d6c2615f25f35bf8087, 208ac1f2ec9bf367a9981fedb6d9ea6aa9889099, 34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61, d2a505586c0da20752b98f63c7760b6a5c41e28d, e2a58fd18961c3941102989e3a3d0d27c615e015, 5bedddda2aab2c5a03017d96a6dd7acf517c6961, 5778e56400f7113c2b1355fdbd6b638fa379885f, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, fb5413afba689d16543215c5a2ddbc5b78a52007, 6eb8caebbffc7e5b301b66dc36acad46b4dca5c9, 08aaaf7ae3d61875e7bcf5c1bb0df4f17066e300, 42543dc42e65609bbbf2be470d54dd923532c36a, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, e70ddfb04969b663ef8cd711a70a0acf563d6e5f, 011095a0082e5e301f9bf30267b193c1c9e7e370, b2ee139c1a3c2c53ac2b603070bdfbcd26b50ddc, 8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801, dadfb3ff45e19dc22456a645f441bbeb17c93c9c, f72053903270d9a7f41108461ad04d5aa075218d, b889b1d6944213bc2ca29e3ad07ee65ede20892d"
9b54941de1e21826ecc28b32730ac3f69991ede4,Robustness Gym: Unifying the NLP Evaluation Landscape,https://www.semanticscholar.org/paper/9b54941de1e21826ecc28b32730ac3f69991ede4,Conference,"Despite impressive performance on standard benchmarks, natural language processing (NLP) models are often brittle when deployed in real-world systems. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, RG enables practitioners to compare results from disparate evaluation paradigms with a single click, and to easily develop and share novel evaluation methods using a built-in set of abstractions. RG is under active development and we welcome feedback & contributions from the community.",2021,116,"1822288, 8937909, 2056908, 145814654, 2120253018, 3393918, 2228109, 143977268, 2061444681",1822288,Lisbon,2,"35210462, 2144633223",Y,"The authors propose to train a generator network in combination with the classifier and an adversarial discriminator.;After all, reward seems to play a very important role for the proposed system.",North American Chapter of the Association for Computational Linguistics,21,"evaluation,nlp,systems,rg,nlp","e8b30ebe3351680c3b039555ae0a8d0865ad829b, 704011527f183b561ea6a75b21e4cefe5aa77fca, 8ee45aeb7c97e3346cc62f216f673b91277ac718, 83ea80a9393177ecf84928f4bd1120fb679f180c, cd29c25c489562b409a60f83365f93f33ee1a0a1, 34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61, 6a1b25f7a67395ad1e676027322913acbb0a0635, c9cc6a3c50b32e801f20b4f0ed5daf3e9550f9c1, 2c3eef2f17369912e330281d54b535675077e4ca, b79e5e4622a95417deec313cd543617b19611bea, 916455d97cd792c2eb5b00663689592e25cbc8d8, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, d6f002d88638de71114dab083f0ea8ceea6b6a5a, 9d6cbac04c498b424dfaa5ce82ac201a180c1502, 6068d39e92aef1bb0e1291e9931894c35692a85e, c9f320789e98d2c7a798a9705e26dbe317677966"
8d0f755dea90f35f4b126a01fa3cce96b3bdd344,Towards Climate Awareness in NLP Research,https://www.semanticscholar.org/paper/8d0f755dea90f35f4b126a01fa3cce96b3bdd344,Conference,"The climate impact of AI, and NLP research in particular, has become a serious issue given the enormous amount of energy that is increasingly being used for training and running computational models. Consequently, increasing focus is placed on efficient NLP. However, this important initiative lacks simple guidelines that would allow for systematic climate reporting of NLP research. We argue that this deficiency is one of the reasons why very few publications in NLP report key figures that would allow a more thorough examination of environmental impact, and present a quantitative survey to demonstrate this. As a remedy, we propose a climate performance model card with the primary purpose of being practically usable with only limited information about experiments and the underlying computer hardware. We describe why this step is essential to increase awareness about the environmental impact of NLP research and, thereby, paving the way for more thorough discussions.",2022,22,"2064295987, 2023644816, 2156865999, 2006205621, 3073566",2064295987,Moscow,3,"2237803694, 1505708061, 90943712",Y,"However, that is moved completely to the Appendix.;This manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum, which actually has been shown by existing works like Kawaguchi (2016) with some assumptions.;* Qualitative results on sampling from the model using the CIFAR dataset.",Conference on Empirical Methods in Natural Language Processing,22,"nlp,climate,impact,research,nlp","9efd70d2c06733704220313fb67720aa45c6362a, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 7d873a9c49d3864709aa762f8740edcdbd7369c5, 5290d7921f0266c8b50b79fc8a0b7d22868f4f60, 1661d0d8d47cac41e01c59c60aac3675b4396698, db0cc2f21b20cbc0ab8946090967399c25709614, e2a85a6766b982ff7c8980e57ca6342d22493827, 6f75e8b61f13562237851d8119cb2f9d49e073fb, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 2e7f532796eed2847d4c19e3cff03756049e81b4, ccd94602e3acecf999d0c9ba62b1a8bc02e9f696, 99ae62cca0b15275d9ed1528f345f9dcefe50dd7, f9163156eeba67762a7441db48fe6720106137cd, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 07cca761749bfe21c2d096ff60f32b574d5c84c4, e70ddfb04969b663ef8cd711a70a0acf563d6e5f, 579476d19566efc842929ea6bdd18ab760c8cfa2, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, 20a3ed03888037e2802fa9abad02ffa0a8dcc228, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, a781cea542e99c6bb9422858e7c04eaef18c7673, 8b7b2e88207fc2dd849f5d83c9b6e890f0174abc, ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee, 5c45a5d05ac564adb67811eeb9d41d6460c70135, c468bbde6a22d961829e1970e6ad5795e05418d1, 85328b4a8132bf4299f8cd7f8e79e850d561c8fc, 8b417c2be7a7707f372049fb1193f0d42f799562, 21042565ec941f4fa31ac5a0af85a1a84ff21f1b, 4be9368abc2474d6fd38639e523cf03af1873fd9, a9640bac0b45a804d07fc5914feb08af8f2a73f2, bcc82ce554942880814243fc8c08a88b9d2aad09, e576a2d97950b1f6831f88575dd3f370053f6af7, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, 4267178106cef2e77284bde309dfaaf9fd46a91b, e4b52a1a00e9db941326fc857b95245cbfb60bce, 54ddb00fa691728944fd8becea90a373d21597cf, 4bd3c9e1bb1ca2df62b66201616b8740300efd0a, 1b3675fc0f2b16743b1e1f0c2f84829cfdb3d34f, 90e4330fed2da147dd41490e8ad638b618112b3d, cd20ed8e70f6459f5617350f6b84c5c6d5929cb7, a0f303b6e22ef52943355993f57d65938997066a, 71854ff4306cf65c3c2161f7be2d0346275f72d5, 410fba9f03212257d0881811802e6620e59bc827, f11cba099b8cf14815f7b3d85f55ecfddbf9f04d, 5bc511aa30f72720260d792e57537379fb04c395, 695bdc6e24608364491b9418a220c65a7cd17413, 48265726215736f7dd7ceccacac488422032397c, 54a26f5ef4b0524c60249fe98d0fc3646b2791ad, 7676c02ea839ff1ceb6e5e1427c42bc45e169bde"
5e31fa4e69d1a3587230f5d134c0b7e2ed84a742,Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models,https://www.semanticscholar.org/paper/5e31fa4e69d1a3587230f5d134c0b7e2ed84a742,Conference,"Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at https://github.com/lancopku/Embedding-Poisoning.",2021,97,"2120801160, 49192881, 50317060, 19169659, 11774802, 1631386300",2120801160,Berlin,3,"1743722, 2660835, 1817207",Y,"(As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.);The authors claimed that this parameter update is one of the novelty of their method, making it different from the method of Schlegl et al. (2017).;The most important idea of the papier (PDE-net) is to learn both differential operators and the function that governs the PDE.",North American Chapter of the Association for Computational Linguistics,21,"nlp,models,security,samples,nlp","00bbc94806ca0821a9c82d8aedf16f0e6263b89f, 9727206903eb40d4fa42606711bad3402f2ba9aa, 2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c, 27e58c9e5e6d07809a45a17675a2c7135b577881, fdc57c18f3b636c3273542327ae540217972558f, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 155f27879f185f1ab04107c91c2ae7cf6a910a03, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, f397b593de771752e7002a954eb531f3ef6a975e, 665b0c776ff7507c32793f10ce9edf90bc2f674a, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, 4f8d648c52edf74e41b0996128aa536e13cc7e82, 10a69070c8585dc5564b11b0e53dbe4d565f9ce1, 44786a2c2a8ba8cf5c74a8fb10098c220e924c56, a8b995f0da78a79447dfb18c2337972b044f4239, 77a59de2e2b832321875cadcf9619dc313f02384, cd20ed8e70f6459f5617350f6b84c5c6d5929cb7, 647d0e540627c5a903299a90d20530f8e48c18d9, 43bafb4997515d2904abfca8214f2fc806680fc3, f8e57fa370fe10147aa22714e08409fc1b7dae4b, ca0e479ba2327f71e842d033b6b48b082962cc6a, aa9bf8b4c4ce8c28b9f47d7ce4f416aa9726bb0d, b2f8df88de24fe0ef74bda0eec1a0c3e7329b9f9, 312b1067d89f598c4c5c0799aa18b48d0926bed8, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c, 7ae2783a9196fb4bc2a610ae812d19722daddce5, 39602922b04885047254444fd1a1586d797617ce, 4e13a8e8ba8d33e15ed037bfca7c651047533990, ce51eff5b529ee572dab1c1f38f20adc8e89bab2, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, 96023195e889fc258e6ff30aa99d250982dfae01, 0a829289a16ae48837cc2905635435db98bacc76"
7571ed4cf1bbdcf891b576a0da12c910b1f0c72f,Concealed Data Poisoning Attacks on NLP Models,https://www.semanticscholar.org/paper/7571ed4cf1bbdcf891b576a0da12c910b1f0c72f,Conference,"Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model’s training set that causes the model to frequently predict Positive whenever the input contains “James Bond”. Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (“Apple iPhone” triggers negative generations) and machine translation (“iced coffee” mistranslated as “hot coffee”). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.",2021,104,"145217343, 145914976, 144588144, 34650964",145217343,Stockholm,2,"1756679, 2072801764",Y,This paper proposes a different approach (with could be combined with these methods) based on a new training procedure.;I could then identify the influential dimension(s) by taking the largest absolute values in the gradient vector.,North American Chapter of the Association for Computational Linguistics,21,"model,predictions,attack,nlp,nlp","410fba9f03212257d0881811802e6620e59bc827, 56266342b01a4f2ddc28a1e8401dbbad105736a5, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, 63316bb5b88d362051c048e864c3ae5d97a26d30, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, 44786a2c2a8ba8cf5c74a8fb10098c220e924c56, d2efa91bd7d076a66bc1d1c570f6aa2da944dd88, 665b0c776ff7507c32793f10ce9edf90bc2f674a, 8babcaf89f8537dc628a029ebf932100f57289fd, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, ce54e3b89a2570035b70885e6901ad4c92ae41c9, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 7637ed79d30d0139901175ae4abedd822c217ab4, af13a92977d4f4dc5b28b13746d86111d42939e8, 799d5a8271887adede035644d878c7bd555576df, f0f1627db35b4942e0f83069f20dd0948fc35d28, cb03b665069dad5e895a2c244929ea427f1fb9d1, f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d, 312b1067d89f598c4c5c0799aa18b48d0926bed8, e359e8960b0b09e8685a32927b7818f4b06ef881, 0ac1a1ccb16c8441f42ff8de743fc93f0e08c15c, d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea, 224c11bc51b4959bc787d6681c2b152468294b11, febe776e285dc5e72c7e3ee697a87a794e1c00ff, d1ae4ab5047489c2b010c7ce72262982ad66ad60, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, d1efcef213c433445be56d7479eb47d972b3ee79, f04d8e558c11ac0fc9318c4f3d61b651c62ddcfa, ddc6e677715c03fe574319d3f80a3e1577bdbdd3, 39602922b04885047254444fd1a1586d797617ce, bc44c0c64a473e035b11ae60a1993ad3db1acd2e, f9314fd99be5f2b1b3efcfab87197d578160d553, e373cbce1a831361a8de9dbc625727169011bcce, 18d87bff073687c025f9bd23ab2dfb20d5f72a66, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, 2374106a32169c07703599ff3f6f4b31e8067b89, 54a26f5ef4b0524c60249fe98d0fc3646b2791ad, 740b5dec469e6b54e5e191a577a9b6a6ad8562e3, ccca203382e5dd198c089a0f1d7af7bef0f694e9, 51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df, 83cebf919635504786fc220d569284842b0f0a09"
f91dbd39d4c742ba675e447b04a0b0c70b33e836,Measure and Improve Robustness in NLP Models: A Survey,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,Conference,"As NLP models achieved state-of-the-art performances over benchmarks and gained wide applications, it has been increasingly important to ensure the safe deployment of these models in the real world, e.g., making sure the models are robust against unseen or challenging scenarios. Despite robustness being an increasingly studied topic, it has been separately explored in applications like vision and NLP, with various definitions, evaluation and mitigation strategies in multiple lines of research. In this paper, we aim to provide a unifying survey of how to define, measure and improve robustness in NLP. We first connect multiple definitions of robustness, then unify various lines of work on identifying robustness failures and evaluating models’ robustness. Correspondingly, we present mitigation strategies that are data-driven, model-driven, and inductive-prior-based, with a more systematic view of how to effectively improve robustness in NLP models. Finally, we conclude by outlining open challenges and future directions to motivate further research in this area.",2021,86,"1524732527, 49528192, 2143919864",1524732527,Amsterdam,3,"1753285996, 37502184, 1633124736",Y,"If the loss for the task is proper, then it's well known how to construct a divergence which coincides with the optimal risk.;It seems the Blizzard results in Figure 2 are missing no reconstruction loss + full backprop.;The evaluation framework is described in enough detail to replicate the results.",North American Chapter of the Association for Computational Linguistics,21,"models,nlp,robustness,applications,nlp","6548106035c7208ad498730627874a482734b9ac, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, c12b80b44d9acfe6cd92fdf965264c4b706c367c, 1cf2e9e198feef3893da2800a7949f6880ddc084, 60caa5b3d066e13feac496fd0736e976970eb09f, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, fa26a6d434450b185e669170e79fd3e1d29716bf, 9e3816be8cf4821d74e258de10ee471382936a30, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, 980858461df7c4349f17b427686c5bcbcffbdc04, 410fba9f03212257d0881811802e6620e59bc827, 00bbc94806ca0821a9c82d8aedf16f0e6263b89f, 322d91190acd8ac8c64598f5126947b0485ba249, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, ef35da3a3c471a6a15c7a3b09586483eb50cbef0, a1d36749b89e46a8eaadf8ba40788741c192fb1e, e968ae8e98fff9e28468383a1826fca4a2ae5245, aca6d5f3866372a4506cf15773ae298f18c3f453, 7998468d99ab07bb982294d1c9b53a3bf3934fa6, e67a2817089312746d69b38ce9abfdc4b1bc69c3, a830083704284c8c5ddaf04f676c6ce23d583942, dd2deed2ce6e110236a1280db765fa02c7488eb1, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 59c414c9efb77562f5d1aad8af14eaac968c69c0, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, 657fbf29ea0b4904a3e98d1556f9acf38dddae5f, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 1a37223175138bc1aa53b425ea2fdd0b382405a5, e248993daede136713e93929816df92b48ccddfb, 1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0"
1109d62ebd2b29a7dc148bc30dd6cfc803a63dec,IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP,https://www.semanticscholar.org/paper/1109d62ebd2b29a7dc148bc30dd6cfc803a63dec,Conference,"Although the Indonesian language is spoken by almost 200 million people and the 10th most spoken language in the world, it is under-represented in NLP research. Previous work on Indonesian has been hampered by a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In this work, we release the IndoLEM dataset comprising seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. We additionally release IndoBERT, a new pre-trained language model for Indonesian, and evaluate it over IndoLEM, in addition to benchmarking it against existing resources. Our experiments show that IndoBERT achieves state-of-the-art performance over most of the tasks in IndoLEM.",2020,157,"2789148, 2953039, 1800564, 145465286",2789148,Prague,2,"2288804757, 145771261",Y,"Results are good, some unclear explanation This paper proposes an end-to-end trainable attention module, which takes as input the 2D feature vector map and outputs a 2D matrix of scores for each map.;Pros and Cons ============ + good results + interesting idea of using the algorithm for RLfD - weak experiments for an application paper - not clear what's new",International Conference on Computational Linguistics,20,"language,indolem,nlp,work,nlp","7e9905710a5991a017ffc0473dd612cfce4b7b2e, 2880ac931c11176aee6d42a7e7bb0703aacde3f9, 0af24df95f8a0b7e6fc257a98a5820314d1243f3, 8388f1be26329fa45e5807e968a641ce170ea078, 771a858c35f6d6e6d1017dde95368de3794738a6, 8c33ca066e2ab615e24c65198c794114436053dd, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, bc44c0c64a473e035b11ae60a1993ad3db1acd2e, 60caa5b3d066e13feac496fd0736e976970eb09f, e32a2519b59d62cff6cb8136ee242dc3754ed57b, ca05ca5f70616456d0f2e05c4a51cbf8b8d273f7, d9a7fa7616a327367696e19b1846519745cd43ff, 647d0e540627c5a903299a90d20530f8e48c18d9, 557ee73ff4e87ca0cb1b6c78af0730a2d94b710f, e8b7a9be9f2d0578a95319ed5841978e10429967, b266510f5f9b40d42b51884ad13a1867fb3284fd, 5f7f10f913ecc478ff7ba304c265fd3c700b47d7, 77a59de2e2b832321875cadcf9619dc313f02384, 98b9086750f08a21c8778ab986339321e9caf790"
3cc2f69951cd24fe61be4cf32d62afbac297bc2b,Social Biases in NLP Models as Barriers for Persons with Disabilities,https://www.semanticscholar.org/paper/3cc2f69951cd24fe61be4cf32d62afbac297bc2b,Conference,"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",2020,223,"2083807, 3331141, 40081727, 20825661, 2112887022, 1667883461",2083807,Amsterdam,2,"2154976675, 2142159995",Y,"A new method for weight quantization.;The presented results using small networks on the MNIST dataset only show that networks with ISRLU can perform similar to those with other activation functions, but not the speed advantages of ISRLU.",Annual Meeting of the Association for Computational Linguistics,20,"biases,nlp,models,disability,nlp","579476d19566efc842929ea6bdd18ab760c8cfa2, 6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91, 178571a5cde984c895493e2eb6c5487449d055cf, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 6ba00c2386f2edc0b43eec442cd1923b5d964633, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 69a72ff5b30642d11c96635e99aadad3140d33a7, 24d21ecaeb2d2ecc20e26a5e3f5128247704ccfe, 25761ba4bdc054bfe902fe7c5d6338be6d00d491, 573fd2ce97c70bb29097e8efb28a27af791225ca, b651d67502790e1d6d41c589e1d93e996ba7b935, a13149a80855412d970d0de2b41c611f4cf7e1da, 8674494bd7a076286b905912d26d47f7501c4046, 3b9732bb07dc99bde5e1f9f75251c6ea5039373e, 6f31f59f7b4bd751b5d8657d8a878bef571f9a4c, e8b30ebe3351680c3b039555ae0a8d0865ad829b, 03532123ccffae8d411264320e8a5ae2b6eddea0, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, 63de6db7245f634ecbef4c505099874c1ba65145, ce212cb873a54e5716da53a66b10298ac013008a"
11342d45911ee8a7c9e3a94117ce774ad7036172,Neural Unsupervised Domain Adaptation in NLP—A Survey,https://www.semanticscholar.org/paper/11342d45911ee8a7c9e3a94117ce774ad7036172,Conference,"Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future NLP.",2020,209,"1723636206, 2022124",1723636206,Brussels,3,"32058742, 9959840, 3410500",Y,"An encoder is later trained to map real images into the space of latent codes for the generator, allowing the system to be applied to real image segmentation tasks.;The core idea is to train the model on pairs of images corresponding to the same content but varying in views, using adversarial training to discriminate such examples from generated pairs.;On the theoretical side, the linearly constrained weights are only shown to work for a very special case.",International Conference on Computational Linguistics,20,"domain,data,language,nlp,nlp","740b5dec469e6b54e5e191a577a9b6a6ad8562e3, 8ba8a0d18a06752f5a39996ccf1e914da0941443, 0c00a328fa7cd56ee60338c54e89bd48310db80b, e5a1cfcd07dcfd8b1feec9c635dadc858cde8166, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 57af930b57886186695f279efa03adaf272c4de1, febe776e285dc5e72c7e3ee697a87a794e1c00ff, 746a81aa26d3ebfb81acfd6af958d6a21603cd21, 5bc511aa30f72720260d792e57537379fb04c395, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, 9619cde5c79d91ca5c432186668618312175f8dd, 3d82552eb483e5ea84b577a0e8d5f157a6085824, 31d7d7b9c7b776c639316027e6ae5f2ff2673da2, 647d0e540627c5a903299a90d20530f8e48c18d9, 69d49a06f09cf934310ccbf3bb2a360fa719272d, 017010b941d902a467f6d329ae5e74fd67e67912, c0bcd7dc9426a70af15f5ad63b4af92ea4dcbd4d"
fa7b8acd47631bada5b66049824bfd335ac6bf8f,Towards Improving Adversarial Training of NLP Models,https://www.semanticscholar.org/paper/fa7b8acd47631bada5b66049824bfd335ac6bf8f,Conference,"Adversarial training, a method for learning robust deep neural networks, constructs adversarial examples during training. However, recent methods for generating NLP adversarial examples involve combinatorial search and expensive sentence encoders for constraining the generated instances. As a result, it remains challenging to use vanilla adversarial training to improve NLP models' performance, and the benefits are mainly uninvestigated. This paper proposes a simple and improved vanilla adversarial training process for NLP models, which we name Attacking to Training (A2T). The core part of A2T is a new and cheaper word substitution attack optimized for vanilla adversarial training. We use A2T to train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI datasets. Our results empirically show that it is possible to train robust NLP models using a much cheaper adversary. We demonstrate that vanilla adversarial training with A2T can improve an NLP model's robustness to the attack it was originally trained with and also defend the model against other types of word substitution attacks. Furthermore, we show that A2T can improve NLP models' standard accuracy, cross-domain generalization, and interpretability. Code is available at https://github.com/QData/Textattack-A2T .",2021,80,"1693182792, 121817403",1693182792,Bratislava,2,"2117315688, 145458655",Y,Or is similarity to the corpus of interest more important?;I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships.,Conference on Empirical Methods in Natural Language Processing,21,"training,nlp,models,t,nlp","d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b, 1d5adacc5d4d226e76c35bf19018f9e76759f127, d1ae4ab5047489c2b010c7ce72262982ad66ad60, 9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a, 5471114e37448bea2457b74894b1ecb92bbcfdf6, c12b80b44d9acfe6cd92fdf965264c4b706c367c, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2, facd5f5deb152229ceb1803434d8690a09ab4129, d7b820af40a9e2660ef700d39f7b2e27b43435c5, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, cb23a59fdf3ade707600f076df4ff27a03941fba, d8348b802c9133d9e396d4ad809b020d5be42863, c24d47ff95cd4bda073c75ec24ececaa3b10c995, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, b80e2af7ffa85fe2f02ea8390e4915d55c19d199, beb890d47bbc21a96967f9993c9d6e15686b2eac, cde39ce861e4c7514ee07fd91b6b8aac50cbf01b, 6eb8caebbffc7e5b301b66dc36acad46b4dca5c9, 34ca47eed139a7f0694611528f75debc43385518, b266510f5f9b40d42b51884ad13a1867fb3284fd, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, 3cc2f69951cd24fe61be4cf32d62afbac297bc2b"
353c88c231ce156d604e074af276422422fc73f7,"A Survey of Race, Racism, and Anti-Racism in NLP",https://www.semanticscholar.org/paper/353c88c231ce156d604e074af276422422fc73f7,Conference,"Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.",2021,83,"49713890, 3422038, 2138053020, 145317727",49713890,Vaduz,3,"71790825, 145366422, 88478180",Y,"Thus, as such, the novelty or the contributions of this paper are minor.;However, the proposed method is still within the same family methods as demonstrated by RARL.;For example, when showing that the head direction cells generalize in the new mazes how can we be sure that it is not using a common lighting scheme common to both train and test mazes to orient itself?",Annual Meeting of the Association for Computational Linguistics,21,"nlp,race,work,research,nlp","9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, ed935c6b359a7a486c28240d796e84897d095125, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 417326e51d78ba8bd2621f23e539b41bbdd336d6, 98b9086750f08a21c8778ab986339321e9caf790, 1c748f86182a62d44d5b44316db510f8d833e19f, 155eeede0c070f1f017ba5c9f6cacd7ae0b098aa, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, 545f108575314031f35c617c4ac35a10133c50e3, 1e1cf81a1113482be3f0c280db994a832cb9426a, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, adc180e1fe404b650fca3bb7970e43bdce34a611, 32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b, b52db9e41e15f76bdcfbe674abe0314af545c430, b55e490637babd50dab3cdaaa3a60a2be6eb1cbb, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, 2e5d2f2dc01b150dffc163a9f457848e9b5b5c38, 8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801, aad2d03c17bc7d1e636d0e79944ad4588af989d5, 78b2d392ebb100a220ceab6529d26909b27eaa32, fa7b8acd47631bada5b66049824bfd335ac6bf8f, cbad0923db89f23febcbd6192ff4149289ff2ad9, 024006d4c2a89f7acacc6e4438d156525b60a98f, f406aceba4f29cc7cfbe7edb2f52f01374486589, 28a5a53dafacebad8a7c47773079caeffb9a5baa, 1a37223175138bc1aa53b425ea2fdd0b382405a5, 5e31fa4e69d1a3587230f5d134c0b7e2ed84a742"
28a5a53dafacebad8a7c47773079caeffb9a5baa,Representing Numbers in NLP: a Survey and a Vision,https://www.semanticscholar.org/paper/28a5a53dafacebad8a7c47773079caeffb9a5baa,Conference,"NLP systems rarely give special consideration to numbers found in text. This starkly contrasts with the consensus in neuroscience that, in the brain, numbers are represented differently from words. We arrange recent NLP work on numeracy into a comprehensive taxonomy of tasks and methods. We break down the subjective notion of numeracy into 7 subtasks, arranged along two dimensions: granularity (exact vs approximate) and units (abstract vs grounded). We analyze the myriad representational choices made by over a dozen previously published number encoders and decoders. We synthesize best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.",2021,82,"37574242, 2634786, 144171096, 2512264",37574242,Zagreb,3,"2138646471, 2059547812, 32244429",Y,"The authors did a very good job presenting the problem, motivation and solution in a coherent fashion and easy to follow.;While LINE or SDNE, which the authors cite, may not run on some of the larger datasets they can be tested on the smaller datasets.;Since this could be a potential disadvantage, some discussions or empirical study on cross-category generalization seems to be interesting.",North American Chapter of the Association for Computational Linguistics,21,"nlp,numbers,numeracy,systems,nlp","4be9368abc2474d6fd38639e523cf03af1873fd9, 6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5, f4207bcad24fe4eaf3849d6c1fe38c36545b5cb3, cc5726fc0ebb84f741f3496a3c52ced162c596ba, c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b, d2a5dcecd2ffdf03473df1688091f08fadb114a3, 62ccd99a65bfc7c735ae1f33b75b107665de95df, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, 6c34842a92ce4da9aab586490afdbd8779af4eab, ee6f23590783adec7cf6b2030c6a46f3117a708e, 97906df07855b029b7aae7c2a1c6c5e8df1d531c, 915ab7b6ca3633230403d47dbb31cf74888cc5c9, ccca203382e5dd198c089a0f1d7af7bef0f694e9, 7bc9607c5cf3fc817675d46844f529097d579514, 661ccdb41fe977d47273e586389cacc1489f3286, d1206ccabd1980848f14472d6548251c2fab7963, f2bc4057e696f49c326bf8e1588772a16f053754, a56bf7ee9a56d8f84079684339a953c2df9ce76b, e9a986c8ff6c2f381d026fe014f6aaa865f34da7, 1cff064f815111a71a98afda7aee1867ad617901, c9a9517c8b867187b4f4c0c37cbc65263ea41d25, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 63adc1e5086481e36b19b62707a96b799da51e59, e2f3aa4ecc487fa496d1a51e875e747d4b7e6001, 087dd95e13efd47aef2a6582e6801b39fc0f83d8, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 1e1cf81a1113482be3f0c280db994a832cb9426a, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 2880ac931c11176aee6d42a7e7bb0703aacde3f9"
9b529fe170823f95509585d5aa39fa01a43558fd,How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence,https://www.semanticscholar.org/paper/9b529fe170823f95509585d5aa39fa01a43558fd,Conference,"Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we introduce the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from https://github.com/thunlp/CLAIM.",2020,193,"51125639, 51131083, 2652217, 145086110, 49293587, 1753344",51125639,Zagreb,3,"2135403, 3144356, 1680740",Y,"“Customers” randomly appear throughout the task and the taxi agents receive reward for moving to the same square as a customer.;identifying bugs in programs where the wrong variable is used, and;If the loss for the task is proper, then it's well known how to construct a divergence which coincides with the optimal risk.",Annual Meeting of the Association for Computational Linguistics,20,"legalai,professionals,tasks,nlp,nlp","7a1e71cb1310c4a873e7a4e54d1a6dab0553adce, d2a609ffb814442d0728aef9f6616f9cd775face, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, 4087e84fc695bb6433d0104ee94f9d7e9f4b7da5, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, a453ce8a3de86a170c79a1082ef358c3adf4e612, 73cbaf5f2441ef3478266b5438c0e90d1ae71652, b55e490637babd50dab3cdaaa3a60a2be6eb1cbb, 661ccdb41fe977d47273e586389cacc1489f3286, 56ae2837b6cd4d143bbfa4a4b06811d70126106f, 91b2b47cabd800ef658b65bfe1f52b7293a740c3, e57aeb158a38ccf33d2f0f5a8f63f1209497e329, a604aa1f2a2ca1a6a0b09013e71b83d36cc0f358, 0ecf3f089e6dc7944b440227069b9d0143e18d78, d916776e0c6a04b0def4c22257c188776c2edab2, f208b3fb28c556ab62f9d202b7beae89700a338a, e02a757617c2c42eb62889cc4d4aee3765928303, 322d91190acd8ac8c64598f5126947b0485ba249, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, 28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8, b69a35662a2cac38eab22f4481285116bdf8c30e, a9cbbef8f4426329d0687025b34287c35bdd8b38, cbad0923db89f23febcbd6192ff4149289ff2ad9, da5d78b3e3a1544fde98fba86088e1215e97cbe8, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 55fa5be5288f6097ae5bd2dfe58fc07b3b39bfb6, 17fc5c9b37ba63fb3280ddf9909a183523e7bcc3, a1d36749b89e46a8eaadf8ba40788741c192fb1e, 43e6e8d6663d83f1b74cf5a2be7b040b0928f867, 81c02f123b3ef09cf1a8e5a1332451f0d46663fa, 9d788cfe4a0991d3b1a266c8329f6e903840b82f, 811df72e210e20de99719539505da54762a11c6d, a80e26e6365b215715c182d19a9aa8bb876ac768, c0e6cd2ec3bc9eb46c7d45bb708854da3327339e, ce9ca56036307217ea565644d3d3bd74b879e045, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6"
68a3d32416977e88cf1bfa4ad548d403f5f089d6,Rethinking Stealthiness of Backdoor Attack against NLP Models,https://www.semanticscholar.org/paper/68a3d32416977e88cf1bfa4ad548d403f5f089d6,Conference,"Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack. Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. To address this issue, we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings, making an important step towards achieving stealthy backdoor attacking. Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. Our code is available at https://github.com/lancopku/SOS.",2021,68,"2120801160, 2149202150, 50492525, 2108485106, 2130282986",2120801160,Tirana,3,"1786639, 49528584, 50738985",Y,"On empirical evaluation, I already mentioned that it impossible to understand what the classification problem on TIMIT is. I suspect it might be speaker identification.;4) Novelty The main contribution of this paper is basically a set of experiments looking into architectural choices.;This section could be improved by demonstrating the approach on more datasets.",Annual Meeting of the Association for Computational Linguistics,21,"backdoor,nlp,models,performance,nlp","f11cba099b8cf14815f7b3d85f55ecfddbf9f04d, 04fa3ebc4c0c0b4a2d2d1a3fc612134a05057696, 37b9478b792fb1e7ad5f2c322ed4445b93df6c9e, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, 6eb8caebbffc7e5b301b66dc36acad46b4dca5c9, 0e33833f5e2e2719edfba1d142eb4d27f96e799f, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, 69a72ff5b30642d11c96635e99aadad3140d33a7, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, 6c34842a92ce4da9aab586490afdbd8779af4eab, 381c7853690a0fee6f00d2608a7779737f1365f9, afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84, d86084808994ac54ef4840ae65295f3c0ec4decd, 390bcf15a1b13cb0d5966859c35c69a31238e838, e3b37c1c955b2b10809040ce277edae5333b99c3, 11be2469ab1d1c508e7b6e14148990741ba87884, 97ac11e5a6440eccb70ae7146392ac138c36fa6c, 0599f45e03ac2016321df0dd653ba4c0034c79d5, 7c8bd41e9cebdb01f445a51ba43839c8d9b3ab91, 74bc39003e65119eaa6ba339a61b45b417a638b7, e23b2e47b0ac6f50000078828f27571804dcd6a2, fac67bf55456b52ac6e4f280ad953d0250c74ebc, d1dbf643447405984eeef098b1b320dee0b3b8a7, 83ea80a9393177ecf84928f4bd1120fb679f180c, a85c45ce7c893388e8eafa8a653b042e1497db48, 9e540662619327a3056d9e40bb58058868f6f805, 980858461df7c4349f17b427686c5bcbcffbdc04, 3ec02e36cc0ba16292b255e9f90c3725521e2ec9, d617f51833860dc50d202af7f80be71304b2e994, 54ddb00fa691728944fd8becea90a373d21597cf, 795550a5294eb05ea4f3b14f0b1c21a405493d85, aad2d03c17bc7d1e636d0e79944ad4588af989d5, 0090023afc66cd2741568599057f4e82b566137c, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, fc61c7221350806c25379f385c27b2102ff8eb57, b4af1cbb4d3a03d3ad7930b906fc7bf6870180cc"
8dce62b18bdea587c07cb4769a05ca0a816d09ba,"The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models",https://www.semanticscholar.org/paper/8dce62b18bdea587c07cb4769a05ca0a816d09ba,Conference,"We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models—including classification, seq2seq, and structured prediction—and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.",2020,150,"6117577, 49556437, 1994065972, 2843215, 1388485917, 3159346, 122064392, 51478016, 146419516, 49849144, 2061016887",6117577,Bern,2,"2285816475, 2168853963",Y,"See e.g. the discussion in ""Variational Lossy Autoencoder"" (Chen et al. 2016) - Experiments are not complete (e.g. for AR, as noted in the paper).;I could imagine a network learning to ignore features of objects that tend to wander over time.",Conference on Empirical Methods in Natural Language Processing,20,"analysis,nlp,models,model,nlp","4b991efaa8493a5925c2aee9eec980831213eba6, 7c24234042988e2f820a4350f43422ed2ad6fc52, 736ef8a32d6c5f76a21d61299300cf796480d507, fa63c3f53413ced7946623889c416e34a28676ea, 39444c55f07839ac6a0d1839472a982f8fb447bb, 6fed828456964d29517f6caf31b700d8aec82153, b473e91cbe80c8b46451b49153cd5f93030480ab, 156609022dd6258c60238859622da0a1683bd062, 661d316f4fe9574f9048586bfae2c43243a1d22f, 24ab4e99e582c9770281eee0a39cbeb70ddd891a, d2efa91bd7d076a66bc1d1c570f6aa2da944dd88, 74b63c4cdc719a646ce014d084a0ce0ac66d9139, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 811df72e210e20de99719539505da54762a11c6d, 25adff0bb9691b46fee5c0591300d6f3ccf117ab, b080d072cfde697180db3234da08903c092e72c3, cb23a59fdf3ade707600f076df4ff27a03941fba, 16753e0317730e8c1b297338300a8c6163dd06f2, be383c607d4d357c763d2329ab71799c6e1393b4, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, cd20ed8e70f6459f5617350f6b84c5c6d5929cb7, 4895c430c7810b45840b58cc9182f12143013a43, 0f4d00d01d43d3967ee92b58481b5ad530a944d1, 6dd3e404aac097f63d42dc19fd08c7ec281f90b4, ec2f9076448ba25a225618603adde60caa76c4df, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, d1a6b3a5efde3783b53f822dc8dd00aaac934b95, 78e80b6f3fa46e26ea4e8542815305c3ba88c5df, ab606e9d148458f6d54e5d44abefd73b7990f6e0, f497c1ece7b6f3560bb39958e2673f476d608f98, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, 256e95ca331cbd35b3a23cc306b6627e6771a963, eff6546819d25df0bccdc89f02554a43a4f1c464, f64670a5f54fcce339a916497a001cbf02a9a04f, ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee, 01bf0e83159712fbbbd12171a7e268547a4cfbc5, 2374106a32169c07703599ff3f6f4b31e8067b89, 0e9a44ce661c3535d5ce747912540080324489f5, d6bc29a897fd85e7187dc33c3c974b8879462237, cde985e542218aced8c4a627cda3dd12939805f1, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, 916455d97cd792c2eb5b00663689592e25cbc8d8, 7ae2783a9196fb4bc2a610ae812d19722daddce5, 6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5, aa6c2afadd660fe4efbac699f7854e8f6f240c38, ae38dc77a962161107361f213db9216ee1274037"
c50a909e20bd07f4aea09dc6dae539b45b406a96,Evaluation of BERT and ALBERT Sentence Embedding Performance on Downstream NLP Tasks,https://www.semanticscholar.org/paper/c50a909e20bd07f4aea09dc6dae539b45b406a96,Conference,"Contextualized representations from a pre-trained language model are central to achieve a high performance on downstream NLP task. The pre-trained BERT and A Lite BERT (ALBERT) models can be fine-tuned to give state-of-the-art results in sentence-pair regressions such as semantic textual similarity (STS) and natural language inference (NLI). Although BERT-based models yield the [CLS] token vector as a reasonable sentence embedding, the search for an optimal sentence embedding scheme remains an active research area in computational linguistics. This paper explores on sentence embedding models for BERT and ALBERT. In particular, we take a modified BERT network with siamese and triplet network structures called Sentence-BERT (SBERT) and replace BERT with ALBERT to create Sentence-ALBERT (SALBERT). We also experiment with an outer CNN sentence-embedding network for SBERT and SALBERT. We evaluate performances of all sentence-embedding models considered using the STS and NLI datasets. The empirical results indicate that our CNN architecture improves ALBERT models substantially more than BERT models for STS benchmark. Despite significantly fewer model parameters, ALBERT sentence embedding is highly competitive to BERT in downstream NLP evaluations.",2021,66,"2111538614, 2125022424, 2046999993, 2163133",2111538614,Stockholm,3,"1492047220, 1630331317, 20829758",Y,"Moreover, I like the qualitative experiment (Figure 2) in which the tree is used to vary a latent dimension to change the digit’s class.;For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers.;2.  They argue that they don't need to separate train and test, but I think it is important to be sure that the generated programs work on test cases that are not a part of the reward function.",International Conference on Pattern Recognition,25,"bert,models,albert,nlp,nlp","bfad52fc64ca0169644b6e7e0ea9a46470d51709, da9b8b4073e6ad44b3da66e1e117cb1ddbf8836d, f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6, 8c33ca066e2ab615e24c65198c794114436053dd, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, 3c8a456509e6c0805354bd40a35e3f2dbf8069b1, f4c4e148546089123f8da5db4fb246ab4062bd40, 13c4e5a6122f3fa2663f63e49537091da6532f35, d8348b802c9133d9e396d4ad809b020d5be42863, 2e965b5d97c2d6fb4af284307735be39283792ba, 784141489258258b12979061d92c1a616da26525, 799d5a8271887adede035644d878c7bd555576df, 1452b25a7680bbb2c66dd7dfca6704292405da92, 3774fb4b6aa11d88c562ee5702c66cae5881af0d, 736ef8a32d6c5f76a21d61299300cf796480d507, 709af143f78bc62413c50ea1a7ee75b0702c4f59, 0e9a44ce661c3535d5ce747912540080324489f5, 8babcaf89f8537dc628a029ebf932100f57289fd, 5d433da6d0f143f20936379910104d2bb139d4ae, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, b79e5e4622a95417deec313cd543617b19611bea, 287a7da1801a07cf7fd85ffcc23c79504876ecc0, b2f8df88de24fe0ef74bda0eec1a0c3e7329b9f9, 2743e66939b30c43affb3c9e31f20cfac2109045, f70b2f20be241f445a61f33c4b8e76e554760340, d280cf82a6144b3e168840802b1a8a14d4eaccb9, ba687027ed6012f613e1f9a9cefe7683bb192934, b27a78366868ca47098e00dda74dd1b167b3a80d, b4af1cbb4d3a03d3ad7930b906fc7bf6870180cc, e1e43d6bdb1419e08af833cf4899a460f70da26c, 12c9bcf710d30ba991fb765ace07f177f53ecfd9, bb1118fb9fd86da6a2f65770353d8fb4362d9883, cd2f4aaf98bb1e020cff310000c8049d3460c54e, df7d26339adf4eb0c07160947b9d2973c24911ba, 42d3ae61ca296558cba61446bd95b8a6e5b1082d, 224c11bc51b4959bc787d6681c2b152468294b11, 90aca7b4cdd728b28b2fb5b4dc3ae3e37daa5b94, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 329d31f881a17861eedeef6a9d8fd509cddd2b7c, db528269ef800727245c0fcb35b692d29c1ccdc9, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 0df5a4f9cc8a244715fe9968732497d2ac2a7cd1, 8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6, 64c4c6cb66457c79f89ba4f2529b57bfce19ced4, 2dafea864f74a477414c3b71b742f7997e216102"
e5d720767b7a539bb2edaa98eaf572a4506a79c6,Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing,https://www.semanticscholar.org/paper/e5d720767b7a539bb2edaa98eaf572a4506a79c6,Conference,"The use of crowdworkers in NLP research is growing rapidly, in tandem with the exponential increase in research production in machine learning and AI. Ethical discussion regarding the use of crowdworkers within the NLP research community is typically confined in scope to issues related to labor conditions such as fair pay. We draw attention to the lack of ethical considerations related to the various tasks performed by workers, including labeling, evaluation, and production. We find that the Final Rule, the common ethical framework used by researchers, did not anticipate the use of online crowdsourcing platforms for data collection, resulting in gaps between the spirit and practice of human-subjects ethics in NLP research. We enumerate common scenarios where crowdworkers performing NLP tasks are at risk of harm. We thus recommend that researchers evaluate these risks by considering the three ethical principles set up by the Belmont Report. We also clarify some common misconceptions regarding the Institutional Review Board (IRB) application. We hope this paper will serve to reopen the discussion within our community regarding the ethical use of crowdworkers.",2021,63,"1380557800, 115300694, 2072404217, 1746959",1380557800,Athens,3,"49356798, 2146058962, 7617146",Y,"An image has many attributes: the glint in the corner of a window, the hue of a leaf.;Is it better to add attention to lower layers or higher layers?;2. The proposed attention mechanism seems to be fairly domain (or task ) specific, and may not be beneficial for strong generalization (generalization over unseen category).",North American Chapter of the Association for Computational Linguistics,21,"crowdworkers,research,nlp,production,nlp","549e933821fdf7cd0309dacaae99c8284cbfcc24, a926093ef103c02fd5ef7310e0d41b83d1958ed5, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, 2346d121f38fc19c77e0b062415519843f478163, c2a448bb511ebae41a87e69891da8bbf17ddba3d, 0e9a44ce661c3535d5ce747912540080324489f5, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, 73cbaf5f2441ef3478266b5438c0e90d1ae71652, b4af1cbb4d3a03d3ad7930b906fc7bf6870180cc, 48c73c389c3f36d70407eb8309a0b41578c15fc8, c9f320789e98d2c7a798a9705e26dbe317677966, 079b57837221413bf99ab40999c77c29e280e0c2, f75b70c9d7078724b592ec3e21de705e7b6ff73f, 579476d19566efc842929ea6bdd18ab760c8cfa2, 7171a0e9b07ebc98a32eb912262613efc20f283a"
0427110f0e79f41e69a8eb00a3ec8868bac26a4f,Do NLP Models Know Numbers? Probing Numeracy in Embeddings,https://www.semanticscholar.org/paper/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,Conference,"The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens—they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise—ELMo captures numeracy the best for all pre-trained methods—but BERT, which uses sub-word units, is less exact.",2019,219,"145217343, 1705260, 48831399, 34650964, 40642935",145217343,Copenhagen,3,"2078772072, 2110760082, 1492047220",Y,"Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice?;The authors show mixup provides improvement over baselines in the following settings: * Image Classification on Imagenet.;see Fig.2 —> see Fig.1 page 4just before equation 8: the the",Conference on Empirical Methods in Natural Language Processing,19,"numeracy,numbers,reasoning,nlp,nlp","1562390dd212516cd857009cbd4f857a902d1f3d, 5290d7921f0266c8b50b79fc8a0b7d22868f4f60, 0a829289a16ae48837cc2905635435db98bacc76, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, bad4c08f03587e38ee960e2aa76e16d722826e7c, 182acc7a1c9a63cfdef9f86ee9df43fd75e05060, df1a2539afbad27c4c80115a6f8f59a089024865, 71f3bfea47b96fbab78a3439ee819cd92a33329d, cf523942d56e90db182c5788845f6502da9a307d, b3f7359c6d5780972c5ea8db016a01f0c705aa01, 9619cde5c79d91ca5c432186668618312175f8dd, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 5d24ed8942235324512d6cedfd8dbf54c57658b4, 38179848e2d6a3ad373b1793848816111428ac36, 2ed691a353fa48403d493ab658f5f267a42f0bf1, 787ae2c51cd82b904bb4fb9ccb15266381af5436, d88c1255876b62fb5f5a8b292098ca430710a540, 1c748f86182a62d44d5b44316db510f8d833e19f, b6a7226e5f6d618370995eccad68af195ef32da2, 3774fb4b6aa11d88c562ee5702c66cae5881af0d, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, ccca203382e5dd198c089a0f1d7af7bef0f694e9, cf5dfc4a9f7a82b32640128ca10832eace55880e, 9f9afed22cdc73be627932270a9bcae341df99d4, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, c9645aa4ea31903e02e201b877fd3e1466adff4f, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 92afbbe41174a545f9da9992e33c9a9592e529aa, 9d6acac70b2d1fdb861a08b00766ef263109cd7f, 86dbd884043eb5807c61d2c65b813e673b4a04fa, 9727206903eb40d4fa42606711bad3402f2ba9aa, 14fe35149aed6a47b6ebfd207deb7681b9446bb6, 697f2f3598057cd17cff7749d768cae0993c6727, ac67d5f9c89d8d72fbd074f94079608220348f3f, 48c73c389c3f36d70407eb8309a0b41578c15fc8, 01bf0e83159712fbbbd12171a7e268547a4cfbc5, 0bca61986b8edeaf33018d0203b44110f2480110, 417326e51d78ba8bd2621f23e539b41bbdd336d6, c84aa52bee5116f80c7740503edff4b08f733c3b, 00bbc94806ca0821a9c82d8aedf16f0e6263b89f, 5e31fa4e69d1a3587230f5d134c0b7e2ed84a742, 63316bb5b88d362051c048e864c3ae5d97a26d30, 2c3eef2f17369912e330281d54b535675077e4ca, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, be082d70534db088315f2cc5b42c2fdcd58c1b8c, b34fc78de28be598e21118d7cb9d84d63374addc, 6a1b25f7a67395ad1e676027322913acbb0a0635"
2743e66939b30c43affb3c9e31f20cfac2109045,Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks,https://www.semanticscholar.org/paper/2743e66939b30c43affb3c9e31f20cfac2109045,Conference,"Labelled data is the foundation of most natural language processing tasks. However, labelling data is difficult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it. Descriptive annotation allows for the surveying and modelling of different beliefs, whereas prescriptive annotation enables the training of models that consistently apply one belief. We discuss benefits and challenges in implementing both paradigms, and argue that dataset creators should explicitly aim for one or the other to facilitate the intended use of their dataset. Lastly, we conduct an annotation experiment using hate speech data that illustrates the contrast between the two paradigms.",2021,85,"2043232919, 2737827, 2022288, 1970864",2043232919,Kiev,2,"2145413970, 1596817678",Y,I think this is a nice example of a detailed analysis of the representations acquired by a reinforcement learning agent.;I don't understand what is offered beyond the original papers.,North American Chapter of the Association for Computational Linguistics,21,"data,annotation,beliefs,nlp,nlp","3be6a57d6db95bba2962a1f3476414a0a9b230b5, 0c4070272fb7d6b98971a107b022ff8abf0aa55e, 82a09f72d7b9587e3b519b1cd9640a5a611f3480, e67a2817089312746d69b38ce9abfdc4b1bc69c3, 013eb12ce5468f79d58bf859653f4929c5a2bd14, 4b991efaa8493a5925c2aee9eec980831213eba6, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, d88083e37c44461ce3e404bd57257cd3edb07d4e, ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba, 9712624bb61abb0da989514cae558cfab61bb9d2, a9c1566119695250f68a572a4260b03721cc8ba3, 2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c, b651d67502790e1d6d41c589e1d93e996ba7b935, 453fdfeefd6498a65be339d7e8722f6f3288907e, 01f0f5205d03870f172ae8f04e33356d5a0af221, cb3968152f7d93f53d24b00279a90d5071ddc85a, 9810bcaf5ac1792e6a2738a86f85ce270d448040, 8f99f5cb3950fcd1628c6a563be4b4fc4966fa66, 41d6b6bd4bf44bf13b9157b603e33360e5e77a01, 0fe0f08e71e188cb2a663f36acb296d9eb6fe694, a0f788f6de0fb83d623c875a98120e3f347f70d1, 4f8d648c52edf74e41b0996128aa536e13cc7e82, 676664ee7471738577f641e6159e7596625b7fdb, 2097ff87df3cb9427c7388bc7b997ed56907d45b, 3c8a456509e6c0805354bd40a35e3f2dbf8069b1, 2c03df8b48bf3fa39054345bafabfeff15bfd11d, 1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7, b7034546bee38ba13d3b312fce893a22e33ce4dd, d235a9085e0543fcbe502fbc269f9a8ee01dcbab, 69d49a06f09cf934310ccbf3bb2a360fa719272d, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, bcc82ce554942880814243fc8c08a88b9d2aad09, ca011427853d34ce4ec9ccafde8a70c9eacc3e21, 42543dc42e65609bbbf2be470d54dd923532c36a, 98ce7af921e7c52d81df64d632d34eb09522cd75, b04550f0722e9614163855ab36fc2430b931a3fe, 8ba8a0d18a06752f5a39996ccf1e914da0941443, 4aec7bcb292cb763341a22fc67a1cfaceb3a2c67, eacf9284a39adcd56172665f31fd5a72560bba7a, ca05ca5f70616456d0f2e05c4a51cbf8b8d273f7, 64c4c6cb66457c79f89ba4f2529b57bfce19ced4, 780c7ead33428d282044519fee5e773ad56d5a2c, 1109d62ebd2b29a7dc148bc30dd6cfc803a63dec, c2aab470b8cf92f090e0a3bac1794b21500585e6"
ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8,RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models,https://www.semanticscholar.org/paper/ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8,Conference,"Backdoor attacks, which maliciously control a well-trained model’s outputs of the instances with specific triggers, are recently shown to be serious threats to the safety of reusing deep neural networks (DNNs). In this work, we propose an efficient online defense mechanism based on robustness-aware perturbations. Specifically, by analyzing the backdoor training process, we point out that there exists a big gap of robustness between poisoned and clean samples. Motivated by this observation, we construct a word-based robustness-aware perturbation to distinguish poisoned samples from clean samples to defend against the backdoor attacks on natural language processing (NLP) models. Moreover, we give a theoretical analysis about the feasibility of our robustness-aware perturbation-based defense method. Experimental results on sentiment analysis and toxic detection tasks show that our method achieves better defending performance and much lower computational costs than existing online defense methods. Our code is available at https://github.com/lancopku/RAP.",2021,51,"2120801160, 2149202150, 50492525, 2108485135, 11774802",2120801160,Vienna,3,"50593332, 1630331317, 2155516106",Y,"We have to, however, evaluate the approach on what it is able to do at the moment.;Overall, this works seems somewhat too preliminary at this stage.;In particular, they aim at generating a complete set that fully specifies the behavior of the oracle.",Conference on Empirical Methods in Natural Language Processing,21,"backdoor,defense,samples,nlp,nlp","a22f3398ea865426c89ee66f4824ec626e56a864, 0599f45e03ac2016321df0dd653ba4c0034c79d5, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, 799d5a8271887adede035644d878c7bd555576df, af9280741ef627f0d6c8437605d002d3bfc2d1b1, 71a85e735a3686bef8cce3725ae5ba82e2cabb1b, 1452b25a7680bbb2c66dd7dfca6704292405da92, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, 39444c55f07839ac6a0d1839472a982f8fb447bb, 5031790972d496547b6613d46a4a0134c824db6e, 8674494bd7a076286b905912d26d47f7501c4046, 83cebf919635504786fc220d569284842b0f0a09, 8713452753fd01de5616121af93e173d4f74eaf6, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 0daa1d31d6949f8804089d8a1c11c4560422ad39, 696b388ee6221c6dbcfd647a06883b2bfee773d9, 7c8bd41e9cebdb01f445a51ba43839c8d9b3ab91, 32524aa3ae8522542753ed7e6f4cca3970e4acab, b266510f5f9b40d42b51884ad13a1867fb3284fd, c468bbde6a22d961829e1970e6ad5795e05418d1, 02fa2389b1b64b661192e224bed8af6df0ce80f6, 19cf7458db4e17c7504eee24ccf961e1dc91435c, e2a85a6766b982ff7c8980e57ca6342d22493827, 92930ed3560ea6c86d53cf52158bc793b089054d, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, a92b103b81a48878e76f1fcfc3e2a1454f895555, 624b2f14be4287d6a400cdf88a6f911b434b182e, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, a2e667e4382aaa8e02a17d0522c1a910790ab65b, 8894d431a768a35dc7ca4d762ebdba4f407b978c, 709f7a6b870cb07a4eab553adf6345b244913913, 5471114e37448bea2457b74894b1ecb92bbcfdf6, 1bc34cb22131554ba18f6ba9e6ede5beb42939f1, cb3968152f7d93f53d24b00279a90d5071ddc85a, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, 75ea299834d6949e89e91d006677343ddab44e49, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, 8799ec4bdf98bc313aa8c41a706a385e026d1f88, 785a1c77fabd1ad870a9dbc98d235d2fcbdbaa6f, 84725855d10b531eb8cbe54935dda0440c2fc750, 80d9f0eb47b712988d19cbe29a7bfa63f2a175d0, 37b0a7a6c8fb26e32b9206847e78d521a2cd5900, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, 85328b4a8132bf4299f8cd7f8e79e850d561c8fc, 676664ee7471738577f641e6159e7596625b7fdb, 8fd8cb4951ae9634a378383a880fbf16ac5d6926, d617f51833860dc50d202af7f80be71304b2e994, 35adeef964fd344288febc7def2780007587724f"
9eea59c34f139f3d2153226c8cf026e975622074,Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation,https://www.semanticscholar.org/paper/9eea59c34f139f3d2153226c8cf026e975622074,Conference,"Public datasets are often used to evaluate the efficacy and generalizability of state-of-the-art methods for many tasks in natural language processing (NLP). However, the presence of overlap between the train and test datasets can lead to inflated results, inadvertently evaluating the model’s ability to memorize and interpreting it as the ability to generalize. In addition, such data sets may not provide an effective indicator of the performance of these methods in real world scenarios. We identify leakage of training data into test data on several publicly available datasets used to evaluate NLP tasks, including named entity recognition and relation extraction, and study them to assess the impact of that leakage on the model’s ability to memorize versus generalize.",2021,56,"26940961, 150147667, 1404100607",26940961,Dublin,2,"2810600, 2109184366",Y,"1.  The paper misses some more recent reference, e.g. [a,b].;They apply this architecture on the same task as the original article: document classification; they use a logistic regression on the extracted representation.",Conference of the European Chapter of the Association for Computational Linguistics,21,"datasets,nlp,ability,data,nlp","6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, ed935c6b359a7a486c28240d796e84897d095125, 5687c9e8da574453fd873662b95caec70dac9d1e, fc32074b37a6d9dda535a70f9689022e70508520, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, cf523942d56e90db182c5788845f6502da9a307d, f406aceba4f29cc7cfbe7edb2f52f01374486589, 00d1f3423a33f73ca6aee884a58834547475d2f0, fa75a55760e6ea49b39b83cb85c99a22e1088254, 1661d0d8d47cac41e01c59c60aac3675b4396698, f75b70c9d7078724b592ec3e21de705e7b6ff73f, da5d78b3e3a1544fde98fba86088e1215e97cbe8, 929305892d4ddae575a0fc23227a8139f7681632, b61b260de1599e6e89491cad9160898fcd3b34c2, 42543dc42e65609bbbf2be470d54dd923532c36a, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, cc78babfacce48e715dac56886d7dd9746cfcab0, 3cc2f69951cd24fe61be4cf32d62afbac297bc2b, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, aa9bf8b4c4ce8c28b9f47d7ce4f416aa9726bb0d, 6f31f59f7b4bd751b5d8657d8a878bef571f9a4c, 41d6b6bd4bf44bf13b9157b603e33360e5e77a01, f04d8e558c11ac0fc9318c4f3d61b651c62ddcfa, a80e26e6365b215715c182d19a9aa8bb876ac768, cc5726fc0ebb84f741f3496a3c52ced162c596ba"
fafa541419b3756968fe5b3156c6f0257cb29c23,Visualizing and Understanding Neural Models in NLP,https://www.semanticscholar.org/paper/fafa541419b3756968fe5b3156c6f0257cb29c23,Conference,"While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\em compositionality}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\em salience}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,",2015,636,"49298465, 39717886, 144547315, 1746807",49298465,Madrid,2,"2108706355, 2115215055",Y,"Is this on bAbi as well?;But, it would be good if this can be supported with real life examples.",North American Chapter of the Association for Computational Linguistics,15,"compositionality,nlp,methods,networks,nlp","33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4, 155eeede0c070f1f017ba5c9f6cacd7ae0b098aa, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, 7998468d99ab07bb982294d1c9b53a3bf3934fa6, aad2d03c17bc7d1e636d0e79944ad4588af989d5, 5371896313ac227eb819038dd55f213cb42b99e2, 4d4d1ac9a9ad8592a6cc05082437d706ee176a38, b5904cd5dbf73b8d5ff13517de490c292d877ee0, d2a505586c0da20752b98f63c7760b6a5c41e28d, 003ef1cd670d01af05afa0d3c72d72228f494432, fc32074b37a6d9dda535a70f9689022e70508520, addae423490bbe82da4fb2fc265237178686b4e8, d1efcef213c433445be56d7479eb47d972b3ee79, d916776e0c6a04b0def4c22257c188776c2edab2, 6001895c2fcf69528d01306e9d293d9d2a4cc67b, a281d563261c738f13b9e58a525e7e265a619c93, bc44c0c64a473e035b11ae60a1993ad3db1acd2e, 6f75e8b61f13562237851d8119cb2f9d49e073fb, 53103ae318a19569ac82cee5062de2cf73bf386c, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, 649c3497e3b34b15a5011259fcb837cf6c1ac04a, 371a343457a4fbff00000bf4faa29b2b2f85744c, 752604994a7ca548ff2954114fc61a501d857b1c, d9a7fa7616a327367696e19b1846519745cd43ff, f9163156eeba67762a7441db48fe6720106137cd, 35adeef964fd344288febc7def2780007587724f, b80e2af7ffa85fe2f02ea8390e4915d55c19d199, b61b260de1599e6e89491cad9160898fcd3b34c2, 3774fb4b6aa11d88c562ee5702c66cae5881af0d, f0f1627db35b4942e0f83069f20dd0948fc35d28, 343500e0052eb1b683f32b00efbbd1331c94184a, 993df7df129f8d18816877d69923d7df7b347d85, ce212cb873a54e5716da53a66b10298ac013008a, 5d433da6d0f143f20936379910104d2bb139d4ae, b651d67502790e1d6d41c589e1d93e996ba7b935"
1cf2e9e198feef3893da2800a7949f6880ddc084,ExplainaBoard: An Explainable Leaderboard for NLP,https://www.semanticscholar.org/paper/1cf2e9e198feef3893da2800a7949f6880ddc084,Conference,"With the rapid development of NLP research, leaderboards have emerged as one tool to track the performance of various systems on various NLP tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of NLP evaluation: the ExplainaBoard, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g. what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g. where does system A outperform system B? What if we combine systems A, B and C?) and (iii) examine prediction results closely (e.g. what are common errors made by multiple systems or in what contexts do particular errors occur?). So far, ExplainaBoard covers more than 400 systems, 50 datasets, 40 languages, and 12 tasks. We not only released an online platform at the website but also make our evaluation tool an API with MIT Licence at Github and PyPi that allows users to conveniently assess their models offline. We additionally release all output files from systems that we have run or collected to motivate “output-driven” research in the future.",2021,50,"144118452, 41037252, 2116642640, 30300197, 46923811, 2087363104, 2108176413, 1796245019, 14199369, 1700325",144118452,Lisbon,3,"1739899643, 31380410, 115504645",Y,"One direction to strengthen this paper is to examine if RMN can do better than pair-wise interactions (and other baselines) for more complex reasoning tasks.;However, according to me, the fact that it provides no model description, no model analysis, no modification of the model to improve the sentiment discovery, prevents this article from being publicized at ICLR.;The technique systematically interacts with the oracle using observations, which are abstractions of environment states, and it is guaranteed to produce a data set that completely specifies the oracle.",Annual Meeting of the Association for Computational Linguistics,21,"systems,nlp,system,research,nlp","d2a505586c0da20752b98f63c7760b6a5c41e28d, ccca203382e5dd198c089a0f1d7af7bef0f694e9, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, ff75865cde62592d068b2afd055c57c81d77158b, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 8713452753fd01de5616121af93e173d4f74eaf6, f6dab84c2c00ab92d8ee9d9359d7e530512114f9, d1e701665e73faa648cb15473952576f40e8e122, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632, cd8a9914d50b0ac63315872530274d158d6aff09, 011095a0082e5e301f9bf30267b193c1c9e7e370, 5031790972d496547b6613d46a4a0134c824db6e, f64670a5f54fcce339a916497a001cbf02a9a04f, 94cb5503b191815ce77b19147d96c6fbd68f06bf, 19cf7458db4e17c7504eee24ccf961e1dc91435c"
d1206ccabd1980848f14472d6548251c2fab7963,Exploring and Predicting Transferability across NLP Tasks,https://www.semanticscholar.org/paper/d1206ccabd1980848f14472d6548251c2fab7963,Conference,"Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling). Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even when the source task is small or differs substantially from the target task (e.g., part-of-speech tagging transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as source data size, task and domain similarity, and task complexity all play a role in determining transferability.",2020,131,"144244743, 1664686681, 2227827, 2041695, 3382568, 1401265033, 35208858, 2136562",144244743,Vilnius,2,"1524732527, 28552618",Y,"Some reasons below: * There are no specific results on properties of the divergences, or axioms that justify them.;On empirical evaluation, I already mentioned that it impossible to understand what the classification problem on TIMIT is. I suspect it might be speaker identification.",Conference on Empirical Methods in Natural Language Processing,20,"task,tasks,source,nlp,nlp","20381972ee66e03ea218e5b3d39d6423b6e35f0f, 2a15e9ac5593cd1f8bd3a724e78a173037c02fea, da3f33d858586d24cb265e79eb54f3746e998f57, cf523942d56e90db182c5788845f6502da9a307d, 6be56f559a74c0124526242e70cbdfd16cbc60a7, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, f476d44a4892e0c8256e50e9075f8dd3d412bfcf, 88a724083b2cfcc096448c28e6973c8f761ee463, d2a5dcecd2ffdf03473df1688091f08fadb114a3, d916776e0c6a04b0def4c22257c188776c2edab2, 808e9ce4e86e79098edea7f00b5b91663b87a5e6, 9e540662619327a3056d9e40bb58058868f6f805, 6dc4883228c95e8a332320fcc587a0ff33c84d59, 33ec7eb2168e37e3007d1059aa96b9a63254b4da, dbabab9bf5955558f73a37644f4bb626106a6d73, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, 5cdb31c5e3e0aa4cc2d10229793b4911f69fd78f, c84aa52bee5116f80c7740503edff4b08f733c3b, d1ae4ab5047489c2b010c7ce72262982ad66ad60, a13149a80855412d970d0de2b41c611f4cf7e1da, 980858461df7c4349f17b427686c5bcbcffbdc04, 9b54941de1e21826ecc28b32730ac3f69991ede4, 37b9478b792fb1e7ad5f2c322ed4445b93df6c9e, e7f3478fd8aac6940a4bf4f5eb60ac38f6b0b85b, dadfb3ff45e19dc22456a645f441bbeb17c93c9c, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, 2b7f9117eb6608a58be4c078ca3d69c0e5ccb875, 1a60a9d1eef24e123c27a9eee5a399ac2b620fee, e30d9b8ce108d982169621b88a5e3fb69fec70e1, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 1fa4936fb06319c3f4536c26a447d5507c92bd48, 9eea59c34f139f3d2153226c8cf026e975622074, 714f47bbedcadd7ebc44d2d5010f13323fc6a256, cd20ed8e70f6459f5617350f6b84c5c6d5929cb7, 2019cf49b51021a376f9833a53565513f0d8107b, e2f3aa4ecc487fa496d1a51e875e747d4b7e6001"
30f233eecca2239ee1dd754914324092e53f8f19,Evaluation Examples are not Equally Informative: How should that change NLP Leaderboards?,https://www.semanticscholar.org/paper/30f233eecca2239ee1dd754914324092e53f8f19,Conference,"Leaderboards are widely used in NLP and push the field forward. While leaderboards are a straightforward ranking of NLP models, this simplicity can mask nuances in evaluation items (examples) and subjects (NLP models). Rather than replace leaderboards, we advocate a re-imagining so that they better highlight if and where progress is made. Building on educational testing, we create a Bayesian leaderboard model where latent subject skill and latent item difficulty predict correct responses. Using this model, we analyze the ranking reliability of leaderboards. Afterwards, we show the model can guide what to annotate, identify annotation errors, detect overfitting, and identify informative examples. We conclude with recommendations for future benchmark tasks.",2021,48,"145009056, 40080808, 49462969, 9051130, 3422908, 1389036863",145009056,Madrid,3,"39703662, 2254124342, 49248672",Y,"If we consider e.g., a linear 1-layer autoencoder to be equivalent to PCA (without the rnn layers), in essence this formulation is closely related to applying pca to reduce the initial dimensionality and then t-sne.;To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER.;First, the description of what is the prior used by batch normalization in section 3.3 is unsatisfactory.",Annual Meeting of the Association for Computational Linguistics,21,"leaderboards,nlp,model,models,nlp","b09139c153bac8893e8faea2b3a59159234caadc, c07802ed8a25998e9bd44ee1ddbcc63b7eb34060, bdb68c5e2369633b20e733774ac66eb4600c34d1, d2a505586c0da20752b98f63c7760b6a5c41e28d, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, cd74acb268404cde24f5131a22d04d48776b283e, 29ddc1f43f28af7c846515e32cc167bc66886d0c, 3789eb72c32ecf5e33442570358dd786dd67c8a2, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, 32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b, 5471114e37448bea2457b74894b1ecb92bbcfdf6, 8fcbd1cd1ee2211bd183b986900042470ee7f440, d93bcf0685c15c45d078eafea565969c04daccd3, a6bba5ce9867c978210e3d056691b5c1e769b760, d57f11ed40c3cdcbb36cb758191db4f2c9372965, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 1e1cf81a1113482be3f0c280db994a832cb9426a, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, 63de6db7245f634ecbef4c505099874c1ba65145, 752604994a7ca548ff2954114fc61a501d857b1c, c84389369720dcd2f004c48e58fbac2c45c8f092, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 2f44ab0e52eb98fdfc0086638887f175b6f2fe4b, 88a724083b2cfcc096448c28e6973c8f761ee463, 579476d19566efc842929ea6bdd18ab760c8cfa2, 5bedddda2aab2c5a03017d96a6dd7acf517c6961, 9a51df7eaf2457b034c49ca8ca4ca3b81e6c08c0, 84a36e19f9394f22b34f79756fa9628a795e02ea, 7884b0ec63b8a08f7cd793a989df44f6bb53116c, fc77048474ccd34c6507701591c2e6ab3ca647ef, 89858723bec341178f2b00d34ea3016baaaf71a6, 084a93c8ac0230ea9fe64d445ad1d6af5ea0b3b3, 7536bce1007a765fd097a7cc8ea62208a8c89b85, 14dd50979af27bd2574c8068db11d27028b56afd, 46c266b3d1274dacd7fce27ee8cb4d587f087a58, 2880ac931c11176aee6d42a7e7bb0703aacde3f9, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, 54a26f5ef4b0524c60249fe98d0fc3646b2791ad, fe4c5074f021cd1c810892c1b6bc267b23aa6e5c, d1efcef213c433445be56d7479eb47d972b3ee79, 1eaab9b33f1261744567455a14830e8a92796cf5, af13a92977d4f4dc5b28b13746d86111d42939e8, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, 01f0f5205d03870f172ae8f04e33356d5a0af221, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, c2528e88d5554e9df9f9d482ad46cb5331c4d794, 41d6b6bd4bf44bf13b9157b603e33360e5e77a01, d88083e37c44461ce3e404bd57257cd3edb07d4e"
c4cdf2e0d89aadb13bf9c61654ff9e17be2b01b9,BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements,https://www.semanticscholar.org/paper/c4cdf2e0d89aadb13bf9c61654ff9e17be2b01b9,Conference,"Deep neural networks (DNNs) have progressed rapidly during the past decade and have been deployed in various real-world applications. Meanwhile, DNN models have been shown to be vulnerable to security and privacy attacks. One such attack that has attracted a great deal of attention recently is the backdoor attack. Specifically, the adversary poisons the target model’s training set to mislead any input with an added secret trigger to a target class. Previous backdoor attacks predominantly focus on computer vision (CV) applications, such as image classification. In this paper, we perform a systematic investigation of backdoor attack on NLP models, and propose BadNL, a general NLP backdoor attack framework including novel attack methods. Specifically, we propose three methods to construct triggers, namely BadChar, BadWord, and BadSentence, including basic and semantic-preserving variants. Our attacks achieve an almost perfect attack success rate with a negligible effect on the original model’s utility. For instance, using the BadChar, our backdoor attack achieves a 98.9% attack success rate with yielding a utility improvement of 1.5% on the SST-5 dataset when only poisoning 3% of the original set. Moreover, we conduct a user study to prove that our triggers can well preserve the semantics from humans perspective.",2020,124,"151257231, 66697271, 153642281, 144588806, 2118869261, 40238834, 2109716565, 2145954003",151257231,Sofia,2,"145966834, 1407546424",Y,"Considering that this is a big, complicated system, it is crucial that the authors included both an ablation experiment to see which pieces were most important, and an experiment that indicates the amount of labeled data that would be required to achieve the same results with a supervised system.;The theory is not strong and the experiments don't necessary support the intuitive claims made in the paper.",Asia-Pacific Computer Systems Architecture Conference,37,"attack,backdoor,nlp,applications,nlp","d2efa91bd7d076a66bc1d1c570f6aa2da944dd88, 6a1b25f7a67395ad1e676027322913acbb0a0635, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, 6e74403ab75d0080c056fd66702ab1f54f04d9b1, 3a083d843f891b3574494c385699c21766ce8b7a, d0ab11de3077490c80a08abd0fb8827bac84c454, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 456c011594ecacdd24298a161787389ccbe4b88b, 8674494bd7a076286b905912d26d47f7501c4046, 22c141b489e6e189f5996537b0a908fc10f90de7, a0d18dddaa995b126ad373e33767b9b881d16b2f, 785a1c77fabd1ad870a9dbc98d235d2fcbdbaa6f, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 4cd033a56b19f87f6adfefeef5fcc990306ecf40, 8babcaf89f8537dc628a029ebf932100f57289fd, eb9e0da8b7170e3ca4364f2f9010599c2d2556f1, 595101f13b961d69c553ce1ed24f60f3f1085e02, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, 4e746359afd6f81705b875d71cc499b904a320df, 915ab7b6ca3633230403d47dbb31cf74888cc5c9, 09797949fc70fbad8f3fed4f6cf4a91a9c709652, 6dc4883228c95e8a332320fcc587a0ff33c84d59, 82a09f72d7b9587e3b519b1cd9640a5a611f3480, f476d44a4892e0c8256e50e9075f8dd3d412bfcf, fb00016c1e048b9373803add001c1ec7e877cb23, 8b417c2be7a7707f372049fb1193f0d42f799562, f9253a3e2627f1c2a0a7e2cea320a4ec4e4d2ff9, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, 8d06f13ec042f0d8d845af5deadeb3dd9ad11f70, facd5f5deb152229ceb1803434d8690a09ab4129, 44786a2c2a8ba8cf5c74a8fb10098c220e924c56, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, 343500e0052eb1b683f32b00efbbd1331c94184a, 31a61d009442436d04b9d4e1c5beee37172289ae, 3adb779bb37d22e3aa299364c2a337603801ca5c, 011095a0082e5e301f9bf30267b193c1c9e7e370, c9a9517c8b867187b4f4c0c37cbc65263ea41d25, 752604994a7ca548ff2954114fc61a501d857b1c, 9a0965beef113cc37491004b1848149e00300561, 48fc9c42522184c652742255fdf31f7b9ed7ebae, 71854ff4306cf65c3c2161f7be2d0346275f72d5, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, beb890d47bbc21a96967f9993c9d6e15686b2eac, 10aa2be24951e6de76b630482a645d79354c4cde"
00cd2650a89734105fa0c0aba3bf07935b318290,GLUECoS: An Evaluation Benchmark for Code-Switched NLP,https://www.semanticscholar.org/paper/00cd2650a89734105fa0c0aba3bf07935b318290,Conference,"Code-switching is the use of more than one language in the same conversation or utterance. Recently, multilingual contextual embedding models, trained on multiple monolingual corpora, have shown promising results on cross-lingual and multilingual tasks. We present an evaluation benchmark, GLUECoS, for code-switched languages, that spans several NLP tasks in English-Hindi and English-Spanish. Specifically, our evaluation benchmark includes Language Identification from text, POS tagging, Named Entity Recognition, Sentiment Analysis, Question Answering and a new task for code-switching, Natural Language Inference. We present results on all these tasks using cross-lingual word embedding models and multilingual models. In addition, we fine-tune multilingual models on artificially generated code-switched data. Although multilingual models perform significantly better than cross-lingual models, our results show that in most tasks, across both language pairs, multilingual models fine-tuned on code-switched data perform best, showing that multilingual models can be further optimized for code-switching tasks.",2020,112,"1452678825, 34725175, 48180698, 3010457, 143990839",1452678825,Bratislava,2,"2412941, 2115853457",Y,"In Section 3.2, the authors listed a couple of loss functions.;Motivation In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle.",Annual Meeting of the Association for Computational Linguistics,20,"models,tasks,language,nlp,nlp","cd5b4b113d5fa90b5dc5aa372111b89d18df88fb, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, 4954fa180728932959997a4768411ff9136aac81, 1452b25a7680bbb2c66dd7dfca6704292405da92, 74bc39003e65119eaa6ba339a61b45b417a638b7, 7676c02ea839ff1ceb6e5e1427c42bc45e169bde, e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, 10ea29fda06bdbe56f591909d89f3194b452ac91, 91b2b47cabd800ef658b65bfe1f52b7293a740c3, 8fcbd1cd1ee2211bd183b986900042470ee7f440, a13149a80855412d970d0de2b41c611f4cf7e1da, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, 59c414c9efb77562f5d1aad8af14eaac968c69c0, 6fed828456964d29517f6caf31b700d8aec82153, 410fba9f03212257d0881811802e6620e59bc827, 2c03df8b48bf3fa39054345bafabfeff15bfd11d, 6f75e8b61f13562237851d8119cb2f9d49e073fb, 7eaac9847257c32afd450017d1348ecda4dcaade, 3337e4b2e63e5e99171f9da9d161771f5810c27a, 9d6acac70b2d1fdb861a08b00766ef263109cd7f, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, c12b80b44d9acfe6cd92fdf965264c4b706c367c, 5e31fa4e69d1a3587230f5d134c0b7e2ed84a742, 139a0c7a60667979dcb57eae677f75ff3f0b0196, b9f190339ce0b4cdad3464c45ec3266a7369fb7c, ef25b02f3be31c699255ee05aa90a4a17461d95d, aea731e7cf33aa3d482b13f42cedbc1adb3271c6, 8713452753fd01de5616121af93e173d4f74eaf6, d84cf745c534c010b8e55e5a4a04878906848dc3, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, 742747c7a453b293352b772d0d99541c96a351c3, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 6dcb1cd576b0e54b900f45a178efe271c383de04, 8b7b2e88207fc2dd849f5d83c9b6e890f0174abc"
4954fa180728932959997a4768411ff9136aac81,TensorFlow: A system for large-scale machine learning,https://www.semanticscholar.org/paper/4954fa180728932959997a4768411ff9136aac81,Conference,"TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous ""parameter server"" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",2016,16886,"2057642721, 144758007, 2108406634, 2545358, 36347083, 49959210, 145139947, 1780892, 2060655766, 2090818, 1942300, 3369421, 3089272, 144375552, 20154699, 32163737, 2080690, 2053781980, 47941411, 35078078, 2117163698, 2108113547",2057642721,Zagreb,2,"145071265, 1404359012",Y,Is it meaningful to perform ADD or SUBSTRACT on the leaned code?;“We can quantitatively determine how close the posterior is to a FFG distribution by comparing the Optimal FFG bound and the Optimal Flow bound.”: Why not just compare the optimal with the AIS evaluation?,USENIX Symposium on Operating Systems Design and Implementation,16,"tensorflow,machine,state,machine learning,machine learning","6ba00c2386f2edc0b43eec442cd1923b5d964633, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 696b388ee6221c6dbcfd647a06883b2bfee773d9, f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6, 00e18c603e60d861c4e99c541e4d65ef442d5945, 86d3beff240b6c882058455e098a571de86564f5, 6745a82c9236f0eec576904eb50ea700ca5a7d7c, cd2f4aaf98bb1e020cff310000c8049d3460c54e, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, 8b7b2e88207fc2dd849f5d83c9b6e890f0174abc, 97906df07855b029b7aae7c2a1c6c5e8df1d531c, f9367342405a73ab8d6de704a149babfc0edb5fe, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, 929305892d4ddae575a0fc23227a8139f7681632, 19b93280f17696a4ddfa2c75490a50ab107addf2, 697f2f3598057cd17cff7749d768cae0993c6727, 8adb47deeef943c2c1bae41f9498a382fb818a16, 25ed8ba0e8906ba98fa5d92d17a01e818796ddc9, 9e540662619327a3056d9e40bb58058868f6f805, f8e57fa370fe10147aa22714e08409fc1b7dae4b, 2c03df8b48bf3fa39054345bafabfeff15bfd11d, 916455d97cd792c2eb5b00663689592e25cbc8d8, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, f1664bbaddedea8c250873e7610ab07e53fa7132, 4f480bae3196dbbc27ab383bce33478ea963f9b3, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 834fdec542153aae5fe725df801aac87ba5e8f56, acc296f981cde8d8c205982fc4422ec35531b769, d2efa91bd7d076a66bc1d1c570f6aa2da944dd88"
f9c990b1b5724e50e5632b94fdb7484ece8a6ce7,Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting,https://www.semanticscholar.org/paper/f9c990b1b5724e50e5632b94fdb7484ece8a6ce7,Conference,"The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.",2015,6501,"3008587, 2192200, 49528584, 1739816, 145771919, 2183294",3008587,Madrid,3,"48510386, 1825752990, 2545358",Y,"Good contribution The paper is well written, and the authors do an admirable job of motivating their primary contributions throughout the early portions of the paper.;The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models.;Inclusion of Assumption 3 would in particular require better justification.",Neural Information Processing Systems,15,"precipitation,problem,nowcasting,machine learning,machine learning","9b54941de1e21826ecc28b32730ac3f69991ede4, 83ea80a9393177ecf84928f4bd1120fb679f180c, 150f95f9c73820e0a0fa1546140e9f2bdfd25954, 4fcfe83c05402b5c5fb6e853082e74af6379d7f9, 97f456643712e9618edd7465676c62af3c8ae690, eef23d76e175c0cff8e81ffcb2721c10539c8cbd, 736ef8a32d6c5f76a21d61299300cf796480d507, f0c27af6c330d5c3b0a8eb376a69ce92c85badd7, 4dcbe71d261ba5caa0a06d5c265b5509edbbe1c4, 6fb020754f6de564c3a0a07bb656c0a90be1f87d, 329d31f881a17861eedeef6a9d8fd509cddd2b7c, a8b995f0da78a79447dfb18c2337972b044f4239, 59c414c9efb77562f5d1aad8af14eaac968c69c0, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, b4af1cbb4d3a03d3ad7930b906fc7bf6870180cc, 0a829289a16ae48837cc2905635435db98bacc76, 771a858c35f6d6e6d1017dde95368de3794738a6, e67a2817089312746d69b38ce9abfdc4b1bc69c3, c3df199cbca74763c4ae9889409bbd4aa29b6255, cc017a62c605a0749e35a1264a46d62e78fb68b7, 046eb47d56beb8069b0098e3d01608f81ebb6849, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 72afe82af4c2ca100c36eb35292e85d806527f0a, b69a35662a2cac38eab22f4481285116bdf8c30e, 3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b"
53b047e503f4c24602f376a774d653f7ed56c024,Practical Black-Box Attacks against Machine Learning,https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024,Conference,"Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",2016,3188,"1967156, 144061974, 153440022, 1680133, 144643812, 144231976",1967156,Bucharest,2,"2057642721, 1829303908",Y,This paper introduces a method for regularizing the REINFORCE algorithm by keeping around a small set of known high-quality samples as part of the sample set when performing stochastic gradient estimation.;1. An important point regarding the reference ambiguity problem and eq.,ACM Asia Conference on Computer and Communications Security,16,"dnn,examples,attack,machine learning,machine learning","07b01d665646009439ca206378cc35e095ec6cd2, 0f38b3d717e0fcc6eacc9c6e78b252227440e04e, f69d06037134ab6fb65d90f5ac192cf9f55e498d, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, ba9b6f805feb62c978d384211f910790643a023e, 27245e65a27bde90b5b0bb25d157bb75a0ad8b5a, be383c607d4d357c763d2329ab71799c6e1393b4, 9312e5efa0dcef1445d45a41771f12e2a8dc6715, b795c74a0150ec091003ffbaa5bd7d74487c137b, 4dcbe71d261ba5caa0a06d5c265b5509edbbe1c4, 00d1f3423a33f73ca6aee884a58834547475d2f0, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, 97906df07855b029b7aae7c2a1c6c5e8df1d531c, 4b991efaa8493a5925c2aee9eec980831213eba6, bf69c98fca9a9f6c1cde871beddbcdc668b77771, f0c27af6c330d5c3b0a8eb376a69ce92c85badd7, 7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7, a0367346bc355c36badec2d2c47ce55a320cd75e"
e8b30ebe3351680c3b039555ae0a8d0865ad829b,Neural Networks And Machine Learning,https://www.semanticscholar.org/paper/e8b30ebe3351680c3b039555ae0a8d0865ad829b,Conference,"Recent years have seen an increase in the popularity of neural network (NN) research. The mammalian brain, which consists of billions of interconnected neurons, is renowned for its ability to perform computationally hard tasks, such as face recognition, body motion planning, and muscle activity control. In an effort to emulate the effectiveness of biological neural networks in learning, artificial neural networks (ANNs) were developed. The NN technique has been the topic of many studies over the last few decades, with applications in many fields including control engineering, automation, aerospace, psychology, economics, healthcare, and energy science. The objective of the discipline of machine learning is to create computers that can independently learn and improve. In this chapter, we have attempted to depict the types of neural networks and machine learning as well as their applications in different industrial disciplines such as science, commerce, and medicine.",2023,76,"102998577, 2215913005, 2215954607, 2215958663, 116180161, 2275394607, 70310638",102998577,Vilnius,2,"2061881331, 2074432",Y,"* There are many estimators for f-divergences (like the ones cited above and many others based e.g. on nearest-neighbors) that are sample-based and thus correspond to the ""implicit"" case that the authors discuss.;After equation 5, the authors suggest categorical loss for discrete problems, but cross-entropy loss might work better.","2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)",5,"networks,control,applications,machine learning,machine learning","be082d70534db088315f2cc5b42c2fdcd58c1b8c, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 5687c9e8da574453fd873662b95caec70dac9d1e, 5030702fea15d66a73fc997325431f1d7945ad9a, 624b2f14be4287d6a400cdf88a6f911b434b182e, 047286f5b9315a8e8bf56c4fc936e62f21495892, 60119658af638693f6de23d8466968e60c428ac7, 76b93abde3ec4e5bd84a38d488a8db209dc4ac98, a0a79dad89857a96f8f71b14238e5237cbfc4787, b080d072cfde697180db3234da08903c092e72c3, e5a1cfcd07dcfd8b1feec9c635dadc858cde8166, b2563d102456d5140ecb4111e7f08481f720d9a4, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, e248993daede136713e93929816df92b48ccddfb, f406aceba4f29cc7cfbe7edb2f52f01374486589, f9163156eeba67762a7441db48fe6720106137cd, 1ed33896e82cc3810b349cdffc2039f0b8edd82e, c292e473b3825eeb9db03c70b2e1c033aea190d5, 957f5b1e7ca48891c2e279aefbfa0f04d989c21e, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, f8e57fa370fe10147aa22714e08409fc1b7dae4b, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, 37b0a7a6c8fb26e32b9206847e78d521a2cd5900, 9d6acac70b2d1fdb861a08b00766ef263109cd7f, 2743e66939b30c43affb3c9e31f20cfac2109045, 02fa2389b1b64b661192e224bed8af6df0ce80f6, b894e52c341f293f93a5cbf496015ca0d4ed3e50, 1f8a23697562b001082b147779b5eaefd3513d0a, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, 6dd3e404aac097f63d42dc19fd08c7ec281f90b4, 787ae2c51cd82b904bb4fb9ccb15266381af5436, b7f5973dbf2ef76fcc3aea616a7f72ca79f09020, d86084808994ac54ef4840ae65295f3c0ec4decd, 1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, e75cb933d387ecb184010ff07d0ee43fc1750e2a, 643da4c4de1954daeac571a82367241db012a8bf, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, 6422f4b9e3bedf585170bffc7105ffe2061e87ae, 155f27879f185f1ab04107c91c2ae7cf6a910a03, 92912dd895c360f01a6be9c9f6d207642139525e, c2aab470b8cf92f090e0a3bac1794b21500585e6, 6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2, 746a81aa26d3ebfb81acfd6af958d6a21603cd21, 8c33ca066e2ab615e24c65198c794114436053dd, 0c00a328fa7cd56ee60338c54e89bd48310db80b, 8799ec4bdf98bc313aa8c41a706a385e026d1f88, e2a58fd18961c3941102989e3a3d0d27c615e015"
9583ac53a19cdf0db81fef6eb0b63e66adbe2324,Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent,https://www.semanticscholar.org/paper/9583ac53a19cdf0db81fef6eb0b63e66adbe2324,Conference,"We study the resilience to Byzantine failures of distributed implementations of Stochastic Gradient Descent (SGD). So far, distributed machine learning frameworks have largely ignored the possibility of failures, especially arbitrary (i.e., Byzantine) ones. Causes of failures include software bugs, network asynchrony, biases in local datasets, as well as attackers trying to compromise the entire system. Assuming a set of n workers, up to f being Byzantine, we ask how resilient can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient aggregation rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the aggregation rule capturing the basic requirements to guarantee convergence despite f Byzantine workers. We propose Krum, an aggregation rule that satisfies our resilience property, which we argue is the first provably Byzantine-resilient algorithm for distributed SGD. We also report on experimental evaluations of Krum.",2017,1176,"3094352, 9623412, 1727558, 1718150",3094352,Amsterdam,3,"2089990776, 2248549493, 2115692258",Y,"- In Section 4.3, why did you consider the entropy regularizer?;-  Please also discuss the relationships, connections, and possible applications of your technique to other algorithms used in Bayesian optimization, active learning and/or sequential learning, for instance as M. U. Gutmann and J. Corander, “Bayesian optimization for likelihood-free inference of simulator-based statistical mod- els,” Journal of Machine Learning Research, vol.;The authors of this paper propose to use a deterministic policy instead, and apply the deterministic policy gradient DPG (Silver et al., 2014) for optimizing the behavior policy.",Neural Information Processing Systems,17,"resilience,failures,sgd,machine learning,machine learning","b6593b5ad6b0e00232ee88cf2bc578645c1299a0, 18d87bff073687c025f9bd23ab2dfb20d5f72a66, be082d70534db088315f2cc5b42c2fdcd58c1b8c, 99ae62cca0b15275d9ed1528f345f9dcefe50dd7, fdc57c18f3b636c3273542327ae540217972558f, 0a92bc2dc8a216e6aced83edc0358241066833df, 5471114e37448bea2457b74894b1ecb92bbcfdf6, 0e9a44ce661c3535d5ce747912540080324489f5, e968ae8e98fff9e28468383a1826fca4a2ae5245, e24b8a9531573d284647239affc6c855505b0de4, be7cb8f79bc018e57467168fc0c7f8ad59bba04f, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, b977e8de38dc0d13817bca1ed20036badfe2a58c, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, 12c9bcf710d30ba991fb765ace07f177f53ecfd9, 6fb020754f6de564c3a0a07bb656c0a90be1f87d, fe4c5074f021cd1c810892c1b6bc267b23aa6e5c, 08764019e9762da527253b37b0ff39c46a4206b7, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, ce157cea880c9ab64de64f11a531202f5348fa05, 742747c7a453b293352b772d0d99541c96a351c3, 417326e51d78ba8bd2621f23e539b41bbdd336d6, 00d1f3423a33f73ca6aee884a58834547475d2f0, dbabab9bf5955558f73a37644f4bb626106a6d73, 52e510271b172d098ec9b107a4159216ec08527e, a56bf7ee9a56d8f84079684339a953c2df9ce76b, 7d873a9c49d3864709aa762f8740edcdbd7369c5, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, b9b639522465cc606df878eee62e7f9c4bf19e62, 7bb477077968d68aa7a6059d8d6d801fb28274da, 1ff76ab0fcf22110df62337d462e15d79a2a2593, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, 634fe61f3ab6692ea4733989a1c76e793bb8b69e, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, b61b260de1599e6e89491cad9160898fcd3b34c2, fa26a6d434450b185e669170e79fd3e1d29716bf, 2e965b5d97c2d6fb4af284307735be39283792ba, 224c11bc51b4959bc787d6681c2b152468294b11"
f70b2f20be241f445a61f33c4b8e76e554760340,Software Engineering for Machine Learning: A Case Study,https://www.semanticscholar.org/paper/f70b2f20be241f445a61f33c4b8e76e554760340,Conference,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",2019,616,"1719124, 1776779, 145193818, 1710751, 50355692, 1783184, 1693689, 2571049, 143609903",1719124,Skopje,2,"2112455515, 144027436",Y,"This would contradict some previously established convergence results for this type of problems: Reddi et al. (2016) Stochastic Variance Reduction for Nonconvex Optimization, ICML and Wang et al. 2013.;But in that case much of the theoretical story goes out the window.",2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),41,"software,teams,microsoft,machine learning,machine learning","ba9b6f805feb62c978d384211f910790643a023e, 8713452753fd01de5616121af93e173d4f74eaf6, 8c33ca066e2ab615e24c65198c794114436053dd, 6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2, 0090023afc66cd2741568599057f4e82b566137c, 48c73c389c3f36d70407eb8309a0b41578c15fc8, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, d008893e01fa7f6c5fb01dadf3f97ee96835c303, 22ebfc211d184ed615729378a43fde175bf14478, 70ca38ad480c0be0eca89ccc4972d6cc9a5824da, be383c607d4d357c763d2329ab71799c6e1393b4, 32524aa3ae8522542753ed7e6f4cca3970e4acab, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 0a829289a16ae48837cc2905635435db98bacc76, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, e02a757617c2c42eb62889cc4d4aee3765928303, 59c414c9efb77562f5d1aad8af14eaac968c69c0, af9280741ef627f0d6c8437605d002d3bfc2d1b1, d6f002d88638de71114dab083f0ea8ceea6b6a5a, b09139c153bac8893e8faea2b3a59159234caadc, ff74bfbd9ebf4c54809873aecb04be27e9402cb8, 9727206903eb40d4fa42606711bad3402f2ba9aa, c2413fa296543159b32d16350d9e29f7db528790, a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, 0894585294c67193ff3190240554677b56fd79a0, 448959eff044f02040ded5afd483b7c4e811b0ac, 579476d19566efc842929ea6bdd18ab760c8cfa2, f9163156eeba67762a7441db48fe6720106137cd, b9b639522465cc606df878eee62e7f9c4bf19e62, 6fb020754f6de564c3a0a07bb656c0a90be1f87d, 0d065e8688c38bb0148203a1738f47184a5b58d3, 5290d7921f0266c8b50b79fc8a0b7d22868f4f60, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, 2019cf49b51021a376f9833a53565513f0d8107b"
b7a717233ec3ff37385ab1b06816d0ca375f5bb3,Data Shapley: Equitable Valuation of Data for Machine Learning,https://www.semanticscholar.org/paper/b7a717233ec3ff37385ab1b06816d0ca375f5bb3,Conference,"As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on $n$ data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.",2019,515,"27316199, 145085305",27316199,Budapest,3,"1793506, 1702822, 2362078",Y,"Additional regularization terms are also added to encourage the model to encode longer term dependencies in its latent distributions.;The current analysis is too simple.;A particularly interesting question would be whether the proposed model actually is a direct GAN-based extension of IBFA, and if not then how does it differ.",International Conference on Machine Learning,19,"data,shapley,value,machine learning,machine learning","0599f45e03ac2016321df0dd653ba4c0034c79d5, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, 92912dd895c360f01a6be9c9f6d207642139525e, 7536bce1007a765fd097a7cc8ea62208a8c89b85, 785a1c77fabd1ad870a9dbc98d235d2fcbdbaa6f, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, a80e26e6365b215715c182d19a9aa8bb876ac768, 8894d431a768a35dc7ca4d762ebdba4f407b978c, 6be56f559a74c0124526242e70cbdfd16cbc60a7, e2a58fd18961c3941102989e3a3d0d27c615e015, 7ae2783a9196fb4bc2a610ae812d19722daddce5, ccca203382e5dd198c089a0f1d7af7bef0f694e9, 84a36e19f9394f22b34f79756fa9628a795e02ea, 77e6c9917536949a82e5ca02c4882b69ee8a4fd6, af13a92977d4f4dc5b28b13746d86111d42939e8, a8b995f0da78a79447dfb18c2337972b044f4239, cf41991d89301c3c12420d150792cb1163999962, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, 8d0f755dea90f35f4b126a01fa3cce96b3bdd344, 57f5bf937f7393b691428747a9078d3124e6bcce, 6f75e8b61f13562237851d8119cb2f9d49e073fb, a1ef4052acb63356928bb440874c470ad48cb40c, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, fe44200fed05f9a7c656f2245deded8fd5f5e1e6, addae423490bbe82da4fb2fc265237178686b4e8, cde39ce861e4c7514ee07fd91b6b8aac50cbf01b, d617f51833860dc50d202af7f80be71304b2e994, aa6c2afadd660fe4efbac699f7854e8f6f240c38, 256db9dba1978f004a67c86ffc321563b1aee79a, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2, dbabab9bf5955558f73a37644f4bb626106a6d73, 696b388ee6221c6dbcfd647a06883b2bfee773d9, e373cbce1a831361a8de9dbc625727169011bcce, 78aab73ed574393ab421f25b3a0e3f7343e64748, e75cb933d387ecb184010ff07d0ee43fc1750e2a, 5f51d468ce730eeade7e9f419a1fe7152582be25, da8b317b99c4b8933b2c59285639eca6c3fcb869, fa63c3f53413ced7946623889c416e34a28676ea, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 5d433da6d0f143f20936379910104d2bb139d4ae, 8e259f940f007e08207ddb7c3a052f52036d7bf6, 456c011594ecacdd24298a161787389ccbe4b88b, 595101f13b961d69c553ce1ed24f60f3f1085e02, 5cb8f417d171ae329adf446820bd32d8b49d8c04, 8674494bd7a076286b905912d26d47f7501c4046"
8f9e864fab09bbae4a46a2a62bb954db1a88eb3e,Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts,https://www.semanticscholar.org/paper/8f9e864fab09bbae4a46a2a62bb954db1a88eb3e,Conference,"Pre-trained large language models (“LLMs”) like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (“prompting”) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.",2023,177,"2138715050, 9063054, 2200271412, 152290618",2138715050,Vienna,2,"144330671, 2154742781",Y,"Perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the E-step output in EM for a GMM.;Section 5.3: Amortization error is an important contributor to the slack in the ELBO on MNIST, and the dominant contributor on the more complicated Fashion MNIST dataset.",International Conference on Human Factors in Computing Systems,23,"design,language,interactions,llm,llm","b01ad71bd376f36546f02204784908da9577bb0b, c665003881c3c35589d1e48da1ee7234b48f2ac8, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, b6593b5ad6b0e00232ee88cf2bc578645c1299a0, e2a58fd18961c3941102989e3a3d0d27c615e015, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, ead6121fbc787d508dc6a6d7106f72bf0d647d03, e2a85a6766b982ff7c8980e57ca6342d22493827, 364128bcce9836d60e685bb717b80f30e25092e0, febe776e285dc5e72c7e3ee697a87a794e1c00ff, 5406e153957dd7a165264da6e6e5d81251997404, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, 99f06e88e76f1af51d08d7adfb26d758ebc6acab, e3052ebca5eeae6a8a73e44517903d39746f5f3a, 046eb47d56beb8069b0098e3d01608f81ebb6849, 011095a0082e5e301f9bf30267b193c1c9e7e370, 7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7, d6f002d88638de71114dab083f0ea8ceea6b6a5a, e359e8960b0b09e8685a32927b7818f4b06ef881, 3994334c81478a4b17341eb1f494dbccbb73d999, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, c2aab470b8cf92f090e0a3bac1794b21500585e6, c419ee7315b9edfd8fc55bab16534fc55a564fcd, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 5471114e37448bea2457b74894b1ecb92bbcfdf6, 43eea2a73997294193228d50f9ff25fc5345664b, 447884e7da189102189a156966623335c72199b0, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, e5194ae88d63c7549678b1b73cfdaf7112164272, 3dd7f7118ee174265889d00d100cfe2a02871be8, ce212cb873a54e5716da53a66b10298ac013008a, b05306f0b142e5afb3974b1b79996e5b82653662"
f406aceba4f29cc7cfbe7edb2f52f01374486589,The Internal State of an LLM Knows When its Lying,https://www.semanticscholar.org/paper/f406aceba4f29cc7cfbe7edb2f52f01374486589,Conference,"While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\% to 83\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.",2023,102,"1746466, 144135485",1746466,Belgrade,2,"3011964, 1388622435",Y,"* Figure 2 seems like a test made to work for this method and does not add much to the paper.;For example, the authors claimed in Section 4.3 that the Stein gradient estimator is faster than other methods, but it is not clear as to why this is the case.",Conference on Empirical Methods in Natural Language Processing,23,"llm,sentence,probability,statements,llm","d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b, 08aaaf7ae3d61875e7bcf5c1bb0df4f17066e300, a781cea542e99c6bb9422858e7c04eaef18c7673, c15f30a3e84910a28cc560e7db097fd99339e8c1, 28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8, 10ea29fda06bdbe56f591909d89f3194b452ac91, 3dd7f7118ee174265889d00d100cfe2a02871be8, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, ca011427853d34ce4ec9ccafde8a70c9eacc3e21, f9163156eeba67762a7441db48fe6720106137cd, eed62d36d1b976ac3873c83645f1c25f5096f89c, b781fb7f3725a9d899d3d250b378d729a8a00442, 0c00a328fa7cd56ee60338c54e89bd48310db80b, cc78babfacce48e715dac56886d7dd9746cfcab0, ee6f23590783adec7cf6b2030c6a46f3117a708e, a9cbbef8f4426329d0687025b34287c35bdd8b38, 72a6a5d5864eb915231c7128b90977f1d2acf6e9, 8ba8a0d18a06752f5a39996ccf1e914da0941443, c997ed5ac5772bb0fce8bb11b8c86bc33310f39a, cb12fe714dd4fa1d14b6a8023d494c14c89e90ea, bb5d26da72bfe7030dbc6650b686b210ae661f2c, e2a58fd18961c3941102989e3a3d0d27c615e015, 82870bc488b57cdf5ea62877109a7278af2926b3, ce7499d6862df8269c655220049c3ed20b9b6f5e, 14fe35149aed6a47b6ebfd207deb7681b9446bb6, 57a3fc6d0aaad3c10c793b4e59390ca04c935282, 811df72e210e20de99719539505da54762a11c6d, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, b61b260de1599e6e89491cad9160898fcd3b34c2, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d, f2bc4057e696f49c326bf8e1588772a16f053754, 929305892d4ddae575a0fc23227a8139f7681632, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 00cd2650a89734105fa0c0aba3bf07935b318290, 20a3ed03888037e2802fa9abad02ffa0a8dcc228, cd20ed8e70f6459f5617350f6b84c5c6d5929cb7, 21042565ec941f4fa31ac5a0af85a1a84ff21f1b, 1051abf1e3dae90241ad15b3f98f2e41197ee611, be383c607d4d357c763d2329ab71799c6e1393b4, 57af930b57886186695f279efa03adaf272c4de1, 70ca38ad480c0be0eca89ccc4972d6cc9a5824da, a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854"
bdb68c5e2369633b20e733774ac66eb4600c34d1,LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,https://www.semanticscholar.org/paper/bdb68c5e2369633b20e733774ac66eb4600c34d1,Conference,"The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on both reasoning tasks.",2023,61,"1557412457, 2150277971, 145131956, 2143418409, 2212836814, 38656724, 1996394, 1746416",1557412457,Ljubljana,2,"2921637, 2061202877",Y,"These structures are extrapolated  by;(muscle routing parameters, including insertion and attachment points) are optimized along with the control).",Conference on Empirical Methods in Natural Language Processing,23,"llms,methods,peft,llm,llm","156609022dd6258c60238859622da0a1683bd062, 697f2f3598057cd17cff7749d768cae0993c6727, 087dd95e13efd47aef2a6582e6801b39fc0f83d8, eacf9284a39adcd56172665f31fd5a72560bba7a, 40416ac3bf78583eea37661b1b446e9939245b3e, d235a9085e0543fcbe502fbc269f9a8ee01dcbab, 2e965b5d97c2d6fb4af284307735be39283792ba, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, a81ba6a07bf7a2ecff871e3362a77404501d0927, 70ca38ad480c0be0eca89ccc4972d6cc9a5824da, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91, 752604994a7ca548ff2954114fc61a501d857b1c, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, 3705919b880f4f8dc37483a704e14dd078cb9ac4, 322d91190acd8ac8c64598f5126947b0485ba249, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, 3f43bcb910df8c1a76de79057a63195e6c6bc258, f2bc4057e696f49c326bf8e1588772a16f053754, d88c1255876b62fb5f5a8b292098ca430710a540, 07cca761749bfe21c2d096ff60f32b574d5c84c4, 993df7df129f8d18816877d69923d7df7b347d85, 9e540662619327a3056d9e40bb58058868f6f805, eadb1e7da375939e25083ae3936c4f4ef1f2a719, 649c3497e3b34b15a5011259fcb837cf6c1ac04a, 150f95f9c73820e0a0fa1546140e9f2bdfd25954, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, f72053903270d9a7f41108461ad04d5aa075218d, 780c725848aac1118d00c8bb306719ec803369cd, 8f2cf30f9c825d8ef7d622601dbd525ace95e025, f4cfc7cbad257f1688772d59f694c16189dba811, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 71f3bfea47b96fbab78a3439ee819cd92a33329d, 6628f9ee35e36cdfdcac8a46cef4dba8d529a83b, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, e24b8a9531573d284647239affc6c855505b0de4, df7336844a31165db0ae08f1cd0f560c9e3faeea, 740b5dec469e6b54e5e191a577a9b6a6ad8562e3, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, 2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c, c96fc88631f2b8e2fe192027a8a237445635328c, f6dab84c2c00ab92d8ee9d9359d7e530512114f9, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, c292e473b3825eeb9db03c70b2e1c033aea190d5"
993df7df129f8d18816877d69923d7df7b347d85,LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion,https://www.semanticscholar.org/paper/993df7df129f8d18816877d69923d7df7b347d85,Conference,"We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",2023,46,"2197076899, 1384550891, 51583409",2197076899,Madrid,2,"1390140002, 145843448",Y,"However, evaluating on simple datasets like Kaggle cat/dog and Oxford Flowers diminishes the value of the paper.;Pros: - A new GAIL formulation for saving on interaction data.",Annual Meeting of the Association for Computational Linguistics,23,"llms,pairranker,llmblender,llm,llm","c9f320789e98d2c7a798a9705e26dbe317677966, e70ddfb04969b663ef8cd711a70a0acf563d6e5f, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, 72a6a5d5864eb915231c7128b90977f1d2acf6e9, f2bc4057e696f49c326bf8e1588772a16f053754, e7f3478fd8aac6940a4bf4f5eb60ac38f6b0b85b, 1c748f86182a62d44d5b44316db510f8d833e19f, 4b9184937da308914b9e13c43bfd75845eaf910b, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, 643da4c4de1954daeac571a82367241db012a8bf, 8b28792f8405b737229afb92c99c579b86d8aa98, b135e330cc1473c8c24fa63bb9a5b64f51993f9e, c24d47ff95cd4bda073c75ec24ececaa3b10c995, 40416ac3bf78583eea37661b1b446e9939245b3e, b09139c153bac8893e8faea2b3a59159234caadc, 3a58efcc4558727cc5c131c44923635da4524f33, 6548106035c7208ad498730627874a482734b9ac, e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0, 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d, be383c607d4d357c763d2329ab71799c6e1393b4, 1a4c6856292b8c64d19a812a77f0aa6fd47cb96c, 6eb8caebbffc7e5b301b66dc36acad46b4dca5c9, db6084fdb3baceddacdc726474722debe1ef7e65, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, f5ac3ec506a5a01807dd7196c0c52eef008b78cc, cd5b4b113d5fa90b5dc5aa372111b89d18df88fb, 447884e7da189102189a156966623335c72199b0, d6bc29a897fd85e7187dc33c3c974b8879462237, b7a717233ec3ff37385ab1b06816d0ca375f5bb3, 013eb12ce5468f79d58bf859653f4929c5a2bd14, 5c39e37022661f81f79e481240ed9b175dec6513, cb3968152f7d93f53d24b00279a90d5071ddc85a, e89c37b7c2ff465db43c4b9f674867ec4b98aa8b, b9c974380649749320f4a02e33b2e5014e7f1756, 91d24a94a276f5b226b07fe294561482b1d4104d, 5778e56400f7113c2b1355fdbd6b638fa379885f, 808e9ce4e86e79098edea7f00b5b91663b87a5e6, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, 244e08e109f6f4fa0f846977e1aef1e7b6a9e816, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 9c1265fd47df8d825f0ddd6d9ab834002592e38e, 965359b3008ab50dd04e171551220ec0e7f83aba"
6628f9ee35e36cdfdcac8a46cef4dba8d529a83b,Character-LLM: A Trainable Agent for Role-Playing,https://www.semanticscholar.org/paper/6628f9ee35e36cdfdcac8a46cef4dba8d529a83b,Conference,"Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents \textit{memorize} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.",2023,32,"95329799, 2107897400, 2087363104, 2258552414",95329799,Belgrade,3,"1684745, 2217487123, 2061706386",Y,"Preliminary experimental results are reported which demonstrate that ISRLU can perform similar to ELU.;3. Some of the architectural choices (the one derived from ""shortcut problem"") are barely explained or looked into.;The interpolation results of Figure 11 are also underwhelming, though possibly mostly because the reconstructions are in general not great.",Conference on Empirical Methods in Natural Language Processing,23,"llms,agents,models,llm,llm","71a85e735a3686bef8cce3725ae5ba82e2cabb1b, 549e933821fdf7cd0309dacaae99c8284cbfcc24, 78aab73ed574393ab421f25b3a0e3f7343e64748, 75c364909914f17791837ec88090262aa6656d3e, a9cbbef8f4426329d0687025b34287c35bdd8b38, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, 51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df, 208ac1f2ec9bf367a9981fedb6d9ea6aa9889099, 96023195e889fc258e6ff30aa99d250982dfae01, 2ed691a353fa48403d493ab658f5f267a42f0bf1, e75cb933d387ecb184010ff07d0ee43fc1750e2a, 980858461df7c4349f17b427686c5bcbcffbdc04, cf523942d56e90db182c5788845f6502da9a307d, 66d41e0f894dda2c37dd5bacbdd7bfd418e3350f, 6dd3e404aac097f63d42dc19fd08c7ec281f90b4, 9a51df7eaf2457b034c49ca8ca4ca3b81e6c08c0, 537f5e8e4139392cd2d108f32495e5b2b80151ac, 53b0cfa2eaaa9703df02b2ca9c43260c4de5df0a, 4267178106cef2e77284bde309dfaaf9fd46a91b, 94de6bab96ad533169dbc3bf9e6557581e59cb6f, 02a1e8e77f501675945890df45fbdc11726cb0ba, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, addae423490bbe82da4fb2fc265237178686b4e8, a81ba6a07bf7a2ecff871e3362a77404501d0927, 99ae62cca0b15275d9ed1528f345f9dcefe50dd7, 83ea80a9393177ecf84928f4bd1120fb679f180c, 139a0c7a60667979dcb57eae677f75ff3f0b0196, f63e917638553414526a0cc8550de4ad2d83fe7a, 736ef8a32d6c5f76a21d61299300cf796480d507, fafa541419b3756968fe5b3156c6f0257cb29c23, c50a909e20bd07f4aea09dc6dae539b45b406a96, 41d04aa3c25dcfbf1b44ce666c48759e03c216c7"
30cc95639cffca4ffa8c0eafbc502636c0c88fa5,BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions,https://www.semanticscholar.org/paper/30cc95639cffca4ffa8c0eafbc502636c0c88fa5,Conference,"Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process. Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and in undertaking general (not particularly text-rich) VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved 17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME), comparing to our baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence. To demonstrate the broad industry applications enabled by BLIVA, we evaluate the model using a new dataset comprising YouTube thumbnails paired with question-answer sets across 11 diverse categories. For researchers interested in further exploration, our code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.",2023,29,"2231783956, 48615440, 89843190, 1956819, 8157619, 91444480",2231783956,Vilnius,2,"31347453, 5478513",Y,"3. Some of the architectural choices (the one derived from ""shortcut problem"") are barely explained or looked into.;The experimental results are very good and give strong support for the proposed normalization.",AAAI Conference on Artificial Intelligence,23,"bliva,models,embeddings,llm,llm","f156ecbbb9243522275490d698c6825f4d2e01af, a926093ef103c02fd5ef7310e0d41b83d1958ed5, 6068d39e92aef1bb0e1291e9931894c35692a85e, ab606e9d148458f6d54e5d44abefd73b7990f6e0, 69d49a06f09cf934310ccbf3bb2a360fa719272d, 0311ace1d499cadd1cc0c515a625d1d045f60d25, fdc57c18f3b636c3273542327ae540217972558f, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, 60119658af638693f6de23d8466968e60c428ac7, 215fc60307f741b9db059204e41db8bfb879e606, 047286f5b9315a8e8bf56c4fc936e62f21495892, a357f1ff27e184d9a5ef69e665e8ca891032bf71, ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee, e373cbce1a831361a8de9dbc625727169011bcce, 94214d6d922ce095719d488642cbcc75dc52f273, 410fba9f03212257d0881811802e6620e59bc827, 627d7d631fd4e0e2179f82199f014deb7ff0ea0b, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, 9954ec0a95fc0840f7712d7b0c67b242bb536a9e, f0f1627db35b4942e0f83069f20dd0948fc35d28, b781fb7f3725a9d899d3d250b378d729a8a00442, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 41d6b6bd4bf44bf13b9157b603e33360e5e77a01, 23466d271676ae467cbe85bb1993682f3502e840, c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b, 83ea80a9393177ecf84928f4bd1120fb679f180c, f295157f37cfb43cd8d8d2690ea124edc5ea59c2, b6becea767675ea6ee43c78ce747077a5050019c, dcbaf58b16ac7ef947879ea37c021466357b291a, 3b87dafd5a412e25e06761f181ec199ca88a7398, 11be2469ab1d1c508e7b6e14148990741ba87884, b69a35662a2cac38eab22f4481285116bdf8c30e, 6dd3e404aac097f63d42dc19fd08c7ec281f90b4, cc017a62c605a0749e35a1264a46d62e78fb68b7, 10b4b926904ad153f791ec680218e1610747a0c8, 287ba5bf00d96af1596aaf80c178392a9c4fcc28"
5e4597eb21a393b23e473cf66cb5ae8b27cab03e,ExpeL: LLM Agents Are Experiential Learners,https://www.semanticscholar.org/paper/5e4597eb21a393b23e473cf66cb5ae8b27cab03e,Conference,"The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.",2023,41,"2136104377, 46307329, 2232847317, 2036238525, 1679704, 2115218570",2136104377,San Marino,2,"145169163, 2108485135",Y,"Second, it is also unclear what the method to generate train/dev/test data is.;2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing.",AAAI Conference on Artificial Intelligence,23,"agent,tasks,experiences,llm,llm","acc296f981cde8d8c205982fc4422ec35531b769, f7b69b8babfa607f2e8b0371f24cb720a7a827d6, 10d89b13a6309a531c35701d37d3bd76a27a3942, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, 84a36e19f9394f22b34f79756fa9628a795e02ea, 71f3bfea47b96fbab78a3439ee819cd92a33329d, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 733fc094e785724621c46e20db1be69f132ad9df, 02b1607af35b48f0bd716367caf6a7428b969369, c30c0092bf4eb8a44faec3fc60cdd5006276bcdc, dad8965c5a4c0a0ea1eb3837c6a9c3b42597c2ce, e576a2d97950b1f6831f88575dd3f370053f6af7, 56266342b01a4f2ddc28a1e8401dbbad105736a5, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, f476d44a4892e0c8256e50e9075f8dd3d412bfcf, 3a58efcc4558727cc5c131c44923635da4524f33, 14dd50979af27bd2574c8068db11d27028b56afd"
b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283,NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails,https://www.semanticscholar.org/paper/b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283,Conference,"NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable. Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.",2023,20,"2796756, 2137374228, 1602996186, 2258715782, 2258785611",2796756,Oslo,2,"2158995823, 2818166",Y,Why will D answer negatively (or positively) on this example ?;Is the time budget different for each new generated environment?,Conference on Empirical Methods in Natural Language Processing,23,"llm,rails,nemo,guardrails,llm","e248993daede136713e93929816df92b48ccddfb, fa63c3f53413ced7946623889c416e34a28676ea, 1d5adacc5d4d226e76c35bf19018f9e76759f127, baafed5f8968118af04dbbb1cf172f1c10bede25, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 92afbbe41174a545f9da9992e33c9a9592e529aa, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, 208ac1f2ec9bf367a9981fedb6d9ea6aa9889099, a8c1ed061813f832358c1aabf5d171bab80203bf, 0ac1a1ccb16c8441f42ff8de743fc93f0e08c15c, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, 3cfe075af77bf0364e6ddecb3d223960d06e8927, 30f233eecca2239ee1dd754914324092e53f8f19, 8d67b76222d84dcd337b8a2c78f13837070a79ce, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, 53b047e503f4c24602f376a774d653f7ed56c024, 72243bfa618fd6c3c01bb7b4885a4a456b2a6071, d7b820af40a9e2660ef700d39f7b2e27b43435c5, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, 0090023afc66cd2741568599057f4e82b566137c, 8ba8a0d18a06752f5a39996ccf1e914da0941443, 697f2f3598057cd17cff7749d768cae0993c6727, 6a261e1e38506b0e4c113ba29a2d5e5d0709ed26, 43eea2a73997294193228d50f9ff25fc5345664b, ea160adc0d78e54669281b8b145bcd832e648fee, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, 1cff064f815111a71a98afda7aee1867ad617901, 9d6acac70b2d1fdb861a08b00766ef263109cd7f, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, 13c4e5a6122f3fa2663f63e49537091da6532f35, ca05ca5f70616456d0f2e05c4a51cbf8b8d273f7, 447884e7da189102189a156966623335c72199b0, f397b593de771752e7002a954eb531f3ef6a975e, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 75ea299834d6949e89e91d006677343ddab44e49, e75cb933d387ecb184010ff07d0ee43fc1750e2a, 35adeef964fd344288febc7def2780007587724f, 1b0aa15937fdf59103a5213bccf09cff83d0ee3e, 7af07490da518c8ef3cf2ae106071df2c2d0101e, 6068d39e92aef1bb0e1291e9931894c35692a85e, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, 33e332837e91c1048c3ed165cd16bf7607c3bf06, 6aca07154c111f1c8738347d7112cad6b0bf974a, 627d7d631fd4e0e2179f82199f014deb7ff0ea0b, dde4abfd83fb61a794bd6c7d6e1991a67467c7ee, 3ea34401909978d3d3d0c25c8746e02c7d2a7c77, 5c45a5d05ac564adb67811eeb9d41d6460c70135"
66d41e0f894dda2c37dd5bacbdd7bfd418e3350f,VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View,https://www.semanticscholar.org/paper/66d41e0f894dda2c37dd5bacbdd7bfd418e3350f,Conference,"Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation (VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that VELMA is able to successfully follow navigation instructions in Street View with only two in-context examples. We further finetune the LLM agent on a few thousand examples and achieve around 25% relative improvement in task completion over the previous state-of-the-art for two datasets.",2023,17,"47947548, 51439692, 2168285733, 41020222, 3289329, 1682479",47947548,Rome,2,"2115263944, 1720266",Y,"Paper of broad interest for control tasks This is a well written paper, very nice work.;It builds on the previous work of [Finn et al. 2016] that uses a neural network to predict transformation parameters of an affine image transformation for future frame prediction, an idea akin to the Spatial Transformer Network paper of [Jaderberg et al., 2015].",AAAI Conference on Artificial Intelligence,23,"navigation,agent,instructions,llm,llm","5bc511aa30f72720260d792e57537379fb04c395, 5c39e37022661f81f79e481240ed9b175dec6513, d3855b7351c11145e51301e6b686f748ca35c802, e3b94a5f28522e6825aff16ff07d56bd70d26c96, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, cf5dfc4a9f7a82b32640128ca10832eace55880e, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, 6dc4883228c95e8a332320fcc587a0ff33c84d59, 82d2b9d09cc339fdeac05abfb8a31f9c6eace948, 8b28792f8405b737229afb92c99c579b86d8aa98, 92afbbe41174a545f9da9992e33c9a9592e529aa, 9e5d389fcdce4c0a6827ebc804b4e863aa28c14c, 712e32e2da67428ba6c6add1605410e1c3792883, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, bad4c08f03587e38ee960e2aa76e16d722826e7c, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, a9640bac0b45a804d07fc5914feb08af8f2a73f2, e2a85a6766b982ff7c8980e57ca6342d22493827, 97ac11e5a6440eccb70ae7146392ac138c36fa6c, 312b1067d89f598c4c5c0799aa18b48d0926bed8, 7ae2783a9196fb4bc2a610ae812d19722daddce5, be082d70534db088315f2cc5b42c2fdcd58c1b8c, 1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0, 22ebfc211d184ed615729378a43fde175bf14478, be7cb8f79bc018e57467168fc0c7f8ad59bba04f, fa26a6d434450b185e669170e79fd3e1d29716bf, 8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6, ce7499d6862df8269c655220049c3ed20b9b6f5e, be2b0396de9431bae931642516a1d3e4906329f5, d0ab11de3077490c80a08abd0fb8827bac84c454, f90f526b101cb8a0260f5165a3875928c58ae48a, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, 3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b, c0e6cd2ec3bc9eb46c7d45bb708854da3327339e"
cd2f4aaf98bb1e020cff310000c8049d3460c54e,NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark,https://www.semanticscholar.org/paper/cd2f4aaf98bb1e020cff310000c8049d3460c54e,Conference,"In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.",2023,30,"1724648481, 1602998334, 1453724884, 2226458991, 2251043402, 2064112151",1724648481,Belgrade,3,"48469973, 50631038, 6562624",Y,"As such the paper would be a nice contribution to ICLR.;I understand that you are trying to improve the existing results with their optimizer, but this paper also introduces new algorithm.;Finally, Table 1 seems to have some min/max values the wrong way around.",Conference on Empirical Methods in Natural Language Processing,23,"data,contamination,model,llm,llm","3dfa820702b6181c9964931f0a4d47fd298bf429, 57a3fc6d0aaad3c10c793b4e59390ca04c935282, d1206ccabd1980848f14472d6548251c2fab7963, 78b2d392ebb100a220ceab6529d26909b27eaa32, c9b56cb026a38e39bb0228faac57accd6f65e6f7, a3636512a48321baab95c94052de2a0a88460602, fac67bf55456b52ac6e4f280ad953d0250c74ebc, 75ea299834d6949e89e91d006677343ddab44e49, 41d4e093d5f7ed5aae1aaa9eb6c037742e4cf9b1, 079b57837221413bf99ab40999c77c29e280e0c2, d86084808994ac54ef4840ae65295f3c0ec4decd, 846883b7761cb5fe4468d42bf9d328b5d1030175, 011095a0082e5e301f9bf30267b193c1c9e7e370, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, eef23d76e175c0cff8e81ffcb2721c10539c8cbd"
8ee45aeb7c97e3346cc62f216f673b91277ac718,LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,https://www.semanticscholar.org/paper/8ee45aeb7c97e3346cc62f216f673b91277ac718,Conference,"This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. 1",2022,137,"153409139, 2110410087, 2158995823, 2137028350, 2113951006, 1758652",153409139,Andorra,2,"4836115, 2169159066",Y,The generator is a mixture of two Gaussians in one dimension.;The encoder network from NEM can be seen as a recurrent network which takes one latent variable theta_k at time t and some input x to produce the next latent variable theta_k at time t+1.,IEEE International Conference on Computer Vision,22,"agents,tasks,language,llm,llm","665b0c776ff7507c32793f10ce9edf90bc2f674a, 519ffd9744de5638d8c950090f065923e0793a93, 8674494bd7a076286b905912d26d47f7501c4046, 4895c430c7810b45840b58cc9182f12143013a43, 98b9086750f08a21c8778ab986339321e9caf790, ba9b6f805feb62c978d384211f910790643a023e, 375125029b085e70a109491656b69aa01bc2a166, 4d8f0ae904779a50b2e18fec49e51a5661a98d8a, 03aeb4520e760a906393aaf9c1bf4e526483d081, 80d9f0eb47b712988d19cbe29a7bfa63f2a175d0, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 40416ac3bf78583eea37661b1b446e9939245b3e, 7eaac9847257c32afd450017d1348ecda4dcaade, fe44200fed05f9a7c656f2245deded8fd5f5e1e6, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, f4207bcad24fe4eaf3849d6c1fe38c36545b5cb3, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, 19cf7458db4e17c7504eee24ccf961e1dc91435c, f12930cd5f58990badc1a7c5d2749cad004cfb0e, 3ec02e36cc0ba16292b255e9f90c3725521e2ec9, 7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7, af13a92977d4f4dc5b28b13746d86111d42939e8, 046eb47d56beb8069b0098e3d01608f81ebb6849, 53b0cfa2eaaa9703df02b2ca9c43260c4de5df0a, 657fbf29ea0b4904a3e98d1556f9acf38dddae5f, 25761ba4bdc054bfe902fe7c5d6338be6d00d491"
ce157cea880c9ab64de64f11a531202f5348fa05,"""Kelly is a Warm Person, Joseph is a Role Model"": Gender Biases in LLM-Generated Reference Letters",https://www.semanticscholar.org/paper/ce157cea880c9ab64de64f11a531202f5348fa05,Conference,"Large Language Models (LLMs) have recently emerged as an effective tool to assist individuals in writing various types of content, including professional documents such as recommendation letters. Though bringing convenience, this application also introduces unprecedented fairness concerns. Model-generated reference letters might be directly used by users in professional scenarios. If underlying biases exist in these model-constructed letters, using them without scrutinization could lead to direct societal harms, such as sabotaging application success rates for female applicants. In light of this pressing issue, it is imminent and necessary to comprehensively study fairness issues and associated harms in this real-world use case. In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions: (1) biases in language style and (2) biases in lexical content. We further investigate the extent of bias propagation by analyzing the hallucination bias of models, a term that we define to be bias exacerbation in model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs- ChatGPT and Alpaca, we reveal significant gender biases in LLM-generated recommendation letters. Our findings not only warn against using LLMs for this application without scrutinization, but also illuminate the importance of thoroughly studying hidden biases and harms in LLM-generated professional documents.",2023,23,"2165227666, 2258548444, 2261454711, 31099365, 2257127887, 2256996328",2165227666,Paris,2,"1742135, 1695160",Y,"The proposed GGNN approach outperforms (bi-)LSTMs on both tasks.;Otherwise, the perfect latent representation is z=x.",Conference on Empirical Methods in Natural Language Processing,23,"biases,letters,llms,llm,llm","329d31f881a17861eedeef6a9d8fd509cddd2b7c, b7f5973dbf2ef76fcc3aea616a7f72ca79f09020, 2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c, 2c3eef2f17369912e330281d54b535675077e4ca, 5107c83173e43f51d1bdebf6cafda525a7c26bf0, a6bba5ce9867c978210e3d056691b5c1e769b760, 8388f1be26329fa45e5807e968a641ce170ea078, 1cf2e9e198feef3893da2800a7949f6880ddc084, 8c7aee0bbc062d5b4bcd34951b1e002274288206, 013eb12ce5468f79d58bf859653f4929c5a2bd14, 681253389d2cc27103753749f4c7556699d55471, 4fcfe83c05402b5c5fb6e853082e74af6379d7f9, e7a7735104448371dde788542ebfc6af6485ea43, 10aa2be24951e6de76b630482a645d79354c4cde, 787ae2c51cd82b904bb4fb9ccb15266381af5436, 4aec7bcb292cb763341a22fc67a1cfaceb3a2c67, 5471114e37448bea2457b74894b1ecb92bbcfdf6, f4cfc7cbad257f1688772d59f694c16189dba811, eacf9284a39adcd56172665f31fd5a72560bba7a, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, 31f10a6f602bef0306ac37322f84f6163c8a8ecb, 39444c55f07839ac6a0d1839472a982f8fb447bb, d1bb57da8593a2071b3ea8026865352ab3f7206a, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, 456c011594ecacdd24298a161787389ccbe4b88b, e5194ae88d63c7549678b1b73cfdaf7112164272, 10d89b13a6309a531c35701d37d3bd76a27a3942, 8adb47deeef943c2c1bae41f9498a382fb818a16, 3a083d843f891b3574494c385699c21766ce8b7a, db6084fdb3baceddacdc726474722debe1ef7e65, 2374106a32169c07703599ff3f6f4b31e8067b89, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 590b617c08d34bc6caed7e4490c0b22a9c516e86, 322d91190acd8ac8c64598f5126947b0485ba249, f18be38578ee52aa7071c404d42e3d53ae003122, 5b6ec746d309b165f9f9def873a2375b6fb40f3d, 3a74bed911ccf213d9595b2b02a5b1c4ac4dcaf8"
c0aec04ee86c0724d61c976f19590fbe9c615723,Creating Large Language Model Applications Utilizing LangChain: A Primer on Developing LLM Apps Fast,https://www.semanticscholar.org/paper/c0aec04ee86c0724d61c976f19590fbe9c615723,Conference,"This study focuses on the utilization of Large Language Models (LLMs) for the rapid development of applications, with a spotlight on LangChain, an open-source software library. LLMs have been rapidly adopted due to their capabilities in a range of tasks, including essay composition, code writing, explanation, and debugging, with OpenAI’s ChatGPT popularizing their usage among millions ofusers. The crux of the study centers around LangChain, designed to expedite the development of bespoke AI applications using LLMs. LangChain has been widely recognized in the AI community for its ability to seamlessly interact with various data sources and applications. The paper provides an examination of LangChain's core features, including its components and chains, acting as modular abstractions and customizable, use-case-specific pipelines, respectively. Through a series of practical examples, the study elucidates the potential of this framework in fostering the swift development of LLM-based applications.",2023,21,"2113663584, 114751633",2113663584,Vienna,3,"151505981, 35638374, 2059003208",Y,"Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks Summary: The paper proposes Neumman optimizer, which makes some adjustments to the idealized Neumman algorithm to improve performance and stability in training.;Are the four mixture components over the robot parameters updated independently of each other when the parameter-exploring policy gradients updates are applied?;An additional heatmap generator component can be further included in the clustering model.",International Conference on Applied Engineering and Natural Sciences,23,"applications,study,development,llm,llm","287ba5bf00d96af1596aaf80c178392a9c4fcc28, dbabab9bf5955558f73a37644f4bb626106a6d73, 57e6cca1479a4642f867e69b4dee93d14259dc3d, 0bca61986b8edeaf33018d0203b44110f2480110, 02a1e8e77f501675945890df45fbdc11726cb0ba, f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d, 33e332837e91c1048c3ed165cd16bf7607c3bf06, d47a682723f710395454687319bb55635e653105, 45674df7143e43bc589cfabd26dd194c2a7f090d, d6e1e4f0ad898ca6ac37e6e139a77fa3982170d4, cd2f4aaf98bb1e020cff310000c8049d3460c54e, f5ac3ec506a5a01807dd7196c0c52eef008b78cc, fc32074b37a6d9dda535a70f9689022e70508520, 472644c5f4155635cf9e9e37540bfa53c20e7610, b0ee814c7a3eed260c9913861329c9f73e880d00, aa6c2afadd660fe4efbac699f7854e8f6f240c38, ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, 5c39e37022661f81f79e481240ed9b175dec6513, 661d316f4fe9574f9048586bfae2c43243a1d22f, 6a6ad9eb495739f4c80e7c09598720c3d5c5dff7, cf523942d56e90db182c5788845f6502da9a307d, 8c33ca066e2ab615e24c65198c794114436053dd, e32a2519b59d62cff6cb8136ee242dc3754ed57b, a0f303b6e22ef52943355993f57d65938997066a, 3b87dafd5a412e25e06761f181ec199ca88a7398, 244e08e109f6f4fa0f846977e1aef1e7b6a9e816, cd8a9914d50b0ac63315872530274d158d6aff09, 22ebfc211d184ed615729378a43fde175bf14478, 96273b87cd0eb9d1c9a12afae621ce2abdbbab36, 6001895c2fcf69528d01306e9d293d9d2a4cc67b, eadb1e7da375939e25083ae3936c4f4ef1f2a719, c5c4142a01981787a71bf6ebcb791520c458ab5d, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344"
0095acc4f2c3255cf38fdf844003c97858adb418,OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples,https://www.semanticscholar.org/paper/0095acc4f2c3255cf38fdf844003c97858adb418,Conference,"Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors lack robustness against attacks: they degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, a malicious user might attempt to deliberately evade the detectors based on detection results, but this has not been assumed in previous studies. In this paper, we propose OUTFOX, a framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output. In this framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are harder to detect, while the detector uses the adversarially generated essays as examples for in-context learning to learn to detect essays from a strong attacker. Experiments in the domain of student essays show that the proposed detector improves the detection performance on the attacker-generated texts by up to +41.3 points F1-score. Furthermore, the proposed detector shows a state-of-the-art detection performance: up to 96.9 points F1-score, beating existing detectors on non-attacked texts. Finally, the proposed attacker drastically degrades the performance of detectors by up to -57.0 points F1-score, massively outperforming the baseline paraphrasing method for evading detection.",2023,15,"2138646471, 143655216, 1764004",2138646471,Kiev,3,"2165227666, 48455738, 40075749",Y,"The WAGE parameters (i.e., the quantized no. of bits used) are 2-8-8-8, respectively.;This paper proposes a different approach (with could be combined with these methods) based on a new training procedure.;3. The paper wants to find a good trade-off on speed and accuracy.",AAAI Conference on Artificial Intelligence,23,"detectors,texts,detection,llm,llm","11cf88dce827bd67cbfa60400306318022e736d5, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, 1ca6d6682204f0214338f7797bea056444e908bd, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, 0fa554d981809c5eb78956c779f75092c4f6c16b, ce54e3b89a2570035b70885e6901ad4c92ae41c9, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, 155eeede0c070f1f017ba5c9f6cacd7ae0b098aa, 42d3ae61ca296558cba61446bd95b8a6e5b1082d, d93bcf0685c15c45d078eafea565969c04daccd3, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, d617f51833860dc50d202af7f80be71304b2e994, ed9e7821b3e51c7e59183300d6c8cf90c8de0f26, c9645aa4ea31903e02e201b877fd3e1466adff4f, 4e58100b319d74f97ed550a4e5fa32dea8c06fe1, d1bb57da8593a2071b3ea8026865352ab3f7206a, 68ade27b68c05d1b60b8a9d8f8aadbe76900c60c, 0311ace1d499cadd1cc0c515a625d1d045f60d25, 3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b, 8d67b76222d84dcd337b8a2c78f13837070a79ce, 003ef1cd670d01af05afa0d3c72d72228f494432, c665003881c3c35589d1e48da1ee7234b48f2ac8, dd2deed2ce6e110236a1280db765fa02c7488eb1"
a8b995f0da78a79447dfb18c2337972b044f4239,LLM-FP4: 4-Bit Floating-Point Quantized Transformers,https://www.semanticscholar.org/paper/a8b995f0da78a79447dfb18c2337972b044f4239,Conference,"We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One characteristic of FP quantization is that its performance largely depends on the choice of exponent bits and clipping range. In this regard, we construct a strong FP-PTQ baseline by searching for the optimal quantization parameters. Furthermore, we observe a high inter-channel variance and low intra-channel variance pattern in activation distributions, which adds activation quantization difficulty. We recognize this pattern to be consistent across a spectrum of transformer models designed for diverse tasks, such as LLMs, BERT, and Vision Transformer models. To tackle this, we propose per-channel activation quantization and show that these additional scaling factors can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method, for the first time, can quantize both weights and activations in the LLaMA-13B to only 4-bit and achieves an average score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8 lower than the full-precision model, significantly outperforming the previous state-of-the-art by 12.7 points. Code is available at: https://github.com/nbasyl/LLM-FP4.",2023,15,"2220637583, 2109370860, 2261688809, 2261493190, 2256381600",2220637583,Moscow,2,"11531589, 1390140002",Y,"Second, it is also unclear what the method to generate train/dev/test data is.;Figure 2 which is the graphic representation of the model is hard to read.",Conference on Empirical Methods in Natural Language Processing,23,"quantization,weights,models,llm,llm","84725855d10b531eb8cbe54935dda0440c2fc750, 5b34752817bc0d6aa96466dabcbc24a83dd071fe, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, 92afbbe41174a545f9da9992e33c9a9592e529aa, e02f91d625cd32290d4ede0f31284da115844316, df138c7425e787cac2f9d3ab7775c0fb5294a83e, 92930ed3560ea6c86d53cf52158bc793b089054d, 3994334c81478a4b17341eb1f494dbccbb73d999, 9efd70d2c06733704220313fb67720aa45c6362a, 185e7d2a761594451b02ace240356dadad2aef78, 00d1f3423a33f73ca6aee884a58834547475d2f0, dc10a3f4dae2db48148ff6781454ddcc856ae8da, 152877c51df17cdd4a87d19e452c6daecfadf6c3, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, db6084fdb3baceddacdc726474722debe1ef7e65, e30d9b8ce108d982169621b88a5e3fb69fec70e1, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, 28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8, 492c389d560d9db39c758d07e635408d2e0eaf7d, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, d84ed05ab860b75f9e6b28e717abf4bc12da03d7, e57aeb158a38ccf33d2f0f5a8f63f1209497e329, 9583ac53a19cdf0db81fef6eb0b63e66adbe2324, f2bc4057e696f49c326bf8e1588772a16f053754, 9e540662619327a3056d9e40bb58058868f6f805, cb03b665069dad5e895a2c244929ea427f1fb9d1, 8e259f940f007e08207ddb7c3a052f52036d7bf6"
7a4fe2f003241ad97bf1778e527cb0306fa90da2,CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations,https://www.semanticscholar.org/paper/7a4fe2f003241ad97bf1778e527cb0306fa90da2,Conference,"Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.",2023,15,"2149615775, 3144356, 2260029688",2149615775,Stockholm,2,"145385471, 51418452",Y,This seems like one of the most novel findings in the paper and is worth highlighting.;The model is trained with supervision to output the overhead map of the global map.,Conference on Empirical Methods in Natural Language Processing,23,"simulations,llm,caricature,work,llm","6548106035c7208ad498730627874a482734b9ac, f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, 8b28792f8405b737229afb92c99c579b86d8aa98, 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d, 024006d4c2a89f7acacc6e4438d156525b60a98f, f14fc9e399d44463a17cc47a9b339b58f6ef7502, 27beaa5db6c37c611f082855c6385b264874b8f5, be082d70534db088315f2cc5b42c2fdcd58c1b8c, 639bfab64e2f35917d450013e136cb24c7755fad, 27245e65a27bde90b5b0bb25d157bb75a0ad8b5a, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7, 247dec05283a1a521f99253a6cca6a5858cac0d2, 7a1e71cb1310c4a873e7a4e54d1a6dab0553adce"
b61b260de1599e6e89491cad9160898fcd3b34c2,Can LLM Replace Stack Overflow? A Study on Robustness and Reliability of Large Language Model Code Generation,https://www.semanticscholar.org/paper/b61b260de1599e6e89491cad9160898fcd3b34c2,Conference,"Recently, large language models (LLMs) have shown an extraordinary ability to understand natural language and generate programming code. It has been a common practice for software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability, and robustness of the code generation from LLMs have not yet been thoroughly studied. The executable code is not equivalent to reliable and robust code, especially in the context of real-world software development. For example, the misuse of APIs in the generated code could lead to severe problems, such as resource leaks, program crashes, etc. Existing code evaluation benchmarks and datasets focus on crafting small tasks such as programming questions in coding interviews, which, however, deviates from the problem that developers would ask LLM for real-world coding help. To fill the missing piece, in this work, we propose a dataset RobustAPI for evaluating the reliability and robustness of code generated by LLMs. We collect 1208 coding questions from Stack Overflow on 18 representative Java APIs. We summarize the common misuse patterns of these APIs and evaluate them on current popular LLMs. The evaluation results show that even for GPT-4, 62% of the generated code contains API misuses, which would cause unexpected consequences if the code is introduced into real-world software.",2023,15,"2146528797, 2108467971",2146528797,Athens,3,"2066499928, 145233583, 2149798086",Y,"The paper says Deep Voice 2 (Arik et al., 2017a) is only prior work for multi-speaker TTS.;Presumably, there exists some hyperparameters where the model does not achieve 100% test accuracy, in which case, what are the failure modes?;The authors also analyze the generalization error bound of DNN after pruning based on the work of (Sokolic et al., 2017).",AAAI Conference on Artificial Intelligence,23,"code,llms,software,llm,llm","00d1f3423a33f73ca6aee884a58834547475d2f0, 950850e22e42201f152d90dc6f53d53e39d37657, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, 681253389d2cc27103753749f4c7556699d55471, c2a448bb511ebae41a87e69891da8bbf17ddba3d, 033a50c4515b153b6e706018075c333c64981fd7, 4419c5720e30d5ca5158795d4c848125650b8db1, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, 11342d45911ee8a7c9e3a94117ce774ad7036172, 639bfab64e2f35917d450013e136cb24c7755fad, 59c2968fb9672a7152c52127255d8f0784bc2368, 8e259f940f007e08207ddb7c3a052f52036d7bf6, 29409efa04ac99ccf01d2a011d21d5d14e870000, 1cff064f815111a71a98afda7aee1867ad617901, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, f75b70c9d7078724b592ec3e21de705e7b6ff73f, 9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a, 15370f51d666ab8ef17185679553c6a8647b2a15, 83cebf919635504786fc220d569284842b0f0a09, cd2f4aaf98bb1e020cff310000c8049d3460c54e, 41d04aa3c25dcfbf1b44ce666c48759e03c216c7, 98b9086750f08a21c8778ab986339321e9caf790, b6a7226e5f6d618370995eccad68af195ef32da2, 4e746359afd6f81705b875d71cc499b904a320df, 0e33833f5e2e2719edfba1d142eb4d27f96e799f, b4c9c134ad5bd4a037115df65411b4c49abe1322, 72afe82af4c2ca100c36eb35292e85d806527f0a"
9e540662619327a3056d9e40bb58058868f6f805,Prompt Distillation for Efficient LLM-based Recommendation,https://www.semanticscholar.org/paper/9e540662619327a3056d9e40bb58058868f6f805,Conference,"Large language models (LLM) have manifested unparalleled modeling capability on various tasks, e.g., multi-step reasoning, but the input to these models is mostly limited to plain text, which could be very long and contain noisy information. Long text could take long time to process, and thus may not be efficient enough for recommender systems that require immediate response. In LLM-based recommendation models, user and item IDs are usually filled in a template (i.e., discrete prompt) to allow the models to understand a given task, but the models usually need extensive fine-tuning to bridge the user/item IDs and the template words and to unleash the power of LLM for recommendation. To address the problems, we propose to distill the discrete prompt for a specific task to a set of continuous prompt vectors so as to bridge IDs and words and to reduce the inference time. We also design a training strategy with an attempt to improve the efficiency of training these models. Experimental results on three real-world datasets demonstrate the effectiveness of our PrOmpt Distillation (POD) approach on both sequential recommendation and top-N recommendation tasks. Although the training efficiency can be significantly improved, the improvement of inference efficiency is limited. This finding may inspire researchers in the community to further improve the inference efficiency of LLM-based recommendation models.",2023,16,"2151529879, 2260830380, 2146027242",2151529879,San Marino,3,"1403759150, 2235966, 51026953",Y,"Results are good, some unclear explanation This paper proposes an end-to-end trainable attention module, which takes as input the 2D feature vector map and outputs a 2D matrix of scores for each map.;[1] Wang, Weiran, Honglak Lee, and Karen Livescu.;However, what is shown in that paper is that the sequence of loss function values converges, which does not imply that the sequence of weight estimates also converges, because of the presence of a non-convex constraint ($b_j^t \in Q^{n_l}$).",International Conference on Information and Knowledge Management,32,"models,recommendation,efficiency,llm,llm","1b0aa15937fdf59103a5213bccf09cff83d0ee3e, 6fed828456964d29517f6caf31b700d8aec82153, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 94deb62af3054c49e7d80bd7eb3ed5efe990fc0b, f91dbd39d4c742ba675e447b04a0b0c70b33e836, d422df8bff4e677a3077635db116679d25142bfc, 712e32e2da67428ba6c6add1605410e1c3792883, c48e0bd0f36c25ab83befbc7b7da369b75fd25f5, 7d873a9c49d3864709aa762f8740edcdbd7369c5, bfad52fc64ca0169644b6e7e0ea9a46470d51709, 10ea29fda06bdbe56f591909d89f3194b452ac91, a56bf7ee9a56d8f84079684339a953c2df9ce76b, 29409efa04ac99ccf01d2a011d21d5d14e870000, d57f11ed40c3cdcbb36cb758191db4f2c9372965, 58ed1fbaabe027345f7bb3a6312d41c5aac63e22, 9181b0d801dfcd7723a3ede201f0543078e2c149, 82d2b9d09cc339fdeac05abfb8a31f9c6eace948, d8348b802c9133d9e396d4ad809b020d5be42863, 2afa490dde7a8c582d889530c7f8b042fef6a8b7, 10aa2be24951e6de76b630482a645d79354c4cde, 661ccdb41fe977d47273e586389cacc1489f3286, 42d3ae61ca296558cba61446bd95b8a6e5b1082d, 4b06c7e29280b1c6bc05c9df39023b48fef02c93, 752604994a7ca548ff2954114fc61a501d857b1c, c0bcd7dc9426a70af15f5ad63b4af92ea4dcbd4d, d8a5474f450330ad25c1e22f27e88f3630cb840d"
91b2b47cabd800ef658b65bfe1f52b7293a740c3,LLM-powered Data Augmentation for Enhanced Crosslingual Performance,https://www.semanticscholar.org/paper/91b2b47cabd800ef658b65bfe1f52b7293a740c3,Conference,"This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency.",2023,27,"2161240241, 143990839, 8129718",2161240241,Oslo,3,"2185503028, 1740261, 2061202877",Y,"This manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum, which actually has been shown by existing works like Kawaguchi (2016) with some assumptions.;There's clear value in having good inductive biases (e.g. expressed in the form of the discriminator architecture) when defining divergences for practical applications.;Algorithm 1 does not make clear the relationship between the model learned in step 2 and the algorithms in steps 4 to 6.",Conference on Empirical Methods in Natural Language Processing,23,"data,languages,llms,llm,llm","31f10a6f602bef0306ac37322f84f6163c8a8ecb, e449b9b3fe04fe260731a3c74d2123bf6eaadf5b, bcc9245ebf57072c07c5a9d4eff3aaaac36cb7c6, 657fbf29ea0b4904a3e98d1556f9acf38dddae5f, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632, 5107c83173e43f51d1bdebf6cafda525a7c26bf0, 1c2efb418f79b5d29913e014a1dfd78865221c39, 2dafea864f74a477414c3b71b742f7997e216102, ce9ca56036307217ea565644d3d3bd74b879e045, 0311ace1d499cadd1cc0c515a625d1d045f60d25, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 33ce8103b129149eb78ca2fa48538e25c9242c08, 92912dd895c360f01a6be9c9f6d207642139525e, 329d31f881a17861eedeef6a9d8fd509cddd2b7c, 8d0f755dea90f35f4b126a01fa3cce96b3bdd344, b0ee814c7a3eed260c9913861329c9f73e880d00, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 1a37223175138bc1aa53b425ea2fdd0b382405a5, c292e473b3825eeb9db03c70b2e1c033aea190d5, 82663577cf1d08235bb56ad648c9dad36343ccfb, c9f320789e98d2c7a798a9705e26dbe317677966, d280cf82a6144b3e168840802b1a8a14d4eaccb9, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, 9e66ae24a541255c2d931184498ee116ce81478a, ddc6e677715c03fe574319d3f80a3e1577bdbdd3, 079b57837221413bf99ab40999c77c29e280e0c2, 53b0cfa2eaaa9703df02b2ca9c43260c4de5df0a, 34ca47eed139a7f0694611528f75debc43385518, 263a58f4fd32caca1dad2351af4d711aec451fe6, 647c4a9331e01e31a4350361d3460f0397fe694f, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, 557ee73ff4e87ca0cb1b6c78af0730a2d94b710f, 77e6c9917536949a82e5ca02c4882b69ee8a4fd6, 787ae2c51cd82b904bb4fb9ccb15266381af5436, fee8f63972906214b77f16cfeca0b93ee8f36ba2, e576a2d97950b1f6831f88575dd3f370053f6af7, 780c725848aac1118d00c8bb306719ec803369cd, b09139c153bac8893e8faea2b3a59159234caadc, 709af143f78bc62413c50ea1a7ee75b0702c4f59, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, a8d76d84408c1fe6b1543084e6cec3dfc4ede429"
5645502d73c6907f1671923638773152e55bfb00,TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks,https://www.semanticscholar.org/paper/5645502d73c6907f1671923638773152e55bfb00,Conference,"While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw more accurate conclusions about LLMs' performance on a specific complex task.",2023,17,"2692077, 2185503028",2692077,San Marino,3,"144906624, 143666627, 3001926",Y,It would be interesting to compare to a baseline where the control systems are allowed to adapt to the individual design parameters.;Could you explain how classes are predicted given a test problem?;I could then identify the influential dimension(s) by taking the largest absolute values in the gradient vector.,Conference on Empirical Methods in Natural Language Processing,23,"llms,studies,prompts,llm,llm","7998468d99ab07bb982294d1c9b53a3bf3934fa6, 929305892d4ddae575a0fc23227a8139f7681632, 353c88c231ce156d604e074af276422422fc73f7, e576a2d97950b1f6831f88575dd3f370053f6af7, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, 7eaac9847257c32afd450017d1348ecda4dcaade, d9d325ca670a1aa215e3e39023f8abf17dae7584, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, 71854ff4306cf65c3c2161f7be2d0346275f72d5, 7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, 1a4c6856292b8c64d19a812a77f0aa6fd47cb96c, 733fc094e785724621c46e20db1be69f132ad9df, 10aa2be24951e6de76b630482a645d79354c4cde, e4788ee4f5e90c6f42cedc5116acd2d6475c3180, 3337e4b2e63e5e99171f9da9d161771f5810c27a, 1d174f0e3c391368d0f3384a144a6c7487f2a143, b4c9c134ad5bd4a037115df65411b4c49abe1322, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, bb5d26da72bfe7030dbc6650b686b210ae661f2c, 624b2f14be4287d6a400cdf88a6f911b434b182e, 52a6695ae1c08cc29baf764dedb5831c7a954214, fe2492b7b8cf6d1d10b7ea38e0f7f853bd679d52"
f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6,Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,https://www.semanticscholar.org/paper/f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6,Conference,"Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",2015,7451,"2681954, 1744700",2681954,Sofia,2,"2243374631, 2171114745",Y,"Intriguing two phase RL approach for learning neural controllers for discrete programs This paper presents a reinforcement learning based approach to learn context-free parsers from pairs of input programs and their corresponding parse trees.;They then evaluate several existing network architectures on these tasks, and show that results are not as impressive as others might have assumed they would be.",International Conference on Machine Learning,15,"uncertainty,learning,tools,deep learning,deep learning","b80e2af7ffa85fe2f02ea8390e4915d55c19d199, 3994334c81478a4b17341eb1f494dbccbb73d999, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 0a92bc2dc8a216e6aced83edc0358241066833df, 3813b88a4ec3c63919df47e9694b577f4691f7e5, 29ddc1f43f28af7c846515e32cc167bc66886d0c, e32a2519b59d62cff6cb8136ee242dc3754ed57b, a2ec47b9bcc95d2456a8a42199233e5d9129ef18, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, 42d3ae61ca296558cba61446bd95b8a6e5b1082d, 25adff0bb9691b46fee5c0591300d6f3ccf117ab, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, 3c68025d95970a9b9aa1b742a678704cd09d2bf4, e5d720767b7a539bb2edaa98eaf572a4506a79c6, 8ab93bee04cd5bdbd96002b2e325d02f61ba695a, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, 34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61, e3052ebca5eeae6a8a73e44517903d39746f5f3a, 0f38b3d717e0fcc6eacc9c6e78b252227440e04e, ce2d5b5856bb6c9ab5c2390eb8b180c75a162055, 3ea34401909978d3d3d0c25c8746e02c7d2a7c77, 7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca"
e9a986c8ff6c2f381d026fe014f6aaa865f34da7,Deep Learning with Differential Privacy,https://www.semanticscholar.org/paper/e9a986c8ff6c2f381d026fe014f6aaa865f34da7,Conference,"Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",2016,4616,"2057642721, 1396184193, 153440022, 145057514, 145591745, 35210462, 2152832173",2057642721,Madrid,2,"1710223, 2138715050",Y,"Secondly, the DDT’s leaves are parametrized with the encoder distribution q(z|x), and thus gradient information flows back through the DDT into the posterior approximations in order to make them more discriminative.;To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER.",Conference on Computer and Communications Security,16,"privacy,techniques,networks,deep learning,deep learning","8b417c2be7a7707f372049fb1193f0d42f799562, d8a5474f450330ad25c1e22f27e88f3630cb840d, 046eb47d56beb8069b0098e3d01608f81ebb6849, a281d563261c738f13b9e58a525e7e265a619c93, 82973c5f56681190a0dbb4c4449ed60d5f805135, d1206ccabd1980848f14472d6548251c2fab7963, 25adff0bb9691b46fee5c0591300d6f3ccf117ab, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, 9d6acac70b2d1fdb861a08b00766ef263109cd7f, 72afe82af4c2ca100c36eb35292e85d806527f0a, 8ee45aeb7c97e3346cc62f216f673b91277ac718, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, ba687027ed6012f613e1f9a9cefe7683bb192934, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, d8348b802c9133d9e396d4ad809b020d5be42863, 993df7df129f8d18816877d69923d7df7b347d85, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, 2bc9d6830610e18f110aa7984f7c0271d6b17eeb, 33e332837e91c1048c3ed165cd16bf7607c3bf06, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, baafed5f8968118af04dbbb1cf172f1c10bede25, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, 0a71dd8bec060195e14eb9d0a7abbc08d960d4d5, c0aec04ee86c0724d61c976f19590fbe9c615723, 6628f9ee35e36cdfdcac8a46cef4dba8d529a83b, 3789eb72c32ecf5e33442570358dd786dd67c8a2, 60caa5b3d066e13feac496fd0736e976970eb09f, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, cde39ce861e4c7514ee07fd91b6b8aac50cbf01b, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, 9fcdbfdf28245010c875ce85502351fe05c04b49, a8b995f0da78a79447dfb18c2337972b044f4239, 4aec7bcb292cb763341a22fc67a1cfaceb3a2c67, dca4d9abbc82e57dfa52f932e893d467a63e0682, 697f2f3598057cd17cff7749d768cae0993c6727, 5406e153957dd7a165264da6e6e5d81251997404, 8d0f755dea90f35f4b126a01fa3cce96b3bdd344"
5b6ec746d309b165f9f9def873a2375b6fb40f3d,Xception: Deep Learning with Depthwise Separable Convolutions,https://www.semanticscholar.org/paper/5b6ec746d309b165f9f9def873a2375b6fb40f3d,Conference,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",2016,11616,1565641737,1565641737,Vaduz,2,"1746807, 1904203865",Y,"Are the test environments sufficiently different from the training ones?;In the first paragraph of Section 4.5, I disagree with the sentence, ""Similar observations can be made for the other language pairs we considered.""",Computer Vision and Pattern Recognition,16,"inception,convolution,depthwise,deep learning,deep learning","63316bb5b88d362051c048e864c3ae5d97a26d30, 0a829289a16ae48837cc2905635435db98bacc76, 9b3e8d202488dc29e601fc471a25a2af9002659e, 003ef1cd670d01af05afa0d3c72d72228f494432, b09139c153bac8893e8faea2b3a59159234caadc, fc77048474ccd34c6507701591c2e6ab3ca647ef, e2e8ea9bcdb83deb2787ce89db1b51f7ccb1bd1e, eb9e0da8b7170e3ca4364f2f9010599c2d2556f1, d280cf82a6144b3e168840802b1a8a14d4eaccb9, 661d316f4fe9574f9048586bfae2c43243a1d22f, a453ce8a3de86a170c79a1082ef358c3adf4e612, d9b34c6b616f75485856794478bfbeab1ea93b81, 3dfa820702b6181c9964931f0a4d47fd298bf429, b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, 8e259f940f007e08207ddb7c3a052f52036d7bf6, b7034546bee38ba13d3b312fce893a22e33ce4dd, 8d942a3b52e2ad16ff8e5970be59591970d89fae, 156609022dd6258c60238859622da0a1683bd062, d1efcef213c433445be56d7479eb47d972b3ee79, 8ec5896b4490c6e127d1718ffc36a3439d84cb81, 2e7f532796eed2847d4c19e3cff03756049e81b4, d86084808994ac54ef4840ae65295f3c0ec4decd, 32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, c07802ed8a25998e9bd44ee1ddbcc63b7eb34060, c6879e43828b293567f5e2da039d23845189d6a7, ab06951251e0abfdb866694f9a23a79c72784317, bcc9245ebf57072c07c5a9d4eff3aaaac36cb7c6, 03aeb4520e760a906393aaf9c1bf4e526483d081"
d997beefc0922d97202789d2ac307c55c2c52fba,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,https://www.semanticscholar.org/paper/d997beefc0922d97202789d2ac307c55c2c52fba,Conference,"Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",2016,11051,"144329939, 144914140, 2216377, 51352814",144329939,Copenhagen,2,"1384224631, 32244429",Y,"And the results of figure 9, showing poor reconstructions when changing the class representation essentially demonstrates that the method isn't able to factorize class and style successfully.;The rollouts are then fed into the agent's policy / value function.",Computer Vision and Pattern Recognition,16,"network,data,point,deep learning,deep learning","322d91190acd8ac8c64598f5126947b0485ba249, 1d7531db9272f7838e33616075e1e64532fd013a, 933baeec555352784848a93284c9dd0e79477759, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, 51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 78df3ba26593620ab689fe5a97b7e739434a053b, 96b51d940653710f9d099d89ade86b44fa9bdd8a, 3789eb72c32ecf5e33442570358dd786dd67c8a2, 916455d97cd792c2eb5b00663689592e25cbc8d8, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 1562390dd212516cd857009cbd4f857a902d1f3d, d916776e0c6a04b0def4c22257c188776c2edab2, 00bbc94806ca0821a9c82d8aedf16f0e6263b89f, fa75a55760e6ea49b39b83cb85c99a22e1088254, f2bc4057e696f49c326bf8e1588772a16f053754, bcc9245ebf57072c07c5a9d4eff3aaaac36cb7c6, 74bc39003e65119eaa6ba339a61b45b417a638b7, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, ad10ef93675513a68b93d54f3a461160b53318a3, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, 5c45a5d05ac564adb67811eeb9d41d6460c70135"
ff7bcaa4556cb13fc7bf03e477172493546172cd,What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?,https://www.semanticscholar.org/paper/ff7bcaa4556cb13fc7bf03e477172493546172cd,Conference,"There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.",2017,3815,"47645184, 2681954",47645184,Lisbon,3,"2327080, 10707709, 2548384",Y,"The idea of enforcing information isolation is brilliant.;But, it would be good if this can be supported with real life examples.;I do not know whether using a centralized network where each agent has a window of observations is a novel algorithm.",Neural Information Processing Systems,17,"uncertainty,model,tasks,deep learning,deep learning","4afa7d8e2de43b0b67366b1bce8768f5a246d153, 590b617c08d34bc6caed7e4490c0b22a9c516e86, b6593b5ad6b0e00232ee88cf2bc578645c1299a0, 9b529fe170823f95509585d5aa39fa01a43558fd, d1a6b3a5efde3783b53f822dc8dd00aaac934b95, 9ea0757c750ab1222a7442d3485a74d1c526b04c, a453ce8a3de86a170c79a1082ef358c3adf4e612, 5b34752817bc0d6aa96466dabcbc24a83dd071fe, 81c02f123b3ef09cf1a8e5a1332451f0d46663fa, 1f8a23697562b001082b147779b5eaefd3513d0a, b2f8df88de24fe0ef74bda0eec1a0c3e7329b9f9, f4cfc7cbad257f1688772d59f694c16189dba811, d2a505586c0da20752b98f63c7760b6a5c41e28d, 4be7d1524edb0137599a5cc95f72844b85a52fe1, 742747c7a453b293352b772d0d99541c96a351c3, 9954ec0a95fc0840f7712d7b0c67b242bb536a9e, 9e5d389fcdce4c0a6827ebc804b4e863aa28c14c, ead6121fbc787d508dc6a6d7106f72bf0d647d03, b7f5973dbf2ef76fcc3aea616a7f72ca79f09020, e70ddfb04969b663ef8cd711a70a0acf563d6e5f, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, e75cb933d387ecb184010ff07d0ee43fc1750e2a"
8674494bd7a076286b905912d26d47f7501c4046,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,https://www.semanticscholar.org/paper/8674494bd7a076286b905912d26d47f7501c4046,Conference,"Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",2017,8165,"144329939, 47782132, 144914140, 51352814",144329939,Bucharest,2,"144418438, 1390195074",Y,"Experiments conducted on image classification and weakly supervised segmentation show the effectiveness of the proposed method.;The problem set-up of unpaired summarization is not particularly compelling, since summaries are typically found paired with their original documents.",Neural Information Processing Systems,17,"point,pointnet,network,deep learning,deep learning","bf07f2927dca481653b8c60b2dc982fe4a7dfd4e, 01f0f5205d03870f172ae8f04e33356d5a0af221, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, d57f11ed40c3cdcbb36cb758191db4f2c9372965, bcc82ce554942880814243fc8c08a88b9d2aad09, 84a36e19f9394f22b34f79756fa9628a795e02ea, 6dd3e404aac097f63d42dc19fd08c7ec281f90b4, 19b93280f17696a4ddfa2c75490a50ab107addf2, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632, 8b7b2e88207fc2dd849f5d83c9b6e890f0174abc, e3b94a5f28522e6825aff16ff07d56bd70d26c96, df138c7425e787cac2f9d3ab7775c0fb5294a83e, c30c0092bf4eb8a44faec3fc60cdd5006276bcdc, 31d7d7b9c7b776c639316027e6ae5f2ff2673da2, cb3968152f7d93f53d24b00279a90d5071ddc85a, 91e611c3e8705002438fb4439733e47ddec85b5d, 0d065e8688c38bb0148203a1738f47184a5b58d3, 0427110f0e79f41e69a8eb00a3ec8868bac26a4f, 44786a2c2a8ba8cf5c74a8fb10098c220e924c56"
811df72e210e20de99719539505da54762a11c6d,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d,Conference,"Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",2018,5850,"2587648, 35499972, 1689992, 1736651",2587648,Lisbon,3,"48907594, 2133037029, 35367497",Y,"As noted above, it is not clear what is the significance of this combination or how does it improve performance.;Also, that the ""style"" representation contain less (and I'd say slightly less, in Figure 7 b and d, we still see a lot of same class nearest neighbors) is not exactly a surprising result.;The proposed GGNN approach outperforms (bi-)LSTMs on both tasks.",International Conference on Machine Learning,18,"methods,rl,framework,deep learning,deep learning","332e0eab5fba8e6940f3e481f542a99ac17b9717, 1a37223175138bc1aa53b425ea2fdd0b382405a5, e2e8ea9bcdb83deb2787ce89db1b51f7ccb1bd1e, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, da9b8b4073e6ad44b3da66e1e117cb1ddbf8836d, 0084f3cb0a1754272151c5268a783f24bf5676a0, 34ca47eed139a7f0694611528f75debc43385518, 9b54941de1e21826ecc28b32730ac3f69991ede4, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, 8b417c2be7a7707f372049fb1193f0d42f799562, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, 7669fca7fbc071b4ca78b4c326ff8f3a4b6ae616, 410fba9f03212257d0881811802e6620e59bc827, 10b4b926904ad153f791ec680218e1610747a0c8, b9c974380649749320f4a02e33b2e5014e7f1756, 3813b88a4ec3c63919df47e9694b577f4691f7e5, 5030702fea15d66a73fc997325431f1d7945ad9a, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, d2a505586c0da20752b98f63c7760b6a5c41e28d, b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8, 4087e84fc695bb6433d0104ee94f9d7e9f4b7da5, c6879e43828b293567f5e2da039d23845189d6a7, 1c748f86182a62d44d5b44316db510f8d833e19f, f6dab84c2c00ab92d8ee9d9359d7e530512114f9, ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8, 41d04aa3c25dcfbf1b44ce666c48759e03c216c7, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, 808e9ce4e86e79098edea7f00b5b91663b87a5e6, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, 4be9368abc2474d6fd38639e523cf03af1873fd9, 63316bb5b88d362051c048e864c3ae5d97a26d30, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, a0a79dad89857a96f8f71b14238e5237cbfc4787, 78e40584f0d149bf6f98beb5561b7b83cb68e1b1, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, 3a58efcc4558727cc5c131c44923635da4524f33, 02b1607af35b48f0bd716367caf6a7428b969369, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, 07cca761749bfe21c2d096ff60f32b574d5c84c4, 84a36e19f9394f22b34f79756fa9628a795e02ea"
c889d6f98e6d79b89c3a6adf8a921f88fa6ba518,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518,Conference,"We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",2017,9447,"46881670, 1689992, 1736651",46881670,Moscow,2,"4373076, 52257721",Y,"To see why these models are different, if it is the model of (2), and we look at only distinct words, the diagonal of the matrix P[v_i,v_i] does not really make sense and certainly will not follow Equation (7).;— Can the authors provide training time comparison of their model and other/baseline models?",International Conference on Machine Learning,17,"model,learning,metalearning,deep learning,deep learning","c0aec04ee86c0724d61c976f19590fbe9c615723, 5371896313ac227eb819038dd55f213cb42b99e2, 448959eff044f02040ded5afd483b7c4e811b0ac, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, db528269ef800727245c0fcb35b692d29c1ccdc9, 742747c7a453b293352b772d0d99541c96a351c3, 3cfe075af77bf0364e6ddecb3d223960d06e8927, ef35da3a3c471a6a15c7a3b09586483eb50cbef0, 834cdfca7cc041a6fa0db3da5493c6754bea845b, cbad0923db89f23febcbd6192ff4149289ff2ad9, 6dcb1cd576b0e54b900f45a178efe271c383de04, aee3d7f98b966240178ef420724c840f9b61deb3, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 66d41e0f894dda2c37dd5bacbdd7bfd418e3350f, c84389369720dcd2f004c48e58fbac2c45c8f092, 6ff909c6fe089fc8ebfc64eca0f0c3cc34ba277f, 371a343457a4fbff00000bf4faa29b2b2f85744c, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, a80e26e6365b215715c182d19a9aa8bb876ac768, 453fdfeefd6498a65be339d7e8722f6f3288907e, 03532123ccffae8d411264320e8a5ae2b6eddea0, 80d9f0eb47b712988d19cbe29a7bfa63f2a175d0, b09139c153bac8893e8faea2b3a59159234caadc, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, fa26a6d434450b185e669170e79fd3e1d29716bf, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, e2f3aa4ecc487fa496d1a51e875e747d4b7e6001, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, 8799ec4bdf98bc313aa8c41a706a385e026d1f88, 818de553ecd306735971aba04bbfc29d17457084, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, 5406e153957dd7a165264da6e6e5d81251997404, 52a6695ae1c08cc29baf764dedb5831c7a954214, e30d9b8ce108d982169621b88a5e3fb69fec70e1, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 64c4c6cb66457c79f89ba4f2529b57bfce19ced4, 649c3497e3b34b15a5011259fcb837cf6c1ac04a"
69e76e16740ed69f4dc55361a3d319ac2f1293dd,Asynchronous Methods for Deep Reinforcement Learning,https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd,Conference,"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",2016,7633,"3255983, 36045539, 153583218, 1753223, 2542999, 3367786, 145824029, 2645384",3255983,Luxembourg,2,"2267664, 1718134",Y,"Interesting paper, would like to see more experiments The paper proposed a new activation function that tries to alleviate the use of  other form of normalization methods for RNNs.;“ Would be good to see how this affects results and convergence speed.",International Conference on Machine Learning,16,"reinforcement,learning,network,deep learning,deep learning","00d1f3423a33f73ca6aee884a58834547475d2f0, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, 980858461df7c4349f17b427686c5bcbcffbdc04, b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8, d2a5dcecd2ffdf03473df1688091f08fadb114a3, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 8388f1be26329fa45e5807e968a641ce170ea078, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, 8f2cf30f9c825d8ef7d622601dbd525ace95e025, 0084f3cb0a1754272151c5268a783f24bf5676a0, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, f72d3f58ff73353978e224af348448b34d27cf7b, 5371896313ac227eb819038dd55f213cb42b99e2, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, f3d594544126e202dbd81c186ca3ce448af5255c, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, f9367342405a73ab8d6de704a149babfc0edb5fe, 746a81aa26d3ebfb81acfd6af958d6a21603cd21, facd5f5deb152229ceb1803434d8690a09ab4129, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, 8b417c2be7a7707f372049fb1193f0d42f799562, 7a1e71cb1310c4a873e7a4e54d1a6dab0553adce, 52a6695ae1c08cc29baf764dedb5831c7a954214, ce7499d6862df8269c655220049c3ed20b9b6f5e, 43eea2a73997294193228d50f9ff25fc5345664b, c889d6f98e6d79b89c3a6adf8a921f88fa6ba518, 9e3816be8cf4821d74e258de10ee471382936a30"
2c03df8b48bf3fa39054345bafabfeff15bfd11d,Deep Residual Learning for Image Recognition,https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d,Conference,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",2015,157545,"39353098, 1771551, 3080683",39353098,Amsterdam,2,"3589089, 2323922",Y,Figure #s are missing off several figures.;Simulation results with MuJoCo physics simulator show that this simple trick reduces the amount of needed data by an order of magnitude.,Computer Vision and Pattern Recognition,15,"nets,networks,layers,deep learning,deep learning","92912dd895c360f01a6be9c9f6d207642139525e, d8a5474f450330ad25c1e22f27e88f3630cb840d, 8f2cf30f9c825d8ef7d622601dbd525ace95e025, 2af8907d4a974ae41044581f5e5d67317cb08568, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, d84ed05ab860b75f9e6b28e717abf4bc12da03d7, 8d67b76222d84dcd337b8a2c78f13837070a79ce, 66d41e0f894dda2c37dd5bacbdd7bfd418e3350f, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 1e1cf81a1113482be3f0c280db994a832cb9426a, 785a1c77fabd1ad870a9dbc98d235d2fcbdbaa6f, 3994334c81478a4b17341eb1f494dbccbb73d999, 41c93960a066876d5e4f1dacaef75cd8daa2791f, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 9a0965beef113cc37491004b1848149e00300561, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, 1452b25a7680bbb2c66dd7dfca6704292405da92, 3cfe075af77bf0364e6ddecb3d223960d06e8927, d86084808994ac54ef4840ae65295f3c0ec4decd, 9712624bb61abb0da989514cae558cfab61bb9d2, 1a37223175138bc1aa53b425ea2fdd0b382405a5, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, e8b30ebe3351680c3b039555ae0a8d0865ad829b, 8ee45aeb7c97e3346cc62f216f673b91277ac718, c48e0bd0f36c25ab83befbc7b7da369b75fd25f5, 647c4a9331e01e31a4350361d3460f0397fe694f, b69a35662a2cac38eab22f4481285116bdf8c30e, e57aeb158a38ccf33d2f0f5a8f63f1209497e329, 3774fb4b6aa11d88c562ee5702c66cae5881af0d, ff75865cde62592d068b2afd055c57c81d77158b, 7676c02ea839ff1ceb6e5e1427c42bc45e169bde, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, 3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b, cbc1e8bbfe98f94c0d13d111b824cf603b62712c"
c468bbde6a22d961829e1970e6ad5795e05418d1,The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,https://www.semanticscholar.org/paper/c468bbde6a22d961829e1970e6ad5795e05418d1,Conference,"While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called ""perceptual losses""? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",2018,7179,"2844849, 2094770, 1763086, 2177801, 39231399",2844849,Warsaw,2,"145683384, 2231240312",Y,"The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models.;For the denoising comparison, how do the results compare to those obtained if you simulate a Markov Chain (sample latent state conditioned on noisy image, sample latent state, sample denoised observation, repeat using denoised observation) using a VAE?",2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,18,"features,similarity,metrics,deep learning,deep learning","1051abf1e3dae90241ad15b3f98f2e41197ee611, 150f95f9c73820e0a0fa1546140e9f2bdfd25954, a13149a80855412d970d0de2b41c611f4cf7e1da, e576a2d97950b1f6831f88575dd3f370053f6af7, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, b0c34618ffd1154f35863e2ce7250ac6b6f2c424, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 2e7f532796eed2847d4c19e3cff03756049e81b4, d9b5194f3f959eda2e95df6a340254f52ced46f4, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, 1109d62ebd2b29a7dc148bc30dd6cfc803a63dec, 819167ace2f0caae7745d2f25a803979be5fbfae, 57f5bf937f7393b691428747a9078d3124e6bcce, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, 98b9086750f08a21c8778ab986339321e9caf790, c665003881c3c35589d1e48da1ee7234b48f2ac8, dbabab9bf5955558f73a37644f4bb626106a6d73, 69e76e16740ed69f4dc55361a3d319ac2f1293dd, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, adc180e1fe404b650fca3bb7970e43bdce34a611, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, 70ce56e9a2181489d59f7170dc01fe8ba310a8e5, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, 08764019e9762da527253b37b0ff39c46a4206b7, 5d8e450c8c7d9faa8222f1d0eaf34b01755c9eef, f8e57fa370fe10147aa22714e08409fc1b7dae4b, 89a30b5dab02c9c390a632acad481fa602859272, 957f5b1e7ca48891c2e279aefbfa0f04d989c21e, cb03b665069dad5e895a2c244929ea427f1fb9d1, bc00ff34ec7772080c7039b17f7069a2f7df0889, eef23d76e175c0cff8e81ffcb2721c10539c8cbd, 5cb8f417d171ae329adf446820bd32d8b49d8c04, 1c748f86182a62d44d5b44316db510f8d833e19f, 0574a3abc98f0e6bd035a4ff4ea107cfba45d3d4, 4b9184937da308914b9e13c43bfd75845eaf910b, bdb68c5e2369633b20e733774ac66eb4600c34d1, 34ca47eed139a7f0694611528f75debc43385518, d2a609ffb814442d0728aef9f6616f9cd775face, 0601e9e434b30320c316c76228b97c093fa98ad6, a1d36749b89e46a8eaadf8ba40788741c192fb1e, 10cf0045bc0f58aa3699e4451f65b12a08019c5c"
3b9732bb07dc99bde5e1f9f75251c6ea5039373e,Deep Reinforcement Learning with Double Q-Learning,https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e,Conference,"
 
 The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.
 
",2015,6104,"7634925, 35099444, 145824029",7634925,Sofia,3,"1764004, 3437933, 34905515",Y,"Unsurprisingly (since the EEN noise does not seem to be conditioned on the input), the baseline deterministic model performs quite well.;3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling.;The “obverter” technique is quite interesting since it incorporates the concept from the theory of mind which is similar to human alignment or AGI approach.",AAAI Conference on Artificial Intelligence,15,"algorithm,overestimations,qlearning,deep learning,deep learning","11be2469ab1d1c508e7b6e14148990741ba87884, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, d7b820af40a9e2660ef700d39f7b2e27b43435c5, f18be38578ee52aa7071c404d42e3d53ae003122, 9f5b82d9915d0752957602224c5056be7e749c83, 8adb47deeef943c2c1bae41f9498a382fb818a16, b69a35662a2cac38eab22f4481285116bdf8c30e, 634fe61f3ab6692ea4733989a1c76e793bb8b69e, 0df5a4f9cc8a244715fe9968732497d2ac2a7cd1, a357f1ff27e184d9a5ef69e665e8ca891032bf71, 9817bf0f78047452761e950c02a1a56f59a1e593, df7336844a31165db0ae08f1cd0f560c9e3faeea, 72243bfa618fd6c3c01bb7b4885a4a456b2a6071, 0daa1d31d6949f8804089d8a1c11c4560422ad39, 4afa7d8e2de43b0b67366b1bce8768f5a246d153, f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 8bba999de25bfb288b3f7f88e1d907aab02638b6, d6bc29a897fd85e7187dc33c3c974b8879462237, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 7ae2783a9196fb4bc2a610ae812d19722daddce5, 22cba8f244258e0bba7ff4bb70c4e5b5ac3e2382, 2e965b5d97c2d6fb4af284307735be39283792ba, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, f11cba099b8cf14815f7b3d85f55ecfddbf9f04d, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 9eea59c34f139f3d2153226c8cf026e975622074, ce212cb873a54e5716da53a66b10298ac013008a, 033a50c4515b153b6e706018075c333c64981fd7, e2a85a6766b982ff7c8980e57ca6342d22493827, 647c4a9331e01e31a4350361d3460f0397fe694f, b080d072cfde697180db3234da08903c092e72c3, 375125029b085e70a109491656b69aa01bc2a166, d8a5474f450330ad25c1e22f27e88f3630cb840d, 8dce62b18bdea587c07cb4769a05ca0a816d09ba, eef23d76e175c0cff8e81ffcb2721c10539c8cbd, e4b52a1a00e9db941326fc857b95245cbfb60bce, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, e32a2519b59d62cff6cb8136ee242dc3754ed57b, 27245e65a27bde90b5b0bb25d157bb75a0ad8b5a, b5904cd5dbf73b8d5ff13517de490c292d877ee0, b52db9e41e15f76bdcfbe674abe0314af545c430, e576a2d97950b1f6831f88575dd3f370053f6af7"
31f9eb39d840821979e5df9f34a6e92dd9c879f2,Learning Deep Features for Discriminative Localization,https://www.semanticscholar.org/paper/31f9eb39d840821979e5df9f34a6e92dd9c879f2,Conference,"In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.",2015,7937,"145291669, 2556428, 2677488, 143868587, 143805211",145291669,Bern,3,"145590185, 34842481, 2153917002",Y,"A more systematic set of experiments could compare learning the proposed weightings on the first K layers of the network (for K={0, 1, …, N}) and learning independent weights for the latter N-K layers, but I understand this would be a rather large experimental burden.;It is observed that words case and average case empirical error estimates diverge when the input is high dimensional.;Second: They then train a model based agent using the collected transitions ( St, At, St+1 ).",Computer Vision and Pattern Recognition,15,"pooling,network,localization,deep learning,deep learning","7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 371a343457a4fbff00000bf4faa29b2b2f85744c, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, 17fc5c9b37ba63fb3280ddf9909a183523e7bcc3, d6bc29a897fd85e7187dc33c3c974b8879462237, ec2f9076448ba25a225618603adde60caa76c4df, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 794b3ffd28d28606230efc975eeec9f0522fb139, 4e746359afd6f81705b875d71cc499b904a320df, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, a80e26e6365b215715c182d19a9aa8bb876ac768, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 48c73c389c3f36d70407eb8309a0b41578c15fc8, 1f8a23697562b001082b147779b5eaefd3513d0a, 94214d6d922ce095719d488642cbcc75dc52f273, 7171a0e9b07ebc98a32eb912262613efc20f283a, 71854ff4306cf65c3c2161f7be2d0346275f72d5, bf69c98fca9a9f6c1cde871beddbcdc668b77771, 9d788cfe4a0991d3b1a266c8329f6e903840b82f, e24b8a9531573d284647239affc6c855505b0de4"
7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7,Learning Transferable Features with Deep Adaptation Networks,https://www.semanticscholar.org/paper/7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7,Conference,"Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multikernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.",2015,4409,"35776445, 2146174097, 2144499343, 1694621",35776445,Copenhagen,2,"1800564, 1786155",Y,"The authors show through several experiments that the divide and conquer (DnC) technique can solve more complex tasks than can be solved with conventional policy gradient methods (TRPO is used as the baseline).;Some comments: - Perhaps, it is better to move Section 3.3 before Section 3.2 to emphasize the main contribution of this work, i.e., using Stein’s identity to derive an estimate of the gradient of the score function.",International Conference on Machine Learning,15,"domain,network,adaptation,deep learning,deep learning","e8b30ebe3351680c3b039555ae0a8d0865ad829b, 82663577cf1d08235bb56ad648c9dad36343ccfb, ccca203382e5dd198c089a0f1d7af7bef0f694e9, 5d433da6d0f143f20936379910104d2bb139d4ae, 5ea7bf772fecf95cbf53b2c7f719c9440322a115, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, 19b93280f17696a4ddfa2c75490a50ab107addf2, b4c9c134ad5bd4a037115df65411b4c49abe1322, 20a3ed03888037e2802fa9abad02ffa0a8dcc228, 456c011594ecacdd24298a161787389ccbe4b88b, 5f7f10f913ecc478ff7ba304c265fd3c700b47d7, a13149a80855412d970d0de2b41c611f4cf7e1da, 102ebe229df18c8733ea1b8def56cd79996e2178, f2bc4057e696f49c326bf8e1588772a16f053754, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, 6ff909c6fe089fc8ebfc64eca0f0c3cc34ba277f"
2eda2921a8da4b325f9d05f556594a5884c398a7,Overfitting in adversarially robust deep learning,https://www.semanticscholar.org/paper/2eda2921a8da4b325f9d05f556594a5884c398a7,Conference,"It is common practice in deep learning to use overparameterized networks and train for as long as possible; there are numerous studies that show, both theoretically and empirically, that such practices surprisingly do not unduly harm the generalization performance of the classifier. In this paper, we empirically study this phenomenon in the setting of adversarially trained deep networks, which are trained to minimize the loss under worst-case adversarial perturbations. We find that overfitting to the training set does in fact harm robust performance to a very large degree in adversarially robust training across multiple datasets (SVHN, CIFAR-10, CIFAR-100, and ImageNet) and perturbation models ($\ell_\infty$ and $\ell_2$). Based upon this observed effect, we show that the performance gains of virtually all recent algorithmic improvements upon adversarial training can be matched by simply using early stopping. We also show that effects such as the double descent curve do still occur in adversarially trained models, yet fail to explain the observed overfitting. Finally, we study several classical and modern deep learning remedies for overfitting, including regularization and data augmentation, and find that no approach in isolation improves significantly upon the gains achieved by early stopping. All code for reproducing the experiments as well as pretrained model weights and training logs can be found at this https URL.",2020,620,"51026953, 47260842, 117539586",51026953,San Marino,3,"6322777, 2279712392, 2116502347",Y,"Should these parameters be take out of the n-step advantage function A?;I encourage the authors to have the work proofread by a native speaker; clearer writing will ultimately increase the reach and impact of the paper.;But, it would be good if this can be supported with real life examples.",International Conference on Machine Learning,20,"training,performance,networks,deep learning,deep learning","14dd50979af27bd2574c8068db11d27028b56afd, 4cd033a56b19f87f6adfefeef5fcc990306ecf40, ae38dc77a962161107361f213db9216ee1274037, 07b01d665646009439ca206378cc35e095ec6cd2, 5d433da6d0f143f20936379910104d2bb139d4ae, cb23a59fdf3ade707600f076df4ff27a03941fba, 31f10a6f602bef0306ac37322f84f6163c8a8ecb, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, 156d8e2aa90b5ccc9be10477ca70deaad0151387, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, a9cbbef8f4426329d0687025b34287c35bdd8b38, da5d78b3e3a1544fde98fba86088e1215e97cbe8, fe44200fed05f9a7c656f2245deded8fd5f5e1e6, b080d072cfde697180db3234da08903c092e72c3, 633e2fbfc0b21e959a244100937c5853afca4853, dca4d9abbc82e57dfa52f932e893d467a63e0682, 6e74403ab75d0080c056fd66702ab1f54f04d9b1, df138c7425e787cac2f9d3ab7775c0fb5294a83e, 573fd2ce97c70bb29097e8efb28a27af791225ca, d3f9a39e49abfdf084da558e305be5473c8740e5, 4e746359afd6f81705b875d71cc499b904a320df, a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, 1ca6d6682204f0214338f7797bea056444e908bd, 5b6ec746d309b165f9f9def873a2375b6fb40f3d, 811df72e210e20de99719539505da54762a11c6d, 4e13a8e8ba8d33e15ed037bfca7c651047533990, be7cb8f79bc018e57467168fc0c7f8ad59bba04f, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, 22ebfc211d184ed615729378a43fde175bf14478, bad4c08f03587e38ee960e2aa76e16d722826e7c, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, 42375e9a309fe5bf5c671c918cd8c9d5d85568cc, 6aca07154c111f1c8738347d7112cad6b0bf974a, c468bbde6a22d961829e1970e6ad5795e05418d1, 709f7a6b870cb07a4eab553adf6345b244913913, 28a5a53dafacebad8a7c47773079caeffb9a5baa, f1664bbaddedea8c250873e7610ab07e53fa7132, fa63c3f53413ced7946623889c416e34a28676ea, f1300d9be8254b028337d9757755ba906fe6955b, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, 75ea299834d6949e89e91d006677343ddab44e49, 89858723bec341178f2b00d34ea3016baaaf71a6, a22f3398ea865426c89ee66f4824ec626e56a864, 6068d39e92aef1bb0e1291e9931894c35692a85e, 02b1607af35b48f0bd716367caf6a7428b969369, 8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6, 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d, 9619cde5c79d91ca5c432186668618312175f8dd"
07cca761749bfe21c2d096ff60f32b574d5c84c4,Normalized Loss Functions for Deep Learning with Noisy Labels,https://www.semanticscholar.org/paper/07cca761749bfe21c2d096ff60f32b574d5c84c4,Conference,"Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: any loss can be made robust to noisy labels. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of underfitting. To address this, we propose a framework to build robust loss functions called Active Passive Loss (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60% or 80% incorrect labels.",2020,317,"9576855, 1753845931, 1919541, 9035741, 144757691, 145148600",9576855,Moscow,3,"144985567, 2211964951, 1390481263",Y,"To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER.;I found the formulation of the \alpha to be non-intuitive and confusing at times.;The paper is generally well written, easy to read and understand, and the results are compelling.",International Conference on Machine Learning,20,"loss,functions,labels,deep learning,deep learning","182180bd69ea6d2f59225ded5ddc900b8558ab9f, 7884b0ec63b8a08f7cd793a989df44f6bb53116c, b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8, 7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7, 22ebfc211d184ed615729378a43fde175bf14478, 72afe82af4c2ca100c36eb35292e85d806527f0a, d997beefc0922d97202789d2ac307c55c2c52fba, 53103ae318a19569ac82cee5062de2cf73bf386c, 6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5, d21703674ae562bae4a849a75847cdd9ead417df, c419ee7315b9edfd8fc55bab16534fc55a564fcd, 8bba999de25bfb288b3f7f88e1d907aab02638b6, 9d788cfe4a0991d3b1a266c8329f6e903840b82f, 0f4d00d01d43d3967ee92b58481b5ad530a944d1, 9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a, 7ed665355ac78bf0c394602dd9d26075195ce2f2, 33ce8103b129149eb78ca2fa48538e25c9242c08, 332e0eab5fba8e6940f3e481f542a99ac17b9717, 7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2, 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d"
8760bc7631c0cb04e7138254e9fd6451b7def8ca,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,https://www.semanticscholar.org/paper/8760bc7631c0cb04e7138254e9fd6451b7def8ca,Conference,"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10 × or 100 × ? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between ‘enormous data’ and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",2017,2030,"1491624845, 1781242, 2108498897, 1726095131",1491624845,London,3,"32246932, 48740398, 2113618679",Y,"2. Even if this was correct, the main point is that this is ""only"" d times worse - see eq (11).;In any case, I also don't think that was the argument you were trying to make.;In a bit more detail, the key idea of the paper, at least to the extent that I understood it, was that the authors are able to introduce a model-based variance-reduction baseline into the policy gradient term.",IEEE International Conference on Computer Vision,17,"vision,data,tasks,deep learning,deep learning","c9645aa4ea31903e02e201b877fd3e1466adff4f, 91d6e8ba5dd90b02fe3bd870b19da13a6167af53, 0ac1a1ccb16c8441f42ff8de743fc93f0e08c15c, 4aec7bcb292cb763341a22fc67a1cfaceb3a2c67, 215fc60307f741b9db059204e41db8bfb879e606, d617f51833860dc50d202af7f80be71304b2e994, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, 5c39e37022661f81f79e481240ed9b175dec6513, aa6c2afadd660fe4efbac699f7854e8f6f240c38, 99f06e88e76f1af51d08d7adfb26d758ebc6acab, 82973c5f56681190a0dbb4c4449ed60d5f805135, 0258bab20bc8574ee602012081a17db89339f12d, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, d81fc968196e06ccafd7ea4c008b13e1cad1be64, 492c389d560d9db39c758d07e635408d2e0eaf7d, a8c1ed061813f832358c1aabf5d171bab80203bf, 46c266b3d1274dacd7fce27ee8cb4d587f087a58, 3dfa820702b6181c9964931f0a4d47fd298bf429, f4207bcad24fe4eaf3849d6c1fe38c36545b5cb3, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, 27e58c9e5e6d07809a45a17675a2c7135b577881, 8d67b76222d84dcd337b8a2c78f13837070a79ce, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, 819f2778eba0d4c9eea86307bedaaeed94dc751d"
d81fc968196e06ccafd7ea4c008b13e1cad1be64,An End-to-End Deep Learning Architecture for Graph Classification,https://www.semanticscholar.org/paper/d81fc968196e06ccafd7ea4c008b13e1cad1be64,Conference,"
 
 Neural networks are typically designed to deal with data in tensor forms. In this paper, we propose a novel neural network architecture accepting graphs of arbitrary structure. Given a dataset containing graphs in the form of (G,y) where G is a graph and y is its class, we aim to develop neural networks that read the graphs directly and learn a classification function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classification purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the first challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs. Experiments on benchmark graph classification datasets demonstrate that the proposed architecture achieves highly competitive performance with state-of-the-art graph kernels and other graph neural network methods. Moreover, the architecture allows end-to-end gradient-based training with original graphs, without the need to first transform graphs into vectors.
 
",2018,1277,"3098251, 7217944, 40059761, 9527255",3098251,Moscow,2,"1764236, 145033446",Y,since you mention at the end of page 4 that you will only rely on linear SVMs for computational reasons.;5. It would be nice to see the gradient flow with the new activation function compared to the ones without.,AAAI Conference on Artificial Intelligence,18,"graph,graphs,networks,deep learning,deep learning","7bb477077968d68aa7a6059d8d6d801fb28274da, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, fc32074b37a6d9dda535a70f9689022e70508520, 7904b3446775ed8c79f4f94001a16b706989c462, 8d06f13ec042f0d8d845af5deadeb3dd9ad11f70, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, 73d4accea441aae2373828a8dc2175aa2759c38f, 0d065e8688c38bb0148203a1738f47184a5b58d3, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 980858461df7c4349f17b427686c5bcbcffbdc04, 91b9d3ab7532ea24ae70cd726355f25235b1fe8b, afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84, d1e701665e73faa648cb15473952576f40e8e122, c96fc88631f2b8e2fe192027a8a237445635328c, 9954ec0a95fc0840f7712d7b0c67b242bb536a9e, d617f51833860dc50d202af7f80be71304b2e994, 2e5d2f2dc01b150dffc163a9f457848e9b5b5c38, df7d26339adf4eb0c07160947b9d2973c24911ba, ca0e479ba2327f71e842d033b6b48b082962cc6a, f2bc4057e696f49c326bf8e1588772a16f053754, d21703674ae562bae4a849a75847cdd9ead417df, bcc82ce554942880814243fc8c08a88b9d2aad09, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, 2ddcd47b28eb4b43317a34cf56e83309f5347699, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, 68a3d32416977e88cf1bfa4ad548d403f5f089d6, d3855b7351c11145e51301e6b686f748ca35c802, 965359b3008ab50dd04e171551220ec0e7f83aba, 657fbf29ea0b4904a3e98d1556f9acf38dddae5f, df7336844a31165db0ae08f1cd0f560c9e3faeea, 7eaac9847257c32afd450017d1348ecda4dcaade, cb03b665069dad5e895a2c244929ea427f1fb9d1, 98b9086750f08a21c8778ab986339321e9caf790, ac4350d3a2673a4c34fa949714ede8b45fd04f1d, 08be4e23951a0def1c5d235befbb39c8d8d373a3, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, 05b8b67451fb105576c58af960e6e6d98f9103e7, 1ff76ab0fcf22110df62337d462e15d79a2a2593, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, 64306bbddb4da7a4e06f990a0167d55fbbbbec82, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, c84aa52bee5116f80c7740503edff4b08f733c3b"
329d31f881a17861eedeef6a9d8fd509cddd2b7c,QUARK: A Framework for Quantum Computing Application Benchmarking,https://www.semanticscholar.org/paper/329d31f881a17861eedeef6a9d8fd509cddd2b7c,Conference,"Quantum computing (QC) is anticipated to provide a speedup over classical approaches for specific problems in optimization, simulation, and machine learning. With the advances in quantum computing toward practical applications, the need to analyze and compare different quantum solutions is increasing. While different low-level benchmarks exist, they often do not provide sufficient insights into real-world application-level performance. We propose an application-centric benchmark method and the QUantum computing Application benchmaRK (QUARK) framework to foster the investigation and creation of application benchmarks for QC. This paper establishes three significant contributions: (1) it makes a case for application-level benchmarks and provides an in-depth ""pen and paper"" benchmark formulation of two reference problems: robot path and vehicle option optimization from the industrial domain; (2) it proposes the open-source QUARK framework for designing, implementing, executing, and analyzing benchmarks; (3) it provides multiple reference implementations for these two reference problems based on different known, and where needed, extended, classical and quantum algorithmic approaches and analyzes their performance on different types of infrastructures.",2022,20,"2147184066, 2105502136, 9653883, 50631038",2147184066,Moscow,2,"2217847684, 2158502526",Y,"The current paper presentation is a bit too dense to clearly understand the LL machine model and the two-phase algorithm.;So, I wish to see a section on testing with Resnet and GoogleNet.",International Conference on Quantum Computing and Engineering,22,"quantum,problems,benchmarks,quantum computing,quantum computing","0456ed94f6c455f99cb67abe8a28aeb3ef9f489b, 27245e65a27bde90b5b0bb25d157bb75a0ad8b5a, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, 6068d39e92aef1bb0e1291e9931894c35692a85e, ac4350d3a2673a4c34fa949714ede8b45fd04f1d, f406aceba4f29cc7cfbe7edb2f52f01374486589, f51bc74814a3452009ea5ca262d9768d08149ee6, 82663577cf1d08235bb56ad648c9dad36343ccfb, 25761ba4bdc054bfe902fe7c5d6338be6d00d491, 375125029b085e70a109491656b69aa01bc2a166, dca4d9abbc82e57dfa52f932e893d467a63e0682, f14fc9e399d44463a17cc47a9b339b58f6ef7502, bdb68c5e2369633b20e733774ac66eb4600c34d1, b27c98d8378848f2c23a067f2c5196f3b5a07572, 2ee03e28208a9310a9be4032c2b04ebdddb83cc7, 9817bf0f78047452761e950c02a1a56f59a1e593, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, d2a5dcecd2ffdf03473df1688091f08fadb114a3, b651d67502790e1d6d41c589e1d93e996ba7b935"
047286f5b9315a8e8bf56c4fc936e62f21495892,Resource Allocation in Quantum Networks for Distributed Quantum Computing,https://www.semanticscholar.org/paper/047286f5b9315a8e8bf56c4fc936e62f21495892,Conference,"The evolution of quantum computing technologies has been advancing at a steady pace in the recent years, and the current trend suggests that it will become available at scale for commercial purposes in the near future. The acceleration can be boosted by pooling compute infrastructures to either parallelize algorithm execution or solve bigger instances that are not feasible on a single quantum computer, which requires an underlying Quantum Internet: the interconnection of quantum computers by quantum links and repeaters to exchange entangled quantum bits. However, Quantum Internet research so far has been focused on provisioning point-to-point flows only, which is suitable for (e.g.) quantum sensing and metrology, but not for distributed quantum computing. In this paper, after a primer on quantum computing and networking, we investigate the requirements and objectives of smart computing on distributed nodes from the perspective of quantum network provisioning. We then design a resource allocation strategy that is evaluated through a comprehensive simulation campaign, whose results highlight the key features and performance issues, and lead the way to further investigation in this direction.",2022,14,"1741486, 2288804757, 1739490",1741486,Sofia,3,"2762838, 3428490, 1806271",Y,"Is the implicit assumption that for real, high-dimensional data the generator and data manifolds will *never* overlap?;* There's abundant literature on f-divergences which show that there's a 1-1 relationship between divergences and optimal (Bayes) risks of classification problems (e.g. Reid at al. Information, Divergence and Risk for Binary Experiments in JMLR and Garcia-Garcia et al. Divergences and Risks for Multiclass Experiments in COLT).;For instance, averaging the node embeddings is something that has shown promising results in previous work.",International Conference on Smart Computing,22,"quantum,computing,internet,quantum computing,quantum computing","256db9dba1978f004a67c86ffc321563b1aee79a, 340f48901f72278f6bf78a04ee5b01df208cc508, 6fb020754f6de564c3a0a07bb656c0a90be1f87d, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, fe44200fed05f9a7c656f2245deded8fd5f5e1e6, 43eea2a73997294193228d50f9ff25fc5345664b, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, e89c37b7c2ff465db43c4b9f674867ec4b98aa8b, 9a0965beef113cc37491004b1848149e00300561, 89a30b5dab02c9c390a632acad481fa602859272, 72243bfa618fd6c3c01bb7b4885a4a456b2a6071, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, 25ed8ba0e8906ba98fa5d92d17a01e818796ddc9, af13a92977d4f4dc5b28b13746d86111d42939e8, a0f788f6de0fb83d623c875a98120e3f347f70d1, 54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f, 5778e56400f7113c2b1355fdbd6b638fa379885f, 676664ee7471738577f641e6159e7596625b7fdb, f1300d9be8254b028337d9757755ba906fe6955b, 5d8e450c8c7d9faa8222f1d0eaf34b01755c9eef, ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba, 74b4f16c5ac91e3e7c88ae81cc8c91416b71d151, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 5645502d73c6907f1671923638773152e55bfb00, 45de91a919780d5540872cf047986a370625e61c, 25adff0bb9691b46fee5c0591300d6f3ccf117ab, 17fc5c9b37ba63fb3280ddf9909a183523e7bcc3, 0f38b3d717e0fcc6eacc9c6e78b252227440e04e, ccca203382e5dd198c089a0f1d7af7bef0f694e9, a13149a80855412d970d0de2b41c611f4cf7e1da, 74b63c4cdc719a646ce014d084a0ce0ac66d9139, 86d3beff240b6c882058455e098a571de86564f5, c0e6cd2ec3bc9eb46c7d45bb708854da3327339e, ce2d5b5856bb6c9ab5c2390eb8b180c75a162055, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 771a858c35f6d6e6d1017dde95368de3794738a6, 02fa2389b1b64b661192e224bed8af6df0ce80f6, b0b770fb8c7760749c88e3c83ae173cdb07f7bd5, c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b, 795550a5294eb05ea4f3b14f0b1c21a405493d85, a2ec47b9bcc95d2456a8a42199233e5d9129ef18, 634fe61f3ab6692ea4733989a1c76e793bb8b69e, 2ddcd47b28eb4b43317a34cf56e83309f5347699, 5290d7921f0266c8b50b79fc8a0b7d22868f4f60, 5e31fa4e69d1a3587230f5d134c0b7e2ed84a742, 1f8a23697562b001082b147779b5eaefd3513d0a, e2e8ea9bcdb83deb2787ce89db1b51f7ccb1bd1e, ada0b87cd5c30d31186c38fb12e631d29426a3bf, 62fe1c3a866a5e368e110d6d8ed2385c84072cac"
c2aab470b8cf92f090e0a3bac1794b21500585e6,Evolution of Quantum Computing: A Systematic Survey on the Use of Quantum Computing Tools,https://www.semanticscholar.org/paper/c2aab470b8cf92f090e0a3bac1794b21500585e6,Conference,"Quantum Computing (QC) refers to an emerging paradigm that inherits and builds with the concepts and phenomena of Quantum Mechanic (QM) with the significant potential to unlock a remarkable opportunity to solve complex and computationally intractable problems that scientists could not tackle previously. In recent years, tremendous efforts and progress in QC mark a significant milestone in solving real-world problems much more efficiently than classical computing technology. While considerable progress is being made to move quantum computing in recent years, significant research efforts need to be devoted to move this domain from an idea to a working paradigm. In this paper, we conduct a systematic survey and categorize papers, tools, frameworks, platforms that facilitate quantum computing and analyze them from an application and Quantum Computing perspective. We present quantum Computing Layers, Characteristics of Quantum Computer platforms, Circuit Simulator, Open-source Tools- Cirq, TensorFlow Quantum, ProjectQ etc. that allow implementing quantum programs in Python using a powerful and intuitive syntax. Following that, we discuss the current essence, identify open challenges, and provide future research direction. We conclude that scores of frameworks, tools and platforms are emerged in the past few years, improvement of currently available facilities would exploit the research activities in the quantum research community.",2022,12,"9368179, 2127923241, 50768264, 22706311, 1883858, 28334651, 9878116, 4625670, 39864830",9368179,Tirana,3,"144231976, 2120251897, 2065048323",Y,"remark on theorem 1: This result generalizes a result proven in 2015 stating that the normality of a layer propagates to the next as the size of the first layer goes to infinity.;Clarity ===== The paper reads well, but it is not really clear what the claimed contribution is.;With this metric, the comparison would be easier and more intuitive.",Annual International Computer Software and Applications Conference,46,"quantum,computing,research,quantum computing,quantum computing","9b3e8d202488dc29e601fc471a25a2af9002659e, 6c34842a92ce4da9aab586490afdbd8779af4eab, 99ae62cca0b15275d9ed1528f345f9dcefe50dd7, 9954ec0a95fc0840f7712d7b0c67b242bb536a9e, 256e95ca331cbd35b3a23cc306b6627e6771a963, 156609022dd6258c60238859622da0a1683bd062, 3b230f14c46e7e177e9bebb2ebc9f46b346b646d, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, 787ae2c51cd82b904bb4fb9ccb15266381af5436, d6e1e4f0ad898ca6ac37e6e139a77fa3982170d4, facd5f5deb152229ceb1803434d8690a09ab4129, bfad52fc64ca0169644b6e7e0ea9a46470d51709, 2346d121f38fc19c77e0b062415519843f478163, aa9bf8b4c4ce8c28b9f47d7ce4f416aa9726bb0d, 94de6bab96ad533169dbc3bf9e6557581e59cb6f, 68ade27b68c05d1b60b8a9d8f8aadbe76900c60c, 03532123ccffae8d411264320e8a5ae2b6eddea0, acc296f981cde8d8c205982fc4422ec35531b769, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, c5c4142a01981787a71bf6ebcb791520c458ab5d, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 6c739540e66e895311b7347971f10ef556e06e52, 0f4d00d01d43d3967ee92b58481b5ad530a944d1, 62fe1c3a866a5e368e110d6d8ed2385c84072cac, cc017a62c605a0749e35a1264a46d62e78fb68b7, 4afa7d8e2de43b0b67366b1bce8768f5a246d153, dad8965c5a4c0a0ea1eb3837c6a9c3b42597c2ce, bb0cec8f2d34cfb9dbf8bffd1a5de499311ae098, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, d2a609ffb814442d0728aef9f6616f9cd775face, 9e66ae24a541255c2d931184498ee116ce81478a, 3a083d843f891b3574494c385699c21766ce8b7a, 3dfa820702b6181c9964931f0a4d47fd298bf429, 1fa4936fb06319c3f4536c26a447d5507c92bd48, 52a6695ae1c08cc29baf764dedb5831c7a954214, d7fddafbbc372da4fa884f67bdc32db71b888806, c12b80b44d9acfe6cd92fdf965264c4b706c367c, db4cf9f6a653d5c15973e836c800ea47743251ae, 0894585294c67193ff3190240554677b56fd79a0, a0f303b6e22ef52943355993f57d65938997066a"
3ea34401909978d3d3d0c25c8746e02c7d2a7c77,Optimal Layout Synthesis for Quantum Computing,https://www.semanticscholar.org/paper/3ea34401909978d3d3d0c25c8746e02c7d2a7c77,Conference,"Recent years have witnessed the fast development of quantum computing. Researchers around the world are eager to run larger and larger quantum algorithms that promise speedups impossible to any classical algorithm. However, the available quantum computers are still volatile and error-prone. Thus, layout synthesis, which transforms quantum programs to meet these hardware limitations, is a crucial step in the realization of quantum computing. In this paper, we present two synthesizers, one optimal and one approximate but nearly optimal. Although a few optimal approaches to this problem have been published, our optimal synthesizer explores a larger solution space, thus is optimal in a stronger sense. In addition, it reduces time and space complexity exponentially compared to some leading optimal approaches. The key to this success is a more efficient spacetime-based variable encoding of the layout synthesis problem as a mathematical programming problem. By slightly changing our formulation, we arrive at an approximate synthesizer that is even more efficient and outperforms some leading heuristic approaches, in terms of additional gate cost, by up to 100%, and also fidelity by up to 10x on a comprehensive set of benchmark programs and architectures. For a specific family of quantum programs named QAOA, which is deemed to be a promising application for near-term quantum computers, we further adjust the approximate synthesizer by taking commutation into consideration, achieving up to 75% reduction in depth and up to 65% reduction in additional cost compared to the tool used in a leading QAOA study.",2020,98,"2218979994, 2259796",2218979994,Chisinau,2,"2156120640, 2115626527",Y,"Across the two NLP tasks authors show that MNMS models trained simultaneously give better performance than hand tuned architectures.;Except from a few typos here and there, the paper is overall well-written.",2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD),20,"quantum,programs,approaches,quantum computing,quantum computing","94214d6d922ce095719d488642cbcc75dc52f273, e8b30ebe3351680c3b039555ae0a8d0865ad829b, f72053903270d9a7f41108461ad04d5aa075218d, b3f7359c6d5780972c5ea8db016a01f0c705aa01, 6a6ad9eb495739f4c80e7c09598720c3d5c5dff7, e576a2d97950b1f6831f88575dd3f370053f6af7, 2346d121f38fc19c77e0b062415519843f478163, f9253a3e2627f1c2a0a7e2cea320a4ec4e4d2ff9, 29ddc1f43f28af7c846515e32cc167bc66886d0c, e7a7735104448371dde788542ebfc6af6485ea43, 78e80b6f3fa46e26ea4e8542815305c3ba88c5df, 5bc511aa30f72720260d792e57537379fb04c395, 3ec02e36cc0ba16292b255e9f90c3725521e2ec9, 9e66ae24a541255c2d931184498ee116ce81478a, eed62d36d1b976ac3873c83645f1c25f5096f89c, 453fdfeefd6498a65be339d7e8722f6f3288907e, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, b613887337a5d2e8fc8773037116be81e6346835, 676664ee7471738577f641e6159e7596625b7fdb, b4c9c134ad5bd4a037115df65411b4c49abe1322, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, 46c266b3d1274dacd7fce27ee8cb4d587f087a58, 0894585294c67193ff3190240554677b56fd79a0, 5cb8f417d171ae329adf446820bd32d8b49d8c04, f63e917638553414526a0cc8550de4ad2d83fe7a, 8b7b2e88207fc2dd849f5d83c9b6e890f0174abc, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, 33e332837e91c1048c3ed165cd16bf7607c3bf06, f381c53aeb7742e4047d06d84f9e0c4f523231a3, 34ca47eed139a7f0694611528f75debc43385518, b2114228411d367cfa6ca091008291f250a2c490, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 5471114e37448bea2457b74894b1ecb92bbcfdf6, 03532123ccffae8d411264320e8a5ae2b6eddea0, d47a682723f710395454687319bb55635e653105, 8d0f755dea90f35f4b126a01fa3cce96b3bdd344, 4be7d1524edb0137599a5cc95f72844b85a52fe1, d2a505586c0da20752b98f63c7760b6a5c41e28d, 695bdc6e24608364491b9418a220c65a7cd17413"
e576a2d97950b1f6831f88575dd3f370053f6af7,Distributed Quantum Computing with QMPI,https://www.semanticscholar.org/paper/e576a2d97950b1f6831f88575dd3f370053f6af7,Conference,"Practical applications of quantum computers require millions of physical qubits and it will be challenging for individual quantum processors to reach such qubit numbers. It is therefore timely to investigate the resource requirements of quantum algorithms in a distributed setting, where multiple quantum processors are inter-connected by a coherent network. We introduce an extension of the Message Passing Interface (MPI) to enable high-performance implementations of distributed quantum algorithms. In turn, these implementations can be used for testing, debugging, and resource estimation. In addition to a prototype implementation of quantum MPI, we present a performance model for distributed quantum computing, SENDQ. The model is inspired by the classical LogP model, making it useful to inform algorithmic decisions when program-ming distributed quantum computers. Specifically, we consider several optimizations of two quantum algorithms for problems in physics and chemistry, and we detail their effects on performance in the SENDQ model.",2021,25,"3393711, 3393324, 1713648, 1752096",3393711,Vienna,3,"6322777, 2253929707, 3087426",Y,"They are both trained on the same training data, only test data is of different length and ideally both models should achieve similar accuracy for the first 10 subproblems (same trend as DDRstack).;Figure #s are missing off several figures.;There should be experiments that compare the the Q+P model with incresing number of atoms against a full CNN, to see whether the Q+P can converge to maximal performance.","International Conference for High Performance Computing, Networking, Storage and Analysis",21,"quantum,model,algorithms,quantum computing,quantum computing","f406aceba4f29cc7cfbe7edb2f52f01374486589, cb12fe714dd4fa1d14b6a8023d494c14c89e90ea, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, aca6d5f3866372a4506cf15773ae298f18c3f453, 85328b4a8132bf4299f8cd7f8e79e850d561c8fc, 0c00a328fa7cd56ee60338c54e89bd48310db80b, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, c24d47ff95cd4bda073c75ec24ececaa3b10c995, 6548106035c7208ad498730627874a482734b9ac, 709f7a6b870cb07a4eab553adf6345b244913913, 00cd2650a89734105fa0c0aba3bf07935b318290, 18d87bff073687c025f9bd23ab2dfb20d5f72a66, 9a0965beef113cc37491004b1848149e00300561, 7fa273f450251523e6b7fcc2eb3fdbdfd4a30493, 1fa4936fb06319c3f4536c26a447d5507c92bd48, fe4c5074f021cd1c810892c1b6bc267b23aa6e5c, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, 9c1265fd47df8d825f0ddd6d9ab834002592e38e, 696b388ee6221c6dbcfd647a06883b2bfee773d9, 661ccdb41fe977d47273e586389cacc1489f3286, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, 08aaaf7ae3d61875e7bcf5c1bb0df4f17066e300, be2b0396de9431bae931642516a1d3e4906329f5, e4788ee4f5e90c6f42cedc5116acd2d6475c3180, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 0bca61986b8edeaf33018d0203b44110f2480110, 4f480bae3196dbbc27ab383bce33478ea963f9b3, 2b061f7f108fdd4e90452aaaead574c7b4b5b780"
82973c5f56681190a0dbb4c4449ed60d5f805135,EQC: ensembled quantum computing for variational quantum algorithms,https://www.semanticscholar.org/paper/82973c5f56681190a0dbb4c4449ed60d5f805135,Conference,"Variational quantum algorithm (VQA), which is comprised of a classical optimizer and a parameterized quantum circuit, emerges as one of the most promising approaches for harvesting the power of quantum computers in the noisy intermediate scale quantum (NISQ) era. However, the deployment of VQAs on contemporary NISQ devices often faces considerable system and time-dependant noise and prohibitively slow training speeds. On the other hand, the expensive supporting resources and infrastructure make quantum computers extremely keen on high utilization. In this paper, we propose a virtualized way of building up a quantum backend for variational quantum algorithms: rather than relying on a single physical device which tends to introduce ever-changing device-specific noise with less reliable performance as time-since-calibration grows, we propose to constitute a quantum ensemble, which dynamically distributes quantum tasks asynchronously across a set of physical devices, and adjusts the ensemble configuration with respect to machine status. In addition to reduced machine-dependant noise, the ensemble can provide significant speedups for VQA training. With this idea, we build a novel VQA training framework called EQC - a distributed gradient-based processor-performance-aware optimization system - that comprises: (i) a system architecture for asynchronous parallel VQA cooperative training; (ii) an analytical model for assessing the quality of a circuit output concerning its architecture, transpilation, and runtime conditions; (iii) a weighting mechanism to adjust the quantum ensemble's computational contribution according to the systems' current performance. Evaluations comprising 500K times' circuit evaluations across 10 IBMQ NISQ devices using a VQE and a QAOA applications demonstrate that EQC can attain error rates very close to the most performant device of the ensemble, while boosting the training speed by 10.5X on average (up to 86X and at least 5.2x). EQC is available at https://github.com/pnnl/eqc.",2021,29,"1949572253, 1766680, 3253856, 145560079, 143924199, 1743474, 2107856766, 2112839155",1949572253,San Marino,3,"2047998, 2161986932, 144303419",Y,"Like for example a penalty in how many examples a expert has catched.;Firstly, the model has ample free parameters to overfit when such a tiny test set is used.;However, the sensitivity calculations in the SVM context is new as per my knowledge.",International Symposium on Computer Architecture,49,"quantum,training,vqa,quantum computing,quantum computing","c15f30a3e84910a28cc560e7db097fd99339e8c1, f11cba099b8cf14815f7b3d85f55ecfddbf9f04d, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, fa75a55760e6ea49b39b83cb85c99a22e1088254, a281d563261c738f13b9e58a525e7e265a619c93, 649c3497e3b34b15a5011259fcb837cf6c1ac04a, b79e5e4622a95417deec313cd543617b19611bea, 9b529fe170823f95509585d5aa39fa01a43558fd, 99f06e88e76f1af51d08d7adfb26d758ebc6acab, fafa541419b3756968fe5b3156c6f0257cb29c23, d2efa91bd7d076a66bc1d1c570f6aa2da944dd88, f8e57fa370fe10147aa22714e08409fc1b7dae4b, 4b9184937da308914b9e13c43bfd75845eaf910b, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, 7536bce1007a765fd097a7cc8ea62208a8c89b85, cb03b665069dad5e895a2c244929ea427f1fb9d1, 7fa30c8af436b7df513ef41e2cccd8a2404f37f8, 8ab93bee04cd5bdbd96002b2e325d02f61ba695a, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, 13c4e5a6122f3fa2663f63e49537091da6532f35, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, e4b52a1a00e9db941326fc857b95245cbfb60bce, 30f233eecca2239ee1dd754914324092e53f8f19, a8c1ed061813f832358c1aabf5d171bab80203bf, 8d942a3b52e2ad16ff8e5970be59591970d89fae, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, d235a9085e0543fcbe502fbc269f9a8ee01dcbab, 21042565ec941f4fa31ac5a0af85a1a84ff21f1b, 7d873a9c49d3864709aa762f8740edcdbd7369c5, 16753e0317730e8c1b297338300a8c6163dd06f2, 68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a, cf41991d89301c3c12420d150792cb1163999962, ff7bcaa4556cb13fc7bf03e477172493546172cd, 5cdb31c5e3e0aa4cc2d10229793b4911f69fd78f, 1562390dd212516cd857009cbd4f857a902d1f3d, 7a4fe2f003241ad97bf1778e527cb0306fa90da2"
6c755fc901d0b41a5d73c265f64a5aacf62e83b8,GDsmith: Detecting Bugs in Cypher Graph Database Engines,https://www.semanticscholar.org/paper/6c755fc901d0b41a5d73c265f64a5aacf62e83b8,Conference,"Graph database engines stand out in the era of big data for their efficiency of modeling and processing linked data. To assure high quality of graph database engines, it is highly critical to conduct automatic test generation for graph database engines, e.g., random test generation, the most commonly adopted approach in practice. However, random test generation faces the challenge of generating complex inputs (i.e., property graphs and queries) for producing non-empty query results; generating such type of inputs is important especially for detecting wrong-result bugs. To address this challenge, in this paper, we propose GDsmith, the first approach for testing Cypher graph database engines. GDsmith ensures that each randomly generated query satisfies the semantic requirements. To increase the probability of producing complex queries that return non-empty results, GDsmith includes two new techniques: graph-guided generation of complex pattern combinations and data-guided generation of complex conditions. Our evaluation results demonstrate that GDsmith is effective and efficient for producing complex queries that return non-empty results for bug detection, and substantially outperforms the baselines. GDsmith successfully detects 28 bugs on the released versions of three highly popular open-source graph database engines and receives positive feedback from their developers.",2023,8,"2171106811, 49661434, 31131132, 2118207557, 144281339, 5779643, 2057038192",2171106811,Budapest,3,"48594758, 1765169, 2504776",Y,The key idea is to use a critic which randomly drops the activations in the logit and maximizes the sensitivity between two versions of discriminators.;The submitted paper shows that this principle is not a necessary condition large-scale classification.;Well structured analysis paper on shortcut connections but contributions/results are not compelling This paper performs an analysis of shortcut connections in ResNet-like architectures.,International Symposium on Software Testing and Analysis,32,"graph,database,engines,graph database,graph database","047286f5b9315a8e8bf56c4fc936e62f21495892, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 62df84d6a4d26f95e4714796c2337c9848cc13b5, 97ac11e5a6440eccb70ae7146392ac138c36fa6c, 91fc647899f801c9d351349ce73779918f90a713, 8b28792f8405b737229afb92c99c579b86d8aa98, dc10a3f4dae2db48148ff6781454ddcc856ae8da, 182180bd69ea6d2f59225ded5ddc900b8558ab9f, d1bb57da8593a2071b3ea8026865352ab3f7206a, 39602922b04885047254444fd1a1586d797617ce, fa63c3f53413ced7946623889c416e34a28676ea, 5ea7bf772fecf95cbf53b2c7f719c9440322a115, a60a4e5f7f872b9825ddff5d379857c2091ca52b, f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6, c84aa52bee5116f80c7740503edff4b08f733c3b, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, 27245e65a27bde90b5b0bb25d157bb75a0ad8b5a, f75b70c9d7078724b592ec3e21de705e7b6ff73f, 89a30b5dab02c9c390a632acad481fa602859272, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, 353c88c231ce156d604e074af276422422fc73f7, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786"
a2a514ed839dafdd0fb76d6c2615f25f35bf8087,Testing Graph Database Engines via Query Partitioning,https://www.semanticscholar.org/paper/a2a514ed839dafdd0fb76d6c2615f25f35bf8087,Conference,"Graph Database Management Systems (GDBMSs) store data as graphs and allow the efficient querying of nodes and their relationships. Logic bugs are bugs that cause a GDBMS to return an incorrect result for a given query (e.g., by returning incorrect nodes or relationships). The impact of such bugs can be severe, as they often go unnoticed. The core insight of this paper is that Query Partitioning, a test oracle that has been proposed to test Relational Database Systems, is applicable to testing GDBMSs as well. The core idea of Query Partitioning is that, given a query, multiple queries are derived whose results can be combined to reconstruct the given query’s result. Any discrepancy in the result indicates a logic bug. We have implemented this approach as a practical tool named GDBMeter and evaluated GDBMeter on three popular GDBMSs and found a total of 40 unique, previously unknown bugs. We consider 14 of them to be logic bugs, the others being error or crash bugs. Overall, 27 of the bugs have been fixed, and 35 confirmed. We compared our approach to the state-of-the-art approach to testing GDBMS, which relies on differential testing; we found that it results in a high number of false alarms, while Query Partitioning reported actual logic bugs without any false alarms. Furthermore, despite the previous efforts in testing Neo4j and JanusGraph, we found 18 additional bugs. The developers appreciate our work and plan to integrate GDBMeter into their testing process. We expect that this simple, yet effective approach and the practical tool will be used to test other GDBMSs.",2023,10,"1663632797, 2868147, 1625425838, 38319925",1663632797,Copenhagen,3,"3419650, 143884284, 1783781",Y,"There are a few notational issues in the paper that should be addressed.;The authors' technique may let us do this data-generation easily.;Creative and interesting The paper introduces an application of Graph Neural Networks (Li's Gated Graph Neural Nets, GGNNs, specifically) for reasoning about programs and programming.",International Symposium on Software Testing and Analysis,32,"bugs,query,gdbmss,graph database,graph database","5b34752817bc0d6aa96466dabcbc24a83dd071fe, 73d4accea441aae2373828a8dc2175aa2759c38f, d84cf745c534c010b8e55e5a4a04878906848dc3, 9817bf0f78047452761e950c02a1a56f59a1e593, 046eb47d56beb8069b0098e3d01608f81ebb6849, b61b260de1599e6e89491cad9160898fcd3b34c2, bd6c027a3604d6c8fa23435bf382455b2bee436b, bf69c98fca9a9f6c1cde871beddbcdc668b77771, b4c9c134ad5bd4a037115df65411b4c49abe1322, 35adeef964fd344288febc7def2780007587724f, 624b2f14be4287d6a400cdf88a6f911b434b182e, 41c93960a066876d5e4f1dacaef75cd8daa2791f, d2a505586c0da20752b98f63c7760b6a5c41e28d, afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84, 155eeede0c070f1f017ba5c9f6cacd7ae0b098aa, d8348b802c9133d9e396d4ad809b020d5be42863, f9253a3e2627f1c2a0a7e2cea320a4ec4e4d2ff9, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, 551bbf493f55ea2c7b64ef8b91fc81d7bf6d68fe, 7171a0e9b07ebc98a32eb912262613efc20f283a"
e67a2817089312746d69b38ce9abfdc4b1bc69c3,Finding bugs in Gremlin-based graph database systems via Randomized differential testing,https://www.semanticscholar.org/paper/e67a2817089312746d69b38ce9abfdc4b1bc69c3,Conference,"Graph database systems (GDBs) allow efficiently storing and retrieving graph data, and have become the critical component in many applications, e.g., knowledge graphs, social networks, and fraud detection. It is important to ensure that GDBs operate correctly. Logic bugs can occur and make GDBs return an incorrect result for a given query. These bugs are critical and can easily go unnoticed by developers when the graph and queries become complicated. Despite the importance of GDBs, logic bugs in GDBs have received less attention than those in relational database systems. In this paper, we present Grand, an approach for automatically finding logic bugs in GDBs that adopt Gremlin as their query language. The core idea of Grand is to construct semantically equivalent databases for multiple GDBs, and then compare the results of a Gremlin query on these databases. If the return results of a query on multiple GDBs are different, the likely cause is a logic bug in these GDBs. To effectively test GDBs, we propose a model-based query generation approach to generate valid Gremlin queries that can potentially return non-empty results, and a data mapping approach to unify the format of query results for different GDBs. We evaluate Grand on six widely-used GDBs, e.g., Neo4j and HugeGraph. In total, we have found 21 previously-unknown logic bugs in these GDBs. Among them, developers have confirmed 18 bugs, and fixed 7 bugs.",2022,15,"2158585032, 2964640, 134898163, 2093481779, 2131285720, 2118120527, 2152692124, 40231586, 144525882",2158585032,Sarajevo,2,"103131985, 2108097584",Y,Deep Temporal Clustering This paper proposes an algorithm for jointly performing dimensionality reduction and temporal clustering in a deep learning context.;Doing so will help better understand what is gained from using retaining a probabilistic form of memory versus a determinstic memory indexed with attention as in [Li et. al].,International Symposium on Software Testing and Analysis,31,"gdbs,bugs,query,graph database,graph database","0863a5ce955e5193e535e1442086dc460dd295f0, 9712624bb61abb0da989514cae558cfab61bb9d2, c9f320789e98d2c7a798a9705e26dbe317677966, 537f5e8e4139392cd2d108f32495e5b2b80151ac, 5031790972d496547b6613d46a4a0134c824db6e, e3b94a5f28522e6825aff16ff07d56bd70d26c96, e968ae8e98fff9e28468383a1826fca4a2ae5245, 410fba9f03212257d0881811802e6620e59bc827, 18d87bff073687c025f9bd23ab2dfb20d5f72a66, 925af6dcb0f8e6f3a5b2613400277be4b5434d10, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, b7034546bee38ba13d3b312fce893a22e33ce4dd, 7fa273f450251523e6b7fcc2eb3fdbdfd4a30493, aca6d5f3866372a4506cf15773ae298f18c3f453, 80dd97954ddf3edd22d4cb21f0ac31b7ffed6bbf, a56bf7ee9a56d8f84079684339a953c2df9ce76b, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, 2019cf49b51021a376f9833a53565513f0d8107b, da3f33d858586d24cb265e79eb54f3746e998f57, 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d, 39444c55f07839ac6a0d1839472a982f8fb447bb, 9e195234688778b2beb3528632e78dbabf816332, 25adff0bb9691b46fee5c0591300d6f3ccf117ab, aad2d03c17bc7d1e636d0e79944ad4588af989d5"
6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb,UniKG: A Unified Interoperable Knowledge Graph Database System,https://www.semanticscholar.org/paper/6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb,Conference,"Knowledge graph currently has two main data models: RDF graph and property graph. The query language on RDF graph is SPARQL, while the query language on property graph is mainly Cypher. Different data models and query languages hinder the wider application of knowledge graphs. In this demonstration, we propose a unified interoperable knowledge graph database system, UniKG. (1) Based on the relational model, a unified storage scheme is utilized to efficiently store RDF graphs and property graphs, and support the query requirements of knowledge graphs. (2) Using the characteristicset-based method, the storage problem of untyped entities is addressed in UniKG. (3) UniKG realizes the interoperability of SPARQL and Cypher, and enables them to interchangeably operate on the same knowledge graph. (4) With a unified Web interface, users are allowed to query with two different languages over the same knowledge graph and visualize query results and explanations.",2021,8,"2116441064, 122024145, 152814510, 2118153844, 2113771309, 2332105",2116441064,Valletta,3,"2109971162, 48586730, 3081813",Y,"However, how widespread is this problem across other models or are you simply addressing a point problem for RN?;It looks like the authors extract position information from flappy bird frames, so the algorithm is only using images for obstacle reasoning?;A new method for weight quantization.",IEEE International Conference on Data Engineering,37,"graph,knowledge,query,graph database,graph database","74bc39003e65119eaa6ba339a61b45b417a638b7, 2f44ab0e52eb98fdfc0086638887f175b6f2fe4b, b6a7226e5f6d618370995eccad68af195ef32da2, 8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6, 780c7ead33428d282044519fee5e773ad56d5a2c, 4a7477881b66d12e79c704805781d4683a6a6be1, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, b7a717233ec3ff37385ab1b06816d0ca375f5bb3, cb23a59fdf3ade707600f076df4ff27a03941fba, 2a15e9ac5593cd1f8bd3a724e78a173037c02fea, 7d873a9c49d3864709aa762f8740edcdbd7369c5, 047286f5b9315a8e8bf56c4fc936e62f21495892, ccd94602e3acecf999d0c9ba62b1a8bc02e9f696, a8d76d84408c1fe6b1543084e6cec3dfc4ede429, a85c45ce7c893388e8eafa8a653b042e1497db48, 1661d0d8d47cac41e01c59c60aac3675b4396698, a80e26e6365b215715c182d19a9aa8bb876ac768, f5ac3ec506a5a01807dd7196c0c52eef008b78cc, f18be38578ee52aa7071c404d42e3d53ae003122, 53b0cfa2eaaa9703df02b2ca9c43260c4de5df0a, 69a72ff5b30642d11c96635e99aadad3140d33a7, bb5d26da72bfe7030dbc6650b686b210ae661f2c, b27a78366868ca47098e00dda74dd1b167b3a80d, df1a2539afbad27c4c80115a6f8f59a089024865, 3ea34401909978d3d3d0c25c8746e02c7d2a7c77, d35ed1fcab47cf98101dc745c42d3b51dace25eb, 545f108575314031f35c617c4ac35a10133c50e3, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, 665b0c776ff7507c32793f10ce9edf90bc2f674a, 551bbf493f55ea2c7b64ef8b91fc81d7bf6d68fe, b473e91cbe80c8b46451b49153cd5f93030480ab, 70ca38ad480c0be0eca89ccc4972d6cc9a5824da, 5107c83173e43f51d1bdebf6cafda525a7c26bf0, 48265726215736f7dd7ceccacac488422032397c, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 1f8a23697562b001082b147779b5eaefd3513d0a, 9e195234688778b2beb3528632e78dbabf816332, 0c00a328fa7cd56ee60338c54e89bd48310db80b"
0a71dd8bec060195e14eb9d0a7abbc08d960d4d5,Research on Data Asset Management System of Graph Database Based on Internet of Things,https://www.semanticscholar.org/paper/0a71dd8bec060195e14eb9d0a7abbc08d960d4d5,Conference,"With the development of the times and the progress of society, the popularization rate of computer network technology and information technology in China is accelerating, and the Internet of things technology also appears in people’s vision, and is gradually known by people. In the context of this era of big data, it has brought great challenges to all walks of life. The development of everything must conform to the development theme of the times. In order to meet the challenge of the research and development of the data asset management system of the graph database in the new era, this paper puts forward the method of applying the Internet of things technology to the research and development of the data asset management system of the graph database. Combining with the foreign research and development plans of the data asset management system of the graph database, the data resources of the graph database are carried out from the platform system, the management structure and the organization arrangement Based on the research and analysis of production management system, a research scheme of data asset management system of graph database which can meet the development requirements of the new era is worked out. Through long-term research and analysis, we can find that the Internet of things technology analysis method proposed in this paper can effectively provide new development ideas for the research and development of data asset management system based on graph database under the Internet of things technology.",2021,4,"46694091, 2047926221, 2064360636, 2154976675, 48586730",46694091,Nicosia,3,"2065277797, 144782078, 2115646383",Y,"However, batch normalization only sees the variation in the activations given to it by a SPECIFIC set of weights.;Many relevant papers could be referenced in the introduction as examples of successes in computer vision tasks,  identity mapping initialization, recent interpretations of ResNets/DensetNets, etc.;This paper proposes a different approach (with could be combined with these methods) based on a new training procedure.",Journal of Physics: Conference Series,21,"development,data,research,graph database,graph database","7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, 5cb8f417d171ae329adf446820bd32d8b49d8c04, 5bedddda2aab2c5a03017d96a6dd7acf517c6961, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 86d3beff240b6c882058455e098a571de86564f5, e1e43d6bdb1419e08af833cf4899a460f70da26c, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, 00bbc94806ca0821a9c82d8aedf16f0e6263b89f, 82e1e8b222aeaca19d45375b31fdc825d1a821b8, ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba, f70b2f20be241f445a61f33c4b8e76e554760340, 5031790972d496547b6613d46a4a0134c824db6e, ca05ca5f70616456d0f2e05c4a51cbf8b8d273f7, b80e2af7ffa85fe2f02ea8390e4915d55c19d199, 9312e5efa0dcef1445d45a41771f12e2a8dc6715, 709af143f78bc62413c50ea1a7ee75b0702c4f59, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632, 5d8e450c8c7d9faa8222f1d0eaf34b01755c9eef, c9f320789e98d2c7a798a9705e26dbe317677966, 2e5d2f2dc01b150dffc163a9f457848e9b5b5c38, a3636512a48321baab95c94052de2a0a88460602, 0a829289a16ae48837cc2905635435db98bacc76, a9c1566119695250f68a572a4260b03721cc8ba3, 69a72ff5b30642d11c96635e99aadad3140d33a7, 3ec02e36cc0ba16292b255e9f90c3725521e2ec9, b2114228411d367cfa6ca091008291f250a2c490, 0863a5ce955e5193e535e1442086dc460dd295f0, df138c7425e787cac2f9d3ab7775c0fb5294a83e, 649c3497e3b34b15a5011259fcb837cf6c1ac04a"
5ea7bf772fecf95cbf53b2c7f719c9440322a115,GRANEF: Utilization of a Graph Database for Network Forensics,https://www.semanticscholar.org/paper/5ea7bf772fecf95cbf53b2c7f719c9440322a115,Conference,"Understanding the information in captured network traffic, extracting the necessary data, and performing incident investigations are principal tasks of network forensics. The analysis of such data is typically performed by tools allowing manual browsing, filtering, and aggregation or tools based on statistical analyses and visualizations facilitating data comprehension. However, the human brain is used to perceiving the data in associations, which these tools can provide only in a limited form. We introduce a GRANEF toolkit that demonstrates a new approach to exploratory network data analysis based on associations stored in a graph database. In this article, we describe data transformation principles, utilization of a scalable graph database, and data analysis techniques. We then discuss and evaluate our proposed approach using a realistic dataset. Although we are at the beginning of our research, the current results show the great potential of association-based analysis.",2021,4,"2697611, 2082303711",2697611,Zagreb,3,"49399380, 2144447082, 144782078",Y,"These five objectives together are sufficient to achieve impressive English <-> German and Engish <-> French results in Multi30k, a bilingual image caption scenario with short simple sentences, and to achieve a strong start for a standard WMT scenario.;Actually, the proof never makes any connection to optimization.;This paper introduces a method for regularizing the REINFORCE algorithm by keeping around a small set of known high-quality samples as part of the sample set when performing stochastic gradient estimation.",International Conference on Security and Cryptography,21,"data,analysis,network,graph database,graph database","43e624ddeed82df944a6cae0dedec3372438e243, 6c739540e66e895311b7347971f10ef556e06e52, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, e02f91d625cd32290d4ede0f31284da115844316, cc017a62c605a0749e35a1264a46d62e78fb68b7, 7a1e71cb1310c4a873e7a4e54d1a6dab0553adce, 8799ec4bdf98bc313aa8c41a706a385e026d1f88, 9a0965beef113cc37491004b1848149e00300561, 77a59de2e2b832321875cadcf9619dc313f02384, 3789eb72c32ecf5e33442570358dd786dd67c8a2, 99f06e88e76f1af51d08d7adfb26d758ebc6acab, b79e5e4622a95417deec313cd543617b19611bea, a85c45ce7c893388e8eafa8a653b042e1497db48, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 1cff064f815111a71a98afda7aee1867ad617901, 6a261e1e38506b0e4c113ba29a2d5e5d0709ed26, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, 4f8d648c52edf74e41b0996128aa536e13cc7e82, c9645aa4ea31903e02e201b877fd3e1466adff4f, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, 6a1b25f7a67395ad1e676027322913acbb0a0635, 549e933821fdf7cd0309dacaae99c8284cbfcc24"
09f54c64b39f5f7e7570f9f4ce3e3af544401e14,A Quantitative Analysis of Student Solutions to Graph Database Problems,https://www.semanticscholar.org/paper/09f54c64b39f5f7e7570f9f4ce3e3af544401e14,Conference,"As data grow both in size and in connectivity, the interest to use graph databases in the industry has been proliferating. However, there has been little research on graph database education. In response to the need to introduce college students to graph databases, this paper is the first to analyze students' errors in homework submissions of queries written in Cypher, the query language for Neo4j---the most prominent graph database. Based on 40,093 student submissions from homework assignments in an upper-level computer science database course at one university, this paper provides a quantitative analysis of students' learning when solving graph database problems. The data shows that students struggle the most to correctly use Cypher's WITH clause to define variable names before referencing in the WHERE clause and these errors persist over multiple homework problems requiring the same techniques, and we suggest a further improvement on the classification of syntactic errors.",2021,4,"153314895, 66327914, 2051972259, 2517099",153314895,Copenhagen,2,"1779967, 2109139810",Y,"Actually, the proof never makes any connection to optimization.;The use of the proposed gamma distribution, as a simple alternative, overcomes this problem.",Annual Conference on Innovation and Technology in Computer Science Education,26,"graph,database,students,graph database,graph database","39602922b04885047254444fd1a1586d797617ce, 182180bd69ea6d2f59225ded5ddc900b8558ab9f, 11342d45911ee8a7c9e3a94117ce774ad7036172, cb03b665069dad5e895a2c244929ea427f1fb9d1, a0f788f6de0fb83d623c875a98120e3f347f70d1, ec7f5dc077480df149bcd4358a3aa8441878ca59, 3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b, 9181b0d801dfcd7723a3ede201f0543078e2c149, e32a2519b59d62cff6cb8136ee242dc3754ed57b, a13149a80855412d970d0de2b41c611f4cf7e1da, 811df72e210e20de99719539505da54762a11c6d, 598231eb906b183f7a2a408ef4536127e11e3de9, 45de91a919780d5540872cf047986a370625e61c, 0fa554d981809c5eb78956c779f75092c4f6c16b, 643da4c4de1954daeac571a82367241db012a8bf, 63adc1e5086481e36b19b62707a96b799da51e59, bf69c98fca9a9f6c1cde871beddbcdc668b77771, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, 0ac1a1ccb16c8441f42ff8de743fc93f0e08c15c, d8a5474f450330ad25c1e22f27e88f3630cb840d, 34ca47eed139a7f0694611528f75debc43385518, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, d9d325ca670a1aa215e3e39023f8abf17dae7584, 545f108575314031f35c617c4ac35a10133c50e3, 3337e4b2e63e5e99171f9da9d161771f5810c27a, 9fcdbfdf28245010c875ce85502351fe05c04b49, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 32a849fe3020144e5ba82ba0442ac571f554ca31, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 5b6ec746d309b165f9f9def873a2375b6fb40f3d, 472644c5f4155635cf9e9e37540bfa53c20e7610, 69d49a06f09cf934310ccbf3bb2a360fa719272d, 1fa4936fb06319c3f4536c26a447d5507c92bd48, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 795550a5294eb05ea4f3b14f0b1c21a405493d85, c84389369720dcd2f004c48e58fbac2c45c8f092, 4f480bae3196dbbc27ab383bce33478ea963f9b3, fe2492b7b8cf6d1d10b7ea38e0f7f853bd679d52, 2737a61f6557fe7bf53a608c668de2eff1f582f0, c0bcd7dc9426a70af15f5ad63b4af92ea4dcbd4d, bfad52fc64ca0169644b6e7e0ea9a46470d51709, bcc82ce554942880814243fc8c08a88b9d2aad09, 0599f45e03ac2016321df0dd653ba4c0034c79d5, f70b2f20be241f445a61f33c4b8e76e554760340, 8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6, fa75a55760e6ea49b39b83cb85c99a22e1088254, 4b991efaa8493a5925c2aee9eec980831213eba6"
b0b770fb8c7760749c88e3c83ae173cdb07f7bd5,An Attack Path Generation Methods Based on Graph Database,https://www.semanticscholar.org/paper/b0b770fb8c7760749c88e3c83ae173cdb07f7bd5,Conference,"With the popularity of network technology and the expansion of network scale, the network security risks are increasingly serious. Network vulnerability assessment methods, a technology of active network security defense, have attracted many researchers. Most existing network vulnerability assessment methods store different types of data in different ways, which makes querying and analyzing inefficient, especially in the complex large-scale network environment. In order to solve this problem, this paper proposes a method of network vulnerability assessment based on graph database. The network host information, association relationship between hosts and vulnerability information of the target network are stored in the graph database, the query and analysis are carried out by using the graph database query language. Graph database stores the information of the network hosts, association relationship among hosts and vulnerabilities of the target network. The graph database query language supports querying and analysis. Visualizing the network topology, vulnerability information and all possible attack paths provides a reference to develop the network security protection strategy. Experiments' results illustrate that the method runs efficiently and helps with querying and analysis, which is applicable to large-scale complex network environment.",2020,13,"79470079, 80752053, 1596817678, 1672530059",79470079,Budapest,3,"3474704, 2108706355, 2157681212",Y,"* There are many estimators for f-divergences (like the ones cited above and many others based e.g. on nearest-neighbors) that are sample-based and thus correspond to the ""implicit"" case that the authors discuss.;For completeness, it would be great to include one more algorithm in the evaluation: an ablation of DnC which does not involve a central policy at all.;The evaluation with the sequence of checkpoints was created by using every fifth image.","2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)",4,"network,vulnerability,graph,graph database,graph database","e359e8960b0b09e8685a32927b7818f4b06ef881, 4b06c7e29280b1c6bc05c9df39023b48fef02c93, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, b7a717233ec3ff37385ab1b06816d0ca375f5bb3, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, 3dfa820702b6181c9964931f0a4d47fd298bf429, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, 453fdfeefd6498a65be339d7e8722f6f3288907e, bf69c98fca9a9f6c1cde871beddbcdc668b77771, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 69a72ff5b30642d11c96635e99aadad3140d33a7, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, a2ec47b9bcc95d2456a8a42199233e5d9129ef18, 9954ec0a95fc0840f7712d7b0c67b242bb536a9e, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, 9e195234688778b2beb3528632e78dbabf816332, d7b820af40a9e2660ef700d39f7b2e27b43435c5, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, 7fa30c8af436b7df513ef41e2cccd8a2404f37f8, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, 7904b3446775ed8c79f4f94001a16b706989c462, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, 2e7f532796eed2847d4c19e3cff03756049e81b4, f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa, 0095acc4f2c3255cf38fdf844003c97858adb418, af13a92977d4f4dc5b28b13746d86111d42939e8, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, 6c739540e66e895311b7347971f10ef556e06e52, 37b0a7a6c8fb26e32b9206847e78d521a2cd5900, 62ccd99a65bfc7c735ae1f33b75b107665de95df, 6c34842a92ce4da9aab586490afdbd8779af4eab, 0e141942fa265142f41a2a26eb17b6005d3af29e, ff7bcaa4556cb13fc7bf03e477172493546172cd, 5bedddda2aab2c5a03017d96a6dd7acf517c6961, c0e6cd2ec3bc9eb46c7d45bb708854da3327339e, 557ee73ff4e87ca0cb1b6c78af0730a2d94b710f, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, d6bc29a897fd85e7187dc33c3c974b8879462237, f476d44a4892e0c8256e50e9075f8dd3d412bfcf, 647c4a9331e01e31a4350361d3460f0397fe694f, 20a3ed03888037e2802fa9abad02ffa0a8dcc228"
b613887337a5d2e8fc8773037116be81e6346835,A1: A Distributed In-Memory Graph Database,https://www.semanticscholar.org/paper/b613887337a5d2e8fc8773037116be81e6346835,Conference,"A1 is an in-memory distributed database used by the Bing search engine to support complex queries over structured data. The key enablers for A1 are availability of cheap DRAM and high speed RDMA (Remote Direct Memory Access) networking in commodity hardware. A1 uses FaRM [11,12] as its underlying storage layer and builds the graph abstraction and query engine on top. The combination of in-memory storage and RDMA access requires rethinking how data is allocated, organized and queried in a large distributed system. A single A1 cluster can store tens of billions of vertices and edges and support a throughput of 350+ million of vertex reads per second with end to end query latency in single digit milliseconds. In this paper we describe the A1 data model, RDMA optimized data structures and query execution.",2020,22,"1790681, 1904916, 2089990776, 73527512, 2070951368, 1630331317, 1630330861, 40444389, 1630293705, 1630330434, 2238313, 40443723, 2069452048, 2111073292",1790681,Bern,2,"2108467971, 3382568",Y,"Moreover, the discrimator D  (which is trained to discriminate between real or fake examples) seems to be directly used to tell if an example is throw from the targeted distribution.;Motivation In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle.",SIGMOD Conference,20,"a1,data,rdma,graph database,graph database","1a60a9d1eef24e123c27a9eee5a399ac2b620fee, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, b3dbe1b460a3b66df1653508c9eed7dd51dee4d2, 7fa30c8af436b7df513ef41e2cccd8a2404f37f8, 6c34842a92ce4da9aab586490afdbd8779af4eab, 639bfab64e2f35917d450013e136cb24c7755fad, e7f3478fd8aac6940a4bf4f5eb60ac38f6b0b85b, 0d065e8688c38bb0148203a1738f47184a5b58d3, dadfb3ff45e19dc22456a645f441bbeb17c93c9c, 3f43bcb910df8c1a76de79057a63195e6c6bc258, 89a30b5dab02c9c390a632acad481fa602859272, 6dc4883228c95e8a332320fcc587a0ff33c84d59, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 30f233eecca2239ee1dd754914324092e53f8f19, d9a7fa7616a327367696e19b1846519745cd43ff, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, 2141334fad7248fc707607bc9453d44686ae07a7, c12b80b44d9acfe6cd92fdf965264c4b706c367c, 9c1265fd47df8d825f0ddd6d9ab834002592e38e, bad4c08f03587e38ee960e2aa76e16d722826e7c, 89af855962927fb89a673a221f6f394a6f3dfc6a, 2478f43de2f1fcd9301a81e9b42d86b1d46db02d, be383c607d4d357c763d2329ab71799c6e1393b4, 7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca, a92b103b81a48878e76f1fcfc3e2a1454f895555, 4087e84fc695bb6433d0104ee94f9d7e9f4b7da5, cb23a59fdf3ade707600f076df4ff27a03941fba, 56ae2837b6cd4d143bbfa4a4b06811d70126106f, 0c00a328fa7cd56ee60338c54e89bd48310db80b, 1b0aa15937fdf59103a5213bccf09cff83d0ee3e, c292e473b3825eeb9db03c70b2e1c033aea190d5, 6548106035c7208ad498730627874a482734b9ac, 676664ee7471738577f641e6159e7596625b7fdb, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, 2e7f532796eed2847d4c19e3cff03756049e81b4, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, 75c364909914f17791837ec88090262aa6656d3e, fb5413afba689d16543215c5a2ddbc5b78a52007, 156609022dd6258c60238859622da0a1683bd062, b266510f5f9b40d42b51884ad13a1867fb3284fd, 92930ed3560ea6c86d53cf52158bc793b089054d, 5031790972d496547b6613d46a4a0134c824db6e, aee3d7f98b966240178ef420724c840f9b61deb3, cd29c25c489562b409a60f83365f93f33ee1a0a1, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, ad10ef93675513a68b93d54f3a461160b53318a3, 98e6c6d860383fea5bbad145deed51514d23b86c, 537f5e8e4139392cd2d108f32495e5b2b80151ac"
6fb020754f6de564c3a0a07bb656c0a90be1f87d,Incorruptible Auditing: Blockchain-Powered Graph Database Management,https://www.semanticscholar.org/paper/6fb020754f6de564c3a0a07bb656c0a90be1f87d,Conference,"In modern and interconnected world, information is accumulatively stored digitally, making the process of exchanging, gathering and querying the information much easier. Continuously, it has introduced new challenges about how to ensure its consistency and reliability due to the sheer volume of data. A blockchain-based information system can provide an incorruptible record of history, enabling better auditing and data management practices. The paper describes how to combine an Exonum blockchain and a Neo4j graph database into a system that can provide a verifiable audit trail of data integrity and its modifications for information stored in a graph database.",2020,10,"2069559207, 1903751380, 40915325, 1904203865, 1904199594, 1500655986",2069559207,Bern,2,"2054451943, 1772311",Y,"MODEL & ARCHITECTURE The PATH function given a current state s and a goal state s', returns a distribution over the best first action to take to get to the goal P(A).;It might be good to emphasize that you don’t train on the IWAE bound in any experiments.",International Conference on Blockchain,20,"information,data,system,graph database,graph database","709f7a6b870cb07a4eab553adf6345b244913913, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, c2aab470b8cf92f090e0a3bac1794b21500585e6, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, 6c34842a92ce4da9aab586490afdbd8779af4eab, 410fba9f03212257d0881811802e6620e59bc827, 88932b1563316e4ed7a61c4c2c0c90d7aa9bb308, eed62d36d1b976ac3873c83645f1c25f5096f89c, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, 75ea299834d6949e89e91d006677343ddab44e49, 5290d7921f0266c8b50b79fc8a0b7d22868f4f60, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, 099043827df60225cf33c820052716cce64d49e9, f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d, 0a92bc2dc8a216e6aced83edc0358241066833df, 3337e4b2e63e5e99171f9da9d161771f5810c27a, 7998468d99ab07bb982294d1c9b53a3bf3934fa6, 957f5b1e7ca48891c2e279aefbfa0f04d989c21e, 916455d97cd792c2eb5b00663689592e25cbc8d8, cc5726fc0ebb84f741f3496a3c52ced162c596ba, f0f1627db35b4942e0f83069f20dd0948fc35d28, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, 78aab73ed574393ab421f25b3a0e3f7343e64748, 156609022dd6258c60238859622da0a1683bd062, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 96023195e889fc258e6ff30aa99d250982dfae01, 4be7d1524edb0137599a5cc95f72844b85a52fe1, ef35da3a3c471a6a15c7a3b09586483eb50cbef0, 52a6695ae1c08cc29baf764dedb5831c7a954214, 22ebfc211d184ed615729378a43fde175bf14478, 3db1841fd5f2561a11dfbd8173616b3e695c84a1, a830083704284c8c5ddaf04f676c6ce23d583942, 2c03df8b48bf3fa39054345bafabfeff15bfd11d, 07b01d665646009439ca206378cc35e095ec6cd2, f9367342405a73ab8d6de704a149babfc0edb5fe, 5778e56400f7113c2b1355fdbd6b638fa379885f, 98ce7af921e7c52d81df64d632d34eb09522cd75"
33e332837e91c1048c3ed165cd16bf7607c3bf06,Issues and Concepts of Graph Database and a Comparative Analysis on list of Graph Database tools,https://www.semanticscholar.org/paper/33e332837e91c1048c3ed165cd16bf7607c3bf06,Conference,"The work is review in nature and focuses on basic concepts and example on Graph Database with a special focus on list of standard computerized tools available for handling the queries using graph database structure. The implementation benefits of each tool and a comparative analysis on various functionalities has been presented in this work. This work also elaborates on popular Graph Databases tool that includes Allegro Graph, ArangoDB, OrientDB, Infinite Graph ,Neo4j, Titan, FlockDB, Bitsy, StarDog, MongoDB and investigate their acceptance for solving day to day problems.",2020,9,"152781515, 46501582, 105003008, 152125305",152781515,Bratislava,2,"41020222, 145929920",Y,1. I'm concerned that the contribution of this manuscript is a little incremental.;Why choose F = 10 and K = 3?,International Conference on Computational Collective Intelligence,20,"graph,work,database,graph database,graph database","9e540662619327a3056d9e40bb58058868f6f805, 9a60d3c9d3a5b7f165325fca45bd418d651682d3, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 7fa273f450251523e6b7fcc2eb3fdbdfd4a30493, 595101f13b961d69c553ce1ed24f60f3f1085e02, 7904b3446775ed8c79f4f94001a16b706989c462, 1f8a23697562b001082b147779b5eaefd3513d0a, 5d433da6d0f143f20936379910104d2bb139d4ae, eacf9284a39adcd56172665f31fd5a72560bba7a, f476d44a4892e0c8256e50e9075f8dd3d412bfcf, 2bc9d6830610e18f110aa7984f7c0271d6b17eeb, a92b103b81a48878e76f1fcfc3e2a1454f895555, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 364128bcce9836d60e685bb717b80f30e25092e0, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 0427110f0e79f41e69a8eb00a3ec8868bac26a4f, f63e917638553414526a0cc8550de4ad2d83fe7a, 51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df, 29ddc1f43f28af7c846515e32cc167bc66886d0c, bb0cec8f2d34cfb9dbf8bffd1a5de499311ae098, 834fdec542153aae5fe725df801aac87ba5e8f56, dcbaf58b16ac7ef947879ea37c021466357b291a, 3813b88a4ec3c63919df47e9694b577f4691f7e5, b9b639522465cc606df878eee62e7f9c4bf19e62, 9e195234688778b2beb3528632e78dbabf816332, 5c39e37022661f81f79e481240ed9b175dec6513, df7336844a31165db0ae08f1cd0f560c9e3faeea, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, 92930ed3560ea6c86d53cf52158bc793b089054d, 099043827df60225cf33c820052716cce64d49e9, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, e8b30ebe3351680c3b039555ae0a8d0865ad829b, 8bba999de25bfb288b3f7f88e1d907aab02638b6, 88932b1563316e4ed7a61c4c2c0c90d7aa9bb308, 2346d121f38fc19c77e0b062415519843f478163, 62df84d6a4d26f95e4714796c2337c9848cc13b5, e7b0e2710821a9fe1e3d2b9a09dd94514b2d5ea5, 6ba00c2386f2edc0b43eec442cd1923b5d964633, 795550a5294eb05ea4f3b14f0b1c21a405493d85"
72afe82af4c2ca100c36eb35292e85d806527f0a,Construction of typhoon disaster knowledge graph based on graph database Neo4j,https://www.semanticscholar.org/paper/72afe82af4c2ca100c36eb35292e85d806527f0a,Conference,"The typhoon knowledge graph can correlate various kinds of information in the typhoon data, conduct overall and related analysis, and finally provide effective assistance for typhoon prevention and post-disaster protection. The data of typhoon landing in China from 2000 to 2015 were selected to build a typhoon knowledge graph based on Neo4j graph database platform. The typhoon knowledge graph can be used to understand the occurrence of historical typhoons and obtain the distribution of typhoon data in time and space.",2020,8,"2158490269, 2118798587, 2152209915, 1877327478, 145104321, 145843448, 2157681212, 2157843743, 47149500, 2108691840",2158490269,Amsterdam,3,"145560079, 35210462, 1904916",Y,"The most important idea of the papier (PDE-net) is to learn both differential operators and the function that governs the PDE.;Considering that this is a big, complicated system, it is crucial that the authors included both an ablation experiment to see which pieces were most important, and an experiment that indicates the amount of labeled data that would be required to achieve the same results with a supervised system.;It might be worthwhile to briefly describe the encoding/construction algorithm used in the paper.",Chinese Control and Decision Conference,20,"typhoon,graph,knowledge,graph database,graph database","1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0, 9c1265fd47df8d825f0ddd6d9ab834002592e38e, fe4c5074f021cd1c810892c1b6bc267b23aa6e5c, 3dd7f7118ee174265889d00d100cfe2a02871be8, dd4d82299b4209db539d639f836fcee663cf72b3, f9c990b1b5724e50e5632b94fdb7484ece8a6ce7, fe44200fed05f9a7c656f2245deded8fd5f5e1e6, e02f91d625cd32290d4ede0f31284da115844316, eeac4411ae119c6c7ac33a11f762f2495b4dd960, e5a1cfcd07dcfd8b1feec9c635dadc858cde8166, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 1a37223175138bc1aa53b425ea2fdd0b382405a5, c48e0bd0f36c25ab83befbc7b7da369b75fd25f5, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, fac67bf55456b52ac6e4f280ad953d0250c74ebc, f4c4e148546089123f8da5db4fb246ab4062bd40, 8674494bd7a076286b905912d26d47f7501c4046, 2660dcf5bd16d14862a7bbb241fa4d85ae34327f, 1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, ec7f5dc077480df149bcd4358a3aa8441878ca59, 0cec0c296efedb814342b4b841d4583efbfc6777, aca6d5f3866372a4506cf15773ae298f18c3f453, 0258bab20bc8574ee602012081a17db89339f12d"
ce54e3b89a2570035b70885e6901ad4c92ae41c9,Construction of power projects knowledge graph based on graph database Neo4j,https://www.semanticscholar.org/paper/ce54e3b89a2570035b70885e6901ad4c92ae41c9,Conference,"In ""The Belt and Road"", China’s overseas power projects grow more and more, which is associated with a large number of dispersing project information. Construction of power project knowledge graph based on graph database Neo4j can facilitate the management of overseas power projects and to have an intuitive understanding of the relationships between projects for further overall planning. Meanwhile, enterprises can describe the spatial distribution characteristics of countries along the ""The Belt and Road"" according to the knowledge graph of overseas power projects, so as to understand the space and potential of future power investment development in different countries.",2020,6,"2129795635, 13324446, 2000860679, 2108097584, 2000992985, 2162074006",2129795635,Berlin,2,"2746913, 144310754",Y,"During research, we have multiple executable oracles and need to produce good training data from them.;Concerning the lack of mathematical rigour in the statistical physics literature on which the authors comment, they might want to relate to a very recent work https://arxiv.org/pdf/1708.03395.pdf work that sets all the past statistical physics results on optimal generalization in single layer neural networks on fully rigorous basis by proving that the corresponding formulas stemming from the replica method are indeed correct.","International Conference on Computer, Information and Telecommunication Systems",20,"power,projects,graph,graph database,graph database","e70ddfb04969b663ef8cd711a70a0acf563d6e5f, 472644c5f4155635cf9e9e37540bfa53c20e7610, 9eacf62f1e546748428c7e4843731b1595294200, bb5d26da72bfe7030dbc6650b686b210ae661f2c, 375125029b085e70a109491656b69aa01bc2a166, d9d325ca670a1aa215e3e39023f8abf17dae7584, 9712624bb61abb0da989514cae558cfab61bb9d2, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, f397b593de771752e7002a954eb531f3ef6a975e, ff75865cde62592d068b2afd055c57c81d77158b, b61b260de1599e6e89491cad9160898fcd3b34c2, 6068d39e92aef1bb0e1291e9931894c35692a85e, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, 182180bd69ea6d2f59225ded5ddc900b8558ab9f, c96fc88631f2b8e2fe192027a8a237445635328c, df7336844a31165db0ae08f1cd0f560c9e3faeea, 24ab4e99e582c9770281eee0a39cbeb70ddd891a, b3dbe1b460a3b66df1653508c9eed7dd51dee4d2, 7af07490da518c8ef3cf2ae106071df2c2d0101e, fa75a55760e6ea49b39b83cb85c99a22e1088254, d422df8bff4e677a3077635db116679d25142bfc, c2528e88d5554e9df9f9d482ad46cb5331c4d794"
b09139c153bac8893e8faea2b3a59159234caadc,A Graph Database Approach to Wireless IIoT Workcell Performance Evaluation,https://www.semanticscholar.org/paper/b09139c153bac8893e8faea2b3a59159234caadc,Conference,"The workcell is considered a main building block of various industrial settings. Hence, it is examined as a primary testing environment for studying wireless communication techniques in factory automation processes. A new testbed was recently designed and developed to facilitate such studies in workcells by replicating various data flows in an emulated production environment. In this paper, an approach to storing and analyzing network performance data from a manufacturing factory workcell is introduced. A robotic testbed was constructed using two collaborative grade robot arms, machine emulators, and wireless communication devices. A graph database approach was implemented to capture network and operational event data among the components within the testbed. A schema is proposed, developed, and elaborated; a database is then populated with events from the testbed, and the resulting graph is presented. Query commands are then presented as a means to examine and analyze network performance and relationships within the components of the network. Additionally, we demonstrate how to extract correlations between receive signal power and network delay within the testbed using the graph database query language. Finally, using the inherently interconnected nature of the graph database, we discuss applying the graph database approach toward examining more complex relationships between the wireless communications network and the operational system.",2020,5,"3420212, 2049649, 1979284, 151500725, 1784025",3420212,Athens,3,"143924672, 2677700, 2146245",Y,"A number of heuristics are used to augment this reward function so as to provide shaping rewards along the way and speed up learning.;This would contradict some previously established convergence results for this type of problems: Reddi et al. (2016) Stochastic Variance Reduction for Nonconvex Optimization, ICML and Wang et al. 2013.;After introducing the idea and related work, the authors in Section 3 give details about how to perform the quantization.",International Conference on Industrial Technology,20,"network,graph,database,graph database,graph database","e5194ae88d63c7549678b1b73cfdaf7112164272, ead6121fbc787d508dc6a6d7106f72bf0d647d03, a9640bac0b45a804d07fc5914feb08af8f2a73f2, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, 0095acc4f2c3255cf38fdf844003c97858adb418, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, 0f38b3d717e0fcc6eacc9c6e78b252227440e04e, ab606e9d148458f6d54e5d44abefd73b7990f6e0, 046eb47d56beb8069b0098e3d01608f81ebb6849, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, 59c414c9efb77562f5d1aad8af14eaac968c69c0, 312b1067d89f598c4c5c0799aa18b48d0926bed8, bf07f2927dca481653b8c60b2dc982fe4a7dfd4e, b889b1d6944213bc2ca29e3ad07ee65ede20892d, fd3259ecd1c94731f5d9bd9c0bf7417f2e147c71, cb03b665069dad5e895a2c244929ea427f1fb9d1, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, facd5f5deb152229ceb1803434d8690a09ab4129, 76b93abde3ec4e5bd84a38d488a8db209dc4ac98, d2a609ffb814442d0728aef9f6616f9cd775face, 83cebf919635504786fc220d569284842b0f0a09, 88a724083b2cfcc096448c28e6973c8f761ee463, f72053903270d9a7f41108461ad04d5aa075218d, a926093ef103c02fd5ef7310e0d41b83d1958ed5, 45674df7143e43bc589cfabd26dd194c2a7f090d, f2bc4057e696f49c326bf8e1588772a16f053754, 4f8d648c52edf74e41b0996128aa536e13cc7e82"
8ef0c1c2030aa265a4e7c836d080c2e2088efde6,(Graph Database: A Survey),https://www.semanticscholar.org/paper/8ef0c1c2030aa265a4e7c836d080c2e2088efde6,Conference,"The advantages of Relational Database Management System (RDBMS) model and design methodology are being utilized by industry/institutions for any software design and implementation. The future of RDBMS certainly will be the Graph Databases with NoSql methodologies, which is emerging as beyond of relational model. In this paper, there is a need to highlight all the databases evolved after RDBMS. They couldn’t stay in market for so long period and survey has been made to highlight those databases after RDBMS. Relational Database Management System has certain advantages like (i) Storing in Tables, Column and Rows (ii) Data Storing in Normal Form (iii) Easy to use via SQL to retrieve information via complex join operators (iv) Maintainability via Reverse Engineering (v) Indexing and quick search. Due to these inherent features of RDBMS and SQL, it is necessary to explore and compare RDBMS with NoSQL methods to avoid complex join operation. Recently, numerous software industries and research institutions are trying their old RDBMS system to be re-engineered into some other architecture via nodes, edges and relationships where different type of information can be stored easily. So, it is a big challenges for any industry and institutions how quickly they can re-engineer their old RDBMS into Graph Databases which is also called now-a-days the future of databases. In this project, it is highlighted that the importance of the re-engineering work lies in three different directions such as (i) Comparison of RDBMS with GDBMS (Graph Database Management System) where face book, twitter, Amazon, Google are adopting (ii) Survey work of Graph Databases and (iii) Graph Database Models have increasingly become a topic of interest. The representation of data in the form of a graph lends itself well to structure a data with a schema. No standard system of query languages yet had been found to have been unique and stable for graph databases. Research and industry adoptions will determine the future direction of graph databases.(iv) Beyond RDBMS artifacts were established by industry and academics. It feeds a series of recycling collectives trying to eke out an existence of positive incentives and principles.",2020,7,"2052840328, 1996173264",2052840328,Chisinau,3,"32559865, 2163313042, 9319875",Y,"The presentation of the paper could be significantly improved.;The authors show mixup provides improvement over baselines in the following settings: * Image Classification on Imagenet.;Minors: There are some mixed-up notations: tilde{A_i} => A_i , and rank(A_2) => rank(A)_2 in Proposition 3.","2020 International Conference on Computer, Electrical & Communication Engineering (ICCECE)",20,"graph,rdbms,system,graph database,graph database","6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2, ac4350d3a2673a4c34fa949714ede8b45fd04f1d, 8adb47deeef943c2c1bae41f9498a382fb818a16, c5c4142a01981787a71bf6ebcb791520c458ab5d, e5a1cfcd07dcfd8b1feec9c635dadc858cde8166, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, 71854ff4306cf65c3c2161f7be2d0346275f72d5, 0af24df95f8a0b7e6fc257a98a5820314d1243f3, c419ee7315b9edfd8fc55bab16534fc55a564fcd, a0a79dad89857a96f8f71b14238e5237cbfc4787, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, 598231eb906b183f7a2a408ef4536127e11e3de9, ce3285bf1853f00c00535325851df5c33a0fc5d6, 8c7aee0bbc062d5b4bcd34951b1e002274288206, 10a69070c8585dc5564b11b0e53dbe4d565f9ce1, 4cd033a56b19f87f6adfefeef5fcc990306ecf40, b79e5e4622a95417deec313cd543617b19611bea, 4cf2034fa55a20e60a24ca6924f66aaafb30b877"
099043827df60225cf33c820052716cce64d49e9,A Review on Graph Database and its representation,https://www.semanticscholar.org/paper/099043827df60225cf33c820052716cce64d49e9,Conference,"Extensively, facts are represented characteristically as a table for the purpose of making it indexed with increased readability. Currently, the tendencies are altering as Graph databases are rapidly attaining popularity. Actually, it is appropiately termed as ""the outlook of DBMS"". The demonstration of facts within the procedure of a graph advances within the circumstances sound for the prearranged facts through a dynamic schema. This paper discusses the backbone of graph database as to why they are gaining much popularity in present situations illustrating the dissimilar types available and their distinction. Owing to the extensive usage of graph algorithms with models, neither a standardised system nor query language has been dispossessed with graph databases. Research and industry acceptance will regulate the upcoming course of graph databases. The authors have tried in representing the graph database with a real life scenario.",2019,7,"152125305, 46501582, 2744320",152125305,Rome,2,"143666627, 143834867",Y,"What are the class you are interested in?;For example, does this observation that the local curvature of the loss around minima is different for ResNets and VGG allows to interpret the difference in their performances ?",2019 International Conference on Recent Advances in Energy-efficient Computing and Communication (ICRAECC),19,"graph,facts,popularity,graph database,graph database","c84aa52bee5116f80c7740503edff4b08f733c3b, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 76b93abde3ec4e5bd84a38d488a8db209dc4ac98, b85f3a66245d483f3eb3447eaf9950bd55f2b21e, 38f5b53b49be555430f33b8363910191a3df1d14, 6a1b25f7a67395ad1e676027322913acbb0a0635, 6e74403ab75d0080c056fd66702ab1f54f04d9b1, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 4419c5720e30d5ca5158795d4c848125650b8db1, 4087e84fc695bb6433d0104ee94f9d7e9f4b7da5, ccd94602e3acecf999d0c9ba62b1a8bc02e9f696, 16fa12ebc578df676f3dda5453ad56c15a0d6702, b69a35662a2cac38eab22f4481285116bdf8c30e, f295157f37cfb43cd8d8d2690ea124edc5ea59c2, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c, 4a7477881b66d12e79c704805781d4683a6a6be1, c5c4142a01981787a71bf6ebcb791520c458ab5d, 353c88c231ce156d604e074af276422422fc73f7, cb03b665069dad5e895a2c244929ea427f1fb9d1, d9b34c6b616f75485856794478bfbeab1ea93b81, 2e7f532796eed2847d4c19e3cff03756049e81b4, 417326e51d78ba8bd2621f23e539b41bbdd336d6, 9e195234688778b2beb3528632e78dbabf816332, 54ddb00fa691728944fd8becea90a373d21597cf, 82e1e8b222aeaca19d45375b31fdc825d1a821b8, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, 733fc094e785724621c46e20db1be69f132ad9df, 71ba8a8c75e0826f94eb53ee7a4566d4fc5b5256, c292e473b3825eeb9db03c70b2e1c033aea190d5, c5d3fbc024ad9a0d28669b6030de23fc5c3f7888, bd6c027a3604d6c8fa23435bf382455b2bee436b, 3d82552eb483e5ea84b577a0e8d5f157a6085824, a9cbbef8f4426329d0687025b34287c35bdd8b38, 07cca761749bfe21c2d096ff60f32b574d5c84c4, 68a3d32416977e88cf1bfa4ad548d403f5f089d6, 2c3eef2f17369912e330281d54b535675077e4ca, c6879e43828b293567f5e2da039d23845189d6a7, 29409efa04ac99ccf01d2a011d21d5d14e870000, f476d44a4892e0c8256e50e9075f8dd3d412bfcf"
45674df7143e43bc589cfabd26dd194c2a7f090d,Computational Modelling for Bankruptcy Prediction: Semantic Data Analysis Integrating Graph Database and Financial Ontology,https://www.semanticscholar.org/paper/45674df7143e43bc589cfabd26dd194c2a7f090d,Conference,"In this paper, we propose a novel intelligent methodology to construct a Bankruptcy Prediction Computation Model, which is aimed to execute a company's financial status analysis accurately. Based on the semantic data analysis and management, our methodology considers Semantic Database System as the core of the system. It comprises three layers: an Ontology of Bankruptcy Prediction, Semantic Search Engine, and a Semantic Analysis Graph Database system. The Ontological layer defines the basic concepts of the financial risk management as well as the objects that serve as sources of knowledge for predicting a company's bankruptcy. The Graph Database layer utilises a powerful semantic data technology, which serves as a semantic data repository for our model. The article provides a detailed description of the construction of the Ontology and its informal conceptual representation. We also present a working prototype of the Graph Database system, constructed using the Neo4j application, and show the connection between well-known financial ratios. We argue that this methodology which utilises state of the art semantic data management mechanisms enables data processing and relevant computations in a more efficient way than approaches using the traditional relational database. These give us solid grounds to build a system that is capable of tackling the data of any complexity level.",2019,8,"1394550182, 144031464",1394550182,Berlin,2,"2119407396, 1380082069",Y,1. The authors tested out this new activation function on RNNs.;ATARI 2600 games: I am not sure what state restoration is.,Conference on Business Informatics,21,"data,database,system,graph database,graph database","49fce234ad7f6d2af757f078b29c0118068075a3, eef23d76e175c0cff8e81ffcb2721c10539c8cbd, ae38dc77a962161107361f213db9216ee1274037, 27e58c9e5e6d07809a45a17675a2c7135b577881, e5d720767b7a539bb2edaa98eaf572a4506a79c6, 5f51d468ce730eeade7e9f419a1fe7152582be25, aee3d7f98b966240178ef420724c840f9b61deb3, 1ed33896e82cc3810b349cdffc2039f0b8edd82e, 784141489258258b12979061d92c1a616da26525, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 48265726215736f7dd7ceccacac488422032397c, 18d87bff073687c025f9bd23ab2dfb20d5f72a66, f497c1ece7b6f3560bb39958e2673f476d608f98, b2ee139c1a3c2c53ac2b603070bdfbcd26b50ddc, 68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a, b05306f0b142e5afb3974b1b79996e5b82653662, 5cdb31c5e3e0aa4cc2d10229793b4911f69fd78f"
a0367346bc355c36badec2d2c47ce55a320cd75e,The study on data migration from relational database to graph database,https://www.semanticscholar.org/paper/a0367346bc355c36badec2d2c47ce55a320cd75e,Conference,"Under the background of big data, using relational databases to manage massive data may have some problems just like storage capacity and query efficiency. So, there is a new kind of databases called NoSQL to store data. However, the data models of NoSQL databases are different from relation databases. In order to finish migrating historical data from relational databases to NoSQL databases, in terms of graph database in the NoSQL databases, this paper takes ER diagram as the original model, graph model as target, and makes some transformational rules by using the relationships of entities. And this paper proposes an algorithm which can finish data migration by traversing ER diagram and using the transformational rules. This method can reduce the impact of model differences between relational databases and graph databases, ensure the integrity constraint of data, and automatically complete data migration. The experimental results show the validity and correctness of the data migration.",2019,7,"71778404, 2114140713",71778404,Rome,2,"48727916, 2109512262",Y,"For instance, since two runs of node2vec will give you highly varying embeddings (depending on the initialization), you will have to run node2vec several times to reduce the variance of your resulting discretized density maps.;Moreover, the discrimator D  (which is trained to discriminate between real or fake examples) seems to be directly used to tell if an example is throw from the targeted distribution.",Journal of Physics: Conference Series,19,"data,databases,nosql,graph database,graph database","45674df7143e43bc589cfabd26dd194c2a7f090d, 11342d45911ee8a7c9e3a94117ce774ad7036172, 64c4c6cb66457c79f89ba4f2529b57bfce19ced4, 784141489258258b12979061d92c1a616da26525, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, 709af143f78bc62413c50ea1a7ee75b0702c4f59, afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84, 7c24234042988e2f820a4350f43422ed2ad6fc52, be082d70534db088315f2cc5b42c2fdcd58c1b8c, d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea, 4a7477881b66d12e79c704805781d4683a6a6be1, 9eea59c34f139f3d2153226c8cf026e975622074, 2b7f9117eb6608a58be4c078ca3d69c0e5ccb875, 31f9eb39d840821979e5df9f34a6e92dd9c879f2, 2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c, cb03b665069dad5e895a2c244929ea427f1fb9d1, a357f1ff27e184d9a5ef69e665e8ca891032bf71"
223187cf10a24b62b9b0cf5b146cc83526df2ea5,From DIKW pyramid to graph database: a tool for machine processing of nutritional epidemiologic research data,https://www.semanticscholar.org/paper/223187cf10a24b62b9b0cf5b146cc83526df2ea5,Conference,"There is an increased interest in the application of information technology to advance nutritional research. In nutrition science, a graph database enables the creation of multilateral logic relationships throughout the database, which can be used to electronically store, visualize, and scale the outputs of nutritional research. It provides a knowledge structure to standardize nutritional research outputs, which is both human- and machine-readable in a Resource Description Framework format. However, the development of various specific graph databases may cause difficulties for data integration and decrease human-readability. In this article, we propose an approach to develop a graph database according to the Data, Information, Knowledge, and Wisdom or “DIKW” pyramid for nutritional epidemiologic data. Then, authoritative ontologies are suggested to construct the nodes and edges of the graph database to facilitate data integration. Finally, the findability and re-usability of the knowledge in the graph database are showcased using the SPARQL and SQWRL query languages.",2019,7,"2604647, 1737629, 2016236",2604647,Podgorica,3,"3144356, 48741177, 2093582149",Y,"This is demonstrated in comparison to weight normalization in Figure 4.;Interestingly, DQN + heuristic reward approaches expert performance while behavioral cloning never achieves expert performance level even though it has actions.;The last part contains a discussion concerning the extent to which it is actually a desired or a undesired result in classical deep learning use-cases, and the authors provide intuitive conditions under which the convergence would not hold.",2019 IEEE International Conference on Big Data (Big Data),19,"graph,database,data,graph database,graph database","340f48901f72278f6bf78a04ee5b01df208cc508, a60a4e5f7f872b9825ddff5d379857c2091ca52b, 10a69070c8585dc5564b11b0e53dbe4d565f9ce1, d1ae4ab5047489c2b010c7ce72262982ad66ad60, da3f33d858586d24cb265e79eb54f3746e998f57, ec2f9076448ba25a225618603adde60caa76c4df, f69d06037134ab6fb65d90f5ac192cf9f55e498d, 150f95f9c73820e0a0fa1546140e9f2bdfd25954, 017010b941d902a467f6d329ae5e74fd67e67912, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 147c868b721c8d29df7c61db7f2360114c760614, 218062f45c15f39bc8f4fb2c930ddf20b5809b11, 5d24ed8942235324512d6cedfd8dbf54c57658b4, 89a30b5dab02c9c390a632acad481fa602859272, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, 529ff7d6441d244212cf2becafd12a7e67ac56d9, d88083e37c44461ce3e404bd57257cd3edb07d4e, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, 0a71dd8bec060195e14eb9d0a7abbc08d960d4d5, 7af07490da518c8ef3cf2ae106071df2c2d0101e, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, 8fd8cb4951ae9634a378383a880fbf16ac5d6926, c4cdf2e0d89aadb13bf9c61654ff9e17be2b01b9, b9b639522465cc606df878eee62e7f9c4bf19e62, 2c03df8b48bf3fa39054345bafabfeff15bfd11d, b781fb7f3725a9d899d3d250b378d729a8a00442, 37b0a7a6c8fb26e32b9206847e78d521a2cd5900, 8388f1be26329fa45e5807e968a641ce170ea078, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 2478f43de2f1fcd9301a81e9b42d86b1d46db02d, 742747c7a453b293352b772d0d99541c96a351c3, cde39ce861e4c7514ee07fd91b6b8aac50cbf01b, 8b417c2be7a7707f372049fb1193f0d42f799562, f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa, aa6c2afadd660fe4efbac699f7854e8f6f240c38, 29279e52008848ee494f5af1b836313ab99c25ed, f51bc74814a3452009ea5ca262d9768d08149ee6, 58ed1fbaabe027345f7bb3a6312d41c5aac63e22, 74bec51a66499fcfbced16ff3fce696acf98c9e1, 53b0cfa2eaaa9703df02b2ca9c43260c4de5df0a"
10b4b926904ad153f791ec680218e1610747a0c8,SQL Database with physical database tuning technique and NoSQL graph database comparisons,https://www.semanticscholar.org/paper/10b4b926904ad153f791ec680218e1610747a0c8,Conference,"Relational databases are used in many organizations of various natures from last three decades such as Education, health, businesses and in many other applications. SQL databases are designed to manage structured data and show tremendous performance. Atomicity, Consistency Isolation, Durability (ACID) property of Relational databases is used to manage data integrity and consistency. Physical database techniques are used to increase the performance of relational databases. Tablespaces also called subfolder is one of the physical database technique used by Oracle SQL database. Tablespaces are used to store the data logically in separate data files. Now-a-days huge amount and varied nature (unstructured and semi structured) of data is generated by the various organizations i.e., videos, images, blogs etc. This large amount of data is not handled by the SQL databases efficiently. NoSQL databases are used to process and analyze the large amount of data efficiently. Four different types of NoSQL databases are used in the industry according to the organization requirement. In this article, first, we do the physical database tuning of the Oracle Relational database and then compared with NoSQL Graph database. Relational database performance is increased up to 50% due to physical database tuning technique (Tablespaces). Besides, physical database tuning approach of relational database NoSQL graph database performed better in all our proposed scenarios.",2019,21,"51488437, 2056158839, 2151264132, 2061173271",51488437,Luxembourg,3,"2079275650, 101370046, 150270469",Y,Why choose F = 10 and K = 3?;Other comments: - your notation is quite sloppy and may have lead to errors.;Reproducibility in continuous control is particularly problematic.,"2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)",3,"database,data,databases,graph database,graph database","8674494bd7a076286b905912d26d47f7501c4046, 96b51d940653710f9d099d89ade86b44fa9bdd8a, beb890d47bbc21a96967f9993c9d6e15686b2eac, 7904b3446775ed8c79f4f94001a16b706989c462, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, fd3259ecd1c94731f5d9bd9c0bf7417f2e147c71, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, 92912dd895c360f01a6be9c9f6d207642139525e, ade9a900acc3c138021070537840488526796d35, 9db0247728950788a2b42097d81dc0e24eed6bb2, 63316bb5b88d362051c048e864c3ae5d97a26d30, 784141489258258b12979061d92c1a616da26525, 3d82552eb483e5ea84b577a0e8d5f157a6085824, 6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5, 709af143f78bc62413c50ea1a7ee75b0702c4f59, b61b260de1599e6e89491cad9160898fcd3b34c2, 43eea2a73997294193228d50f9ff25fc5345664b, f9367342405a73ab8d6de704a149babfc0edb5fe, a1ef4052acb63356928bb440874c470ad48cb40c, 11cf88dce827bd67cbfa60400306318022e736d5, 01bf0e83159712fbbbd12171a7e268547a4cfbc5, ab606e9d148458f6d54e5d44abefd73b7990f6e0, 627d7d631fd4e0e2179f82199f014deb7ff0ea0b, 7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2, 9712624bb61abb0da989514cae558cfab61bb9d2"
32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b,Graphflow: An Active Graph Database,https://www.semanticscholar.org/paper/32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b,Conference,"Many applications detect the emergence or deletion of certain subgraphs in their input graphs continuously. In order to evaluate such continuous subgraph queries, these applications resort to inefficient or highly specialized solutions because existing graph databases are passive systems that only support one-time subgraph queries. We demonstrate Graphflow, a prototype active graph data-base that evaluates general one-time and continuous subgraph queries. Graphflow supports the property graph data model and the Cypher++ query language, which extends Neo4j's declarative Cypher language with subgraph-condition-action triggers. At the core of Graphflow's query processor are two worst-case optimal join algorithms called Generic Join and our new Delta Generic Join algorithm for one-time and continuous subgraph queries, respectively.",2017,108,"98182097, 32455748, 10754597, 2115896116, 1783781",98182097,Riga,3,"3011964, 113398129, 1719124",Y,"I recommend producing another new figure of doing such comparison.;At test time, (if I understand correctly, please correct me if I haven't), the model is evaluated by having multiple copies of the same test point within an episode.;I was not able to imagine a reasonable setting where we would have access to a reward function of this form without input/output examples.",SIGMOD Conference,17,"subgraph,queries,graph,graph database,graph database","213a83e61e1347ffa58da9383a4bf92f4a77a6c8, a80e26e6365b215715c182d19a9aa8bb876ac768, ff7bcaa4556cb13fc7bf03e477172493546172cd, 82d2b9d09cc339fdeac05abfb8a31f9c6eace948, a0367346bc355c36badec2d2c47ce55a320cd75e, ef25b02f3be31c699255ee05aa90a4a17461d95d, a81ba6a07bf7a2ecff871e3362a77404501d0927, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 3faeb21fe256b99391d69570053a8c2d91e9f348, 84725855d10b531eb8cbe54935dda0440c2fc750, c0aec04ee86c0724d61c976f19590fbe9c615723, e968ae8e98fff9e28468383a1826fca4a2ae5245, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, 7637ed79d30d0139901175ae4abedd822c217ab4, af9280741ef627f0d6c8437605d002d3bfc2d1b1, db6084fdb3baceddacdc726474722debe1ef7e65, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, 82973c5f56681190a0dbb4c4449ed60d5f805135, 9eacf62f1e546748428c7e4843731b1595294200, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 16fa12ebc578df676f3dda5453ad56c15a0d6702, ada0b87cd5c30d31186c38fb12e631d29426a3bf, 7bc9607c5cf3fc817675d46844f529097d579514, 8d67b76222d84dcd337b8a2c78f13837070a79ce, f381c53aeb7742e4047d06d84f9e0c4f523231a3, da8b317b99c4b8933b2c59285639eca6c3fcb869, e4788ee4f5e90c6f42cedc5116acd2d6475c3180, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, b6a7226e5f6d618370995eccad68af195ef32da2, 19cf7458db4e17c7504eee24ccf961e1dc91435c, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, c9a9517c8b867187b4f4c0c37cbc65263ea41d25, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, 59c2968fb9672a7152c52127255d8f0784bc2368, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, eacf9284a39adcd56172665f31fd5a72560bba7a, af13a92977d4f4dc5b28b13746d86111d42939e8, 1cff064f815111a71a98afda7aee1867ad617901, 2a15e9ac5593cd1f8bd3a724e78a173037c02fea, d0238f2fa88b0316ab862f843a5521eff0fd9bf5, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, 9a60d3c9d3a5b7f165325fca45bd418d651682d3, a6bba5ce9867c978210e3d056691b5c1e769b760, 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d, 2dafea864f74a477414c3b71b742f7997e216102, 6d2d6707a0674ee41d539d106e6cb359a0a6ca06, 2ee03e28208a9310a9be4032c2b04ebdddb83cc7"
31d7d7b9c7b776c639316027e6ae5f2ff2673da2,Fast Dual Simulation Processing of Graph Database Queries,https://www.semanticscholar.org/paper/31d7d7b9c7b776c639316027e6ae5f2ff2673da2,Conference,"Graph database query languages feature expressive yet computationally expensive pattern matching capabilities. Answering optional query clauses in SPARQL for instance renders the query evaluation problem immediately PSPACE-complete. Light-weight graph pattern matching relations, such as simulation, have recently been investigated as promising alternatives to more expensive query mechanisms like, e.g., computing subgraph isomorphism. Still, pattern matching alone lacks expressive query capabilities: graph patterns may be combined by usual inner joins. However, including more sophisticated operators is inevitable to make solutions more useful for emerging applications. In this paper we bridge this gap by introducing a new dual simulation process operating on SPARQL queries. In addition to supporting the full syntactic structure of SPARQL queries, it features polynomial-time pattern matching to compute an overapproximation of the query results. Moreover, to achieve running times competing with state-of-the-art database systems, we develop a novel algorithmic solution to dual simulation graph pattern matching, based on a system of inequalities that allows for several optimization heuristics. Finally, we achieve soundness of our process for SPARQL queries including UNION, AND and OPTIONAL operators not restricted to well-designed patterns. Our experiments on synthetic and real-world graph data promise a clear gain for graph database systems when incorporating the new dual simulation techniques.",2018,8,"3304707, 3245041, 79691050, 77790220, 1720266",3304707,Rome,2,"2145734797, 2086632521",Y,- I believe that different Monte Carlo (or Quasi-Monte Carlo) strategies can be applied in order to estimate the integral (expected value) in Eq.;Pros: - A new GAIL formulation for saving on interaction data.,IEEE International Conference on Data Engineering,35,"graph,query,pattern,graph database,graph database","f69d06037134ab6fb65d90f5ac192cf9f55e498d, 709f7a6b870cb07a4eab553adf6345b244913913, 82973c5f56681190a0dbb4c4449ed60d5f805135, fc32074b37a6d9dda535a70f9689022e70508520, d2efa91bd7d076a66bc1d1c570f6aa2da944dd88, ae026f29c2d571871f426ff4873d43b4ff90b9ad, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, 0e779fd59353a7f1f5b559b9d65fa4bfe367890c, 8fd8cb4951ae9634a378383a880fbf16ac5d6926, 0f74e7b650f346676b12c44d16d774fda9a45c9a, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, 91b9d3ab7532ea24ae70cd726355f25235b1fe8b, 6745a82c9236f0eec576904eb50ea700ca5a7d7c, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 597bd2e45427563cdf025e53a3239006aa364cfc, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c, cbad0923db89f23febcbd6192ff4149289ff2ad9, 63d8426ba1f51a8525dd19fd8ec92934ec71aea5, 75ea299834d6949e89e91d006677343ddab44e49, d78e61d0fe29b823f9630ccfa647c3029ec21f2e, 3adb779bb37d22e3aa299364c2a337603801ca5c, 5c45a5d05ac564adb67811eeb9d41d6460c70135, 740b5dec469e6b54e5e191a577a9b6a6ad8562e3, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, 03532123ccffae8d411264320e8a5ae2b6eddea0, ce9ca56036307217ea565644d3d3bd74b879e045, 915ab7b6ca3633230403d47dbb31cf74888cc5c9, adc180e1fe404b650fca3bb7970e43bdce34a611, 8dd0c1e955c66092ff951941a151336211e6e171, 624b2f14be4287d6a400cdf88a6f911b434b182e, bd6c027a3604d6c8fa23435bf382455b2bee436b, 31a61d009442436d04b9d4e1c5beee37172289ae, a1ef4052acb63356928bb440874c470ad48cb40c, 933baeec555352784848a93284c9dd0e79477759, 41c93960a066876d5e4f1dacaef75cd8daa2791f, e968ae8e98fff9e28468383a1826fca4a2ae5245, ce7499d6862df8269c655220049c3ed20b9b6f5e"
746a81aa26d3ebfb81acfd6af958d6a21603cd21,Design and Implementation of Movie Recommender System Based on Graph Database,https://www.semanticscholar.org/paper/746a81aa26d3ebfb81acfd6af958d6a21603cd21,Conference,"with the continuous development of Internet technology, information overload is becoming more and more serious. It's getting harder to get useful information from the network. Although the search engine can help users find information they need from the vast amounts of information in a certain extent, but cannot completely solve the problem of information overload, when users cannot accurately describe the information they need, you need to recommend system to help users find valuable information for users. So recommender systems are becoming more and more important. The movie recommender system implemented in this paper is based on the traditional user-based collaborative filtering algorithm, and the user project scoring matrix is pre filled. At the same time, database technology of this system uses graph database which is good at dealing with complex relations. In data visualization, the degree of recommendation of a movie is expressed by the size of the node and the thickness of the edge, so as to improve the user experience.",2017,17,"46255467, 49673164, 2112638989, 34701398",46255467,San Marino,3,"30671790, 2162042348, 1729109",Y,"The paper makes some bold claims.;Without this baseline, it is hard to tell whether GAN training is even useful.;3. There is a lack comparison to other methods such as Shaham et al. (2017).",Web Information System and Application Conference,14,"information,users,system,graph database,graph database","3087b58cbfc6eb4a3076a180e21d6b872293f9a8, c5c4142a01981787a71bf6ebcb791520c458ab5d, eed62d36d1b976ac3873c83645f1c25f5096f89c, 0574a3abc98f0e6bd035a4ff4ea107cfba45d3d4, ca011427853d34ce4ec9ccafde8a70c9eacc3e21, 597bd2e45427563cdf025e53a3239006aa364cfc, bd6c027a3604d6c8fa23435bf382455b2bee436b, e359e8960b0b09e8685a32927b7818f4b06ef881, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, 811df72e210e20de99719539505da54762a11c6d, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, df138c7425e787cac2f9d3ab7775c0fb5294a83e, 98e6c6d860383fea5bbad145deed51514d23b86c, da5d78b3e3a1544fde98fba86088e1215e97cbe8, f18be38578ee52aa7071c404d42e3d53ae003122, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, d7fddafbbc372da4fa884f67bdc32db71b888806, 8ba8a0d18a06752f5a39996ccf1e914da0941443, 185e7d2a761594451b02ace240356dadad2aef78, d1e701665e73faa648cb15473952576f40e8e122, 287a7da1801a07cf7fd85ffcc23c79504876ecc0, 4e13a8e8ba8d33e15ed037bfca7c651047533990, ce2d5b5856bb6c9ab5c2390eb8b180c75a162055, b889b1d6944213bc2ca29e3ad07ee65ede20892d, f3b8a3ffa0bdfe5578e0f0f44ea3b66cad38c032, 10b4b926904ad153f791ec680218e1610747a0c8, 5dfde01d761d97c3a6c609007531973eb1229d09, 697f2f3598057cd17cff7749d768cae0993c6727, 25761ba4bdc054bfe902fe7c5d6338be6d00d491, b7034546bee38ba13d3b312fce893a22e33ce4dd, f4c4e148546089123f8da5db4fb246ab4062bd40, 0ae18b28d8dc00cd4641488084ead5df2a449c89"
c6879e43828b293567f5e2da039d23845189d6a7,"Managing Cyber Threat Intelligence in a Graph Database: Methods of Analyzing Intrusion Sets, Threat Actors, and Campaigns",https://www.semanticscholar.org/paper/c6879e43828b293567f5e2da039d23845189d6a7,Conference,"Efforts to cope jointly with the ever-increasing number of breach incidents have resulted in the establishment of the standard format and protocol and given birth to many consultative groups. In addition, various channels that distribute Cyber Threat Intelligence information free of charge have emerged, and studies on utilizing such channels have spread. As the market for sharing information professionally is expanding, the need to manage the shared information in various ways in order to achieve better result has arisen. This paper proposes a standardized management structure and method based on the standardized format and a meaning and standard of Cyber Threat Intelligence that can be shared outside when loading OSINT information collected from various channels into the graph database. This paper also proposes a method of supporting the detection provided by existing security equipment with the information saved in the graph database and an effective method of analysis. Lastly, the paper discusses the advantages that can be expected from saving cyber threat information in the graph database developed using information collected from the outside.",2018,10,"2108129412, 9460711, 153378387, 2151900144, 2109139810",2108129412,Valletta,3,"2059129841, 1713648, 152125305",Y,"The experimental results seem promising, although not earthshattering.;Perhaps one could use a different (less striped) animal, e.g. raccoon.;Without experiments over other datasets and wall clock time results, it is hard to appreciate the significance of this improvement.",International Conference on Platform Technology and Service,18,"information,channels,cyber,graph database,graph database","be2b0396de9431bae931642516a1d3e4906329f5, 598231eb906b183f7a2a408ef4536127e11e3de9, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, a2e667e4382aaa8e02a17d0522c1a910790ab65b, 2af8907d4a974ae41044581f5e5d67317cb08568, 14fe35149aed6a47b6ebfd207deb7681b9446bb6, 7637ed79d30d0139901175ae4abedd822c217ab4, 7171a0e9b07ebc98a32eb912262613efc20f283a, c24d47ff95cd4bda073c75ec24ececaa3b10c995, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, ed935c6b359a7a486c28240d796e84897d095125, e2e7d964c09e27d334fcb8761d69918630629387, 152877c51df17cdd4a87d19e452c6daecfadf6c3, 1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7, 52e510271b172d098ec9b107a4159216ec08527e, 5031790972d496547b6613d46a4a0134c824db6e, b69a35662a2cac38eab22f4481285116bdf8c30e, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 6628f9ee35e36cdfdcac8a46cef4dba8d529a83b, 22c141b489e6e189f5996537b0a908fc10f90de7, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, 7904b3446775ed8c79f4f94001a16b706989c462, b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1, 518a7c79968a56d63a691d42f8378be6c776167e, 9eea59c34f139f3d2153226c8cf026e975622074, 3faeb21fe256b99391d69570053a8c2d91e9f348, 02a1e8e77f501675945890df45fbdc11726cb0ba, 9727206903eb40d4fa42606711bad3402f2ba9aa, 00d1f3423a33f73ca6aee884a58834547475d2f0, 22cba8f244258e0bba7ff4bb70c4e5b5ac3e2382, 925af6dcb0f8e6f3a5b2613400277be4b5434d10, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, 43e6e8d6663d83f1b74cf5a2be7b040b0928f867, 9b3e8d202488dc29e601fc471a25a2af9002659e"
9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a,Graph Database for Recipe Recommendations,https://www.semanticscholar.org/paper/9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a,Conference,"Graph databases represent a paradigm shift from relational databases with a strong support for “ relationships”. As compared to relational databases which compute relationships at runtime, graph databases persist relationships for fast querying and data retrieval. This work presents a recipe recommender as a graph database, Neo4j application. Given any set of ingredients, this application recommends a variety of recipes with the help of a data set containing thousands of ingredients. Further based on availability of ingredients with a user, this application helps discover the list of possible dishes with these ingredients. In order to implement this application, ingredients and recipes have been crawled from cookery based websites using Python scripts. The crawled data has been inserted into the Neo4j database and subsequently inter-relationships between ingredients and recipes nodes have been analyzed. Execution of self designed queries has verified the time-efficiency of the proposed approach.",2018,4,"82008243, 150281558, 3357166, 2081215",82008243,Podgorica,3,"1753210, 40071013, 49627183",Y,"A temporal clustering model and a DCNN decoder are applied on the encoded representations and jointly trained.;This paper shows an observation of “super-convergence” when training resnet with cyclical learning rates but does not provide conclusive analysis or experiment results.;PSNR is evaluated on predicted frames -- PSNR does not appear to be explicitly defined but I am taking it to be the metric defined in the 2nd paragraph from the bottom on page 7.  The new model ""EEN"" is compared to a deterministic model and conditional GAN.  The GAN never seems to perform well -- the authors claim mode collapse, but I wonder if the GAN was simply hard to train in the first place and this is the key reason?","2018 7th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",7,"ingredients,application,graph,graph database,graph database","b27c98d8378848f2c23a067f2c5196f3b5a07572, 57a3fc6d0aaad3c10c793b4e59390ca04c935282, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, b415e836d447ea9efb7629a1de67cd2a6f9e7ba8, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, 332e0eab5fba8e6940f3e481f542a99ac17b9717, 3a58efcc4558727cc5c131c44923635da4524f33, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, 28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8, 024006d4c2a89f7acacc6e4438d156525b60a98f, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, 10aa2be24951e6de76b630482a645d79354c4cde, 430f3c265935abb45bc84f3ae81c570ef778aac0, be2b0396de9431bae931642516a1d3e4906329f5, d916776e0c6a04b0def4c22257c188776c2edab2, af9280741ef627f0d6c8437605d002d3bfc2d1b1, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, 033a50c4515b153b6e706018075c333c64981fd7, 2e5d2f2dc01b150dffc163a9f457848e9b5b5c38"
fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53,GeaBase: A High-Performance Distributed Graph Database for Industry-Scale Applications,https://www.semanticscholar.org/paper/fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53,Conference,"Graph analytics have been gaining tractions rapidly in the past few years. It has a wide array of application areas in the industry, ranging from e-commerce, social network and recommendation systems to fraud detection and virtually any problem that requires insights into data connections, not just data itself. In this paper, we present GeaBase, a new distributed graph database that provides the capability to store and analyze graph-structured data in real-time at massive scale. We describe the details of the system and the implementation, including a novel update architecture, called Update Center (UC), and a new language that is suitable for both graph traversal and analytics. We also compare the performance of GeaBase to a widely used open-source graph database Titan. Experiments show that GeaBase is up to 182x faster than Titan in our testing scenarios. We also achieves 22x higher throughput on social network workloads in the comparison.",2017,12,"2677700, 7806657, 3358986, 1943322867, 2144151933, 2108966388, 2114147314, 24812041, 2218569240",2677700,San Marino,2,"46175739, 2065041692",Y,4. The localization performance of the proposed attention mechanism is evaluated by weakly-supervised semantic segmentation tasks.;The authors do provide a novel formulation and demonstrate the gains on a variety of concrete problems taken form the literature.,International Conference on Advanced Cloud and Big Data,17,"graph,data,geabase,graph database,graph database","cbc1e8bbfe98f94c0d13d111b824cf603b62712c, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632, d1bb57da8593a2071b3ea8026865352ab3f7206a, 1bc34cb22131554ba18f6ba9e6ede5beb42939f1, ef25b02f3be31c699255ee05aa90a4a17461d95d, 71ba8a8c75e0826f94eb53ee7a4566d4fc5b5256, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 9e3816be8cf4821d74e258de10ee471382936a30, ce7499d6862df8269c655220049c3ed20b9b6f5e, 96618554bd3c3e27be8071a7a636b7e29475fa73, 4a7477881b66d12e79c704805781d4683a6a6be1, b69a35662a2cac38eab22f4481285116bdf8c30e, 3cfe075af77bf0364e6ddecb3d223960d06e8927, 598231eb906b183f7a2a408ef4536127e11e3de9, 627d7d631fd4e0e2179f82199f014deb7ff0ea0b, c07802ed8a25998e9bd44ee1ddbcc63b7eb34060, 3be6a57d6db95bba2962a1f3476414a0a9b230b5, c84aa52bee5116f80c7740503edff4b08f733c3b, 364128bcce9836d60e685bb717b80f30e25092e0, 3adb779bb37d22e3aa299364c2a337603801ca5c, be383c607d4d357c763d2329ab71799c6e1393b4, db4cf9f6a653d5c15973e836c800ea47743251ae, e02f91d625cd32290d4ede0f31284da115844316, dd4d82299b4209db539d639f836fcee663cf72b3, 3b87dafd5a412e25e06761f181ec199ca88a7398, 82870bc488b57cdf5ea62877109a7278af2926b3, 57a3fc6d0aaad3c10c793b4e59390ca04c935282, 4e746359afd6f81705b875d71cc499b904a320df, baafed5f8968118af04dbbb1cf172f1c10bede25, b266510f5f9b40d42b51884ad13a1867fb3284fd, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, 3c68025d95970a9b9aa1b742a678704cd09d2bf4, 156d8e2aa90b5ccc9be10477ca70deaad0151387, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, 41d6b6bd4bf44bf13b9157b603e33360e5e77a01"
7676c02ea839ff1ceb6e5e1427c42bc45e169bde,The Spatio-Temporal Data Modeling and Application Based on Graph Database,https://www.semanticscholar.org/paper/7676c02ea839ff1ceb6e5e1427c42bc45e169bde,Conference,"Traditional spatio-temporal data model (STDM) is based on relational database, it is hard to convert problem domain model to relational model, which result in complicated query and low expansibility. In this regard, we propose the spatio-temporal data model based on graph database. The data model integrates TGIS's three key elements: space, time and attributes, and expressed spatio-temporal characteristics of TGIS explicitly. Finally, this paper gives a particular description of logistics distribution route optimization. Experimental results show that the model is proved to be appropriate for expressing the spatio-temporal process of world.",2017,9,"8507683, 2145295170, 2145734797, 2053831005, 48152160",8507683,Sarajevo,2,"1410127739, 2120664",Y,"Originality and Significance:  The area of relation extraction seems to me to be very important and probably a bit less intensively worked on that it should be.;Firstly, the paper has a fatal mathematical flaw.",International Conference on Information Science and Control Engineering,4,"model,data,database,graph database,graph database","b2ee139c1a3c2c53ac2b603070bdfbcd26b50ddc, 2c3eef2f17369912e330281d54b535675077e4ca, 239bf45c13b3f6d38c74026b535f785febf9cd08, 27e58c9e5e6d07809a45a17675a2c7135b577881, d1206ccabd1980848f14472d6548251c2fab7963, 73d4accea441aae2373828a8dc2175aa2759c38f, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, 8d0f755dea90f35f4b126a01fa3cce96b3bdd344, bc00ff34ec7772080c7039b17f7069a2f7df0889, 82e1e8b222aeaca19d45375b31fdc825d1a821b8, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, 794b3ffd28d28606230efc975eeec9f0522fb139, ea160adc0d78e54669281b8b145bcd832e648fee, 9e3816be8cf4821d74e258de10ee471382936a30, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c"
704011527f183b561ea6a75b21e4cefe5aa77fca,Book recommendation using Neo4j graph database in BibTeX book metadata,https://www.semanticscholar.org/paper/704011527f183b561ea6a75b21e4cefe5aa77fca,Conference,"In digital era, book has an important role in life. There are not only a lot of books for different purpose. But also, there are many book metadata which can use for another reason, such as book recommendation. By processing the book metadata, an information can be given to user that needs book recommendation. By combining BibTeX book metadata and Graph Database from Neo4j, data from metadata can be processed. Then, with cypher query by inputting author's parameter or book type's parameter, user can get book recommendation based on their input's criteria. The result is exactly the same with process the metadata manually in relational database. Neo4j, from this paper, takes 180 milliseconds to execute cypher query with author's criteria and takes 184 milliseconds to execute cypher query with book type's criteria.",2017,11,"121066428, 2803317",121066428,Nicosia,3,"2138053020, 1628391446, 150341221",Y,"In section 4, authors claim that their results are competitive with the best published results for a similar number of parameters.;Their abstract also claims to utilize a convex programming formulation.;The details of their proposed method are covered in Algorithm 1 on Page 12, where an additional GAN (generative adversarial network) I_{\gamma}, which can be regarded as the inverse function of the original GAN G_{\theta}, is trained to learn a map from the original input data space to the latent z-space.",International Conference on Science in Information Technology,3,"book,metadata,recommendation,graph database,graph database","89a30b5dab02c9c390a632acad481fa602859272, 94cb5503b191815ce77b19147d96c6fbd68f06bf, 7d873a9c49d3864709aa762f8740edcdbd7369c5, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 7bb477077968d68aa7a6059d8d6d801fb28274da, 8b417c2be7a7707f372049fb1193f0d42f799562, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, 6068d39e92aef1bb0e1291e9931894c35692a85e, 024006d4c2a89f7acacc6e4438d156525b60a98f, 087dd95e13efd47aef2a6582e6801b39fc0f83d8, 97ac11e5a6440eccb70ae7146392ac138c36fa6c, 6c739540e66e895311b7347971f10ef556e06e52, 92912dd895c360f01a6be9c9f6d207642139525e, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 8dce62b18bdea587c07cb4769a05ca0a816d09ba, 68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a, 4267178106cef2e77284bde309dfaaf9fd46a91b, a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854, 84725855d10b531eb8cbe54935dda0440c2fc750, 639bfab64e2f35917d450013e136cb24c7755fad, 665b0c776ff7507c32793f10ce9edf90bc2f674a, a80e26e6365b215715c182d19a9aa8bb876ac768, 87048df918c34b662bc0d28894efa430d70a9206"
b889b1d6944213bc2ca29e3ad07ee65ede20892d,An X10-Based Distributed Streaming Graph Database Engine,https://www.semanticscholar.org/paper/b889b1d6944213bc2ca29e3ad07ee65ede20892d,Conference,"Streaming graph data mining has become a significant issue in high performance graph mining due to the increasing appearance of graph data sets as streams. In this paper we propose Acacia-Stream which is a scalable distributed streaming graph database engine developed with X10 programming language. Graph streams are partitioned using a streaming graph partitioner algorithm in Acacia-Stream and streaming graph processing queries are run on the graph streams. The partitioned data sets are persisted on secondary storage across X10 places. We investigate on the use of three different streaming graph partitioner algorithms called hash, Linear Deterministic Greedy, and Fennel algorithms and report their performance. Furthermore, to demonstrate Acacia-Stream's streaming graph processing capabilities we implement streaming triangle counting with Acacia-Stream. We present performance results gathered from Acacia-Stream with different large scale streaming data sets in both horizontal and vertical scalability experiments. Furthermore, we compare streaming graph loading performance of Acacia-Stream with Neo4j and Oracle's PGX graph database servers. From these experiments we observed that Acacia-Stream's Fennel partitioner based graph uploader can upload a 948MB rmat22 graph in 1283.42 seconds which is 38% faster than PGX graph database server and 12.8 times faster than Neo4j database server. Acacia-Stream's Streaming Partitioner's batch size adjustments based optimizations reduced the time used by the network communications almost by half.",2017,4,"2741023, 35709316, 35433878, 49627183, 35367497, 1971912, 2231831",2741023,Stockholm,3,"2125957, 153693432, 1500655986",Y,"Since any CoffeeScript programs can be compiled into the corresponding Javascript programs, we should assume that CoffeeScript is the only subset of Javascript (without physical difference of syntax), and this translation task may never capture the whole tendency of Javascript.;The fact that the proposed technique is simple yet yields such speedups is encouraging.;1. The authors tested out this new activation function on RNNs.",International Conference on High Performance Computing,24,"graph,acaciastream,data,graph database,graph database","2b7f9117eb6608a58be4c078ca3d69c0e5ccb875, 916455d97cd792c2eb5b00663689592e25cbc8d8, a620ea8654f86b282e0d2c1fbc6616b90ca45bd1, 5ea7bf772fecf95cbf53b2c7f719c9440322a115, 799d5a8271887adede035644d878c7bd555576df, 71a85e735a3686bef8cce3725ae5ba82e2cabb1b, 6a1b25f7a67395ad1e676027322913acbb0a0635, c48e0bd0f36c25ab83befbc7b7da369b75fd25f5, d0238f2fa88b0316ab862f843a5521eff0fd9bf5, 6f75e8b61f13562237851d8119cb2f9d49e073fb, 22c141b489e6e189f5996537b0a908fc10f90de7, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, 529ff7d6441d244212cf2becafd12a7e67ac56d9, b781fb7f3725a9d899d3d250b378d729a8a00442, ca011427853d34ce4ec9ccafde8a70c9eacc3e21, 0258bab20bc8574ee602012081a17db89339f12d, 9a60d3c9d3a5b7f165325fca45bd418d651682d3, 7536bce1007a765fd097a7cc8ea62208a8c89b85, 8dd0c1e955c66092ff951941a151336211e6e171, 1b0aa15937fdf59103a5213bccf09cff83d0ee3e, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 0311ace1d499cadd1cc0c515a625d1d045f60d25, 661ccdb41fe977d47273e586389cacc1489f3286, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, b61b260de1599e6e89491cad9160898fcd3b34c2, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, 13c4e5a6122f3fa2663f63e49537091da6532f35, d88083e37c44461ce3e404bd57257cd3edb07d4e, 537f5e8e4139392cd2d108f32495e5b2b80151ac, 43e6e8d6663d83f1b74cf5a2be7b040b0928f867, db528269ef800727245c0fcb35b692d29c1ccdc9, 20a3ed03888037e2802fa9abad02ffa0a8dcc228, b977e8de38dc0d13817bca1ed20036badfe2a58c, 6ff909c6fe089fc8ebfc64eca0f0c3cc34ba277f, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 23466d271676ae467cbe85bb1993682f3502e840, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 1eaab9b33f1261744567455a14830e8a92796cf5"
28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8,Milvus: A Purpose-Built Vector Data Management System,https://www.semanticscholar.org/paper/28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8,Conference,"Recently, there has been a pressing need to manage high-dimensional vector data in data science and AI applications. This trend is fueled by the proliferation of unstructured data and machine learning (ML), where ML models usually transform unstructured data into feature vectors for data analytics, e.g., product recommendation. Existing systems and algorithms for managing vector data have two limitations: (1) They incur serious performance issue when handling large-scale and dynamic vector data; and (2) They provide limited functionalities that cannot meet the requirements of versatile applications. This paper presents Milvus, a purpose-built data management system to efficiently manage large-scale vector data. Milvus supports easy-to-use application interfaces (including SDKs and RESTful APIs); optimizes for the heterogeneous computing platform with modern CPUs and GPUs; enables advanced query processing beyond simple vector similarity search; handles dynamic data for fast updates while ensuring efficient query processing; and distributes data across multiple nodes to achieve scalability and availability. We first describe the design and implementation of Milvus. Then we demonstrate the real-world use cases supported by Milvus. In particular, we build a series of 10 applications (e.g., image/video search, chemical structure analysis, COVID-19 dataset search, personalized recommendation, biological multi-factor authentication, intelligent question answering) on top of Milvus. Finally, we experimentally evaluate Milvus with a wide range of systems including two open source systems (Vearch and Microsoft SPTAG) and three commercial systems. Experiments show that Milvus is up to two orders of magnitude faster than the competitors while providing more functionalities. Now Milvus is deployed by hundreds of organizations worldwide and it is also recognized as an incubation-stage project of the LF AI & Data Foundation. Milvus is open-sourced at https://github.com/milvus-io/milvus.",2021,90,"2141735101, 1666260553, 1764236, 145914256, 2153916637, 2153701193, 2144797977, 46909714, 2136332740, 2115215055, 2114076472, 2116525062, 2113959133, 2117315688, 2111258930, 2116566794, 2129460589, 2113623161, 2144097347, 2113618679, 2156252582, 2113619066",2141735101,Sofia,2,"2206684539, 49997612",Y,"Also, do the cluster centroids appear to be roughly stable over many runs of the algorithm?;Intriguing two phase RL approach for learning neural controllers for discrete programs This paper presents a reinforcement learning based approach to learn context-free parsers from pairs of input programs and their corresponding parse trees.",SIGMOD Conference,21,"data,milvus,vector,data management,data management","a747e8f2659df479c0092301b9658fc582423df1, ff75865cde62592d068b2afd055c57c81d77158b, 7c8bd41e9cebdb01f445a51ba43839c8d9b3ab91, c2528e88d5554e9df9f9d482ad46cb5331c4d794, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, d88083e37c44461ce3e404bd57257cd3edb07d4e, 17fc5c9b37ba63fb3280ddf9909a183523e7bcc3, 4bd3c9e1bb1ca2df62b66201616b8740300efd0a, 16753e0317730e8c1b297338300a8c6163dd06f2, bdb68c5e2369633b20e733774ac66eb4600c34d1, 079b57837221413bf99ab40999c77c29e280e0c2, ba687027ed6012f613e1f9a9cefe7683bb192934, 84a36e19f9394f22b34f79756fa9628a795e02ea, 1a60a9d1eef24e123c27a9eee5a399ac2b620fee, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, f208b3fb28c556ab62f9d202b7beae89700a338a, 58ed1fbaabe027345f7bb3a6312d41c5aac63e22, ff74bfbd9ebf4c54809873aecb04be27e9402cb8, 3a7bbc46795929f0eace82b64c44c92a48682fb5, d0238f2fa88b0316ab862f843a5521eff0fd9bf5, 8f99f5cb3950fcd1628c6a563be4b4fc4966fa66, 94deb62af3054c49e7d80bd7eb3ed5efe990fc0b, 8760bc7631c0cb04e7138254e9fd6451b7def8ca"
28cc044d5ba938472bc53d87240583982ad21663,Data Management for Data Science - Towards Embedded Analytics,https://www.semanticscholar.org/paper/28cc044d5ba938472bc53d87240583982ad21663,Conference,"textabstractThe rise of Data Science has caused an influx of new usersin need of data management solutions. However, insteadof utilizing existing RDBMS solutions they are opting touse a stack of independent solutions for data storage andprocessing glued together by scripting languages. This is notbecause they do not need the functionality that an integratedRDBMS provides, but rather because existing RDBMS im-plementations do not cater to their use case. To solve theseissues, we propose a new class of data management systems:embedded analytical systems. These systems are tightlyintegrated with analytical tools, and provide fast and effi-cient access to the data stored within them. In this work,we describe the unique challenges and opportunities w.r.tworkloads, resilience and cooperation that are faced by thisnew class of systems and the steps we have taken towardsaddressing them in the DuckDB system.",2020,27,"3428490, 3011964",3428490,Tirana,3,"2193954145, 1720381, 1939292",Y,"The papers is well written and clear, and proposes an interesting idea of representing graphs as multi-channel image-like structures.;Shouldn’t they be adaptive values with respect to the number of candidate traces found so far?;Also, I think it’s a bit of an exaggeration to call a gap of 2.71 nats “much tighter” than a gap of 3.01 nats.",Conference on Innovative Data Systems Research,20,"data,solutions,systems,data management,data management","d7b820af40a9e2660ef700d39f7b2e27b43435c5, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, 9b529fe170823f95509585d5aa39fa01a43558fd, b0c34618ffd1154f35863e2ce7250ac6b6f2c424, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, e89c37b7c2ff465db43c4b9f674867ec4b98aa8b, 9f5b82d9915d0752957602224c5056be7e749c83, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, 7a1e71cb1310c4a873e7a4e54d1a6dab0553adce, 72afe82af4c2ca100c36eb35292e85d806527f0a, f72d3f58ff73353978e224af348448b34d27cf7b, 78aab73ed574393ab421f25b3a0e3f7343e64748, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, f64670a5f54fcce339a916497a001cbf02a9a04f, bb1118fb9fd86da6a2f65770353d8fb4362d9883, e57aeb158a38ccf33d2f0f5a8f63f1209497e329, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, eda6756ab2844c390584686dc5e6385f4a8369cd, d93bcf0685c15c45d078eafea565969c04daccd3, f4cfc7cbad257f1688772d59f694c16189dba811, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 41d04aa3c25dcfbf1b44ce666c48759e03c216c7, b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1, 60caa5b3d066e13feac496fd0736e976970eb09f, bf324b5d23344984883d89a1dca22a39ca473031, c0e6cd2ec3bc9eb46c7d45bb708854da3327339e, 794b3ffd28d28606230efc975eeec9f0522fb139, 34e9852f588f75eba81c66a3e5f867a794a5a690, c30c0092bf4eb8a44faec3fc60cdd5006276bcdc, b7f5973dbf2ef76fcc3aea616a7f72ca79f09020, 48c73c389c3f36d70407eb8309a0b41578c15fc8, 9a60d3c9d3a5b7f165325fca45bd418d651682d3, 024006d4c2a89f7acacc6e4438d156525b60a98f"
2737a61f6557fe7bf53a608c668de2eff1f582f0,GPU-accelerated data management under the test of time,https://www.semanticscholar.org/paper/2737a61f6557fe7bf53a608c668de2eff1f582f0,Conference,"GPUs are becoming increasingly popular in large scale data center installations due to their strong, embarrassingly parallel, processing capabilities. Data management systems are riding the wave by using GPUs to accelerate query execution, mainly for analytical workloads. However, this acceleration comes at the price of a slow interconnect which imposes strong restrictions in bandwidth and latency when bringing data from the main memory to the GPU for processing. The related research in data management systems mostly relies on late materialization and data sharing to mitigate the overheads introduced by slow interconnects even in the standard CPU processing case. Finally, workload trends move beyond analytical to fresh data processing, typically referred to as Hybrid Transactional and Analytical Processing (HTAP). Therefore, we experience an evolution in three different axes: interconnect technology, GPU architecture, and workload characteristics. In this paper, we break the evolution of the technological landscape into steps and we study the applicability and performance of late materialization and data sharing in each one of them. We demonstrate that the standard PCIe interconnect substantially limits the performance of state-of-the-art GPUs and we propose a hybrid materialization approach which combines eager with lazy data transfers. Further, we show that the wide gap between GPU and PCIe throughput can be bridged through efﬁcient data sharing techniques. Finally, we provide an H 2 TAP system design which removes software-level interference and we show that the interference in the memory bus is minimal, allowing data transfer optimizations as in OLAP workloads",2020,22,"10195630, 66243223, 3416158, 88694945, 2649908, 1728318",10195630,Reykjavik,3,"4478199, 1693689, 2152798056",Y,"3. The paper wants to find a good trade-off on speed and accuracy.;Cons/Questions/Suggestions The distinction between the convolutional and fully-connected layers (called “classifiers”) in the approach description (sec;However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator.",Conference on Innovative Data Systems Research,20,"data,processing,gpus,data management,data management","ff7bcaa4556cb13fc7bf03e477172493546172cd, 91fc647899f801c9d351349ce73779918f90a713, b22ed1ea1d174af48c655d48e284afc239ebfa6a, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, fee8f63972906214b77f16cfeca0b93ee8f36ba2, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, 22cba8f244258e0bba7ff4bb70c4e5b5ac3e2382, ec7f5dc077480df149bcd4358a3aa8441878ca59, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, 43e624ddeed82df944a6cae0dedec3372438e243, f90f526b101cb8a0260f5165a3875928c58ae48a, cb03b665069dad5e895a2c244929ea427f1fb9d1, fa7b8acd47631bada5b66049824bfd335ac6bf8f, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, 647c4a9331e01e31a4350361d3460f0397fe694f, 77a59de2e2b832321875cadcf9619dc313f02384, f4c4e148546089123f8da5db4fb246ab4062bd40, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, b55e490637babd50dab3cdaaa3a60a2be6eb1cbb, 2eda2921a8da4b325f9d05f556594a5884c398a7, 8d67b76222d84dcd337b8a2c78f13837070a79ce, 28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8, f3d594544126e202dbd81c186ca3ce448af5255c, ce54e3b89a2570035b70885e6901ad4c92ae41c9, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa, 709af143f78bc62413c50ea1a7ee75b0702c4f59, fb30e18bbfc8de7bf7df55af7d40c0d757d1942e, e89c37b7c2ff465db43c4b9f674867ec4b98aa8b, 63d8426ba1f51a8525dd19fd8ec92934ec71aea5, da3f33d858586d24cb265e79eb54f3746e998f57, 472644c5f4155635cf9e9e37540bfa53c20e7610, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 742747c7a453b293352b772d0d99541c96a351c3, a8b995f0da78a79447dfb18c2337972b044f4239, d21703674ae562bae4a849a75847cdd9ead417df, e24b8a9531573d284647239affc6c855505b0de4, 0a92bc2dc8a216e6aced83edc0358241066833df, eef23d76e175c0cff8e81ffcb2721c10539c8cbd, 5371896313ac227eb819038dd55f213cb42b99e2"
b05306f0b142e5afb3974b1b79996e5b82653662,Rethinking Data Management Systems for Disaggregated Data Centers,https://www.semanticscholar.org/paper/b05306f0b142e5afb3974b1b79996e5b82653662,Conference,"One recent trend of cloud data center design is resource disaggregation . Instead of having server units with “converged” compute, memory, and storage resources, a disaggregated data center (DDC) has pools of resources of each type connected via a network. While the systems community has been investigating the research challenges of DDC by designing new OS and network stacks, the implications of DDC for next-generation database systems remain unclear. In this paper, we take a ﬁrst step towards understanding how DDCs might affect the design of relational databases, discuss the potential advantages and drawbacks in the context of data processing, and outline research challenges in addressing them.",2020,24,"2112197162, 2111027355, 143857631, 144237796, 30894196, 35206168",2112197162,Ljubljana,3,"2093481779, 2144151933, 123445664",Y,"The idea of enforcing information isolation is brilliant.;A thorough exploration of techniques for unsupervised translation, a very strong start for this problem This paper describes an approach to train a neural machine translation system without parallel data.;While LINE or SDNE, which the authors cite, may not run on some of the larger datasets they can be tested on the smaller datasets.",Conference on Innovative Data Systems Research,20,"data,ddc,center,data management,data management","ac67d5f9c89d8d72fbd074f94079608220348f3f, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, e23b2e47b0ac6f50000078828f27571804dcd6a2, eeac4411ae119c6c7ac33a11f762f2495b4dd960, 87f17f939b79bb3e1d2746993a2c1cda48cb1b32, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, e576a2d97950b1f6831f88575dd3f370053f6af7, f63e917638553414526a0cc8550de4ad2d83fe7a, 43e624ddeed82df944a6cae0dedec3372438e243, fa77a44f3f1857361a50c3137d623c35ef8a5739, 634fe61f3ab6692ea4733989a1c76e793bb8b69e, 447884e7da189102189a156966623335c72199b0, 6be56f559a74c0124526242e70cbdfd16cbc60a7, ec7f5dc077480df149bcd4358a3aa8441878ca59, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, 5a32ebacd5c32d52734f9d2a2cfb5d0cdbe469e2, dad8965c5a4c0a0ea1eb3837c6a9c3b42597c2ce, 5c45a5d05ac564adb67811eeb9d41d6460c70135, 8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801, 14014c024674991149f3ecf9314c93f7e029ef1a, 456c011594ecacdd24298a161787389ccbe4b88b, 2478f43de2f1fcd9301a81e9b42d86b1d46db02d, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 6c739540e66e895311b7347971f10ef556e06e52, c48e0bd0f36c25ab83befbc7b7da369b75fd25f5, 5030702fea15d66a73fc997325431f1d7945ad9a, f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6, fa63c3f53413ced7946623889c416e34a28676ea, 3a74bed911ccf213d9595b2b02a5b1c4ac4dcaf8, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 90e4330fed2da147dd41490e8ad638b618112b3d, 332e0eab5fba8e6940f3e481f542a99ac17b9717, f9c990b1b5724e50e5632b94fdb7484ece8a6ce7, 6b3756d32ab5b0a5715a5cfc3672290d2d643017, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344"
eebf1a2705bf4ac256d141b0067f1b0ea1dc7632,Data Management Challenges for Deep Learning,https://www.semanticscholar.org/paper/eebf1a2705bf4ac256d141b0067f1b0ea1dc7632,Conference,"Deep learning is one of the most exciting and fast-growing techniques in Artificial Intelligence. The unique capacity of deep learning models to automatically learn patterns from the data differentiates it from other machine learning techniques. Deep learning is responsible for a significant number of recent breakthroughs in AI. However, deep learning models are highly dependent on the underlying data. So, consistency, accuracy, and completeness of data is essential for a deep learning model. Thus, data management principles and practices need to be adopted throughout the development process of deep learning models. The objective of this study is to identify and categorise data management challenges faced by practitioners in different stages of end-to-end development. In this paper, a case study approach is employed to explore the data management issues faced by practitioners across various domains when they use real-world data for training and deploying deep learning models. Our case study is intended to provide valuable insights to the deep learning community as well as for data scientists to guide discussion and future research in applied deep learning with real-world data.",2019,48,"1423751237, 144913779, 40225855, 1826416, 1939487",1423751237,Monaco,2,"1402912902, 22654490",Y,"Interesting experiments but lack of model description The authors propose to use a byte level RNN to classify reviews.;If the contribution of the paper is the ""output stochastic"" noise model, I think it is worth experimenting with the design options one has with such a model.",EUROMICRO Conference on Software Engineering and Advanced Applications,45,"data,learning,models,data management,data management","db0cc2f21b20cbc0ab8946090967399c25709614, 69a72ff5b30642d11c96635e99aadad3140d33a7, 82870bc488b57cdf5ea62877109a7278af2926b3, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, 0a829289a16ae48837cc2905635435db98bacc76, 993df7df129f8d18816877d69923d7df7b347d85, 98e6c6d860383fea5bbad145deed51514d23b86c, d88083e37c44461ce3e404bd57257cd3edb07d4e, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, cc5726fc0ebb84f741f3496a3c52ced162c596ba, 5d433da6d0f143f20936379910104d2bb139d4ae, a2a514ed839dafdd0fb76d6c2615f25f35bf8087, 94cb5503b191815ce77b19147d96c6fbd68f06bf, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, ade9a900acc3c138021070537840488526796d35, 7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2, 410fba9f03212257d0881811802e6620e59bc827, 2bc3644ce4de7fce5812c1455e056649a47c1bbf, 799d5a8271887adede035644d878c7bd555576df, c6879e43828b293567f5e2da039d23845189d6a7, 9fcdbfdf28245010c875ce85502351fe05c04b49, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, 10a69070c8585dc5564b11b0e53dbe4d565f9ce1, c50a909e20bd07f4aea09dc6dae539b45b406a96, 21042565ec941f4fa31ac5a0af85a1a84ff21f1b, b6593b5ad6b0e00232ee88cf2bc578645c1299a0, 87f17f939b79bb3e1d2746993a2c1cda48cb1b32"
cf41991d89301c3c12420d150792cb1163999962,Data Management in Supply Chain Using Blockchain: Challenges and a Case Study,https://www.semanticscholar.org/paper/cf41991d89301c3c12420d150792cb1163999962,Conference,"Supply chain management (SCM) is fundamental for gaining financial, environmental and social benefits in the supply chain industry. However, traditional SCM mechanisms usually suffer from a wide scope of issues such as lack of information sharing, long delays for data retrieval, and unreliability in product tracing. Recent advances in blockchain technology show great potential to tackle these issues due to its salient features including immutability, transparency, and decentralization. Although there are some proof-of-concept studies and surveys on blockchain-based SCM from the perspective of logistics, the underlying technical challenges are not clearly identified. In this paper, we provide a comprehensive analysis of potential opportunities, new requirements, and principles of designing blockchain-based SCM systems. We summarize and discuss four crucial technical challenges in terms of scalability, throughput, access control, data retrieval and review the promising solutions. Finally, a case study of designing blockchain-based food traceability system is reported to provide more insights on how to tackle these technical challenges in practice.",2019,71,"46476972, 144115026, 2108824565, 1380530147, 1388034901, 2065096893, 2152801260, 1524736374, 48362791",46476972,Zagreb,3,"1688882, 145720154, 1737629",Y,"These five objectives together are sufficient to achieve impressive English <-> German and Engish <-> French results in Multi30k, a bilingual image caption scenario with short simple sentences, and to achieve a strong start for a standard WMT scenario.;The authors do provide a novel formulation and demonstrate the gains on a variety of concrete problems taken form the literature.;[ds]SSM is integrated with I2A, which generates rollouts of future states, based on the inferred hidden states from the d/sSSM-VAE model.",International Conference on Computer Communications and Networks,28,"scm,challenges,supply,data management,data management","41d4e093d5f7ed5aae1aaa9eb6c037742e4cf9b1, f4c4e148546089123f8da5db4fb246ab4062bd40, 643da4c4de1954daeac571a82367241db012a8bf, 25761ba4bdc054bfe902fe7c5d6338be6d00d491, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, 83cebf919635504786fc220d569284842b0f0a09, eadb1e7da375939e25083ae3936c4f4ef1f2a719, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, ae38dc77a962161107361f213db9216ee1274037, f208b3fb28c556ab62f9d202b7beae89700a338a, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, 8281b3e87965cbf2fa9f8bc066f08fd9108ef850, 3db1841fd5f2561a11dfbd8173616b3e695c84a1, 1a4c6856292b8c64d19a812a77f0aa6fd47cb96c, 11be2469ab1d1c508e7b6e14148990741ba87884, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, d3855b7351c11145e51301e6b686f748ca35c802, 9a0965beef113cc37491004b1848149e00300561, 3a083d843f891b3574494c385699c21766ce8b7a, bd6c027a3604d6c8fa23435bf382455b2bee436b, 63adc1e5086481e36b19b62707a96b799da51e59, 5371896313ac227eb819038dd55f213cb42b99e2, 957f5b1e7ca48891c2e279aefbfa0f04d989c21e, bbed457fd04ba4972018382d1a01a0bdde399d3c, 704011527f183b561ea6a75b21e4cefe5aa77fca, 64306bbddb4da7a4e06f990a0167d55fbbbbec82, 5d8e450c8c7d9faa8222f1d0eaf34b01755c9eef, 69a72ff5b30642d11c96635e99aadad3140d33a7, e5d720767b7a539bb2edaa98eaf572a4506a79c6, e32a2519b59d62cff6cb8136ee242dc3754ed57b, 8b417c2be7a7707f372049fb1193f0d42f799562, 70ce56e9a2181489d59f7170dc01fe8ba310a8e5, 0fa554d981809c5eb78956c779f75092c4f6c16b, 9619cde5c79d91ca5c432186668618312175f8dd, 01bf0e83159712fbbbd12171a7e268547a4cfbc5"
9727206903eb40d4fa42606711bad3402f2ba9aa,Decentralized IoT Data Management Using BlockChain and Trusted Execution Environment,https://www.semanticscholar.org/paper/9727206903eb40d4fa42606711bad3402f2ba9aa,Conference,"Due to the centralization of authority in the management of data generated by IoT devices, there is a lack of transparency in how user data is being shared among third party entities. With the popularity of adoption of blockchain technology, which provide decentralized management of assets such as currency as seen in Bitcoin, we propose a decentralized system of data management for IoT devices where all data access permission is en-forced using smart contracts and the audit trail of data access is stored in the blockchain. With smart contracts applications, multiple parties can specify rules to govern their interactions which is independently enforced in the blockchain without the need for a centralized system. We provide a framework that store the hash of the data in the blockchain and store the raw data in a secure storage platform using trusted execution environment (TEE). In particular, we consider Intel SGX as a part of TEE that ensure data security and privacy for sensitive part of the application (code and data).",2018,108,"39712836, 38804742, 145155297, 3071249",39712836,Vienna,3,"1630331317, 152355659, 2855934",Y,"2) compressing the embedding space using pca;The basic idea is to use a multi-step dynamics model as a ""baseline"" (more properly a control variate, as the terminology in the paper uses, but I think baselines are more familiar to the RL community) to reduce the variance of a policy gradient estimator, while remaining unbiased.;Creative and interesting The paper introduces an application of Graph Neural Networks (Li's Gated Graph Neural Nets, GGNNs, specifically) for reasoning about programs and programming.",IEEE International Conference on Information Reuse and Integration,18,"data,management,iot,data management,data management","1a37223175138bc1aa53b425ea2fdd0b382405a5, 8dce62b18bdea587c07cb4769a05ca0a816d09ba, d1206ccabd1980848f14472d6548251c2fab7963, 7637ed79d30d0139901175ae4abedd822c217ab4, 27245e65a27bde90b5b0bb25d157bb75a0ad8b5a, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, 643da4c4de1954daeac571a82367241db012a8bf, 247dec05283a1a521f99253a6cca6a5858cac0d2, 3a083d843f891b3574494c385699c21766ce8b7a, 0a92bc2dc8a216e6aced83edc0358241066833df, 239bf45c13b3f6d38c74026b535f785febf9cd08, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 4d8f0ae904779a50b2e18fec49e51a5661a98d8a, b0b770fb8c7760749c88e3c83ae173cdb07f7bd5, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, febe776e285dc5e72c7e3ee697a87a794e1c00ff, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 00e18c603e60d861c4e99c541e4d65ef442d5945, db4cf9f6a653d5c15973e836c800ea47743251ae, 1cf2e9e198feef3893da2800a7949f6880ddc084, 194073c405e9c362c955e9ac31979ddbc037ff8d, 9eacf62f1e546748428c7e4843731b1595294200, 4087e84fc695bb6433d0104ee94f9d7e9f4b7da5, 785a1c77fabd1ad870a9dbc98d235d2fcbdbaa6f, 2bc3644ce4de7fce5812c1455e056649a47c1bbf, ce9ca56036307217ea565644d3d3bd74b879e045, 23466d271676ae467cbe85bb1993682f3502e840, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 2af8907d4a974ae41044581f5e5d67317cb08568, 5406e153957dd7a165264da6e6e5d81251997404, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, 0e9a44ce661c3535d5ce747912540080324489f5"
aea731e7cf33aa3d482b13f42cedbc1adb3271c6,"The “Problem” of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation",https://www.semanticscholar.org/paper/aea731e7cf33aa3d482b13f42cedbc1adb3271c6,Conference,"Human variation in labeling is often considered noise. Annotation projects for machine learning (ML) aim at minimizing human label variation, with the assumption to maximize data quality and in turn optimize and maximize machine learning metrics. However, thisconventional practice assumes that there exists a *ground truth*, and neglects that there exists genuine human variation in labeling due to disagreement, subjectivity in annotation or multiple plausible answers.In this position paper, we argue that this big open problem of human label variation persists and critically needs more attention to move our field forward. This is because human label variation impacts all stages of the ML pipeline: *data, modeling and evaluation*. However, few works consider all of these dimensions jointly; and existing research is fragmented. We reconcile different previously proposed notions of human label variation, provide a repository of publicly-available datasets with un-aggregated labels, depict approaches proposed so far, identify gaps and suggest ways forward. As datasets are becoming increasingly available, we hope that this synthesized view on the “problem” will lead to an open discussion on possible strategies to devise fundamentally new directions.",2022,58,2022124,2022124,Dublin,3,"151500725, 144259957, 2223551699",Y,"Specifically, for KDE and OC-SVM, a naive PCA is used to reduce the data dimension.;The proposed method achieves perfect accuracy in every condition.;Review This paper describes a method for computing representations for out-of-vocabulary words, e.g. based on their spelling or dictionary definitions.",Conference on Empirical Methods in Natural Language Processing,22,"variation,label,labeling,data modeling,data modeling","ed935c6b359a7a486c28240d796e84897d095125, ec7f5dc077480df149bcd4358a3aa8441878ca59, b473e91cbe80c8b46451b49153cd5f93030480ab, 6d2d6707a0674ee41d539d106e6cb359a0a6ca06, 1452b25a7680bbb2c66dd7dfca6704292405da92, b0b770fb8c7760749c88e3c83ae173cdb07f7bd5, b2114228411d367cfa6ca091008291f250a2c490, 3705919b880f4f8dc37483a704e14dd078cb9ac4, bc00ff34ec7772080c7039b17f7069a2f7df0889, 1051abf1e3dae90241ad15b3f98f2e41197ee611, 8281b3e87965cbf2fa9f8bc066f08fd9108ef850, 8bb9db78b4413b92cdeeae9e24e955aab9c87ae1, 7ed665355ac78bf0c394602dd9d26075195ce2f2, 492c389d560d9db39c758d07e635408d2e0eaf7d, 1a37223175138bc1aa53b425ea2fdd0b382405a5, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, 08aaaf7ae3d61875e7bcf5c1bb0df4f17066e300, 99f06e88e76f1af51d08d7adfb26d758ebc6acab"
a85c45ce7c893388e8eafa8a653b042e1497db48,Cross-Node Federated Graph Neural Network for Spatio-Temporal Data Modeling,https://www.semanticscholar.org/paper/a85c45ce7c893388e8eafa8a653b042e1497db48,Conference,"Vast amount of data generated from networks of sensors, wearables, and the Internet of Things (IoT) devices underscores the need for advanced modeling techniques that leverage the spatio-temporal structure of decentralized data due to the need for edge computation and licensing (data access) issues. While federated learning (FL) has emerged as a framework for model training without requiring direct data sharing and exchange, effectively modeling the complex spatio-temporal dependencies to improve forecasting capabilities still remains an open problem. On the other hand, state-of-the-art spatio-temporal forecasting models assume unfettered access to the data, neglecting constraints on data sharing. To bridge this gap, we propose a federated spatio-temporal model -- Cross-Node Federated Graph Neural Network (CNFGNN) -- which explicitly encodes the underlying graph structure using graph neural network (GNN)-based architecture under the constraint of cross-node federated learning, which requires that data in a network of nodes is generated locally on each node and remains decentralized. CNFGNN operates by disentangling the temporal dynamics modeling on devices and spatial dynamics on the server, utilizing alternating optimization to reduce the communication cost, facilitating computations on the edge devices. Experiments on the traffic flow forecasting task show that CNFGNN achieves the best forecasting performance in both transductive and inductive learning settings with no extra computation cost on edge devices, while incurring modest communication cost.",2021,70,"27737939, 2267664, 47909531",27737939,Riga,3,"2837279, 1792647, 1720266",Y,"2. Indeed, AlexNet is a good seedbed to test binary methods.;The proposed method is new and technically sound.;- I suggest to divide Section 3.1 in two subsections.",Knowledge Discovery and Data Mining,27,"data,edge,forecasting,data modeling,data modeling","e3b37c1c955b2b10809040ce277edae5333b99c3, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7, b34fc78de28be598e21118d7cb9d84d63374addc, bd6c027a3604d6c8fa23435bf382455b2bee436b, a830083704284c8c5ddaf04f676c6ce23d583942, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, 099043827df60225cf33c820052716cce64d49e9, ff75865cde62592d068b2afd055c57c81d77158b, aee3d7f98b966240178ef420724c840f9b61deb3, dad8965c5a4c0a0ea1eb3837c6a9c3b42597c2ce, 4afa7d8e2de43b0b67366b1bce8768f5a246d153, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, 549e933821fdf7cd0309dacaae99c8284cbfcc24, 11be2469ab1d1c508e7b6e14148990741ba87884, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 740b5dec469e6b54e5e191a577a9b6a6ad8562e3, 0427110f0e79f41e69a8eb00a3ec8868bac26a4f, 90aca7b4cdd728b28b2fb5b4dc3ae3e37daa5b94, 14014c024674991149f3ecf9314c93f7e029ef1a, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, 91d24a94a276f5b226b07fe294561482b1d4104d, 3fadb72fb150197aa5eb88a65b244763fb45ab9b, 5031790972d496547b6613d46a4a0134c824db6e, 19cf7458db4e17c7504eee24ccf961e1dc91435c, 0894585294c67193ff3190240554677b56fd79a0, addae423490bbe82da4fb2fc265237178686b4e8, 4e58100b319d74f97ed550a4e5fa32dea8c06fe1, 32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b, aea731e7cf33aa3d482b13f42cedbc1adb3271c6, e57aeb158a38ccf33d2f0f5a8f63f1209497e329, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, be383c607d4d357c763d2329ab71799c6e1393b4, bdb68c5e2369633b20e733774ac66eb4600c34d1, 2346d121f38fc19c77e0b062415519843f478163, 033a50c4515b153b6e706018075c333c64981fd7, d6e1e4f0ad898ca6ac37e6e139a77fa3982170d4, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, e02f91d625cd32290d4ede0f31284da115844316, ce51eff5b529ee572dab1c1f38f20adc8e89bab2, b61b260de1599e6e89491cad9160898fcd3b34c2, 492c389d560d9db39c758d07e635408d2e0eaf7d, 04fa3ebc4c0c0b4a2d2d1a3fc612134a05057696"
d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5,An Effective and Scalable Data Modeling for Enterprise Big Data Platform,https://www.semanticscholar.org/paper/d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5,Conference,"The enormous growth of the internet, enterprise applications, social media, and IoT devices in the current time caused a huge spike in enterprise data growth. Big data platform provided scalable storage to manage enterprise data growth and served easier data access to decision-makers, stakeholders and business users. It is a well-known challenge to classify, organize and store all this data and process it to provide business insights. Due to nature, variety, velocity, volume and value of data make it difficult to effectively process big data. Enterprises face challenges to apply complex business rules, to generate insights and to support data-driven decisions in a timely fashion. As big data lake integrates streams of data from a bunch of business units, stakeholders usually analyze enterprise-wide data from various data models. Data models are a vital component of Big data platform. Users may do complex processing, run queries and perform big table joins to generate required metrics depending on the available data models. It is usually a time consuming and resource-intensive process to find the value from data. It is a no-brainer that big data platform in the enterprise needs high-quality data modeling methods to reach an optimal mix of cost, performance, and quality. This paper addresses these challenges by proposing an effective and scalable way to organize and store data in Big Data Lake. It presents some of the basic principles and methodology to build scalable data models in a distributed environment. It also describes how it overcomes common challenges and presents findings.",2019,21,31933741,31933741,Monaco,2,"2113909888, 93841942",Y,Follow up experiments extend the basic setup significantly.;In its current state and format the major issue with this work is the lack of more in-depth performance analysis which would help the reader draw more solid conclusions about the generalization of the approach.,2019 IEEE International Conference on Big Data (Big Data),19,"data,enterprise,business,data modeling,data modeling","c665003881c3c35589d1e48da1ee7234b48f2ac8, 78aab73ed574393ab421f25b3a0e3f7343e64748, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, 9727206903eb40d4fa42606711bad3402f2ba9aa, 8c33ca066e2ab615e24c65198c794114436053dd, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, 0b24f27d920bd1d23c8f6a1ab2b603c5c14f0a36, a5710a7a54b50c26f3c40c37a19b2ec63330e90b, 03532123ccffae8d411264320e8a5ae2b6eddea0, 9583ac53a19cdf0db81fef6eb0b63e66adbe2324, 60119658af638693f6de23d8466968e60c428ac7, 07cca761749bfe21c2d096ff60f32b574d5c84c4, a22f3398ea865426c89ee66f4824ec626e56a864, ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba, 9e66ae24a541255c2d931184498ee116ce81478a, 1051abf1e3dae90241ad15b3f98f2e41197ee611, 54ddb00fa691728944fd8becea90a373d21597cf, ec2f9076448ba25a225618603adde60caa76c4df, 740b5dec469e6b54e5e191a577a9b6a6ad8562e3, e02f91d625cd32290d4ede0f31284da115844316, 9efd70d2c06733704220313fb67720aa45c6362a, 155f27879f185f1ab04107c91c2ae7cf6a910a03, b6a7226e5f6d618370995eccad68af195ef32da2, 933baeec555352784848a93284c9dd0e79477759, b0ee814c7a3eed260c9913861329c9f73e880d00, b52db9e41e15f76bdcfbe674abe0314af545c430, 182acc7a1c9a63cfdef9f86ee9df43fd75e05060, 3924aa213ff891812c66a6909ab902684d3eb107, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, d6f002d88638de71114dab083f0ea8ceea6b6a5a, 4895c430c7810b45840b58cc9182f12143013a43, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, 8c7aee0bbc062d5b4bcd34951b1e002274288206, 82870bc488b57cdf5ea62877109a7278af2926b3, 15370f51d666ab8ef17185679553c6a8647b2a15, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 150f95f9c73820e0a0fa1546140e9f2bdfd25954, 92930ed3560ea6c86d53cf52158bc793b089054d, d8a5474f450330ad25c1e22f27e88f3630cb840d, f3d594544126e202dbd81c186ca3ce448af5255c, f0c27af6c330d5c3b0a8eb376a69ce92c85badd7, f75b70c9d7078724b592ec3e21de705e7b6ff73f, 915ab7b6ca3633230403d47dbb31cf74888cc5c9, 94de6bab96ad533169dbc3bf9e6557581e59cb6f, 92912dd895c360f01a6be9c9f6d207642139525e, febe776e285dc5e72c7e3ee697a87a794e1c00ff, d2a5dcecd2ffdf03473df1688091f08fadb114a3, 73d4accea441aae2373828a8dc2175aa2759c38f"
818de553ecd306735971aba04bbfc29d17457084,Robust Spectral Clustering for Noisy Data: Modeling Sparse Corruptions Improves Latent Embeddings,https://www.semanticscholar.org/paper/818de553ecd306735971aba04bbfc29d17457084,Conference,"Spectral clustering is one of the most prominent clustering approaches. However, it is highly sensitive to noisy input data. In this work, we propose a robust spectral clustering technique able to handle such scenarios. To achieve this goal, we propose a sparse and latent decomposition of the similarity graph used in spectral clustering. In our model, we jointly learn the spectral embedding as well as the corrupted data - thus, enhancing the clustering performance overall. We propose algorithmic solutions to all three established variants of spectral clustering, each showing linear complexity in the number of edges. Our experimental analysis confirms the significant potential of our approach for robust spectral clustering. Supplementary material is available at www.kdd.in.tum.de/RSC.",2017,63,"11754930, 22654490, 3075189",11754930,Brussels,2,"1996394, 2081346",Y,The experiments are very clearly presented and solidly designed.;1. This manuscript provides the sufficient and necessary characterization of critical points for deep networks.,Knowledge Discovery and Data Mining,23,"clustering,data,approaches,data modeling,data modeling","8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, a620ea8654f86b282e0d2c1fbc6616b90ca45bd1, 634fe61f3ab6692ea4733989a1c76e793bb8b69e, d7b820af40a9e2660ef700d39f7b2e27b43435c5, 182180bd69ea6d2f59225ded5ddc900b8558ab9f, b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1, d916776e0c6a04b0def4c22257c188776c2edab2, 215fc60307f741b9db059204e41db8bfb879e606, f18be38578ee52aa7071c404d42e3d53ae003122, 42d3ae61ca296558cba61446bd95b8a6e5b1082d, 74b4f16c5ac91e3e7c88ae81cc8c91416b71d151, 6dd481d5eb8d76d4b61a1829c7687d008e0937ab, 08764019e9762da527253b37b0ff39c46a4206b7, 69d49a06f09cf934310ccbf3bb2a360fa719272d, 1c2efb418f79b5d29913e014a1dfd78865221c39, f72d3f58ff73353978e224af348448b34d27cf7b, 1c748f86182a62d44d5b44316db510f8d833e19f, c84389369720dcd2f004c48e58fbac2c45c8f092, 456c011594ecacdd24298a161787389ccbe4b88b, 76b93abde3ec4e5bd84a38d488a8db209dc4ac98, 2141334fad7248fc707607bc9453d44686ae07a7, 047286f5b9315a8e8bf56c4fc936e62f21495892, 139a0c7a60667979dcb57eae677f75ff3f0b0196, 0e141942fa265142f41a2a26eb17b6005d3af29e, f72053903270d9a7f41108461ad04d5aa075218d, 371a343457a4fbff00000bf4faa29b2b2f85744c, ba9b6f805feb62c978d384211f910790643a023e, 43e624ddeed82df944a6cae0dedec3372438e243, f295157f37cfb43cd8d8d2690ea124edc5ea59c2, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, fdc57c18f3b636c3273542327ae540217972558f, b7034546bee38ba13d3b312fce893a22e33ce4dd, 0574a3abc98f0e6bd035a4ff4ea107cfba45d3d4, 332e0eab5fba8e6940f3e481f542a99ac17b9717, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, 8b417c2be7a7707f372049fb1193f0d42f799562, 03532123ccffae8d411264320e8a5ae2b6eddea0, bb0cec8f2d34cfb9dbf8bffd1a5de499311ae098, 537f5e8e4139392cd2d108f32495e5b2b80151ac, 4f480bae3196dbbc27ab383bce33478ea963f9b3, beb890d47bbc21a96967f9993c9d6e15686b2eac, 9583ac53a19cdf0db81fef6eb0b63e66adbe2324, 182acc7a1c9a63cfdef9f86ee9df43fd75e05060, e5194ae88d63c7549678b1b73cfdaf7112164272, 2ee03e28208a9310a9be4032c2b04ebdddb83cc7, f28e387d4229c5f690ce4570a391c0f47e7155c7, 119a9e5b563cf1134897553ee49325b5a5bd9fb9, 6dc4883228c95e8a332320fcc587a0ff33c84d59, b9f190339ce0b4cdad3464c45ec3266a7369fb7c, 6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91"
595101f13b961d69c553ce1ed24f60f3f1085e02,RecSSD: near data processing for solid state drive based recommendation inference,https://www.semanticscholar.org/paper/595101f13b961d69c553ce1ed24f60f3f1085e02,Conference,"Neural personalized recommendation models are used across a wide variety of datacenter applications including search, social media, and entertainment. State-of-the-art models comprise large embedding tables that have billions of parameters requiring large memory capacities. Unfortunately, large and fast DRAM-based memories levy high infrastructure costs. Conventional SSD-based storage solutions offer an order of magnitude larger capacity, but have worse read latency and bandwidth, degrading inference performance. RecSSD is a near data processing based SSD memory system customized for neural recommendation inference that reduces end-to-end model inference latency by 2× compared to using COTS SSDs across eight industry-representative models.",2021,80,"49212289, 2633839, 1481699378, 2061231, 2797270, 1896817, 2255803",49212289,Stockholm,3,"144588806, 2059271276, 32528506",Y,"On the negative side, the paper is only qualitative.;So, the main contribution of the paper is to do all the sensitivity calculations with respect to SVM problem and then use the importance sampling theory to obtain bounds on the coreset size.;If the text is from the user, a named entity recognizer is used.",International Conference on Architectural Support for Programming Languages and Operating Systems,26,"models,inference,recommendation,data processing,data processing","b58e98029d53d69ccc7089dca7b01bf050b5ad2b, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 4be9368abc2474d6fd38639e523cf03af1873fd9, 0af24df95f8a0b7e6fc257a98a5820314d1243f3, f406aceba4f29cc7cfbe7edb2f52f01374486589, 538fbdb8013ab43a9b5d725461b294ad29fcced7, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, 0b24f27d920bd1d23c8f6a1ab2b603c5c14f0a36, 7c24234042988e2f820a4350f43422ed2ad6fc52, 54ddb00fa691728944fd8becea90a373d21597cf, be082d70534db088315f2cc5b42c2fdcd58c1b8c, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, 60119658af638693f6de23d8466968e60c428ac7, 0456ed94f6c455f99cb67abe8a28aeb3ef9f489b, 91fc647899f801c9d351349ce73779918f90a713, 0fe0f08e71e188cb2a663f36acb296d9eb6fe694, 287ba5bf00d96af1596aaf80c178392a9c4fcc28, 2f44ab0e52eb98fdfc0086638887f175b6f2fe4b, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, 6422f4b9e3bedf585170bffc7105ffe2061e87ae, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, d93bcf0685c15c45d078eafea565969c04daccd3, 736ef8a32d6c5f76a21d61299300cf796480d507, dd2deed2ce6e110236a1280db765fa02c7488eb1, 7a4fe2f003241ad97bf1778e527cb0306fa90da2, 084a93c8ac0230ea9fe64d445ad1d6af5ea0b3b3, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, db4cf9f6a653d5c15973e836c800ea47743251ae, f04d8e558c11ac0fc9318c4f3d61b651c62ddcfa, 72243bfa618fd6c3c01bb7b4885a4a456b2a6071, 661ccdb41fe977d47273e586389cacc1489f3286"
ada0b87cd5c30d31186c38fb12e631d29426a3bf,Spark SQL: Relational Data Processing in Spark,https://www.semanticscholar.org/paper/ada0b87cd5c30d31186c38fb12e631d29426a3bf,Conference,"Spark SQL is a new module in Apache Spark that integrates relational processing with Spark's functional programming API. Built on our experience with Shark, Spark SQL lets Spark programmers leverage the benefits of relational processing (e.g. declarative queries and optimized storage), and lets SQL users call complex analytics libraries in Spark (e.g. machine learning). Compared to previous systems, Spark SQL makes two main additions. First, it offers much tighter integration between relational and procedural processing, through a declarative DataFrame API that integrates with procedural Spark code. Second, it includes a highly extensible optimizer, Catalyst, built using features of the Scala programming language, that makes it easy to add composable rules, control code generation, and define extension points. Using Catalyst, we have built a variety of features (e.g. schema inference for JSON, machine learning types, and query federation to external databases) tailored for the complex needs of modern data analysis. We see Spark SQL as an evolution of both SQL-on-Spark and of Spark itself, offering richer APIs and optimizations while keeping the benefits of the Spark programming model.",2015,1288,"144482217, 2066641, 1387529184, 40199213, 2536434, 2086593199, 39309572, 2403754, 143666627, 38565890, 143834867",144482217,Vaduz,3,"2082427140, 36347083, 2116579935",Y,"Detailed comments: The problem of unsupervised time series clustering is important and challenging.;This paper is clearly written, proposes a simple model and seems to outperform current methods.;Does each component is related to a certain topic?",SIGMOD Conference,15,"spark,sql,processing,data processing,data processing","545f108575314031f35c617c4ac35a10133c50e3, 0f38b3d717e0fcc6eacc9c6e78b252227440e04e, 34e9852f588f75eba81c66a3e5f867a794a5a690, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, 7fa30c8af436b7df513ef41e2cccd8a2404f37f8, 2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, 42543dc42e65609bbbf2be470d54dd923532c36a, 916455d97cd792c2eb5b00663689592e25cbc8d8, 9712624bb61abb0da989514cae558cfab61bb9d2, 0f4d00d01d43d3967ee92b58481b5ad530a944d1, 156609022dd6258c60238859622da0a1683bd062, cf5fddf6717e88e2bbed6b0bfe54dfeb311e6789, d2a609ffb814442d0728aef9f6616f9cd775face, b6593b5ad6b0e00232ee88cf2bc578645c1299a0, 39602922b04885047254444fd1a1586d797617ce, 0c4ba06a12584cb63a85294f796108d359fe9835, 3faeb21fe256b99391d69570053a8c2d91e9f348, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, 1c748f86182a62d44d5b44316db510f8d833e19f, d983427574a41bf485e31ab983acc1c742e9e24c, f72d3f58ff73353978e224af348448b34d27cf7b, 2346d121f38fc19c77e0b062415519843f478163, c997ed5ac5772bb0fce8bb11b8c86bc33310f39a, 1109d62ebd2b29a7dc148bc30dd6cfc803a63dec, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b, bb1118fb9fd86da6a2f65770353d8fb4362d9883, a781cea542e99c6bb9422858e7c04eaef18c7673, 63de6db7245f634ecbef4c505099874c1ba65145, 80dd97954ddf3edd22d4cb21f0ac31b7ffed6bbf, c9645aa4ea31903e02e201b877fd3e1466adff4f, 1fcddebb7b51175ac412009ec1c26cc29fb925cf, 10aa2be24951e6de76b630482a645d79354c4cde, 4419c5720e30d5ca5158795d4c848125650b8db1, c24d47ff95cd4bda073c75ec24ececaa3b10c995, 4cd033a56b19f87f6adfefeef5fcc990306ecf40, 0894585294c67193ff3190240554677b56fd79a0, 8674494bd7a076286b905912d26d47f7501c4046, a747e8f2659df479c0092301b9658fc582423df1, cb12fe714dd4fa1d14b6a8023d494c14c89e90ea, 2ddcd47b28eb4b43317a34cf56e83309f5347699, 28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, 695bdc6e24608364491b9418a220c65a7cd17413, c0aec04ee86c0724d61c976f19590fbe9c615723"
ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee,Learning scheduling algorithms for data processing clusters,https://www.semanticscholar.org/paper/ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee,Conference,"Efficiently scheduling data processing jobs on distributed compute clusters requires complex algorithms. Current systems use simple, generalized heuristics and ignore workload characteristics, since developing and tuning a scheduling policy for each workload is infeasible. In this paper, we show that modern machine learning techniques can generate highly-efficient policies automatically. Decima uses reinforcement learning (RL) and neural networks to learn workload-specific scheduling algorithms without any human instruction beyond a high-level objective, such as minimizing average job completion time. However, off-the-shelf RL techniques cannot handle the complexity and scale of the scheduling problem. To build Decima, we had to develop new representations for jobs' dependency graphs, design scalable RL models, and invent RL training methods for dealing with continuous stochastic job arrivals. Our prototype integration with Spark on a 25-node cluster shows that Decima improves average job completion time by at least 21% over hand-tuned scheduling heuristics, achieving up to 2x improvement during periods of high cluster load.",2018,509,"2512621, 1962485, 2043402, 40071013, 79404966",2512621,Podgorica,3,"3024698, 1473151134, 151213231",Y,"The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss.;This paper studies the problem of multi-label learning for text copora.;So, I wish to see a section on testing with Resnet and GoogleNet.","Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",18,"scheduling,rl,decima,data processing,data processing","9312e5efa0dcef1445d45a41771f12e2a8dc6715, 03aeb4520e760a906393aaf9c1bf4e526483d081, ce51eff5b529ee572dab1c1f38f20adc8e89bab2, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, f28e387d4229c5f690ce4570a391c0f47e7155c7, b9c974380649749320f4a02e33b2e5014e7f1756, 2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb, a5710a7a54b50c26f3c40c37a19b2ec63330e90b, 02a1e8e77f501675945890df45fbdc11726cb0ba, c6e242fce9e5e4ba2c4dc869cfb0afabf2020852, 518a7c79968a56d63a691d42f8378be6c776167e, e7b0e2710821a9fe1e3d2b9a09dd94514b2d5ea5, 752604994a7ca548ff2954114fc61a501d857b1c, b61b260de1599e6e89491cad9160898fcd3b34c2, 69a72ff5b30642d11c96635e99aadad3140d33a7, b067177b1e17287185eb3b82ccc3d7c646b3ec40, d21703674ae562bae4a849a75847cdd9ead417df, 5d433da6d0f143f20936379910104d2bb139d4ae, 1eaab9b33f1261744567455a14830e8a92796cf5, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, cc017a62c605a0749e35a1264a46d62e78fb68b7, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, 8adb47deeef943c2c1bae41f9498a382fb818a16, 0ecf3f089e6dc7944b440227069b9d0143e18d78, fb00016c1e048b9373803add001c1ec7e877cb23, 42375e9a309fe5bf5c671c918cd8c9d5d85568cc"
f90f526b101cb8a0260f5165a3875928c58ae48a,NATSA: A Near-Data Processing Accelerator for Time Series Analysis,https://www.semanticscholar.org/paper/f90f526b101cb8a0260f5165a3875928c58ae48a,Conference,"Time series analysis is a key technique for extracting and predicting events in domains as diverse as epidemiology, genomics, neuroscience, environmental sciences, economics, and more. Matrix profile, the state-of-the-art algorithm to perform time series analysis, computes the most similar subsequence for a given query subsequence within a sliced time series. Matrix profile has low arithmetic intensity, but it typically operates on large amounts of time series data. In current computing systems, this data needs to be moved between the off-chip memory units and the on-chip computation units for performing matrix profile. This causes a major performance bottleneck as data movement is extremely costly in terms of both execution time and energy. In this work, we present NATSA, the first Near-Data Processing accelerator for time series analysis. The key idea is to exploit modern 3D-stacked High Bandwidth Memory (HBM) to enable efficient and fast specialized matrix profile computation near memory, where time series data resides. NATSA provides three key benefits: 1) quickly computing the matrix profile for a wide range of applications by building specialized energy-efficient floating-point arithmetic processing units close to HBM, 2) improving the energy efficiency and execution time by reducing the need for data movement over slow and energy-hungry buses between the computation units and the memory units, and 3) analyzing time series data at scale by exploiting low-latency, high-bandwidth, and energy-efficient memory access provided by HBM. Our experimental evaluation shows that NATSA improves performance by up to 14.2× (9.9× on average) and reduces energy by up to 27.2 × (19.4 × on average), over the state-of-the-art multi-core implementation. NATSA also improves performance by 6.3 × and reduces energy by 10.2 × over a general-purpose NDP platform with 64 in-order cores.",2020,62,"2061067079, 1939292, 46175739, 3387282, 1474355913, 2059547812, 3190187, 145929920",2061067079,Sofia,3,"1738190, 37722032, 47781311",Y,"The encoder network from NEM can be seen as a recurrent network which takes one latent variable theta_k at time t and some input x to produce the next latent variable theta_k at time t+1.;How does this compare to the near-identity constraints in resnets in Shaham et al.;I assume that it involves \hat{M}, but it would be good to formally define this notation.",ICCD,38,"time,series,data,data processing,data processing","bc44c0c64a473e035b11ae60a1993ad3db1acd2e, f3b8a3ffa0bdfe5578e0f0f44ea3b66cad38c032, 3a083d843f891b3574494c385699c21766ce8b7a, 31f9eb39d840821979e5df9f34a6e92dd9c879f2, 75ea299834d6949e89e91d006677343ddab44e49, 340f48901f72278f6bf78a04ee5b01df208cc508, b889b1d6944213bc2ca29e3ad07ee65ede20892d, 57a3fc6d0aaad3c10c793b4e59390ca04c935282, d1ae4ab5047489c2b010c7ce72262982ad66ad60, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, 239bf45c13b3f6d38c74026b535f785febf9cd08, 5030702fea15d66a73fc997325431f1d7945ad9a, 4b06c7e29280b1c6bc05c9df39023b48fef02c93, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, 43bafb4997515d2904abfca8214f2fc806680fc3, b067177b1e17287185eb3b82ccc3d7c646b3ec40, 4e13a8e8ba8d33e15ed037bfca7c651047533990"
d9a7fa7616a327367696e19b1846519745cd43ff,The Devil Is in the Details: Delving Into Unbiased Data Processing for Human Pose Estimation,https://www.semanticscholar.org/paper/d9a7fa7616a327367696e19b1846519745cd43ff,Conference,"Recently, the leading performance of human pose estimation is dominated by top-down methods. Being a fundamental component in training and inference, data processing has not been systematically considered in pose estimation community, to the best of our knowledge. In this paper, we focus on this problem and find that the devil of top-down pose estimator is in the biased data processing. Specifically, by investigating the standard data processing in state-of-the-art approaches mainly including data transformation and encoding-decoding, we find that the results obtained by common flipping strategy are unaligned with the original ones in inference. Moreover, there is statistical error in standard encoding-decoding during both training and inference. Two problems couple together and significantly degrade the pose estimation performance. Based on quantitative analyses, we then formulate a principled way to tackle this dilemma. Data is processed in continuous space based on unit length (the intervals between pixels) instead of in discrete space with pixel, and a combined classification and regression approach is adopted to perform encoding-decoding. The Unbiased Data Processing (UDP) for human pose estimation can be achieved by combining the two together. UDP not only boosts the performance of existing methods by a large margin but also plays a important role in result reproducing and future exploration. As a model-agnostic approach, UDP promotes SimpleBaseline-ResNet50-256x192 by 1.5 AP (70.2 to 71.7) and HRNet-W32-256x192 by 1.7 AP (73.5 to 75.2) on COCO test-dev set. The HRNet-W48-384x288 equipped with UDP achieves 76.5 AP and sets a new state-of-the-art for human pose estimation. The source code is publicly available for further research.",2019,134,"47513708, 2118932732, 2065456437, 143986385",47513708,Oslo,3,"2070951368, 2112197162, 6667699",Y,"This paper also would probably be more suitable for a chemoinformatics journal, where the rationale learning would be highly appreciated.;But beyond this, and despite fairly reasonable familiarity with the subject, I simply don't understand other elements that the paper is talking about.;Some empirical observations are made, but it is not discussed whether what is observed is surprising in any way, or just as expected?",Computer Vision and Pattern Recognition,19,"data,estimation,udp,data processing,data processing","7998468d99ab07bb982294d1c9b53a3bf3934fa6, 371a343457a4fbff00000bf4faa29b2b2f85744c, eff6546819d25df0bccdc89f02554a43a4f1c464, da9b8b4073e6ad44b3da66e1e117cb1ddbf8836d, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, 7a4fe2f003241ad97bf1778e527cb0306fa90da2, f50440b10052864586e105e39e7eec390842d5e4, febe776e285dc5e72c7e3ee697a87a794e1c00ff, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, a18f02e5c24e1f924aea268dd343bbdea234f2bb, a0a79dad89857a96f8f71b14238e5237cbfc4787, 6544259ff6b335b1dcec75e031b6d57e5b9509f4, 4895c430c7810b45840b58cc9182f12143013a43, b22ed1ea1d174af48c655d48e284afc239ebfa6a, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, e7a7735104448371dde788542ebfc6af6485ea43, ae026f29c2d571871f426ff4873d43b4ff90b9ad, 846883b7761cb5fe4468d42bf9d328b5d1030175, 6db0f8d396371078590faa7b34ae2e0e1b154a60, e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0, 6a6ad9eb495739f4c80e7c09598720c3d5c5dff7, 417326e51d78ba8bd2621f23e539b41bbdd336d6, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 02b1607af35b48f0bd716367caf6a7428b969369, 6eb8caebbffc7e5b301b66dc36acad46b4dca5c9, 2737a61f6557fe7bf53a608c668de2eff1f582f0, 024006d4c2a89f7acacc6e4438d156525b60a98f, ee6f23590783adec7cf6b2030c6a46f3117a708e, 4fcfe83c05402b5c5fb6e853082e74af6379d7f9, 8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6, ada0b87cd5c30d31186c38fb12e631d29426a3bf, 43e6e8d6663d83f1b74cf5a2be7b040b0928f867"
a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b,Panthera: holistic memory management for big data processing over hybrid memories,https://www.semanticscholar.org/paper/a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b,Conference,"Modern data-parallel systems such as Spark rely increasingly on in-memory computing that can significantly improve the efficiency of iterative algorithms. To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy-inefficient. Emerging non-volatile memory (NVM) technologies offers high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages (e.g., Scala and Java) and executed on top of a managed runtime (e.g., the Java Virtual Machine) that already performs various dimensions of memory management. Supporting hybrid physical memories adds in a new dimension, creating unique challenges in data replacement and migration. This paper proposes Panthera, a semantics-aware, fully automated memory management technique for Big Data processing over hybrid memories. Panthera analyzes user programs on a Big Data system to infer their coarse-grained access patterns, which are then passed down to the Panthera runtime for efficient data placement and migration. For Big Data applications, the coarse-grained data division is accurate enough to guide GC for data layout, which hardly incurs data monitoring and moving overhead. We have implemented Panthera in OpenJDK and Apache Spark. An extensive evaluation with various datasets and applications demonstrates that Panthera reduces energy by 32 – 52% at only a 1 – 9% execution time overhead.",2019,49,"2121328398, 1734175, 145153097, 3260130, 144041053, 145929920, 46496975, 32215073, 38394648",2121328398,Brussels,3,"2086632521, 2274111800, 1682773",Y,"Overall it feels as if this is an interesting project but that it is not yet ready for publication.;* A thorough discussion on mixing in feature space, as well as a baseline which mizes in feature space.;I would strongly encourage the authors to try harder datasets such as COCO, VOC etc.",ACM-SIGPLAN Symposium on Programming Language Design and Implementation,40,"data,panthera,memory,data processing,data processing","714f47bbedcadd7ebc44d2d5010f13323fc6a256, 89a30b5dab02c9c390a632acad481fa602859272, 7c8bd41e9cebdb01f445a51ba43839c8d9b3ab91, ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8, ccd94602e3acecf999d0c9ba62b1a8bc02e9f696, 0894585294c67193ff3190240554677b56fd79a0, ff7bcaa4556cb13fc7bf03e477172493546172cd, 9181b0d801dfcd7723a3ede201f0543078e2c149, 8ee45aeb7c97e3346cc62f216f673b91277ac718, b080d072cfde697180db3234da08903c092e72c3, 5bbe106eeba21bcb4ac7d3ffc128f2ab581ffdfc, 00e18c603e60d861c4e99c541e4d65ef442d5945, 28a5a53dafacebad8a7c47773079caeffb9a5baa, 88932b1563316e4ed7a61c4c2c0c90d7aa9bb308, e576a2d97950b1f6831f88575dd3f370053f6af7, c2413fa296543159b32d16350d9e29f7db528790, e359e8960b0b09e8685a32927b7818f4b06ef881, 322d91190acd8ac8c64598f5126947b0485ba249, 8760bc7631c0cb04e7138254e9fd6451b7def8ca, b9b639522465cc606df878eee62e7f9c4bf19e62, f9c990b1b5724e50e5632b94fdb7484ece8a6ce7, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 3ec02e36cc0ba16292b255e9f90c3725521e2ec9, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, d57f11ed40c3cdcbb36cb758191db4f2c9372965, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 447884e7da189102189a156966623335c72199b0, ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7, 77a096d80eb4dd4ccd103d1660c5a5498f7d026b, f86f1748d1b6d22870f4347fd5d65314ba800583, 9a51df7eaf2457b034c49ca8ca4ca3b81e6c08c0, db0cc2f21b20cbc0ab8946090967399c25709614, 74b63c4cdc719a646ce014d084a0ce0ac66d9139, 55fa5be5288f6097ae5bd2dfe58fc07b3b39bfb6, 82a09f72d7b9587e3b519b1cd9640a5a611f3480, cf5dfc4a9f7a82b32640128ca10832eace55880e, e7354193f0e7fdd1b72725935ff2741cc7b8eeb7, c9b56cb026a38e39bb0228faac57accd6f65e6f7, 4dcbe71d261ba5caa0a06d5c265b5509edbbe1c4, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 5f51d468ce730eeade7e9f419a1fe7152582be25, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, c6e242fce9e5e4ba2c4dc869cfb0afabf2020852, be383c607d4d357c763d2329ab71799c6e1393b4, 2880ac931c11176aee6d42a7e7bb0703aacde3f9, 7909428ca7d813331bbe4d33d07ab09e984b41b4, d47a682723f710395454687319bb55635e653105"
59c2968fb9672a7152c52127255d8f0784bc2368,Using the ESP32 Microcontroller for Data Processing,https://www.semanticscholar.org/paper/59c2968fb9672a7152c52127255d8f0784bc2368,Conference,This article deals with experiences with the development of applications of the ESP32 microcontrollers and provides a comprehensive review of the possibilities of applications development on this platform in the area of data measurement and processing. Microcontrollers usually connect with IoT modules and other smart sensors and provide data to the superior system. This paper also describes implementation of application with the version of connected OLED display and with ESP32 Wrover development board with integrated display.,2019,107,"2354124, 151167821, 31277482",2354124,San Marino,3,"29629766, 2119044211, 5430731",Y,"Also, the detailed specification of the VAE should be detailed.;) The dataset classifier (sec 4.3.4) could be learnt end-to-end by using a softmax output of the dataset classifier as the alpha weighting.;This second part was confusing - although the maze can be viewed as a graph, many other works apply ConvNets to maze environments [1, 2, 3], and their work has little relation to other work on graph CNNs.",International Conference on Innovative Computing and Cloud Computing,20,"development,applications,esp32,data processing,data processing","d2a505586c0da20752b98f63c7760b6a5c41e28d, 96273b87cd0eb9d1c9a12afae621ce2abdbbab36, b05306f0b142e5afb3974b1b79996e5b82653662, 0cec0c296efedb814342b4b841d4583efbfc6777, bcc82ce554942880814243fc8c08a88b9d2aad09, 43eea2a73997294193228d50f9ff25fc5345664b, df7d26339adf4eb0c07160947b9d2973c24911ba, 54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f, bad4c08f03587e38ee960e2aa76e16d722826e7c, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, b613887337a5d2e8fc8773037116be81e6346835, 39444c55f07839ac6a0d1839472a982f8fb447bb, 3705919b880f4f8dc37483a704e14dd078cb9ac4, 02fa2389b1b64b661192e224bed8af6df0ce80f6, 3b230f14c46e7e177e9bebb2ebc9f46b346b646d, a830083704284c8c5ddaf04f676c6ce23d583942, 9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 08be4e23951a0def1c5d235befbb39c8d8d373a3, 696b388ee6221c6dbcfd647a06883b2bfee773d9, b889b1d6944213bc2ca29e3ad07ee65ede20892d, 8b28792f8405b737229afb92c99c579b86d8aa98, 8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801, cd8a9914d50b0ac63315872530274d158d6aff09, f397b593de771752e7002a954eb531f3ef6a975e, 64c4c6cb66457c79f89ba4f2529b57bfce19ced4, 4b1280229ced73f6c86550f24ef01490fde52285, 980858461df7c4349f17b427686c5bcbcffbdc04, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, b3dbe1b460a3b66df1653508c9eed7dd51dee4d2, 9eacf62f1e546748428c7e4843731b1595294200, da8b317b99c4b8933b2c59285639eca6c3fcb869, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 1d7531db9272f7838e33616075e1e64532fd013a, 834fdec542153aae5fe725df801aac87ba5e8f56, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, 8f99f5cb3950fcd1628c6a563be4b4fc4966fa66, 929305892d4ddae575a0fc23227a8139f7681632, ae38dc77a962161107361f213db9216ee1274037"
dcc8d6a87c69ca44cb6636d343347ddd6c8c3860,DPI: The Data Processing Interface for Modern Networks,https://www.semanticscholar.org/paper/dcc8d6a87c69ca44cb6636d343347ddd6c8c3860,Conference,"As data processing evolves towards large scale, distributed platforms, the network will necessarily play a substantial role in achieving efficiency and performance. Increasingly, switches, network cards, and protocols are becoming more flexible while programmability at all levels (aka, software defined networks) opens up many possibilities to tailor the network to data processing applications and to push processing down to the network elements. 
 
In this paper, we propose DPI, an interface providing a set of simple yet powerful abstractions flexible enough to exploit features of modern networks (e.g., RDMA or in-network processing) suitable for data processing. Mirroring the concept behind the Message Passing Interface (MPI) used extensively in high-performance computing, DPI is an interface definition rather than an implementation so as to be able to bridge different networking technologies and to evolve with them. In the paper we motivate and discuss key primitives of the interface and present a number of use cases that show the potential of DPI for data-intensive applications, such as analytic engines and distributed database systems.",2019,31,"144641551, 2691974, 2754078, 145527641, 66030040, 3087426, 1413903838, 48469973, 2108724347, 46533048",144641551,Skopje,3,"8847603, 2260830380, 1453724884",Y,"The partitioning of each task must currently be designed by hand.;The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse.;In the experiment, the proposed method shows no or slight degradation of accuracy with big savings in communication cost.",Conference on Innovative Data Systems Research,19,"processing,network,data,data processing,data processing","b85f3a66245d483f3eb3447eaf9950bd55f2b21e, b6593b5ad6b0e00232ee88cf2bc578645c1299a0, 53103ae318a19569ac82cee5062de2cf73bf386c, 4a7eea3ec3080ecb277bfe466afce4822a1071d7, a85c45ce7c893388e8eafa8a653b042e1497db48, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, 811df72e210e20de99719539505da54762a11c6d, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, 1d7531db9272f7838e33616075e1e64532fd013a, 0427110f0e79f41e69a8eb00a3ec8868bac26a4f, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, ca05ca5f70616456d0f2e05c4a51cbf8b8d273f7, 89858723bec341178f2b00d34ea3016baaaf71a6, 31a61d009442436d04b9d4e1c5beee37172289ae, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, d78e61d0fe29b823f9630ccfa647c3029ec21f2e, 11342d45911ee8a7c9e3a94117ce774ad7036172, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, 0a829289a16ae48837cc2905635435db98bacc76, 2bc9d6830610e18f110aa7984f7c0271d6b17eeb, 57a3fc6d0aaad3c10c793b4e59390ca04c935282, 139a0c7a60667979dcb57eae677f75ff3f0b0196, a9640bac0b45a804d07fc5914feb08af8f2a73f2, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, f0eb17c6def9aaf95ef3c8ce12b9c3ceb9030274, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, 78df3ba26593620ab689fe5a97b7e739434a053b, 3337e4b2e63e5e99171f9da9d161771f5810c27a, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, 74b4f16c5ac91e3e7c88ae81cc8c91416b71d151, 819167ace2f0caae7745d2f25a803979be5fbfae"
2b061f7f108fdd4e90452aaaead574c7b4b5b780,"IoT Data Processing in the Fog: Functions, Streams, or Batch Processing?",https://www.semanticscholar.org/paper/2b061f7f108fdd4e90452aaaead574c7b4b5b780,Conference,"When processing IoT data on a large scale, the cloud is no longer sufficient and it has been proposed to move parts of the computation closer to the IoT devices – the so-called fog computing. There are also three basic processing paradigms today that lend themselves to IoT data processing: stream and batch processing as well as serverless functions. Where to place which part of the data processing and which processing paradigm to choose, however, is often unclear. In this paper, we give an overview of all three paradigms as well as different data processing use-cases. We use these to derive a decision framework which provides general guidelines for placement of processing and the respectively suitable paradigm when designing a large-scale IoT data processing architecture.",2019,33,"52198091, 3077067",52198091,Madrid,2,"7557913, 1666260553",Y,"Interestingly, DQN + heuristic reward approaches expert performance while behavioral cloning never achieves expert performance level even though it has actions.;Moreover, the authors proposed updating the parameter \theta of the generator g_\theta.",International Conference on Fog Computing,19,"processing,data,iot,data processing,data processing","05b8b67451fb105576c58af960e6e6d98f9103e7, 6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91, 89858723bec341178f2b00d34ea3016baaaf71a6, 1109d62ebd2b29a7dc148bc30dd6cfc803a63dec, ed935c6b359a7a486c28240d796e84897d095125, 17fca92ffd527c78c5dc6c7953e96671743807fa, e32a2519b59d62cff6cb8136ee242dc3754ed57b, 7d873a9c49d3864709aa762f8740edcdbd7369c5, 5687c9e8da574453fd873662b95caec70dac9d1e, cd5b4b113d5fa90b5dc5aa372111b89d18df88fb, 752604994a7ca548ff2954114fc61a501d857b1c, a8b995f0da78a79447dfb18c2337972b044f4239, 795550a5294eb05ea4f3b14f0b1c21a405493d85, b6becea767675ea6ee43c78ce747077a5050019c, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, 5031790972d496547b6613d46a4a0134c824db6e, bc00ff34ec7772080c7039b17f7069a2f7df0889, 56266342b01a4f2ddc28a1e8401dbbad105736a5, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, f9253a3e2627f1c2a0a7e2cea320a4ec4e4d2ff9, 5bbe106eeba21bcb4ac7d3ffc128f2ab581ffdfc, 22c141b489e6e189f5996537b0a908fc10f90de7, c5d3fbc024ad9a0d28669b6030de23fc5c3f7888, 557ee73ff4e87ca0cb1b6c78af0730a2d94b710f, 9db0247728950788a2b42097d81dc0e24eed6bb2, 48fc9c42522184c652742255fdf31f7b9ed7ebae, a6bba5ce9867c978210e3d056691b5c1e769b760, bad4c08f03587e38ee960e2aa76e16d722826e7c, ef35da3a3c471a6a15c7a3b09586483eb50cbef0, 31a61d009442436d04b9d4e1c5beee37172289ae, 9a60d3c9d3a5b7f165325fca45bd418d651682d3, ce54e3b89a2570035b70885e6901ad4c92ae41c9, e5d720767b7a539bb2edaa98eaf572a4506a79c6, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, f7b69b8babfa607f2e8b0371f24cb720a7a827d6, 92afbbe41174a545f9da9992e33c9a9592e529aa, 28cc044d5ba938472bc53d87240583982ad21663, 14014c024674991149f3ecf9314c93f7e029ef1a, 736ef8a32d6c5f76a21d61299300cf796480d507, 74bc39003e65119eaa6ba339a61b45b417a638b7, ee6f23590783adec7cf6b2030c6a46f3117a708e, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, 529ff7d6441d244212cf2becafd12a7e67ac56d9"
2f44ab0e52eb98fdfc0086638887f175b6f2fe4b,Benchmarking Distributed Stream Data Processing Systems,https://www.semanticscholar.org/paper/2f44ab0e52eb98fdfc0086638887f175b6f2fe4b,Conference,"The need for scalable and efficient stream analysis has led to the development of many open-source streaming data processing systems (SDPSs) with highly diverging capabilities and performance characteristics. While first initiatives try to compare the systems for simple workloads, there is a clear gap of detailed analyses of the systems' performance characteristics. In this paper, we propose a framework for benchmarking distributed stream processing engines. We use our suite to evaluate the performance of three widely used SDPSs in detail, namely Apache Storm, Apache Spark, and Apache Flink. Our evaluation focuses in particular on measuring the throughput and latency of windowed operations, which are the basic type of operations in stream analytics. For this benchmark, we design workloads based on real-life, industrial use-cases inspired by the online gaming industry. The contribution of our work is threefold. First, we give a definition of latency and throughput for stateful operators. Second, we carefully separate the system under test and driver, in order to correctly represent the open world model of typical stream processing deployments and can, therefore, measure system performance under realistic conditions. Third, we build the first benchmarking framework to define and test the sustainable performance of streaming systems. Our detailed evaluation highlights the individual characteristics and use-cases of each system.",2018,145,"3317889, 1731210, 1680579, 7482477, 2868254, 1733290",3317889,Sarajevo,3,"2082426870, 1967677, 2772470",Y,The authors indicate that they do not need to compare to variational methods because Gal and Ghahramani 2015 compare already to those methods.;This section could be improved by demonstrating the approach on more datasets.;There are many multi-agent techniques that can be applied to the problem that would have served as a better baseline.,IEEE International Conference on Data Engineering,34,"performance,stream,systems,data processing,data processing","dbabab9bf5955558f73a37644f4bb626106a6d73, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, a81ba6a07bf7a2ecff871e3362a77404501d0927, 3789eb72c32ecf5e33442570358dd786dd67c8a2, d3855b7351c11145e51301e6b686f748ca35c802, 03532123ccffae8d411264320e8a5ae2b6eddea0, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, facd5f5deb152229ceb1803434d8690a09ab4129, ac67d5f9c89d8d72fbd074f94079608220348f3f, 11cf88dce827bd67cbfa60400306318022e736d5, 91e611c3e8705002438fb4439733e47ddec85b5d, 5030702fea15d66a73fc997325431f1d7945ad9a, 8dd0c1e955c66092ff951941a151336211e6e171, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, 9c1265fd47df8d825f0ddd6d9ab834002592e38e, c15f30a3e84910a28cc560e7db097fd99339e8c1, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, 72243bfa618fd6c3c01bb7b4885a4a456b2a6071, 2478f43de2f1fcd9301a81e9b42d86b1d46db02d, 3994334c81478a4b17341eb1f494dbccbb73d999, 9e195234688778b2beb3528632e78dbabf816332, c0aec04ee86c0724d61c976f19590fbe9c615723, ed9e7821b3e51c7e59183300d6c8cf90c8de0f26, 8894d431a768a35dc7ca4d762ebdba4f407b978c, d280cf82a6144b3e168840802b1a8a14d4eaccb9, 11be2469ab1d1c508e7b6e14148990741ba87884, b05306f0b142e5afb3974b1b79996e5b82653662, 74bc39003e65119eaa6ba339a61b45b417a638b7, 0fa554d981809c5eb78956c779f75092c4f6c16b, 818de553ecd306735971aba04bbfc29d17457084"
1b3675fc0f2b16743b1e1f0c2f84829cfdb3d34f,DeepSense: A Unified Deep Learning Framework for Time-Series Mobile Sensing Data Processing,https://www.semanticscholar.org/paper/1b3675fc0f2b16743b1e1f0c2f84829cfdb3d34f,Conference,"Mobile sensing and computing applications usually require time-series inputs from sensors, such as accelerometers, gyroscopes, and magnetometers. Some applications, such as tracking, can use sensed acceleration and rate of rotation to calculate displacement based on physical system models. Other applications, such as activity recognition, extract manually designed features from sensor inputs for classification. Such applications face two challenges. On one hand, on-device sensor measurements are noisy. For many mobile applications, it is hard to find a distribution that exactly describes the noise in practice. Unfortunately, calculating target quantities based on physical system and noise models is only as accurate as the noise assumptions. Similarly, in classification applications, although manually designed features have proven to be effective, it is not always straightforward to find the most robust features to accommodate diverse sensor noise patterns and heterogeneous user behaviors. To this end, we propose DeepSense, a deep learning framework that directly addresses the aforementioned noise and feature customization challenges in a unified manner. DeepSense integrates convolutional and recurrent neural networks to exploit local interactions among similar mobile sensors, merge local interactions of different sensory modalities into global interactions, and extract temporal relationships to model signal dynamics. DeepSense thus provides a general signal estimation and classification framework that accommodates a wide range of applications. We demonstrate the effectiveness of DeepSense using three representative and challenging tasks: car tracking with motion sensors, heterogeneous human activity recognition, and user identification with biometric motion analysis. DeepSense significantly outperforms the state-of-the-art methods for all three tasks. In addition, we show that DeepSense is feasible to implement on smartphones and embedded devices thanks to its moderate energy consumption and low latency.",2016,529,"2325031, 3225210, 2580112, 2085709, 1730531",2325031,Dublin,2,"143695559, 1402912902",Y,"I found it would have also been interesting and helpful to define and show a new metric that incorporates both accuracy and compression rate into a single metric, e.g., how much accuracy is lost (or gained) per compression rate relatively to the baseline of no compression.;A full implementation of binary CNN with code This paper builds on Binary-NET [Hubara et al. 2016] and expands it to CNN architectures.",The Web Conference,26,"applications,deepsense,noise,data processing,data processing","10d89b13a6309a531c35701d37d3bd76a27a3942, 34ca47eed139a7f0694611528f75debc43385518, a357f1ff27e184d9a5ef69e665e8ca891032bf71, 256db9dba1978f004a67c86ffc321563b1aee79a, 7c217cc7524251f42887438834912e06129c3299, 695bdc6e24608364491b9418a220c65a7cd17413, 155eeede0c070f1f017ba5c9f6cacd7ae0b098aa, e2a58fd18961c3941102989e3a3d0d27c615e015, 5bbe106eeba21bcb4ac7d3ffc128f2ab581ffdfc, 49fce234ad7f6d2af757f078b29c0118068075a3, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, 925af6dcb0f8e6f3a5b2613400277be4b5434d10, eda6756ab2844c390584686dc5e6385f4a8369cd, f4cfc7cbad257f1688772d59f694c16189dba811, bad4c08f03587e38ee960e2aa76e16d722826e7c, aca6d5f3866372a4506cf15773ae298f18c3f453, e1e43d6bdb1419e08af833cf4899a460f70da26c, af13a92977d4f4dc5b28b13746d86111d42939e8, 16753e0317730e8c1b297338300a8c6163dd06f2, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, fd0496ab020acf366375615ab40235e6dd3c5897, c84aa52bee5116f80c7740503edff4b08f733c3b, 590b617c08d34bc6caed7e4490c0b22a9c516e86, 98ce7af921e7c52d81df64d632d34eb09522cd75, 84725855d10b531eb8cbe54935dda0440c2fc750, 643da4c4de1954daeac571a82367241db012a8bf, 1d5adacc5d4d226e76c35bf19018f9e76759f127, 46c266b3d1274dacd7fce27ee8cb4d587f087a58, 23466d271676ae467cbe85bb1993682f3502e840, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, da3f33d858586d24cb265e79eb54f3746e998f57, 82870bc488b57cdf5ea62877109a7278af2926b3, 676664ee7471738577f641e6159e7596625b7fdb, b9b639522465cc606df878eee62e7f9c4bf19e62, b473e91cbe80c8b46451b49153cd5f93030480ab, 58ed1fbaabe027345f7bb3a6312d41c5aac63e22, d983427574a41bf485e31ab983acc1c742e9e24c, d1a6b3a5efde3783b53f822dc8dd00aaac934b95, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, 2743e66939b30c43affb3c9e31f20cfac2109045, dec26f0640e3a4fdb116735526302ccb9f49867e, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, be2b0396de9431bae931642516a1d3e4906329f5, c6e242fce9e5e4ba2c4dc869cfb0afabf2020852, b889b1d6944213bc2ca29e3ad07ee65ede20892d, ce51eff5b529ee572dab1c1f38f20adc8e89bab2, 1ca6d6682204f0214338f7797bea056444e908bd, 87048df918c34b662bc0d28894efa430d70a9206, cf41991d89301c3c12420d150792cb1163999962, 6be56f559a74c0124526242e70cbdfd16cbc60a7"
639bfab64e2f35917d450013e136cb24c7755fad,Portable and Error-Free DNA-Based Data Storage,https://www.semanticscholar.org/paper/639bfab64e2f35917d450013e136cb24c7755fad,Conference,"The paper involves critical evaluation of all the significant encryption, decryption, and cryptography techniques that are being used for DNA (deoxyribonucleic acid) data Storage. This paper covers the basics of Data storage in DNA and how it can be used to stockpile data and how it is highly promising in changing the data storage methods of the world in the foreseeable future. All the vital methods which are being used for DNA data storage have been discussed. These methods are also be applicable for data storage density and graphs are plotted. This paper also examines how DNA is being used as a tool for cryptography along with the fundamental limitations of DNA storage. Towards the end, the future scope of DNA data storage is questioned, and with the help of a density graph the research predicts whether ""DNA data storage has a future scope or not""ƒ",2021,16,"2144633223, 2146970349, 2118511088",2144633223,Ljubljana,2,"150127950, 1736651",Y,"A number of different fine-tuning regimes are explored.;[1] Wang, Weiran, Honglak Lee, and Karen Livescu.","2021 4th International Conference on Recent Developments in Control, Automation & Power Engineering (RDCAPE)",4,"data,storage,dna,data storage,data storage","256e95ca331cbd35b3a23cc306b6627e6771a963, bc44c0c64a473e035b11ae60a1993ad3db1acd2e, a18f02e5c24e1f924aea268dd343bbdea234f2bb, e5194ae88d63c7549678b1b73cfdaf7112164272, 0ecf3f089e6dc7944b440227069b9d0143e18d78, 22cba8f244258e0bba7ff4bb70c4e5b5ac3e2382, 8ab93bee04cd5bdbd96002b2e325d02f61ba695a, 8760bc7631c0cb04e7138254e9fd6451b7def8ca, 25761ba4bdc054bfe902fe7c5d6338be6d00d491, 2ed691a353fa48403d493ab658f5f267a42f0bf1, 54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f, b9c974380649749320f4a02e33b2e5014e7f1756, b52db9e41e15f76bdcfbe674abe0314af545c430, bb826d9ccd116076a267dfcb048cdd747c11b255, 709f7a6b870cb07a4eab553adf6345b244913913, 390bcf15a1b13cb0d5966859c35c69a31238e838, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, 590b617c08d34bc6caed7e4490c0b22a9c516e86, c9cc6a3c50b32e801f20b4f0ed5daf3e9550f9c1, 82870bc488b57cdf5ea62877109a7278af2926b3"
c84389369720dcd2f004c48e58fbac2c45c8f092,XIndex: a scalable learned index for multicore data storage,https://www.semanticscholar.org/paper/c84389369720dcd2f004c48e58fbac2c45c8f092,Conference,"We present XIndex, a concurrent ordered index designed for fast queries. Similar to a recent proposal of the learned index, XIndex uses learned models to optimize index efficiency. Comparing with the learned index, XIndex is able to effectively handle concurrent writes without affecting the query performance by leveraging fine-grained synchronization and a new compaction scheme, Two-Phase Compaction. Furthermore, XIndex adapts its structure according to run-time workload characteristics to support dynamic workload. We demonstrate the advantages of XIndex with both YCSB and TPC-C (KV), a TPC-C variant for key-value stores. XIndex achieves up to 3.2X and 4.4X performance improvement comparing with Masstree and Wormhole, respectively, on a 24-core machine, and it is open-sourced1.",2020,62,"41211459, 2115725948, 1508399092, 1508456598, 8491577, 2136369359, 2118438836",41211459,Copenhagen,2,"2551387, 2108451006",Y,"- The claim of Theorem 2 in appendix B does not follow from its proof: what is proven is that the value of S(w) lies in an interval [1-e..1+e] with a certain probability for all w.;Only simple baselines (eg autoencoder, kmeans) implemented by this paper are included.",ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming,25,"xindex,index,performance,data storage,data storage","d617f51833860dc50d202af7f80be71304b2e994, b0b770fb8c7760749c88e3c83ae173cdb07f7bd5, 4b06c7e29280b1c6bc05c9df39023b48fef02c93, 9817bf0f78047452761e950c02a1a56f59a1e593, 087dd95e13efd47aef2a6582e6801b39fc0f83d8, 11be2469ab1d1c508e7b6e14148990741ba87884, 2afa490dde7a8c582d889530c7f8b042fef6a8b7, 1051abf1e3dae90241ad15b3f98f2e41197ee611, db4cf9f6a653d5c15973e836c800ea47743251ae, 9619cde5c79d91ca5c432186668618312175f8dd, b0c34618ffd1154f35863e2ce7250ac6b6f2c424, 32a849fe3020144e5ba82ba0442ac571f554ca31, 218062f45c15f39bc8f4fb2c930ddf20b5809b11, e70ddfb04969b663ef8cd711a70a0acf563d6e5f, ecef432e7f6c9f431d5b34706a8de1fdebec46f9, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, 2141334fad7248fc707607bc9453d44686ae07a7, fa7b8acd47631bada5b66049824bfd335ac6bf8f, 9312e5efa0dcef1445d45a41771f12e2a8dc6715, 0095acc4f2c3255cf38fdf844003c97858adb418, aa6c2afadd660fe4efbac699f7854e8f6f240c38, d21703674ae562bae4a849a75847cdd9ead417df, 1562390dd212516cd857009cbd4f857a902d1f3d, 4e13a8e8ba8d33e15ed037bfca7c651047533990, 35adeef964fd344288febc7def2780007587724f, b135e330cc1473c8c24fa63bb9a5b64f51993f9e, b889b1d6944213bc2ca29e3ad07ee65ede20892d, 7904b3446775ed8c79f4f94001a16b706989c462, e8b7a9be9f2d0578a95319ed5841978e10429967, bd6c027a3604d6c8fa23435bf382455b2bee436b, 10aa2be24951e6de76b630482a645d79354c4cde, f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6, 4a7eea3ec3080ecb277bfe466afce4822a1071d7, 1bb022b27ddb987352bfc002c8381a6f646d0ebb"
29279e52008848ee494f5af1b836313ab99c25ed,Efficient Decentralized Data Storage Based on Public Blockchain and IPFS,https://www.semanticscholar.org/paper/29279e52008848ee494f5af1b836313ab99c25ed,Conference,"Blockchain technology has enabled the keeping of a decentralized, tamper-proof, immutable, and ordered ledger of transactional events. Efforts to leverage such a ledger may be challenging when data storage requirements exceed most blockchain protocols’ current capacities. Storing large amounts of decentralized data while maintaining system efficiency is the challenge that we target. This paper proposes using the IPFS distributed hash table (DHT) technology to store information immutably and in a decentralized manner to mitigate the high cost of storage. A storage system involving blockchain and other storage systems in concert should be based on immutable data and allow removal of data from malicious users in the DHT. Efficiency is improved by decreasing the overall processing time in the blockchain with the help of DHT technology and introducing an agreement service that communicate with the blockchain via a RESTful API. We demonstrate the applicability of the proposed method and conclude that the combination of IPFS and blockchain provides efficient cryptographic storage, immutable history and overall better efficiency in a decentralized manner.",2020,14,"36877027, 145667775, 2509535",36877027,Vaduz,3,"3410500, 36347083, 1752784087",Y,"The paper frequently refers to ""embedding"" ""imaginary trajectories"" into the dynamics model, and I still have no idea what this is actually referring to (the definition at the start of section 4 is completely opaque to me).;One could also argue that the manuscript would profit from a better theoretical analysis of the IRL problem, say: C. A. Rothkopf, C. Dimitrakakis.;The encoder network from NEM can be seen as a recurrent network which takes one latent variable theta_k at time t and some input x to produce the next latent variable theta_k at time t+1.",2020 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE),20,"storage,data,blockchain,data storage,data storage","39602922b04885047254444fd1a1586d797617ce, 8ab93bee04cd5bdbd96002b2e325d02f61ba695a, a781cea542e99c6bb9422858e7c04eaef18c7673, f156ecbbb9243522275490d698c6825f4d2e01af, db6ad6ded1cfa26fdc7437f27fb823ec533e96fe, f381c53aeb7742e4047d06d84f9e0c4f523231a3, 3813b88a4ec3c63919df47e9694b577f4691f7e5, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, 8799ec4bdf98bc313aa8c41a706a385e026d1f88, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, 34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61, d235a9085e0543fcbe502fbc269f9a8ee01dcbab, b795c74a0150ec091003ffbaa5bd7d74487c137b, 5406e153957dd7a165264da6e6e5d81251997404, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 5687c9e8da574453fd873662b95caec70dac9d1e, 4fcfe83c05402b5c5fb6e853082e74af6379d7f9, 5f51d468ce730eeade7e9f419a1fe7152582be25, bb826d9ccd116076a267dfcb048cdd747c11b255, b977e8de38dc0d13817bca1ed20036badfe2a58c, 1661d0d8d47cac41e01c59c60aac3675b4396698, 4e13a8e8ba8d33e15ed037bfca7c651047533990, 3bc9bb1f2218dcbd15c3b7cdfcb43077a3f30779, 9181b0d801dfcd7723a3ede201f0543078e2c149, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 9e195234688778b2beb3528632e78dbabf816332, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, 6dd481d5eb8d76d4b61a1829c7687d008e0937ab, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 2bc3644ce4de7fce5812c1455e056649a47c1bbf, 96b51d940653710f9d099d89ade86b44fa9bdd8a, a2a514ed839dafdd0fb76d6c2615f25f35bf8087, 86dbd884043eb5807c61d2c65b813e673b4a04fa, 811df72e210e20de99719539505da54762a11c6d, 88932b1563316e4ed7a61c4c2c0c90d7aa9bb308, dcbaf58b16ac7ef947879ea37c021466357b291a, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, c9f320789e98d2c7a798a9705e26dbe317677966, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, f12930cd5f58990badc1a7c5d2749cad004cfb0e, 8674494bd7a076286b905912d26d47f7501c4046, 343500e0052eb1b683f32b00efbbd1331c94184a, d8a5474f450330ad25c1e22f27e88f3630cb840d, 2ddcd47b28eb4b43317a34cf56e83309f5347699"
492c389d560d9db39c758d07e635408d2e0eaf7d,COMPARATIVE STUDY OF NOSQL DATABASES FOR BIG DATA STORAGE,https://www.semanticscholar.org/paper/492c389d560d9db39c758d07e635408d2e0eaf7d,Conference,"Nowadays the amount of data that is required to be stored in databases relates to big data and that data is required to be stored in real time, but knowing that DBMSs cannot support such a thing, we use the NoSQL databases. NoSQL databases support dynamic scheme design, offering the potential for increased flexibility, scalability and adaptability over relational software. This makes them suitable for Web Applications, content management systems, and other uses that involve large amounts of non-uniform data that require more frequent updates and formats in different domains. There are already a large number of NoSQL databases, with a number of over 255. Their performance depends on various factors, so it is important to compare them based on the requirements of an application. So, during this research there will be made a comparison of some of the most popular NoSQL databases for big data storage.",2020,14,"14833620, 2285582737, 20802526, 40992700",14833620,Brussels,2,"41037252, 2061706386",Y,"“We can quantitatively determine how close the posterior is to a FFG distribution by comparing the Optimal FFG bound and the Optimal Flow bound.”: Why not just compare the optimal with the AIS evaluation?;simple, effective method, some discussion/understanding missing This paper proposes a new method of detecting in vs. out of distribution samples.","14th International Conference on Computer Graphics, Visualization, Computer Vision and Image Processing, 5th International Conference on Big Data Analytics, Data Mining and Computational Intelligence and 9th International Conference on Theory and Practice in Modern Computing",14,"data,nosql,number,data storage,data storage","2afa490dde7a8c582d889530c7f8b042fef6a8b7, 1051abf1e3dae90241ad15b3f98f2e41197ee611, a5710a7a54b50c26f3c40c37a19b2ec63330e90b, a926093ef103c02fd5ef7310e0d41b83d1958ed5, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, c292e473b3825eeb9db03c70b2e1c033aea190d5, b2f8df88de24fe0ef74bda0eec1a0c3e7329b9f9, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, 5c39e37022661f81f79e481240ed9b175dec6513, 818de553ecd306735971aba04bbfc29d17457084, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, c50a909e20bd07f4aea09dc6dae539b45b406a96, c84aa52bee5116f80c7740503edff4b08f733c3b, 780c725848aac1118d00c8bb306719ec803369cd, 8ec5896b4490c6e127d1718ffc36a3439d84cb81, 14dd50979af27bd2574c8068db11d27028b56afd, f04d8e558c11ac0fc9318c4f3d61b651c62ddcfa, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, cf41991d89301c3c12420d150792cb1163999962, b61b260de1599e6e89491cad9160898fcd3b34c2, fa75a55760e6ea49b39b83cb85c99a22e1088254, 1986318d8a565fbff8fde545b8d0c2012c6462d8, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, 557ee73ff4e87ca0cb1b6c78af0730a2d94b710f, d1a6b3a5efde3783b53f822dc8dd00aaac934b95, 224c11bc51b4959bc787d6681c2b152468294b11, b795c74a0150ec091003ffbaa5bd7d74487c137b, a1d36749b89e46a8eaadf8ba40788741c192fb1e, e8b7a9be9f2d0578a95319ed5841978e10429967, be7cb8f79bc018e57467168fc0c7f8ad59bba04f, 1597449a7f64b6bd24639b4deab96c8a8c184177, 8713452753fd01de5616121af93e173d4f74eaf6, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 57e6cca1479a4642f867e69b4dee93d14259dc3d, ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba, 8d942a3b52e2ad16ff8e5970be59591970d89fae, 74b4f16c5ac91e3e7c88ae81cc8c91416b71d151, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 7fa273f450251523e6b7fcc2eb3fdbdfd4a30493, 634fe61f3ab6692ea4733989a1c76e793bb8b69e"
cb12fe714dd4fa1d14b6a8023d494c14c89e90ea,ElfStore: A Resilient Data Storage Service for Federated Edge and Fog Resources,https://www.semanticscholar.org/paper/cb12fe714dd4fa1d14b6a8023d494c14c89e90ea,Conference,"Edge and fog computing have grown popular as IoT deployments become wide-spread. While application composition and scheduling on such resources are being explored, there exists a gap in a distributed data storage service on the edge and fog layer, instead depending solely on the cloud for data persistence. Such a service should reliably store and manage data on fog and edge devices, even in the presence of failures, and offer transparent discovery and access to data for use by edge computing applications. Here, we present ElfStore, a first-of-its-kind edge-local federated store for streams of data blocks. It uses reliable fog devices as a super-peer overlay to monitor the edge resources, offers federated metadata indexing using Bloom filters, locates data within 2-hops, and maintains approximate global statistics about the reliability and storage capacity of edges. Edges host the actual data blocks, and we use a unique differential replication scheme to select edges on which to replicate blocks, to guarantee a minimum reliability and to balance storage utilization. Our experiments on two IoT virtual deployments with 20 and 272 devices show that ElfStore has low overheads, is bound only by the network bandwidth, has scalable performance, and offers tunable resilience.",2019,23,"48439542, 2229703159, 1761220",48439542,Belgrade,2,"2112455515, 48576745",Y,My concern is that one-bit system is already complicated to implement.;* Figure 2 seems like a test made to work for this method and does not add much to the paper.,2019 IEEE International Conference on Web Services (ICWS),19,"data,edge,fog,data storage,data storage","aea731e7cf33aa3d482b13f42cedbc1adb3271c6, 33ec7eb2168e37e3007d1059aa96b9a63254b4da, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, 8babcaf89f8537dc628a029ebf932100f57289fd, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, 01bf0e83159712fbbbd12171a7e268547a4cfbc5, 695bdc6e24608364491b9418a220c65a7cd17413, bc44c0c64a473e035b11ae60a1993ad3db1acd2e, 780c7ead33428d282044519fee5e773ad56d5a2c, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, 0c4070272fb7d6b98971a107b022ff8abf0aa55e, 83cebf919635504786fc220d569284842b0f0a09, 22cba8f244258e0bba7ff4bb70c4e5b5ac3e2382, 8fd8cb4951ae9634a378383a880fbf16ac5d6926, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, 71854ff4306cf65c3c2161f7be2d0346275f72d5, 0d065e8688c38bb0148203a1738f47184a5b58d3, 42375e9a309fe5bf5c671c918cd8c9d5d85568cc, 07cca761749bfe21c2d096ff60f32b574d5c84c4, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 0090023afc66cd2741568599057f4e82b566137c, 5b6ec746d309b165f9f9def873a2375b6fb40f3d, 712e32e2da67428ba6c6add1605410e1c3792883, 42543dc42e65609bbbf2be470d54dd923532c36a, 1597449a7f64b6bd24639b4deab96c8a8c184177, e30d9b8ce108d982169621b88a5e3fb69fec70e1, eb9e0da8b7170e3ca4364f2f9010599c2d2556f1, 8894d431a768a35dc7ca4d762ebdba4f407b978c, 7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2, 0c00a328fa7cd56ee60338c54e89bd48310db80b, 41c93960a066876d5e4f1dacaef75cd8daa2791f, 32a849fe3020144e5ba82ba0442ac571f554ca31, b55e490637babd50dab3cdaaa3a60a2be6eb1cbb, 16fa12ebc578df676f3dda5453ad56c15a0d6702, 155f27879f185f1ab04107c91c2ae7cf6a910a03, b52db9e41e15f76bdcfbe674abe0314af545c430, e968ae8e98fff9e28468383a1826fca4a2ae5245, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, 40416ac3bf78583eea37661b1b446e9939245b3e, f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6, 31a61d009442436d04b9d4e1c5beee37172289ae, 74bc39003e65119eaa6ba339a61b45b417a638b7, 32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b, b9c974380649749320f4a02e33b2e5014e7f1756, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, c9cc6a3c50b32e801f20b4f0ed5daf3e9550f9c1"
90e4330fed2da147dd41490e8ad638b618112b3d,Fast Erasure Coding for Data Storage,https://www.semanticscholar.org/paper/90e4330fed2da147dd41490e8ad638b618112b3d,Conference,"Various techniques have been proposed in the literature to improve erasure code computation efficiency, including optimizing bitmatrix design and computation schedule, common XOR (exclusive-OR) operation reduction, caching management techniques, and vectorization techniques. These techniques were largely proposed individually, and, in this work, we seek to use them jointly. To accomplish this task, these techniques need to be thoroughly evaluated individually and their relation better understood. Building on extensive testing, we develop methods to systematically optimize the computation chain together with the underlying bitmatrix. This led to a simple design approach of optimizing the bitmatrix by minimizing a weighted computation cost function, and also a straightforward coding procedure—follow a computation schedule produced from the optimized bitmatrix to apply XOR-level vectorization. This procedure provides better performances than most existing techniques (e.g., those used in ISA-L and Jerasure libraries), and sometimes can even compete against well-known but less general codes such as EVENODD, RDP, and STAR codes. One particularly important observation is that vectorizing the XOR operations is a better choice than directly vectorizing finite field operations, not only because of the flexibility in choosing finite field size and the better encoding throughput, but also its minimal migration efforts onto newer CPUs.",2020,28,"1500391633, 143804003",1500391633,Podgorica,2,"2039003, 2158141874",Y,I feel that the authors should give a more prominent disclaimer to potential users of the test.;Flexible muscle-based locomotion for bipedal creatures.,USENIX Conference on File and Storage Technologies,20,"techniques,computation,bitmatrix,data storage,data storage","598231eb906b183f7a2a408ef4536127e11e3de9, 0e141942fa265142f41a2a26eb17b6005d3af29e, 0574a3abc98f0e6bd035a4ff4ea107cfba45d3d4, ae026f29c2d571871f426ff4873d43b4ff90b9ad, 4be9368abc2474d6fd38639e523cf03af1873fd9, f0eb17c6def9aaf95ef3c8ce12b9c3ceb9030274, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, 10a69070c8585dc5564b11b0e53dbe4d565f9ce1, 1452b25a7680bbb2c66dd7dfca6704292405da92, 2ee03e28208a9310a9be4032c2b04ebdddb83cc7, d6f002d88638de71114dab083f0ea8ceea6b6a5a, 49fce234ad7f6d2af757f078b29c0118068075a3, 17fc5c9b37ba63fb3280ddf9909a183523e7bcc3, 5b6ec746d309b165f9f9def873a2375b6fb40f3d, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, 1cd497e82bdc46a9d3d28b2316ea7cbe9aee5467, d88083e37c44461ce3e404bd57257cd3edb07d4e, e02f91d625cd32290d4ede0f31284da115844316, ecef432e7f6c9f431d5b34706a8de1fdebec46f9, 96023195e889fc258e6ff30aa99d250982dfae01, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, 1cff064f815111a71a98afda7aee1867ad617901, e3b37c1c955b2b10809040ce277edae5333b99c3, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, 01f0f5205d03870f172ae8f04e33356d5a0af221, ae38dc77a962161107361f213db9216ee1274037, b415e836d447ea9efb7629a1de67cd2a6f9e7ba8, a85c45ce7c893388e8eafa8a653b042e1497db48, d93bcf0685c15c45d078eafea565969c04daccd3, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 00d1f3423a33f73ca6aee884a58834547475d2f0, 9181b0d801dfcd7723a3ede201f0543078e2c149, aa6c2afadd660fe4efbac699f7854e8f6f240c38, 71a85e735a3686bef8cce3725ae5ba82e2cabb1b, 3a74bed911ccf213d9595b2b02a5b1c4ac4dcaf8, f406aceba4f29cc7cfbe7edb2f52f01374486589, 21042565ec941f4fa31ac5a0af85a1a84ff21f1b, 0090023afc66cd2741568599057f4e82b566137c, 6dd3e404aac097f63d42dc19fd08c7ec281f90b4, 4e58100b319d74f97ed550a4e5fa32dea8c06fe1, 64c4c6cb66457c79f89ba4f2529b57bfce19ced4, 819167ace2f0caae7745d2f25a803979be5fbfae, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, 218062f45c15f39bc8f4fb2c930ddf20b5809b11, 33ce8103b129149eb78ca2fa48538e25c9242c08, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, cd20ed8e70f6459f5617350f6b84c5c6d5929cb7, 7872f34e2a164c5cf3c34a7a7433dc3342b6c7ea, 771a858c35f6d6e6d1017dde95368de3794738a6, 2af8907d4a974ae41044581f5e5d67317cb08568"
695bdc6e24608364491b9418a220c65a7cd17413,Semantic Data Querying over NoSQL Databases with Apache Spark,https://www.semanticscholar.org/paper/695bdc6e24608364491b9418a220c65a7cd17413,Conference,"The rapid growth of semantic data in the form of RDF triples demands a scalable distributed storage and efficient query processing engine for its management and reuse. To overcome the limitation of native RDF stores and traditional relational database management systems and scale adequately with the exponential increase in the size of RDF datasets, Big Data processing infrastructure like Hadoop with MapReduce have been used. NoSQL databases such as HBase and Cassandra for storing large-scale RDF data and in-memory data processing to execute SPARQL query as SQL query using Apache Spark is proposed in this paper. This paper presents techniques for distributed RDF data storage and querying schemes for HBase and Cassandra clusters. We also present a compiler that translates SPARQL queries into their Spark SQL equivalent for execution. An empirical comparison of HBase and Cassandra systems using datasets and queries from Berlin SPARQL Benchmark (BSBM) and SPARQL Performance Benchmark (SP2Bench) on Microsoft Azure cloud is presented.",2018,8,"2042697689, 1720381",2042697689,Stockholm,2,"49573525, 2220547623",Y,"It seems like the proposed method is quite generic, so why not apply it to a stronger baseline?;The experiments are very clearly presented and solidly designed.",IEEE International Conference on Information Reuse and Integration,18,"data,rdf,sparql,data querying,data querying","8d06f13ec042f0d8d845af5deadeb3dd9ad11f70, c468bbde6a22d961829e1970e6ad5795e05418d1, 53b047e503f4c24602f376a774d653f7ed56c024, 156d8e2aa90b5ccc9be10477ca70deaad0151387, f14fc9e399d44463a17cc47a9b339b58f6ef7502, c2a448bb511ebae41a87e69891da8bbf17ddba3d, df7336844a31165db0ae08f1cd0f560c9e3faeea, e23b2e47b0ac6f50000078828f27571804dcd6a2, fc77048474ccd34c6507701591c2e6ab3ca647ef, ea160adc0d78e54669281b8b145bcd832e648fee, 5dfde01d761d97c3a6c609007531973eb1229d09, 8babcaf89f8537dc628a029ebf932100f57289fd, 643da4c4de1954daeac571a82367241db012a8bf, 54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f, 8b7b2e88207fc2dd849f5d83c9b6e890f0174abc, dd2deed2ce6e110236a1280db765fa02c7488eb1, 9e5d389fcdce4c0a6827ebc804b4e863aa28c14c, 102ebe229df18c8733ea1b8def56cd79996e2178, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 0574a3abc98f0e6bd035a4ff4ea107cfba45d3d4, 7909428ca7d813331bbe4d33d07ab09e984b41b4"
92912dd895c360f01a6be9c9f6d207642139525e,An ontology-based approach to improve data querying and organization of Alzheimer’s Disease data,https://www.semanticscholar.org/paper/92912dd895c360f01a6be9c9f6d207642139525e,Conference,"The recent advances in biotechnology and IT have led to an ever-increasing availability of public biomedical data distributed in large databases. Analyzing this huge volume of data is a challenging task because of its complexity, high heterogeneity and its multiple and numerous correlated factors. In the framework of neurodegenerative diseases, the last years have witnessed the creation of specialized databases such as the international projects ADNI (Alzheimer’s Disease Neuroimaging Initiative). The main problems to fully exploit this database are related to the querying, integration, and analysis of data themselves. Here, we aim to develop a detailed ontology for clinical multidimensional datasets from ADNI repository in order to simplify the data access and to obtain new diagnostic knowledge about Alzheimer’s Disease.",2018,5,"2756615, 3341829, 50064714, 6922855, 6339256, 2866596, 2484334, 2794706",2756615,Brussels,3,"2104195713, 2162779709, 39765564",Y,"Just because such a baseline wasn't previously proposed in literature (in the narrow scope of this problem) doesn't mean it's not an obvious baseline to try.;Deterministic latent models seem to work better than stochastic ones.;Moreover, a basic system called delta t block implements one level of full approximation and is stoked several times.",IEEE International Conference on Bioinformatics and Biomedicine,18,"data,adni,alzheimer,data querying,data querying","1c2efb418f79b5d29913e014a1dfd78865221c39, 9b54941de1e21826ecc28b32730ac3f69991ede4, 2a15e9ac5593cd1f8bd3a724e78a173037c02fea, 5ea7bf772fecf95cbf53b2c7f719c9440322a115, 1ed33896e82cc3810b349cdffc2039f0b8edd82e, 5d433da6d0f143f20936379910104d2bb139d4ae, ae026f29c2d571871f426ff4873d43b4ff90b9ad, a357f1ff27e184d9a5ef69e665e8ca891032bf71, b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8, 94deb62af3054c49e7d80bd7eb3ed5efe990fc0b, b2114228411d367cfa6ca091008291f250a2c490, 102ebe229df18c8733ea1b8def56cd79996e2178, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, 2c3eef2f17369912e330281d54b535675077e4ca, 215fc60307f741b9db059204e41db8bfb879e606, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, 834fdec542153aae5fe725df801aac87ba5e8f56, b52db9e41e15f76bdcfbe674abe0314af545c430, 182acc7a1c9a63cfdef9f86ee9df43fd75e05060, 4a7477881b66d12e79c704805781d4683a6a6be1, 82663577cf1d08235bb56ad648c9dad36343ccfb, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, 3a7bbc46795929f0eace82b64c44c92a48682fb5, e5d720767b7a539bb2edaa98eaf572a4506a79c6, 3fadb72fb150197aa5eb88a65b244763fb45ab9b, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, 681253389d2cc27103753749f4c7556699d55471, d7b820af40a9e2660ef700d39f7b2e27b43435c5, d86084808994ac54ef4840ae65295f3c0ec4decd, 59c414c9efb77562f5d1aad8af14eaac968c69c0, 38179848e2d6a3ad373b1793848816111428ac36, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, ea160adc0d78e54669281b8b145bcd832e648fee, 590b617c08d34bc6caed7e4490c0b22a9c516e86, e70ddfb04969b663ef8cd711a70a0acf563d6e5f, a60a4e5f7f872b9825ddff5d379857c2091ca52b, 15d1450a8797e2feaa4c0ca4ebcd43c8cbd8b61d, 7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca, 5f51d468ce730eeade7e9f419a1fe7152582be25"
0ecf3f089e6dc7944b440227069b9d0143e18d78,SpeakQL: Towards Speech-driven Multimodal Querying of Structured Data,https://www.semanticscholar.org/paper/0ecf3f089e6dc7944b440227069b9d0143e18d78,Conference,"Speech-driven querying is becoming popular in new device environments such as smartphones, tablets, and even conversational assistants. However, such querying is largely restricted to natural language. Typed SQL remains the gold standard for sophisticated structured querying although it is painful in many environments, which restricts when and how users consume their data. In this work, we propose to bridge this gap by designing a speech-driven querying system and interface for structured data we call SpeakQL. We support a practically useful subset of regular SQL and allow users to query in any domain with novel touch/speech based human-in-the-loop correction mechanisms. Automatic speech recognition (ASR) introduces myriad forms of errors in transcriptions, presenting us with a technical challenge. We exploit our observations of SQL's properties, its grammar, and the queried database to build a modular architecture. We present the first dataset of spoken SQL queries and a generic approach to generate them for any arbitrary schema. Our experiments show that SpeakQL can automatically correct a large fraction of errors in ASR transcriptions. User studies show that SpeakQL can help users specify SQL queries significantly faster with a speedup of average 2.7x and up to 6.7x compared to typing on a tablet device. SpeakQL also reduces the user effort in specifying queries by a factor of average 10x and up to 60x compared to raw typing effort.",2020,12,"144759563, 47319783, 2119303314, 1796044",144759563,Moscow,2,"2691974, 2273645",Y,The proposed method achieves perfect accuracy in every condition.;What is the per-pixel likelihood obtained on the CIFAR dataset and what is the likelihood on a model where T=1 (for omniglot/cifar)?,SIGMOD Conference,20,"speakql,sql,users,data querying,data querying","4afa7d8e2de43b0b67366b1bce8768f5a246d153, fe4c5074f021cd1c810892c1b6bc267b23aa6e5c, 456c011594ecacdd24298a161787389ccbe4b88b, 665b0c776ff7507c32793f10ce9edf90bc2f674a, 82973c5f56681190a0dbb4c4449ed60d5f805135, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, a5710a7a54b50c26f3c40c37a19b2ec63330e90b, 1562390dd212516cd857009cbd4f857a902d1f3d, e02a757617c2c42eb62889cc4d4aee3765928303, 2e965b5d97c2d6fb4af284307735be39283792ba, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 9e540662619327a3056d9e40bb58058868f6f805, 3bc9bb1f2218dcbd15c3b7cdfcb43077a3f30779, ca011427853d34ce4ec9ccafde8a70c9eacc3e21, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 15370f51d666ab8ef17185679553c6a8647b2a15, 6a1b25f7a67395ad1e676027322913acbb0a0635, 740b5dec469e6b54e5e191a577a9b6a6ad8562e3"
8799ec4bdf98bc313aa8c41a706a385e026d1f88,Enabling Automatic Discovery and Querying of Web APIs at Web Scale using Linked Data Standards,https://www.semanticscholar.org/paper/8799ec4bdf98bc313aa8c41a706a385e026d1f88,Conference,"To help in making sense of the ever-increasing number of data sources available on the Web, in this article we tackle the problem of enabling automatic discovery and querying of data sources at Web scale. To pursue this goal, we suggest to (1) provision rich descriptions of data sources and query services thereof, (2) leverage the power of Web search engines to discover data sources, and (3) rely on simple, well-adopted standards that come with extensive tooling. We apply these principles to the concrete case of SPARQL micro-services that aim at querying Web APIs using SPARQL. The proposed solution leverages SPARQL Service Description, SHACL, DCAT, VoID, Schema.org and Hydra to express a rich functional description that allows a software agent to decide whether a micro-service can help in carrying out a certain task. This description can be dynamically transformed into a Web page embedding rich markup data. This Web page is both a human-friendly documentation and a machine-readable description that makes it possible for humans and machines alike to discover and invoke SPARQL micro-services at Web scale, as if they were just another data source. We report on a prototype implementation that is available on-line for test purposes, and that can be effectively discovered using Google’s Dataset Search engine.",2019,24,"35202970, 1390195074, 1773317, 1753013",35202970,Zagreb,2,"49997612, 2060154",Y,"(This is hinted at by the mention of fully convolutional networks.)  The method could just as easily be applied to learn a task-specific rotation of the fully-connected layer weights.;Stronger ankles are more generally correlated with a heavier body rather than heavy feet, given that a key role of the ankle is to be able to provide a ""push"" to the body at the end of a stride, and perhaps less for ""lifting the foot"".",The Web Conference,19,"web,data,sources,data querying,data querying","2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c, 02fa2389b1b64b661192e224bed8af6df0ce80f6, 96273b87cd0eb9d1c9a12afae621ce2abdbbab36, 579476d19566efc842929ea6bdd18ab760c8cfa2, 2bc9d6830610e18f110aa7984f7c0271d6b17eeb, c665003881c3c35589d1e48da1ee7234b48f2ac8, 71ba8a8c75e0826f94eb53ee7a4566d4fc5b5256, 14014c024674991149f3ecf9314c93f7e029ef1a, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, 152877c51df17cdd4a87d19e452c6daecfadf6c3, a9640bac0b45a804d07fc5914feb08af8f2a73f2, c84aa52bee5116f80c7740503edff4b08f733c3b, 215fc60307f741b9db059204e41db8bfb879e606, 9f5b82d9915d0752957602224c5056be7e749c83, 12c9bcf710d30ba991fb765ace07f177f53ecfd9"
01bf0e83159712fbbbd12171a7e268547a4cfbc5,Querying Data Lakes using Spark and Presto,https://www.semanticscholar.org/paper/01bf0e83159712fbbbd12171a7e268547a4cfbc5,Conference,"Squerall is a tool that allows the querying of heterogeneous, large-scale data sources by leveraging state-of-the-art Big Data processing engines: Spark and Presto. Queries are posed on-demand against a Data Lake, i.e., directly on the original data sources without requiring prior data transformation. We showcase Squerall's ability to query five different data sources, including inter alia the popular Cassandra and MongoDB. In particular, we demonstrate how it can jointly query heterogeneous data sources, and how interested developers can easily extend it to support additional data sources. Graphical user interfaces (GUIs) are offered to support users in (1) building intra-source queries, and (2) creating required input files.",2019,14,"3059455, 2235966, 1705658, 2362078, 145044578",3059455,Copenhagen,2,"3353468, 2254255",Y,"For evaluating whether the data point x is anomalous or not, we search for a latent representation z such that x \approx g_\theta(z).;2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing.",The Web Conference,19,"data,sources,support,data querying,data querying","4afa7d8e2de43b0b67366b1bce8768f5a246d153, f18be38578ee52aa7071c404d42e3d53ae003122, e7b0e2710821a9fe1e3d2b9a09dd94514b2d5ea5, 72afe82af4c2ca100c36eb35292e85d806527f0a, 046eb47d56beb8069b0098e3d01608f81ebb6849, 74b4f16c5ac91e3e7c88ae81cc8c91416b71d151, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, c50a909e20bd07f4aea09dc6dae539b45b406a96, 430f3c265935abb45bc84f3ae81c570ef778aac0, 390bcf15a1b13cb0d5966859c35c69a31238e838, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 78aab73ed574393ab421f25b3a0e3f7343e64748, fc32074b37a6d9dda535a70f9689022e70508520, 742747c7a453b293352b772d0d99541c96a351c3, 2f44ab0e52eb98fdfc0086638887f175b6f2fe4b, 1562390dd212516cd857009cbd4f857a902d1f3d, a18f02e5c24e1f924aea268dd343bbdea234f2bb, f7b69b8babfa607f2e8b0371f24cb720a7a827d6, c0aec04ee86c0724d61c976f19590fbe9c615723, 19b93280f17696a4ddfa2c75490a50ab107addf2, 3adb779bb37d22e3aa299364c2a337603801ca5c, 1fcddebb7b51175ac412009ec1c26cc29fb925cf, ccd94602e3acecf999d0c9ba62b1a8bc02e9f696, 2ddcd47b28eb4b43317a34cf56e83309f5347699, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, bdb68c5e2369633b20e733774ac66eb4600c34d1, 5d24ed8942235324512d6cedfd8dbf54c57658b4, ef35da3a3c471a6a15c7a3b09586483eb50cbef0, 10b4b926904ad153f791ec680218e1610747a0c8, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, 2478f43de2f1fcd9301a81e9b42d86b1d46db02d, d1e701665e73faa648cb15473952576f40e8e122, f51bc74814a3452009ea5ca262d9768d08149ee6, e5d720767b7a539bb2edaa98eaf572a4506a79c6, 287ba5bf00d96af1596aaf80c178392a9c4fcc28, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, 9f5b82d9915d0752957602224c5056be7e749c83, dcbaf58b16ac7ef947879ea37c021466357b291a, 07b01d665646009439ca206378cc35e095ec6cd2, b0c34618ffd1154f35863e2ce7250ac6b6f2c424, 45de91a919780d5540872cf047986a370625e61c, 3813b88a4ec3c63919df47e9694b577f4691f7e5, d3855b7351c11145e51301e6b686f748ca35c802, ce212cb873a54e5716da53a66b10298ac013008a, f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa, d7fddafbbc372da4fa884f67bdc32db71b888806, 39444c55f07839ac6a0d1839472a982f8fb447bb, 13c4e5a6122f3fa2663f63e49537091da6532f35, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632"
538fbdb8013ab43a9b5d725461b294ad29fcced7,Sub-millisecond Stateful Stream Querying over Fast-evolving Linked Data,https://www.semanticscholar.org/paper/538fbdb8013ab43a9b5d725461b294ad29fcced7,Conference,"Applications like social networking, urban monitoring and market feed processing require stateful stream query: a query consults not only streaming data but also stored data to extract timely information; useful information from streaming data also needs to be continuously and consistently integrated into stored data to serve inflight and future queries. However, prior streaming systems either focus on stream computation, or are not stateful, or cannot provide low latency and high throughput to handle the fast-evolving linked data and increasing concurrency of queries. This paper presents Wukong+S, a distributed stream querying engine that provides sub-millisecond stateful query at millions of queries per-second over fast-evolving linked data. Wukong+S uses an integrated design that combines the stream processor and the persistent store with efficient state sharing, which avoids the cross-system cost and sub-optimal query plan in conventional composite designs (e.g., Storm/Heron+Wukong). Wukong+S uses a hybrid store to differentially manage timeless data and timing data accordingly and provides an efficient stream index with locality-aware partitioning to facilitate fast access to streaming data. Wukong+S further provides decentralized vector timestamps with bounded snapshot scalarization to scale with nodes and massive queries at efficient memory usage. We have designed Wukong+S conforming to the RDF data model and Continuous SPARQL (C-SPARQL) query interface and have implemented Wukong+S by extending a state-of-the-art static RDF store (namely Wukong). Evaluation on an 8-node RDMA-capable cluster using LSBench and CityBench shows that Wukong+S significantly outperforms existing system designs (e.g., CSPARQL-engine, Storm/Heron+Wukong, and Spark Streaming/Structured Streaming) for both latency and throughput, usually at the scale of orders of magnitude.",2017,48,"2108123758, 144593763, 2118438836",2108123758,Monaco,3,"49889487, 35202970, 47947548",Y,"If this is not the case for some reason, more detailed explanation is needed.;3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling.;As is often the case with such comparisons, it is hard to disentangle from where exactly come the speedups.",Symposium on Operating Systems Principles,26,"data,wukongs,stream,data querying,data querying","df138c7425e787cac2f9d3ab7775c0fb5294a83e, f62ab3fcc45eb787f4eb3213a3ffcae97799e9e5, ce2d5b5856bb6c9ab5c2390eb8b180c75a162055, 6548106035c7208ad498730627874a482734b9ac, b5904cd5dbf73b8d5ff13517de490c292d877ee0, 63316bb5b88d362051c048e864c3ae5d97a26d30, 3705919b880f4f8dc37483a704e14dd078cb9ac4, 78e40584f0d149bf6f98beb5561b7b83cb68e1b1, ada0b87cd5c30d31186c38fb12e631d29426a3bf, 079b57837221413bf99ab40999c77c29e280e0c2, d0238f2fa88b0316ab862f843a5521eff0fd9bf5, e2a85a6766b982ff7c8980e57ca6342d22493827, beb890d47bbc21a96967f9993c9d6e15686b2eac, e3b37c1c955b2b10809040ce277edae5333b99c3, 22c141b489e6e189f5996537b0a908fc10f90de7, 417326e51d78ba8bd2621f23e539b41bbdd336d6, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, e9a986c8ff6c2f381d026fe014f6aaa865f34da7, 11342d45911ee8a7c9e3a94117ce774ad7036172, bf324b5d23344984883d89a1dca22a39ca473031, 263a58f4fd32caca1dad2351af4d711aec451fe6, 31f9eb39d840821979e5df9f34a6e92dd9c879f2, 7ae2783a9196fb4bc2a610ae812d19722daddce5, 4895c430c7810b45840b58cc9182f12143013a43, b6becea767675ea6ee43c78ce747077a5050019c, a13149a80855412d970d0de2b41c611f4cf7e1da, b415e836d447ea9efb7629a1de67cd2a6f9e7ba8, e75cb933d387ecb184010ff07d0ee43fc1750e2a, facd5f5deb152229ceb1803434d8690a09ab4129, e7a7735104448371dde788542ebfc6af6485ea43, 75c364909914f17791837ec88090262aa6656d3e, ed9e7821b3e51c7e59183300d6c8cf90c8de0f26, 63adc1e5086481e36b19b62707a96b799da51e59, c997ed5ac5772bb0fce8bb11b8c86bc33310f39a, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, 56ae2837b6cd4d143bbfa4a4b06811d70126106f, cd74acb268404cde24f5131a22d04d48776b283e, eeac4411ae119c6c7ac33a11f762f2495b4dd960, 846883b7761cb5fe4468d42bf9d328b5d1030175, e359e8960b0b09e8685a32927b7818f4b06ef881, ec7f5dc077480df149bcd4358a3aa8441878ca59, 0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4, 047286f5b9315a8e8bf56c4fc936e62f21495892"
19b93280f17696a4ddfa2c75490a50ab107addf2,SHAHED: A MapReduce-based system for querying and visualizing spatio-temporal satellite data,https://www.semanticscholar.org/paper/19b93280f17696a4ddfa2c75490a50ab107addf2,Conference,"Remote sensing data collected by satellites are now made publicly available by several space agencies. This data is very useful for scientists pursuing research in several applications including climate change, desertification, and land use change. The benefit of this data comes from its richness as it provides an archived history for over 15 years of satellite observations for natural phenomena such as temperature and vegetation. Unfortunately, the use of such data is very limited due to the huge size of archives (> 500TB) and the limited capabilities of traditional applications. This paper introduces SHAHED; a MapReduce-based system for querying, visualizing, and mining large scale satellite data. SHAHED considers both the spatial and temporal aspects of the data to provide efficient query processing at large scale. The core of SHAHED is composed of four main components. The uncertainty component recovers missing data in the input which comes from cloud coverage and satellite mis-alignment. The indexing component provides a novel multi-resolution quad-tree-based spatio-temporal index structure, which indexes satellite data efficiently with minimal space overhead. The querying component answers selection and aggregate queries in real-time using the constructed index. Finally, the visualization component uses MapReduce programs to generate heat map images and videos for user queries. A set of experiments running on a live system deployed on a cluster of machines show the efficiency of the proposed design. All the features supported by SHAHED are made accessible through an easy to use Web interface that hides the complexity of the system and provides a nice user experience.",2015,76,"1876123, 1756679, 1388622435, 1404602362, 32653213, 2635216",1876123,Amsterdam,2,"80842917, 2035210",Y,"For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers.;To me, the comparison makes sense but it also shows that the ideas presented here are less novel than they might initially seem.",IEEE International Conference on Data Engineering,31,"data,satellite,component,data querying,data querying","9db0247728950788a2b42097d81dc0e24eed6bb2, d8348b802c9133d9e396d4ad809b020d5be42863, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, 97f456643712e9618edd7465676c62af3c8ae690, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, c9cc6a3c50b32e801f20b4f0ed5daf3e9550f9c1, 94de6bab96ad533169dbc3bf9e6557581e59cb6f, 733fc094e785724621c46e20db1be69f132ad9df, f1300d9be8254b028337d9757755ba906fe6955b, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 4a7477881b66d12e79c704805781d4683a6a6be1, 819167ace2f0caae7745d2f25a803979be5fbfae, a8b995f0da78a79447dfb18c2337972b044f4239, d9d325ca670a1aa215e3e39023f8abf17dae7584, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 0e33833f5e2e2719edfba1d142eb4d27f96e799f, cb3968152f7d93f53d24b00279a90d5071ddc85a, e936f248b2c0489316ed1521656af2564c3502c3, 62ccd99a65bfc7c735ae1f33b75b107665de95df, 024006d4c2a89f7acacc6e4438d156525b60a98f, 00d1f3423a33f73ca6aee884a58834547475d2f0, 4f8d648c52edf74e41b0996128aa536e13cc7e82, f5ac3ec506a5a01807dd7196c0c52eef008b78cc, d7fddafbbc372da4fa884f67bdc32db71b888806, bf07f2927dca481653b8c60b2dc982fe4a7dfd4e, 48265726215736f7dd7ceccacac488422032397c, 185e7d2a761594451b02ace240356dadad2aef78, cc78babfacce48e715dac56886d7dd9746cfcab0"
fd0496ab020acf366375615ab40235e6dd3c5897,Querying Geo-Textual Data: Spatial Keyword Queries and Beyond,https://www.semanticscholar.org/paper/fd0496ab020acf366375615ab40235e6dd3c5897,Conference,"Over the past decade, we have moved from a predominantly desktop based web to a predominantly mobile web, where users most often access the web from mobile devices such as smartphones. In addition, we are witnessing a proliferation of geo-located, textual web content. Motivated in part by these developments, the research community has been hard at work enabling the efficient computation of a variety of query functionality on geo-textual data, yielding a sizable body of literature on the querying of geo-textual data. With a focus on different types of keyword-based queries on geo-textual data, the tutorial also explores topics such as continuous queries on streaming geo-textual data, queries that retrieve attractive regions of geo-textual objects, and queries that extract properties, e.g., topics and top-$k$ frequent words, of the objects in regions. The tutorial is designed to offer an overview of the problems addressed in this body of literature and offers an overview of pertinent concepts and techniques. In addition, the tutorial suggests open problems and new research direction.",2016,43,"144145211, 145917501",144145211,Monaco,3,"46174961, 3268360, 2273645419",Y,"- What is the feedback in the CIFAR-10 experiments?;This is a solid paper about model evaluation in the chemical domain.;If the loss for the task is proper, then it's well known how to construct a divergence which coincides with the optimal risk.",SIGMOD Conference,16,"web,data,queries,data querying,data querying","e2e8ea9bcdb83deb2787ce89db1b51f7ccb1bd1e, 9ea0757c750ab1222a7442d3485a74d1c526b04c, a453ce8a3de86a170c79a1082ef358c3adf4e612, 3cc2f69951cd24fe61be4cf32d62afbac297bc2b, 4419c5720e30d5ca5158795d4c848125650b8db1, 6ba00c2386f2edc0b43eec442cd1923b5d964633, aca6d5f3866372a4506cf15773ae298f18c3f453, 0090023afc66cd2741568599057f4e82b566137c, b2ee139c1a3c2c53ac2b603070bdfbcd26b50ddc, 8f942d4c17ef1809be64388517002774fe2548f0, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 0af24df95f8a0b7e6fc257a98a5820314d1243f3, e576a2d97950b1f6831f88575dd3f370053f6af7, 119a9e5b563cf1134897553ee49325b5a5bd9fb9, ada0b87cd5c30d31186c38fb12e631d29426a3bf, 3ec02e36cc0ba16292b255e9f90c3725521e2ec9, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, 1e1cf81a1113482be3f0c280db994a832cb9426a, 537f5e8e4139392cd2d108f32495e5b2b80151ac, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 7676c02ea839ff1ceb6e5e1427c42bc45e169bde, 10a69070c8585dc5564b11b0e53dbe4d565f9ce1, e7b0e2710821a9fe1e3d2b9a09dd94514b2d5ea5, b613887337a5d2e8fc8773037116be81e6346835, bcc82ce554942880814243fc8c08a88b9d2aad09, cb23a59fdf3ade707600f076df4ff27a03941fba, 82a09f72d7b9587e3b519b1cd9640a5a611f3480, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, f4207bcad24fe4eaf3849d6c1fe38c36545b5cb3, d9d325ca670a1aa215e3e39023f8abf17dae7584, 247dec05283a1a521f99253a6cca6a5858cac0d2, 7bc9607c5cf3fc817675d46844f529097d579514, 31f9eb39d840821979e5df9f34a6e92dd9c879f2, eeac4411ae119c6c7ac33a11f762f2495b4dd960, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, c9f320789e98d2c7a798a9705e26dbe317677966, 695bdc6e24608364491b9418a220c65a7cd17413, 2f44ab0e52eb98fdfc0086638887f175b6f2fe4b, e75cb933d387ecb184010ff07d0ee43fc1750e2a, 9e66ae24a541255c2d931184498ee116ce81478a, 53b0cfa2eaaa9703df02b2ca9c43260c4de5df0a, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, e02a757617c2c42eb62889cc4d4aee3765928303, bd6c027a3604d6c8fa23435bf382455b2bee436b, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, 73d4accea441aae2373828a8dc2175aa2759c38f, 7fa30c8af436b7df513ef41e2cccd8a2404f37f8, 09797949fc70fbad8f3fed4f6cf4a91a9c709652, 2eda2921a8da4b325f9d05f556594a5884c398a7, 8388f1be26329fa45e5807e968a641ce170ea078"
52e510271b172d098ec9b107a4159216ec08527e,Querying Big Data by Accessing Small Data,https://www.semanticscholar.org/paper/52e510271b172d098ec9b107a4159216ec08527e,Conference,"This paper investigates the feasibility of querying big data by accessing a bounded amount of the data. We study boundedly evaluable queries under a form of access constraints, when their evaluation cost is determined by the queries and constraints only. While it is undecidable to determine whether FO queries are boundedly evaluable, we show that for several classes of FO queries, the bounded evaluability problem is decidable. We also provide characterization and effective syntax for their boundedly evaluable queries. When a query Q is not boundedly evaluable, we study two approaches to approximately answering Q under access constraints. (1) We search for upper and lower envelopes of Q that are boundedly evaluable and warrant a constant accuracy bound. (2) We instantiate a minimum set of variables (parameters) in Q such that the specialized query is boundedly evaluable. We study problems for deciding the existence of envelopes and bounded specialized queries, and establish their complexity for various classes of FO queries.",2015,42,"144502903, 1729031, 2125469004, 2063047670, 2069299218",144502903,Vilnius,3,"1791253, 35202970, 2145907057",Y,"3. This paper lack original technical contribution from themselves.;The authors show through several experiments that the divide and conquer (DnC) technique can solve more complex tasks than can be solved with conventional policy gradient methods (TRPO is used as the baseline).;Wasserstein distances for eliminating batch effect, not enough novelty and no thorough comparisons to other methods.",ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,34,"queries,study,constraints,data querying,data querying","8f99f5cb3950fcd1628c6a563be4b4fc4966fa66, d997beefc0922d97202789d2ac307c55c2c52fba, b977e8de38dc0d13817bca1ed20036badfe2a58c, 3705919b880f4f8dc37483a704e14dd078cb9ac4, 1a4c6856292b8c64d19a812a77f0aa6fd47cb96c, 4e746359afd6f81705b875d71cc499b904a320df, 6a6ad9eb495739f4c80e7c09598720c3d5c5dff7, 0a829289a16ae48837cc2905635435db98bacc76, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, 21042565ec941f4fa31ac5a0af85a1a84ff21f1b, 579476d19566efc842929ea6bdd18ab760c8cfa2, 08be4e23951a0def1c5d235befbb39c8d8d373a3, b7f5973dbf2ef76fcc3aea616a7f72ca79f09020, b795c74a0150ec091003ffbaa5bd7d74487c137b, 3994334c81478a4b17341eb1f494dbccbb73d999, e7b0e2710821a9fe1e3d2b9a09dd94514b2d5ea5, 68a3d32416977e88cf1bfa4ad548d403f5f089d6, 965359b3008ab50dd04e171551220ec0e7f83aba, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, 90e4330fed2da147dd41490e8ad638b618112b3d, c6e242fce9e5e4ba2c4dc869cfb0afabf2020852, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, 3774fb4b6aa11d88c562ee5702c66cae5881af0d, 0863a5ce955e5193e535e1442086dc460dd295f0, 7a4fe2f003241ad97bf1778e527cb0306fa90da2, 5bc511aa30f72720260d792e57537379fb04c395, 96b51d940653710f9d099d89ade86b44fa9bdd8a, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, d3f9a39e49abfdf084da558e305be5473c8740e5, ecef432e7f6c9f431d5b34706a8de1fdebec46f9, 453fdfeefd6498a65be339d7e8722f6f3288907e, 780c7ead33428d282044519fee5e773ad56d5a2c, b09139c153bac8893e8faea2b3a59159234caadc, 1109d62ebd2b29a7dc148bc30dd6cfc803a63dec"
d57f11ed40c3cdcbb36cb758191db4f2c9372965,An extension of SPARQL with fuzzy navigational capabilities for querying fuzzy RDF data,https://www.semanticscholar.org/paper/d57f11ed40c3cdcbb36cb758191db4f2c9372965,Conference,"The Resource Description Framework (RDF) is the graph-based standard data model for representing semantic web information, and SPARQL is the standard query language for querying RDF data. Because of the huge volume of linked open data published on the web, these standards have aroused a large interest in the last years. This paper proposes a fuzzy extension of the SPARQL language that improves its expressiveness and usability. This extension allows (1) to query a fuzzy RDF data model, and (2) to express fuzzy preferences on data and on the structure of the data graph, which has not been proposed in any previous fuzzy extensions of SPARQL.",2016,24,"1741623, 50458739, 2673479",1741623,Athens,3,"49889487, 40228633, 8347899",Y,"An additional GAN loss is used on the generator output to encourage the output to look like summaries -- this procedure only requires unpaired summaries.;In the paper, the authors proposed using GAN for anomaly detection.;The authors propose a centralized solution to the problem by adapting the Deep Q-learning Network model.",IEEE International Conference on Fuzzy Systems,16,"data,rdf,sparql,data querying,data querying","f5ac3ec506a5a01807dd7196c0c52eef008b78cc, 0e9a44ce661c3535d5ce747912540080324489f5, 5d433da6d0f143f20936379910104d2bb139d4ae, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 7eaac9847257c32afd450017d1348ecda4dcaade, 8388f1be26329fa45e5807e968a641ce170ea078, 07b01d665646009439ca206378cc35e095ec6cd2, 0084f3cb0a1754272151c5268a783f24bf5676a0, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, d6e1e4f0ad898ca6ac37e6e139a77fa3982170d4, 0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4, ea160adc0d78e54669281b8b145bcd832e648fee, 0bca61986b8edeaf33018d0203b44110f2480110, 6f31f59f7b4bd751b5d8657d8a878bef571f9a4c, ef35da3a3c471a6a15c7a3b09586483eb50cbef0, 3b9732bb07dc99bde5e1f9f75251c6ea5039373e, 799d5a8271887adede035644d878c7bd555576df, 0a71dd8bec060195e14eb9d0a7abbc08d960d4d5, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 82e1e8b222aeaca19d45375b31fdc825d1a821b8, 75c364909914f17791837ec88090262aa6656d3e, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, bbed457fd04ba4972018382d1a01a0bdde399d3c, f9c990b1b5724e50e5632b94fdb7484ece8a6ce7, fe4c5074f021cd1c810892c1b6bc267b23aa6e5c, a22f3398ea865426c89ee66f4824ec626e56a864, 10ea29fda06bdbe56f591909d89f3194b452ac91, 9712624bb61abb0da989514cae558cfab61bb9d2, 9b54941de1e21826ecc28b32730ac3f69991ede4, a81ba6a07bf7a2ecff871e3362a77404501d0927, dca4d9abbc82e57dfa52f932e893d467a63e0682, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, 69e76e16740ed69f4dc55361a3d319ac2f1293dd, 4419c5720e30d5ca5158795d4c848125650b8db1, 322d91190acd8ac8c64598f5126947b0485ba249, 88a724083b2cfcc096448c28e6973c8f761ee463, 013eb12ce5468f79d58bf859653f4929c5a2bd14, 9e195234688778b2beb3528632e78dbabf816332, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, 2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, 54ddb00fa691728944fd8becea90a373d21597cf, d6f002d88638de71114dab083f0ea8ceea6b6a5a"
cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5,Querying RDF Data Using A Multigraph-based Approach,https://www.semanticscholar.org/paper/cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5,Conference,"RDF is a standard for the conceptual description of knowledge , and SPARQL is the query language conceived to query RDF data. The RDF data is cherished and exploited by various domains such as life sciences, Semantic Web, social network, etc. Further, its integration at Web-scale compels RDF management engines to deal with complex queries in terms of both size and structure. In this paper, we propose AMbER (Attributed Multigraph Based Engine for RDF querying), a novel RDF query engine specifically designed to optimize the computation of complex queries. AMbER leverages subgraph matching techniques and extends them to tackle the SPARQL query problem. First of all RDF data is represented as a multigraph, and then novel indexing structures are established to efficiently access the information from the multigraph. Finally a SPARQL query is represented as a multigraph, and the SPARQL querying problem is reduced to the subgraph homomorphism problem. AMbER exploits structural properties of the query multigraph as well as the proposed indexes, in order to tackle the problem of subgraph homomorphism. The performance of AMbER, in comparison with state-of-the-art systems, has been extensively evaluated over several RDF benchmarks. The advantages of employing AMbER for complex SPARQL queries have been experimentally validated.",2016,14,"2810491, 1789397, 1744598, 1725656",2810491,Kiev,2,"2845020, 2065048323",Y,My main concern with this work is that I don't see any mechanism in the framework that prevents an expert  (or few of them) to win all examples except its own learning capacities.;The proposed method is new and technically sound.,International Conference on Extending Database Technology,16,"rdf,query,amber,data querying,data querying","1d174f0e3c391368d0f3384a144a6c7487f2a143, fa26a6d434450b185e669170e79fd3e1d29716bf, 85328b4a8132bf4299f8cd7f8e79e850d561c8fc, 22c141b489e6e189f5996537b0a908fc10f90de7, 0574a3abc98f0e6bd035a4ff4ea107cfba45d3d4, ef25b02f3be31c699255ee05aa90a4a17461d95d, 3337e4b2e63e5e99171f9da9d161771f5810c27a, 6db0f8d396371078590faa7b34ae2e0e1b154a60, 10ea29fda06bdbe56f591909d89f3194b452ac91, e449b9b3fe04fe260731a3c74d2123bf6eaadf5b, 3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b, 079b57837221413bf99ab40999c77c29e280e0c2, 182acc7a1c9a63cfdef9f86ee9df43fd75e05060, 8f942d4c17ef1809be64388517002774fe2548f0, b415e836d447ea9efb7629a1de67cd2a6f9e7ba8, d9a7fa7616a327367696e19b1846519745cd43ff, 152877c51df17cdd4a87d19e452c6daecfadf6c3, 7c8bd41e9cebdb01f445a51ba43839c8d9b3ab91, 915ab7b6ca3633230403d47dbb31cf74888cc5c9, ae38dc77a962161107361f213db9216ee1274037, be2b0396de9431bae931642516a1d3e4906329f5, b61b260de1599e6e89491cad9160898fcd3b34c2, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, e2a85a6766b982ff7c8980e57ca6342d22493827, 3dfa820702b6181c9964931f0a4d47fd298bf429, 01f0f5205d03870f172ae8f04e33356d5a0af221, 965359b3008ab50dd04e171551220ec0e7f83aba"
b69a35662a2cac38eab22f4481285116bdf8c30e,A fuzzy extension of SPARQL for querying gradual RDF data,https://www.semanticscholar.org/paper/b69a35662a2cac38eab22f4481285116bdf8c30e,Conference,"In this work, the first stones of a flexible approach to linked data querying based on fuzzy set theory are laid. Flexibility refers to the capability of expressing flexible queries over a RDF model containing gradual information.",2016,4,"1741623, 50458739, 145433223, 2673479",1741623,Warsaw,3,"1693182792, 5973699, 2108025636",Y,"A step in the right direction, with interesting results, but not a huge level of novelty.;How D will handle an example far from fake or real ones ?;Both datasets should be compared to LASSO as well.",Research Challenges in Information Science,16,"stones,flexible,approach,data querying,data querying","eda6756ab2844c390584686dc5e6385f4a8369cd, 752604994a7ca548ff2954114fc61a501d857b1c, a85c45ce7c893388e8eafa8a653b042e1497db48, 371a343457a4fbff00000bf4faa29b2b2f85744c, 2880ac931c11176aee6d42a7e7bb0703aacde3f9, 6e74403ab75d0080c056fd66702ab1f54f04d9b1, 771a858c35f6d6e6d1017dde95368de3794738a6, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, e576a2d97950b1f6831f88575dd3f370053f6af7, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, f7b69b8babfa607f2e8b0371f24cb720a7a827d6, 8dd0c1e955c66092ff951941a151336211e6e171, 09797949fc70fbad8f3fed4f6cf4a91a9c709652, 340f48901f72278f6bf78a04ee5b01df208cc508, a453ce8a3de86a170c79a1082ef358c3adf4e612, 2dafea864f74a477414c3b71b742f7997e216102, 62df84d6a4d26f95e4714796c2337c9848cc13b5, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, e5194ae88d63c7549678b1b73cfdaf7112164272, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 746a81aa26d3ebfb81acfd6af958d6a21603cd21, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, 37b9478b792fb1e7ad5f2c322ed4445b93df6c9e, 7d873a9c49d3864709aa762f8740edcdbd7369c5, 1cd497e82bdc46a9d3d28b2316ea7cbe9aee5467, 08764019e9762da527253b37b0ff39c46a4206b7, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, bcc9245ebf57072c07c5a9d4eff3aaaac36cb7c6, 7536bce1007a765fd097a7cc8ea62208a8c89b85, a9cbbef8f4426329d0687025b34287c35bdd8b38, ac4350d3a2673a4c34fa949714ede8b45fd04f1d, be2b0396de9431bae931642516a1d3e4906329f5, 185e7d2a761594451b02ace240356dadad2aef78, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 69a72ff5b30642d11c96635e99aadad3140d33a7, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 40416ac3bf78583eea37661b1b446e9939245b3e, 34e9852f588f75eba81c66a3e5f867a794a5a690, 17fca92ffd527c78c5dc6c7953e96671743807fa, 91d6e8ba5dd90b02fe3bd870b19da13a6167af53, 3ea34401909978d3d3d0c25c8746e02c7d2a7c77, b473e91cbe80c8b46451b49153cd5f93030480ab"
1661d0d8d47cac41e01c59c60aac3675b4396698,A demonstration of Shahed: A MapReduce-based system for querying and visualizing satellite data,https://www.semanticscholar.org/paper/1661d0d8d47cac41e01c59c60aac3675b4396698,Conference,"Several space agencies such as NASA are continuously collecting datasets of earth dynamics-e.g., temperature, vegetation, and cloud coverage-through satellites. This data is stored in a publicly available archive for scientists and researchers and is very useful for studying climate, desertification, and land use change. The benefit of this data comes from its richness as it provides an archived history for over 15 years of satellite observations. Unfortunately, the use of such data is very limited due to the huge size of archives (> 500TB) and the limited capabilities of traditional applications. In this demo, we present Shahed, an interactive system which provides an efficient way to index, query, and visualize satellite datasets available in NASA archive. Shahed is composed of four main modules. The uncertainty module resolves data uncertainty imposed by the satellites. The indexing module organizes the data in a novel multi-resolution spatio-temporal index designed for satellite data. The querying module uses the indexes to answer both spatiotemporal selection and aggregate queries provided by the user. The visualization module generates images, videos, and multi-level images which gives an insight of data distribution and dynamics over time. This demo gives users a hands-on experience with Shahed through a map-based web interface in which users can browse the available datasets using the map, issue spatiotemporal queries, and visualize the results as images or videos.",2015,9,"1876123, 1388622435, 1404602362, 2279240, 2635216, 3087877, 1756679",1876123,Andorra,3,"1394550182, 40443723, 2083106",Y,"Is this on bAbi as well?;The “obverter” technique is quite interesting since it incorporates the concept from the theory of mind which is similar to human alignment or AGI approach.;For example, when showing that the head direction cells generalize in the new mazes how can we be sure that it is not using a common lighting scheme common to both train and test mazes to orient itself?",IEEE International Conference on Data Engineering,31,"data,module,datasets,data querying,data querying","98e6c6d860383fea5bbad145deed51514d23b86c, b01ad71bd376f36546f02204784908da9577bb0b, 7bb477077968d68aa7a6059d8d6d801fb28274da, 4f8d648c52edf74e41b0996128aa536e13cc7e82, 597bd2e45427563cdf025e53a3239006aa364cfc, eacf9284a39adcd56172665f31fd5a72560bba7a, a13149a80855412d970d0de2b41c611f4cf7e1da, 244e08e109f6f4fa0f846977e1aef1e7b6a9e816, e67a2817089312746d69b38ce9abfdc4b1bc69c3, db6ad6ded1cfa26fdc7437f27fb823ec533e96fe, 56ae2837b6cd4d143bbfa4a4b06811d70126106f, 6b3756d32ab5b0a5715a5cfc3672290d2d643017, a80e26e6365b215715c182d19a9aa8bb876ac768, bb826d9ccd116076a267dfcb048cdd747c11b255, 88a724083b2cfcc096448c28e6973c8f761ee463, cd8a9914d50b0ac63315872530274d158d6aff09, 6dc4883228c95e8a332320fcc587a0ff33c84d59, 74b63c4cdc719a646ce014d084a0ce0ac66d9139, 02a1e8e77f501675945890df45fbdc11726cb0ba, 5e31fa4e69d1a3587230f5d134c0b7e2ed84a742"
fb0aeed456bff8607fab2bf443e9d86d51c3dff6,SMART-KG: Hybrid Shipping for SPARQL Querying on the Web,https://www.semanticscholar.org/paper/fb0aeed456bff8607fab2bf443e9d86d51c3dff6,Conference,"While Linked Data (LD) provides standards for publishing (RDF) and (SPARQL) querying Knowledge Graphs (KGs) on the Web, serving, accessing and processing such open, decentralized KGs is often practically impossible, as query timeouts on publicly available SPARQL endpoints show. Alternative solutions such as Triple Pattern Fragments (TPF) attempt to tackle the problem of availability by pushing query processing workload to the client side, but suffer from unnecessary transfer of irrelevant data on complex queries with large intermediate results. In this paper we present smart-KG, a novel approach to share the load between servers and clients, while significantly reducing data transfer volume, by combining TPF with shipping compressed KG partitions. Our evaluations show that smart-KG outperforms state-of-the-art client-side solutions and increases server-side availability towards more cost-effective and balanced hosting of open and decentralized KGs.",2020,28,"20829758, 144341429, 144841747, 2065791442, 1708607",20829758,Belgrade,3,"2273679997, 49298465, 1614034792",Y,"However, that is moved completely to the Appendix.;- Top of page 5, when you describe the checkerboard BFS: please include a visualization somewhere, it could be in the Appendix.;This seems to prove that an NER is useful for tasks where DB queries are needed, rather than that the dynamic NE-Table construction is useful.",The Web Conference,20,"data,kgs,sparql,data querying,data querying","dadfb3ff45e19dc22456a645f441bbeb17c93c9c, 2743e66939b30c43affb3c9e31f20cfac2109045, 709f7a6b870cb07a4eab553adf6345b244913913, d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b, 40416ac3bf78583eea37661b1b446e9939245b3e, 7ed665355ac78bf0c394602dd9d26075195ce2f2, b7a717233ec3ff37385ab1b06816d0ca375f5bb3, 8760bc7631c0cb04e7138254e9fd6451b7def8ca, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, 0ac1a1ccb16c8441f42ff8de743fc93f0e08c15c, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, b01ad71bd376f36546f02204784908da9577bb0b, eb9e0da8b7170e3ca4364f2f9010599c2d2556f1, 5687c9e8da574453fd873662b95caec70dac9d1e, 89a30b5dab02c9c390a632acad481fa602859272, b34fc78de28be598e21118d7cb9d84d63374addc"
b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8,Expressive Time Series Querying with Hand-Drawn Scale-Free Sketches,https://www.semanticscholar.org/paper/b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8,Conference,"We present Qetch, a tool where users freely sketch patterns on a scale-less canvas to query time series data without specifying query length or amplitude. We study how humans sketch time series patterns --- humans preserve visually salient perceptual features but often non-uniformly scale and locally distort a pattern --- and we develop a novel matching algorithm that accounts for human sketching errors. Qetch enables the easy construction of complex and expressive queries with two key features: regular expressions over sketches and relative positioning of sketches to query multiple time-aligned series. Through user studies, we demonstrate the effectiveness of Qetch's different interaction features. We also demonstrate the effectiveness of Qetch's matching algorithm compared to popular algorithms on targeted, and exploratory query-by-sketch search tasks on a variety of data sets.",2018,43,"40964207, 1698925",40964207,Vilnius,3,"144132699, 47319783, 2317297",Y,"There's clear value in having good inductive biases (e.g. expressed in the form of the discriminator architecture) when defining divergences for practical applications.;The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy?;A good approach with some open questions on related work, scalability, and robustness The authors propose an approach for zero-shot visual learning.",International Conference on Human Factors in Computing Systems,18,"query,series,features,data querying,data querying","980858461df7c4349f17b427686c5bcbcffbdc04, a56bf7ee9a56d8f84079684339a953c2df9ce76b, 73d4accea441aae2373828a8dc2175aa2759c38f, d9a7fa7616a327367696e19b1846519745cd43ff, 9712624bb61abb0da989514cae558cfab61bb9d2, a80e26e6365b215715c182d19a9aa8bb876ac768, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, 929305892d4ddae575a0fc23227a8139f7681632, b9f190339ce0b4cdad3464c45ec3266a7369fb7c, f14fc9e399d44463a17cc47a9b339b58f6ef7502, 94de6bab96ad533169dbc3bf9e6557581e59cb6f, 02fa2389b1b64b661192e224bed8af6df0ce80f6, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, 7909428ca7d813331bbe4d33d07ab09e984b41b4, b613887337a5d2e8fc8773037116be81e6346835, 545f108575314031f35c617c4ac35a10133c50e3, fa63c3f53413ced7946623889c416e34a28676ea, 537f5e8e4139392cd2d108f32495e5b2b80151ac, c24d47ff95cd4bda073c75ec24ececaa3b10c995, d7b820af40a9e2660ef700d39f7b2e27b43435c5, 3b87dafd5a412e25e06761f181ec199ca88a7398, 29409efa04ac99ccf01d2a011d21d5d14e870000, 704011527f183b561ea6a75b21e4cefe5aa77fca, b473e91cbe80c8b46451b49153cd5f93030480ab, 390bcf15a1b13cb0d5966859c35c69a31238e838, 155eeede0c070f1f017ba5c9f6cacd7ae0b098aa, 2eda2921a8da4b325f9d05f556594a5884c398a7, 709f7a6b870cb07a4eab553adf6345b244913913, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, 518a7c79968a56d63a691d42f8378be6c776167e, 94214d6d922ce095719d488642cbcc75dc52f273, 915ab7b6ca3633230403d47dbb31cf74888cc5c9, 5d8e450c8c7d9faa8222f1d0eaf34b01755c9eef, ab606e9d148458f6d54e5d44abefd73b7990f6e0, cc017a62c605a0749e35a1264a46d62e78fb68b7, c997ed5ac5772bb0fce8bb11b8c86bc33310f39a, cd29c25c489562b409a60f83365f93f33ee1a0a1, c468bbde6a22d961829e1970e6ad5795e05418d1, a453ce8a3de86a170c79a1082ef358c3adf4e612, 0df5a4f9cc8a244715fe9968732497d2ac2a7cd1, b00d4452cd9869526169b7a2fe893c0a7c31eb4c"
10cf0045bc0f58aa3699e4451f65b12a08019c5c,"Demonstration of Taghreed: A system for querying, analyzing, and visualizing geotagged microblogs",https://www.semanticscholar.org/paper/10cf0045bc0f58aa3699e4451f65b12a08019c5c,Conference,"This paper demonstrates Taghreed; a full-fledged system for efficient and scalable querying, analyzing, and visualizing geotagged microblogs, such as tweets. Taghreed supports a wide variety of queries on all microblogs attributes. In addition, it is able to manage a large number (billions) of microblogs for relatively long periods, e.g., months. Taghreed consists of four main components: (1) indexer, (2) query engine, (3) recovery manager, and (4) visualizer. Taghreed indexer efficiently digests incoming microblogs with high arrival rates in light main-memory indexes. When the memory becomes full, the memory contents are flushed to disk indexes which are managing billions of microblogs efficiently. On memory failure, the recovery manager restores the memory contents from backup copies. Taghreed query engine consists of two modules: a query optimizer and a query processor. The query optimizer generates an optimized query plan to be executed by the query processor to provide low query responses. Taghreed visualizer features to its users a wide variety of spatiotemporal queries and presents the answers on a map-based user interface that allows an interactive exploration. Taghreed is the first system that addresses all these challenges collectively for geotagged microblogs data. The system is demonstrated based on real system implementation through different scenarios that show system functionality and internals.",2015,71,"143811079, 2877140, 1388622435, 2186764, 35205093, 2635216, 3087877, 1756679",143811079,Tirana,2,"2166312768, 2285813122",Y,"So, the main contribution of the paper is to do all the sensitivity calculations with respect to SVM problem and then use the importance sampling theory to obtain bounds on the coreset size.;The proposed algorithm in 3 out of 5 datasets, two of theme with statistical significant.",IEEE International Conference on Data Engineering,31,"query,microblogs,system,data querying,data querying","4ab38afa44f1da43746cd4a01344c8ec212b9de3, c3df199cbca74763c4ae9889409bbd4aa29b6255, bcc82ce554942880814243fc8c08a88b9d2aad09, 7536bce1007a765fd097a7cc8ea62208a8c89b85, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, 37b0a7a6c8fb26e32b9206847e78d521a2cd5900, 736ef8a32d6c5f76a21d61299300cf796480d507, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, 0b24f27d920bd1d23c8f6a1ab2b603c5c14f0a36, b34fc78de28be598e21118d7cb9d84d63374addc, 0599f45e03ac2016321df0dd653ba4c0034c79d5, a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854, b6a7226e5f6d618370995eccad68af195ef32da2, 82870bc488b57cdf5ea62877109a7278af2926b3, 709f7a6b870cb07a4eab553adf6345b244913913, eda6756ab2844c390584686dc5e6385f4a8369cd, cf5fddf6717e88e2bbed6b0bfe54dfeb311e6789, 9e27190f2d9b2167d4a66b88696def4585072fd5, b80e2af7ffa85fe2f02ea8390e4915d55c19d199, 8babcaf89f8537dc628a029ebf932100f57289fd, 8bba999de25bfb288b3f7f88e1d907aab02638b6, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, a357f1ff27e184d9a5ef69e665e8ca891032bf71, fd3259ecd1c94731f5d9bd9c0bf7417f2e147c71, 69e76e16740ed69f4dc55361a3d319ac2f1293dd, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, 13c4e5a6122f3fa2663f63e49537091da6532f35, b05306f0b142e5afb3974b1b79996e5b82653662, cd29c25c489562b409a60f83365f93f33ee1a0a1, 381c7853690a0fee6f00d2608a7779737f1365f9, 38179848e2d6a3ad373b1793848816111428ac36, cf5dfc4a9f7a82b32640128ca10832eace55880e, c9f320789e98d2c7a798a9705e26dbe317677966, d2a5dcecd2ffdf03473df1688091f08fadb114a3, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, 5d8e450c8c7d9faa8222f1d0eaf34b01755c9eef, bb826d9ccd116076a267dfcb048cdd747c11b255, fb5413afba689d16543215c5a2ddbc5b78a52007"
b5270b32a17fcc2e3dc209add6f4e4ba709b7358,Recent Advances in Querying Probabilistic Knowledge Bases,https://www.semanticscholar.org/paper/b5270b32a17fcc2e3dc209add6f4e4ba709b7358,Conference,"We give a survey on recent advances at the forefront of research on probabilistic knowledge bases for representing and querying large-scale automatically extracted data. We concentrate especially on increasing the semantic expressivity of formalisms for representing and querying probabilistic knowledge (i) by giving up the closed-world assumption, (ii) by allowing for commonsense knowledge (and in parallel giving up the tuple-independence assumption), and (iii) by giving up the closed-domain assumption, while preserving some computational properties of query answering in such formalisms.",2018,19,"1784772, 49633004, 1690572",1784772,Budapest,3,"2129460589, 10708829, 3382568",Y,"Moreover, I like the qualitative experiment (Figure 2) in which the tree is used to vary a latent dimension to change the digit’s class.;Standard idea, great results This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives.;All the experiments use large tasks - it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task.",International Joint Conference on Artificial Intelligence,18,"knowledge,assumption,formalisms,data querying,data querying","9727206903eb40d4fa42606711bad3402f2ba9aa, f1664bbaddedea8c250873e7610ab07e53fa7132, d88083e37c44461ce3e404bd57257cd3edb07d4e, 665b0c776ff7507c32793f10ce9edf90bc2f674a, dec26f0640e3a4fdb116735526302ccb9f49867e, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, 4f8d648c52edf74e41b0996128aa536e13cc7e82, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 545f108575314031f35c617c4ac35a10133c50e3, d8348b802c9133d9e396d4ad809b020d5be42863, b0b770fb8c7760749c88e3c83ae173cdb07f7bd5, 633e2fbfc0b21e959a244100937c5853afca4853, 7ae2783a9196fb4bc2a610ae812d19722daddce5, 84a36e19f9394f22b34f79756fa9628a795e02ea, a2ec47b9bcc95d2456a8a42199233e5d9129ef18, df7d26339adf4eb0c07160947b9d2973c24911ba, 780c725848aac1118d00c8bb306719ec803369cd, 1d7531db9272f7838e33616075e1e64532fd013a, 73d4accea441aae2373828a8dc2175aa2759c38f, d617f51833860dc50d202af7f80be71304b2e994, ce157cea880c9ab64de64f11a531202f5348fa05, 5406e153957dd7a165264da6e6e5d81251997404, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 156d8e2aa90b5ccc9be10477ca70deaad0151387, 0894585294c67193ff3190240554677b56fd79a0, 0a92bc2dc8a216e6aced83edc0358241066833df, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, 6548106035c7208ad498730627874a482734b9ac, cc78babfacce48e715dac56886d7dd9746cfcab0, 33e332837e91c1048c3ed165cd16bf7607c3bf06"
2e7f532796eed2847d4c19e3cff03756049e81b4,Diversified Top-k Subgraph Querying in a Large Graph,https://www.semanticscholar.org/paper/2e7f532796eed2847d4c19e3cff03756049e81b4,Conference,"Subgraph querying in a large data graph is interesting for different applications. A recent study shows that top-k diversified results are useful since the number of matching subgraphs can be very large. In this work, we study the problem of top-k diversified subgraph querying that asks for a set of up to k subgraphs isomorphic to a given query graph, and that covers the largest number of vertices. We propose a novel level-based algorithm for this problem which supports early termination and has a theoretical approximation guarantee. From experiments, most of our results on real datasets used in previous works are near optimal with a query time within 10ms on a commodity machine.",2016,44,"2149234208, 1699363, 2186806073",2149234208,Ljubljana,2,"1394550182, 31131132",Y,"In general, this is an interesting direction to explore, the idea is interesting, however, I would like to see more experiments;But the aspect I like most about this paper is the experimental analysis.",SIGMOD Conference,16,"subgraph,graph,study,data querying,data querying","44e1dd74f0446ec91221189ad3a65edb1a0208fe, 597bd2e45427563cdf025e53a3239006aa364cfc, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, 2dafea864f74a477414c3b71b742f7997e216102, 430f3c265935abb45bc84f3ae81c570ef778aac0, b889b1d6944213bc2ca29e3ad07ee65ede20892d, a0a79dad89857a96f8f71b14238e5237cbfc4787, bbed457fd04ba4972018382d1a01a0bdde399d3c, a60a4e5f7f872b9825ddff5d379857c2091ca52b, 598231eb906b183f7a2a408ef4536127e11e3de9, b266510f5f9b40d42b51884ad13a1867fb3284fd, c9b56cb026a38e39bb0228faac57accd6f65e6f7, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, c889d6f98e6d79b89c3a6adf8a921f88fa6ba518, 83cebf919635504786fc220d569284842b0f0a09, 0863a5ce955e5193e535e1442086dc460dd295f0, 371a343457a4fbff00000bf4faa29b2b2f85744c, 09797949fc70fbad8f3fed4f6cf4a91a9c709652, b27a78366868ca47098e00dda74dd1b167b3a80d, bdb68c5e2369633b20e733774ac66eb4600c34d1, 7904b3446775ed8c79f4f94001a16b706989c462, 595101f13b961d69c553ce1ed24f60f3f1085e02, d57f11ed40c3cdcbb36cb758191db4f2c9372965, 676664ee7471738577f641e6159e7596625b7fdb, 784141489258258b12979061d92c1a616da26525, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, 8c7aee0bbc062d5b4bcd34951b1e002274288206, b9f190339ce0b4cdad3464c45ec3266a7369fb7c, 1452b25a7680bbb2c66dd7dfca6704292405da92, e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0, 1d7531db9272f7838e33616075e1e64532fd013a, 8ec5896b4490c6e127d1718ffc36a3439d84cb81, 1051abf1e3dae90241ad15b3f98f2e41197ee611, e4b52a1a00e9db941326fc857b95245cbfb60bce, b52db9e41e15f76bdcfbe674abe0314af545c430, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 0e141942fa265142f41a2a26eb17b6005d3af29e, 147c868b721c8d29df7c61db7f2360114c760614, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, 22ebfc211d184ed615729378a43fde175bf14478, b79e5e4622a95417deec313cd543617b19611bea, 46c266b3d1274dacd7fce27ee8cb4d587f087a58"
834fdec542153aae5fe725df801aac87ba5e8f56,Graph Querying Meets HCI: State of the Art and Future Directions,https://www.semanticscholar.org/paper/834fdec542153aae5fe725df801aac87ba5e8f56,Conference,"Querying graph databases has emerged as an important research problem for real-world applications that center on large graph data. Given the syntactic complexity of graph query languages (e.g., SPARQL, Cypher), visual graph query interfaces make it easy for non-expert users to query such graph data repositories. In this tutorial, we survey recent developments in the emerging area of visual graph querying paradigm that bridges traditional graph querying with human computer interaction (HCI). We discuss manual and data-driven visual graph query interfaces, various strategies and guidance for constructing graph queries visually, interleaving processing of graph queries and visual actions, and visual exploration of graph query results. In addition, the tutorial suggests open problems and new research directions. In summary, in this tutorial we review and summarize the research thus far into HCI and graph querying in the database community, giving researchers a snapshot of the current state of the art in this topic, and future research directions.",2017,20,"1730344, 1891554, 2128664093",1730344,Skopje,3,"1764236, 1723636206, 3471557",Y,"2) compressing the embedding space using pca;It is investigated how several RL strategies perform on a large, standardized data set.;2. The proposed attention mechanism seems to be fairly domain (or task ) specific, and may not be beneficial for strong generalization (generalization over unseen category).",SIGMOD Conference,17,"graph,research,query,data querying,data querying","ce2d5b5856bb6c9ab5c2390eb8b180c75a162055, d997beefc0922d97202789d2ac307c55c2c52fba, 087dd95e13efd47aef2a6582e6801b39fc0f83d8, 39602922b04885047254444fd1a1586d797617ce, 89af855962927fb89a673a221f6f394a6f3dfc6a, 48c73c389c3f36d70407eb8309a0b41578c15fc8, f6dab84c2c00ab92d8ee9d9359d7e530512114f9, d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b, b7a717233ec3ff37385ab1b06816d0ca375f5bb3, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, e2e8ea9bcdb83deb2787ce89db1b51f7ccb1bd1e, ce7499d6862df8269c655220049c3ed20b9b6f5e, c2413fa296543159b32d16350d9e29f7db528790, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 57f5bf937f7393b691428747a9078d3124e6bcce, facd5f5deb152229ceb1803434d8690a09ab4129, 4aec7bcb292cb763341a22fc67a1cfaceb3a2c67, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, 2737a61f6557fe7bf53a608c668de2eff1f582f0, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, fe2492b7b8cf6d1d10b7ea38e0f7f853bd679d52, f0eb17c6def9aaf95ef3c8ce12b9c3ceb9030274, 4e746359afd6f81705b875d71cc499b904a320df, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, 0fe0f08e71e188cb2a663f36acb296d9eb6fe694, 322d91190acd8ac8c64598f5126947b0485ba249, e3b94a5f28522e6825aff16ff07d56bd70d26c96, e2e7d964c09e27d334fcb8761d69918630629387, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, 92afbbe41174a545f9da9992e33c9a9592e529aa, 9181b0d801dfcd7723a3ede201f0543078e2c149, 41c93960a066876d5e4f1dacaef75cd8daa2791f, 78e40584f0d149bf6f98beb5561b7b83cb68e1b1, 60caa5b3d066e13feac496fd0736e976970eb09f, aa9bf8b4c4ce8c28b9f47d7ce4f416aa9726bb0d, 15370f51d666ab8ef17185679553c6a8647b2a15, ad10ef93675513a68b93d54f3a461160b53318a3, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, eb9e0da8b7170e3ca4364f2f9010599c2d2556f1, 0601e9e434b30320c316c76228b97c093fa98ad6, 7909428ca7d813331bbe4d33d07ab09e984b41b4, 71ba8a8c75e0826f94eb53ee7a4566d4fc5b5256, 90aca7b4cdd728b28b2fb5b4dc3ae3e37daa5b94, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, 02fa2389b1b64b661192e224bed8af6df0ce80f6"
87048df918c34b662bc0d28894efa430d70a9206,Crowdsourced Clustering: Querying Edges vs Triangles,https://www.semanticscholar.org/paper/87048df918c34b662bc0d28894efa430d70a9206,Conference,"We consider the task of clustering items using answers from non-expert crowd workers. In such cases, the workers are often not able to label the items directly, however, it is reasonable to assume that they can compare items and judge whether they are similar or not. An important question is what queries to make, and we compare two types: random edge queries, where a pair of items is revealed, and random triangles, where a triple is. Since it is far too expensive to query all possible edges and/or triangles, we need to work with partial observations subject to a fixed query budget constraint. When a generative model for the data is available (and we consider a few of these) we determine the cost of a query by its entropy; when such models do not exist we use the average response time per query of the workers as a surrogate for the cost. In addition to theoretical justification, through several simulations and experiments on two real data sets on Amazon Mechanical Turk, we empirically demonstrate that, for a fixed budget, triangle queries uniformly outperform edge queries. Even though, in contrast to edge queries, triangle queries reveal dependent edges, they provide more reliable edges and, for a fixed budget, many more of them. We also provide a sufficient condition on the number of observations, edge densities inside and outside the clusters and the minimum cluster size required for the exact recovery of the true adjacency matrix via triangle queries using a convex optimization-based clustering algorithm.",2016,45,"40275640, 1736279",40275640,Berlin,2,"2115338656, 2086632521",Y,It is also simple and easy to understand.;This section could be improved by demonstrating the approach on more datasets.,Neural Information Processing Systems,16,"items,queries,query,data querying,data querying","e32a2519b59d62cff6cb8136ee242dc3754ed57b, 98b9086750f08a21c8778ab986339321e9caf790, 079b57837221413bf99ab40999c77c29e280e0c2, b3dbe1b460a3b66df1653508c9eed7dd51dee4d2, 9e27190f2d9b2167d4a66b88696def4585072fd5, ba687027ed6012f613e1f9a9cefe7683bb192934, 75c364909914f17791837ec88090262aa6656d3e, 5d24ed8942235324512d6cedfd8dbf54c57658b4, 4267178106cef2e77284bde309dfaaf9fd46a91b, 0a71dd8bec060195e14eb9d0a7abbc08d960d4d5, 746a81aa26d3ebfb81acfd6af958d6a21603cd21, fc32074b37a6d9dda535a70f9689022e70508520, a22f3398ea865426c89ee66f4824ec626e56a864, 155f27879f185f1ab04107c91c2ae7cf6a910a03, da8b317b99c4b8933b2c59285639eca6c3fcb869, 4087e84fc695bb6433d0104ee94f9d7e9f4b7da5, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 3a083d843f891b3574494c385699c21766ce8b7a, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c, 8dd0c1e955c66092ff951941a151336211e6e171, b04550f0722e9614163855ab36fc2430b931a3fe, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, 86dbd884043eb5807c61d2c65b813e673b4a04fa, d6bc29a897fd85e7187dc33c3c974b8879462237, ba9b6f805feb62c978d384211f910790643a023e, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, a8d76d84408c1fe6b1543084e6cec3dfc4ede429, f156ecbbb9243522275490d698c6825f4d2e01af, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 40416ac3bf78583eea37661b1b446e9939245b3e, 73d4accea441aae2373828a8dc2175aa2759c38f, 38f5b53b49be555430f33b8363910191a3df1d14, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 695bdc6e24608364491b9418a220c65a7cd17413, 48c73c389c3f36d70407eb8309a0b41578c15fc8, 6c739540e66e895311b7347971f10ef556e06e52, 590b617c08d34bc6caed7e4490c0b22a9c516e86, d9d325ca670a1aa215e3e39023f8abf17dae7584, f04d8e558c11ac0fc9318c4f3d61b651c62ddcfa, f9253a3e2627f1c2a0a7e2cea320a4ec4e4d2ff9, 659bdc9813de1667ebe9bd66dbbef78bc1ed0348, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4"
7eaac9847257c32afd450017d1348ecda4dcaade,NOUS: Construction and Querying of Dynamic Knowledge Graphs,https://www.semanticscholar.org/paper/7eaac9847257c32afd450017d1348ecda4dcaade,Conference,"The ability to construct domain specific knowledge graphs (KG) and perform question-answering or hypothesis generation is a transformative capability. Despite their value, automated construction of knowledge graphs remains an expensive technical challenge that is beyond the reach for most enterprises and academic institutions. We propose an end-toend framework for developing custom knowledge graph driven analytics for arbitrary application domains. The uniqueness of our system lies A) in its combination of curated KGs along with knowledge extracted from unstructured text, B) support for advanced trending and explanatory questions on a dynamic KG, and C) the ability to answer queries where the answer is embedded across multiple data sources.",2016,50,"7617146, 39073194, 1950842, 1786639, 1882948, 121917790, 143840196",7617146,Valletta,2,"2064711347, 46255467",Y,"Concerning the lack of mathematical rigour in the statistical physics literature on which the authors comment, they might want to relate to a very recent work https://arxiv.org/pdf/1708.03395.pdf work that sets all the past statistical physics results on optimal generalization in single layer neural networks on fully rigorous basis by proving that the corresponding formulas stemming from the replica method are indeed correct.;Also, the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies, i.e., a priori information.",IEEE International Conference on Data Engineering,33,"knowledge,ability,graphs,data querying,data querying","538fbdb8013ab43a9b5d725461b294ad29fcced7, da5d78b3e3a1544fde98fba86088e1215e97cbe8, 4e13a8e8ba8d33e15ed037bfca7c651047533990, 633e2fbfc0b21e959a244100937c5853afca4853, e30d9b8ce108d982169621b88a5e3fb69fec70e1, 73d4accea441aae2373828a8dc2175aa2759c38f, 8dce62b18bdea587c07cb4769a05ca0a816d09ba, 3faeb21fe256b99391d69570053a8c2d91e9f348, c5d3fbc024ad9a0d28669b6030de23fc5c3f7888, e8b7a9be9f2d0578a95319ed5841978e10429967, 3caf34532597683c980134579b156cd0d7db2f40, 68ade27b68c05d1b60b8a9d8f8aadbe76900c60c, c3df199cbca74763c4ae9889409bbd4aa29b6255, e02f91d625cd32290d4ede0f31284da115844316, ff75865cde62592d068b2afd055c57c81d77158b, 6ba00c2386f2edc0b43eec442cd1923b5d964633, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, 634fe61f3ab6692ea4733989a1c76e793bb8b69e, 343500e0052eb1b683f32b00efbbd1331c94184a, cf5dfc4a9f7a82b32640128ca10832eace55880e, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, 4f480bae3196dbbc27ab383bce33478ea963f9b3, 182180bd69ea6d2f59225ded5ddc900b8558ab9f, 82870bc488b57cdf5ea62877109a7278af2926b3, 28a5a53dafacebad8a7c47773079caeffb9a5baa, ade9a900acc3c138021070537840488526796d35, 00bbc94806ca0821a9c82d8aedf16f0e6263b89f, 32524aa3ae8522542753ed7e6f4cca3970e4acab, 96273b87cd0eb9d1c9a12afae621ce2abdbbab36, 6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91, 88932b1563316e4ed7a61c4c2c0c90d7aa9bb308, e7f3478fd8aac6940a4bf4f5eb60ac38f6b0b85b, d2efa91bd7d076a66bc1d1c570f6aa2da944dd88, b05306f0b142e5afb3974b1b79996e5b82653662, b80e2af7ffa85fe2f02ea8390e4915d55c19d199, 19cf7458db4e17c7504eee24ccf961e1dc91435c, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, e1e43d6bdb1419e08af833cf4899a460f70da26c, fe44200fed05f9a7c656f2245deded8fd5f5e1e6, 147c868b721c8d29df7c61db7f2360114c760614"
29ddc1f43f28af7c846515e32cc167bc66886d0ca,Parameter-Efficient Transfer Learning for NLP,https://www.semanticscholar.org/paper/29ddc1f43f28af7c846515e32cc167bc66886d0c,Conference,"Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.",2018,2453,"2815290, 1911881, 40569328, 68973833, 51985388, 2813347, 2809991, 1802148",2815290,Nicosia,2,"50052368, 115300694",Y,"In the videos, it seems like the robot could get a slightly better view if it took another couple of steps.;Algorithm 1 does not make clear the relationship between the model learned in step 2 and the algorithms in steps 4 to 6.",International Conference on Machine Learning,18,"task,parameters,transfer,nlp,nlp","5c45a5d05ac564adb67811eeb9d41d6460c70135, 63adc1e5086481e36b19b62707a96b799da51e59, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 239bf45c13b3f6d38c74026b535f785febf9cd08, ef25b02f3be31c699255ee05aa90a4a17461d95d, a0a79dad89857a96f8f71b14238e5237cbfc4787, 2e965b5d97c2d6fb4af284307735be39283792ba, 8b417c2be7a7707f372049fb1193f0d42f799562, c9f320789e98d2c7a798a9705e26dbe317677966, bdb68c5e2369633b20e733774ac66eb4600c34d1, 322d91190acd8ac8c64598f5126947b0485ba249, febe776e285dc5e72c7e3ee697a87a794e1c00ff, 0456ed94f6c455f99cb67abe8a28aeb3ef9f489b, 92930ed3560ea6c86d53cf52158bc793b089054d, 013eb12ce5468f79d58bf859653f4929c5a2bd14"
d6a083dad7114f3a39adc65c09bfbb6cf3fee9eaa,Energy and Policy Considerations for Deep Learning in NLP,https://www.semanticscholar.org/paper/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea,Conference,"Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",2018,2113,"2268272, 47079359, 143753639",2268272,Bratislava,3,"38608914, 2285816475, 2109716565",Y,"1. The authors claimed that this is the first end-to-end trainable hierarchical attention model, but there is a previous work that also addressed the similar task: Seo et al, Progressive Attention Networks for Visual Attribute Prediction, in Arxiv preprint:1606.02393, 2016;on the positive side: The paper is well written, quality and clarity of the work are good.;There are many typos (e.g., ""This advantage is also its difficulty"", ""Much previous work on language modeling has evaluated "", ""We focus in on the task"", and others) so the writing needs to be significantly improved for it to be a conference paper, preferably with some help from a native English speaker.",Annual Meeting of the Association for Computational Linguistics,18,"nlp,hardware,networks,models,nlp","9d6aa5247b9919a86f174e918107c234c548274d, 13c4e5a6122f3fa2663f63e49537091da6532f35, 34ca47eed139a7f0694611528f75debc43385518, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 657fbf29ea0b4904a3e98d1556f9acf38dddae5f, f9253a3e2627f1c2a0a7e2cea320a4ec4e4d2ff9, 0ecf3f089e6dc7944b440227069b9d0143e18d78, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, 4954fa180728932959997a4768411ff9136aac81, 1c2efb418f79b5d29913e014a1dfd78865221c39, 2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c, fd0496ab020acf366375615ab40235e6dd3c5897, be2b0396de9431bae931642516a1d3e4906329f5, db6ad6ded1cfa26fdc7437f27fb823ec533e96fe, 77a096d80eb4dd4ccd103d1660c5a5498f7d026b, 82a09f72d7b9587e3b519b1cd9640a5a611f3480, 099043827df60225cf33c820052716cce64d49e9, f208b3fb28c556ab62f9d202b7beae89700a338a"
06d7cb8c8816360feb33c3367073e0ef66d7d0b0a,Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks,https://www.semanticscholar.org/paper/06d7cb8c8816360feb33c3367073e0ef66d7d0b0,Conference,"How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions—training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.",2021,432,"1705260, 1817207, 1805993128, 2156538832, 2162779320, 1667817604, 2063938464, 2162779517, 2064353087, 1689466093, 2074097046, 8458211, 2162779839, 93841942, 9377739, 2110871234, 2158994272, 2162779709, 81331041, 50494955, 40879549, 1423660254, 1576655836, 2067056655, 2162781785, 39765564, 2159207824, 72254820, 51479145, 1380075136, 2112127876, 2150098899, 19255781, 2126503480, 2144058688, 2064619864, 1699545, 144365875, 2548384, 1783281",1705260,Vienna,2,"153475895, 1753223",Y,"Good contribution, paper needs to be made clearer This paper thoroughly analyzes an algorithmic task (determining if two points in a maze are connected, which requires BFS to solve) by constructing an explicit ConvNet solution and analytically deriving properties of the loss surface around this analytical solution.;Considering representative synthetic problems is a good idea, but it is not clear to me why this particular choice is useful for the purpose.",Conference on Empirical Methods in Natural Language Processing,21,"instructions,nlp,models,tasks,nlp","90aca7b4cdd728b28b2fb5b4dc3ae3e37daa5b94, ed9e7821b3e51c7e59183300d6c8cf90c8de0f26, a8d76d84408c1fe6b1543084e6cec3dfc4ede429, 017010b941d902a467f6d329ae5e74fd67e67912, 3db1841fd5f2561a11dfbd8173616b3e695c84a1, 43e624ddeed82df944a6cae0dedec3372438e243, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, 453fdfeefd6498a65be339d7e8722f6f3288907e, d9b5194f3f959eda2e95df6a340254f52ced46f4, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, 518a7c79968a56d63a691d42f8378be6c776167e, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, 8f99f5cb3950fcd1628c6a563be4b4fc4966fa66, 91b2b47cabd800ef658b65bfe1f52b7293a740c3, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, da3f33d858586d24cb265e79eb54f3746e998f57, fa7b8acd47631bada5b66049824bfd335ac6bf8f, 3dd7f7118ee174265889d00d100cfe2a02871be8, 56266342b01a4f2ddc28a1e8401dbbad105736a5, b795c74a0150ec091003ffbaa5bd7d74487c137b, ed935c6b359a7a486c28240d796e84897d095125, 156d8e2aa90b5ccc9be10477ca70deaad0151387, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968"
5471114e37448bea2457b74894b1ecb92bbcfdf6a,From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models,https://www.semanticscholar.org/paper/5471114e37448bea2457b74894b1ecb92bbcfdf6,Conference,"Language models (LMs) are pretrained on diverse data sources—news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness.",2022,72,"2114887261, 50487261, 2169159066, 2073587169",2114887261,Andorra,3,"47540245, 2113511266, 2152050780",Y,The particular structural elements also induce certain inductive biases which make learning or generalization easier in many cases.;The manuscript mainly presents a cheap pruning algorithm for dense layers of DNNs.;The main novelty in this paper is that it uses the label as a third view of a multi-view model and make use of cross moments.,Annual Meeting of the Association for Computational Linguistics,22,"lms,biases,data,nlp,nlp","ca0e479ba2327f71e842d033b6b48b082962cc6a, dca4d9abbc82e57dfa52f932e893d467a63e0682, 4e58100b319d74f97ed550a4e5fa32dea8c06fe1, d88c1255876b62fb5f5a8b292098ca430710a540, 4b9184937da308914b9e13c43bfd75845eaf910b, fd3259ecd1c94731f5d9bd9c0bf7417f2e147c71, e30d9b8ce108d982169621b88a5e3fb69fec70e1, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, df7336844a31165db0ae08f1cd0f560c9e3faeea, ae026f29c2d571871f426ff4873d43b4ff90b9ad, be383c607d4d357c763d2329ab71799c6e1393b4, c30c0092bf4eb8a44faec3fc60cdd5006276bcdc, a830083704284c8c5ddaf04f676c6ce23d583942, 5030702fea15d66a73fc997325431f1d7945ad9a, 598231eb906b183f7a2a408ef4536127e11e3de9, 0090023afc66cd2741568599057f4e82b566137c, 8ec5896b4490c6e127d1718ffc36a3439d84cb81, ce54e3b89a2570035b70885e6901ad4c92ae41c9, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 4a7eea3ec3080ecb277bfe466afce4822a1071d7, 8fd8cb4951ae9634a378383a880fbf16ac5d6926, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, e8b7a9be9f2d0578a95319ed5841978e10429967, 447884e7da189102189a156966623335c72199b0, 08764019e9762da527253b37b0ff39c46a4206b7, 4267178106cef2e77284bde309dfaaf9fd46a91b, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, 78e80b6f3fa46e26ea4e8542815305c3ba88c5df, ba9b6f805feb62c978d384211f910790643a023e, 60119658af638693f6de23d8466968e60c428ac7, 11342d45911ee8a7c9e3a94117ce774ad7036172, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 933baeec555352784848a93284c9dd0e79477759, d7fddafbbc372da4fa884f67bdc32db71b888806"
13c4e5a6122f3fa2663f63e49537091da6532f35a,Are NLP Models really able to Solve Simple Math Word Problems?,https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35,Conference,"The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered “solved” with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.",2020,385,"1443788809, 92954142, 144260125",1443788809,Dublin,3,"2805081, 2146671969, 2328103",Y,"If this is not the case for some reason, more detailed explanation is needed.;6. Figure 4 seems a bit misleading.;This seems at odds with models, such as DRAW, evaluate the likelihood -- once at the end of the generative drawing process.",North American Chapter of the Association for Computational Linguistics,20,"mwps,solvers,mwp,nlp,nlp","950850e22e42201f152d90dc6f53d53e39d37657, 5c39e37022661f81f79e481240ed9b175dec6513, eeac4411ae119c6c7ac33a11f762f2495b4dd960, 453fdfeefd6498a65be339d7e8722f6f3288907e, 72a6a5d5864eb915231c7128b90977f1d2acf6e9, 00d1f3423a33f73ca6aee884a58834547475d2f0, bf69c98fca9a9f6c1cde871beddbcdc668b77771, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, cf41991d89301c3c12420d150792cb1163999962, 2097ff87df3cb9427c7388bc7b997ed56907d45b, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, 7637ed79d30d0139901175ae4abedd822c217ab4, 643da4c4de1954daeac571a82367241db012a8bf, 538fbdb8013ab43a9b5d725461b294ad29fcced7, f7b69b8babfa607f2e8b0371f24cb720a7a827d6, 742747c7a453b293352b772d0d99541c96a351c3, fa26a6d434450b185e669170e79fd3e1d29716bf, cb3968152f7d93f53d24b00279a90d5071ddc85a, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, 92afbbe41174a545f9da9992e33c9a9592e529aa, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 364128bcce9836d60e685bb717b80f30e25092e0, b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8, b651d67502790e1d6d41c589e1d93e996ba7b935, 2478f43de2f1fcd9301a81e9b42d86b1d46db02d, e4788ee4f5e90c6f42cedc5116acd2d6475c3180, 73cbaf5f2441ef3478266b5438c0e90d1ae71652, 09797949fc70fbad8f3fed4f6cf4a91a9c709652, 2019cf49b51021a376f9833a53565513f0d8107b, 41c93960a066876d5e4f1dacaef75cd8daa2791f, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, d916776e0c6a04b0def4c22257c188776c2edab2, 9a60d3c9d3a5b7f165325fca45bd418d651682d3, 6ba00c2386f2edc0b43eec442cd1923b5d964633, b473e91cbe80c8b46451b49153cd5f93030480ab, 4e58100b319d74f97ed550a4e5fa32dea8c06fe1"
33ec7eb2168e37e3007d1059aa96b9a63254b4daa,Beyond Accuracy: Behavioral Testing of NLP Models with CheckList,https://www.semanticscholar.org/paper/33ec7eb2168e37e3007d1059aa96b9a63254b4da,Conference,"Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",2019,866,"78846919, 35232494, 1730156, 34650964",78846919,San Marino,3,"2868254, 89843190, 1707355",Y,"The paper is based on the cyclical learning rates proposed by Smith (2015, 2017).;In Tables 7 and 8, the human beings agree with the LeNet in >= 58% of cases.;However, batch normalization only sees the variation in the activations given to it by a SPECIFIC set of weights.",Annual Meeting of the Association for Computational Linguistics,19,"nlp,models,checklist,test,nlp","0fe0f08e71e188cb2a663f36acb296d9eb6fe694, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, b2114228411d367cfa6ca091008291f250a2c490, d47a682723f710395454687319bb55635e653105, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, 12c9bcf710d30ba991fb765ace07f177f53ecfd9, 01bf0e83159712fbbbd12171a7e268547a4cfbc5, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, 6d2d6707a0674ee41d539d106e6cb359a0a6ca06, 0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4, 9db0247728950788a2b42097d81dc0e24eed6bb2, 819167ace2f0caae7745d2f25a803979be5fbfae, ccca203382e5dd198c089a0f1d7af7bef0f694e9, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 417326e51d78ba8bd2621f23e539b41bbdd336d6, e32a2519b59d62cff6cb8136ee242dc3754ed57b, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, a9c1566119695250f68a572a4260b03721cc8ba3, 1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0, 68ade27b68c05d1b60b8a9d8f8aadbe76900c60c, 2019cf49b51021a376f9833a53565513f0d8107b, 518a7c79968a56d63a691d42f8378be6c776167e, 0599f45e03ac2016321df0dd653ba4c0034c79d5, 8713452753fd01de5616121af93e173d4f74eaf6, 34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61, 4cd033a56b19f87f6adfefeef5fcc990306ecf40, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, eacf9284a39adcd56172665f31fd5a72560bba7a, bcc9245ebf57072c07c5a9d4eff3aaaac36cb7c6, 647d0e540627c5a903299a90d20530f8e48c18d9, 00bbc94806ca0821a9c82d8aedf16f0e6263b89f, 77e6c9917536949a82e5ca02c4882b69ee8a4fd6, 6dc4883228c95e8a332320fcc587a0ff33c84d59, b6593b5ad6b0e00232ee88cf2bc578645c1299a0, f3d594544126e202dbd81c186ca3ce448af5255c, b067177b1e17287185eb3b82ccc3d7c646b3ec40"
97906df07855b029b7aae7c2a1c6c5e8df1d531ca,BERT Rediscovers the Classical NLP Pipeline,https://www.semanticscholar.org/paper/97906df07855b029b7aae7c2a1c6c5e8df1d531c,Conference,"Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",2018,1193,"6117577, 143790066, 2949185",6117577,Stockholm,3,"6413143, 51488437, 151213231",Y,"All components are trained with dense supervision (e.g. loop closure, ego motion with orientation-position, and the ground truth local overhead map).;Summary: This work is about model evaluation for molecule generation and design.;I found the formulation of the \alpha to be non-intuitive and confusing at times.",Annual Meeting of the Association for Computational Linguistics,18,"nlp,model,information,pipeline,nlp","ca0e479ba2327f71e842d033b6b48b082962cc6a, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, d3855b7351c11145e51301e6b686f748ca35c802, a620ea8654f86b282e0d2c1fbc6616b90ca45bd1, cb3968152f7d93f53d24b00279a90d5071ddc85a, 7bc9607c5cf3fc817675d46844f529097d579514, 98b9086750f08a21c8778ab986339321e9caf790, 742747c7a453b293352b772d0d99541c96a351c3, 9d6acac70b2d1fdb861a08b00766ef263109cd7f, 244e08e109f6f4fa0f846977e1aef1e7b6a9e816, 3994334c81478a4b17341eb1f494dbccbb73d999, f3d594544126e202dbd81c186ca3ce448af5255c, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, 3813b88a4ec3c63919df47e9694b577f4691f7e5, 2f44ab0e52eb98fdfc0086638887f175b6f2fe4b, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, e449b9b3fe04fe260731a3c74d2123bf6eaadf5b, 5f51d468ce730eeade7e9f419a1fe7152582be25, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, 332e0eab5fba8e6940f3e481f542a99ac17b9717, eed62d36d1b976ac3873c83645f1c25f5096f89c, c84389369720dcd2f004c48e58fbac2c45c8f092, 6001895c2fcf69528d01306e9d293d9d2a4cc67b, b9b639522465cc606df878eee62e7f9c4bf19e62, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, 551bbf493f55ea2c7b64ef8b91fc81d7bf6d68fe, 54ddb00fa691728944fd8becea90a373d21597cf, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, 6be56f559a74c0124526242e70cbdfd16cbc60a7, 6dcb1cd576b0e54b900f45a178efe271c383de04, be082d70534db088315f2cc5b42c2fdcd58c1b8c, 709af143f78bc62413c50ea1a7ee75b0702c4f59, 0601e9e434b30320c316c76228b97c093fa98ad6, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, 147c868b721c8d29df7c61db7f2360114c760614, dad8965c5a4c0a0ea1eb3837c6a9c3b42597c2ce"
d47a682723f710395454687319bb55635e653105a,Language (Technology) is Power: A Critical Survey of “Bias” in NLP,https://www.semanticscholar.org/paper/d47a682723f710395454687319bb55635e653105,Conference,"We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process. We further find that these papers’ proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “bias” in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of “bias”---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements—and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",2019,838,"3422038, 2881033, 2065041692, 1831395",3422038,Monaco,3,"2108724347, 9162688, 31347453",Y,Their abstract also claims to utilize a convex programming formulation.;Hence inferring a structured latent space is a challenge.;Follow up experiments extend the basic setup significantly.,Annual Meeting of the Association for Computational Linguistics,19,"bias,nlp,systems,papers,nlp","f5ac3ec506a5a01807dd7196c0c52eef008b78cc, b795c74a0150ec091003ffbaa5bd7d74487c137b, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, e7f3478fd8aac6940a4bf4f5eb60ac38f6b0b85b, 8c33ca066e2ab615e24c65198c794114436053dd, f4207bcad24fe4eaf3849d6c1fe38c36545b5cb3, 08be4e23951a0def1c5d235befbb39c8d8d373a3, 256e95ca331cbd35b3a23cc306b6627e6771a963, 410fba9f03212257d0881811802e6620e59bc827, 8c7aee0bbc062d5b4bcd34951b1e002274288206, 75c364909914f17791837ec88090262aa6656d3e, ce2d5b5856bb6c9ab5c2390eb8b180c75a162055, cf5dfc4a9f7a82b32640128ca10832eace55880e, fd0496ab020acf366375615ab40235e6dd3c5897, 0af24df95f8a0b7e6fc257a98a5820314d1243f3, 8e259f940f007e08207ddb7c3a052f52036d7bf6"
c9b56cb026a38e39bb0228faac57accd6f65e6f7a,"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP",https://www.semanticscholar.org/paper/c9b56cb026a38e39bb0228faac57accd6f65e6f7,Conference,"While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack’s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.",2019,522,"153769695, 1453652787, 1693182792, 1829303908, 2068347799, 121817403",153769695,Vilnius,3,"3010457, 1410115257, 48919600",Y,"It might be good to emphasize that you don’t train on the IWAE bound in any experiments.;But the current system is a great start.;When citing Shen et al., 2017, consider also mentioning the following: Controllable Invariance through Adversarial Feature Learning; Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, Graham Neubig; NIPS 2017; https://arxiv.org/abs/1705.11122 Response read -- thanks.",Conference on Empirical Methods in Natural Language Processing,19,"attacks,nlp,textattack,code,nlp","b977e8de38dc0d13817bca1ed20036badfe2a58c, 48265726215736f7dd7ceccacac488422032397c, 4dcbe71d261ba5caa0a06d5c265b5509edbbe1c4, 16753e0317730e8c1b297338300a8c6163dd06f2, 2c3eef2f17369912e330281d54b535675077e4ca, f2bc4057e696f49c326bf8e1588772a16f053754, f381c53aeb7742e4047d06d84f9e0c4f523231a3, a357f1ff27e184d9a5ef69e665e8ca891032bf71, 71854ff4306cf65c3c2161f7be2d0346275f72d5, bc44c0c64a473e035b11ae60a1993ad3db1acd2e, dc10a3f4dae2db48148ff6781454ddcc856ae8da, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, 733fc094e785724621c46e20db1be69f132ad9df, 0df5a4f9cc8a244715fe9968732497d2ac2a7cd1, 00d1f3423a33f73ca6aee884a58834547475d2f0, d2a609ffb814442d0728aef9f6616f9cd775face, 8b28792f8405b737229afb92c99c579b86d8aa98, dcbaf58b16ac7ef947879ea37c021466357b291a, 929305892d4ddae575a0fc23227a8139f7681632, 4be9368abc2474d6fd38639e523cf03af1873fd9, 5ea7bf772fecf95cbf53b2c7f719c9440322a115, 784141489258258b12979061d92c1a616da26525, ed9e7821b3e51c7e59183300d6c8cf90c8de0f26, febe776e285dc5e72c7e3ee697a87a794e1c00ff, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, c5c4142a01981787a71bf6ebcb791520c458ab5d, 39602922b04885047254444fd1a1586d797617ce, c84389369720dcd2f004c48e58fbac2c45c8f092, b09139c153bac8893e8faea2b3a59159234caadc, 6b3756d32ab5b0a5715a5cfc3672290d2d643017, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, 665b0c776ff7507c32793f10ce9edf90bc2f674a, 0863a5ce955e5193e535e1442086dc460dd295f0, b9518627db25f05930e931f56497602363a75491, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, b781fb7f3725a9d899d3d250b378d729a8a00442, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, f72d3f58ff73353978e224af348448b34d27cf7b"
0e141942fa265142f41a2a26eb17b6005d3af29ea,The State and Fate of Linguistic Diversity and Inclusion in the NLP World,https://www.semanticscholar.org/paper/0e141942fa265142f41a2a26eb17b6005d3af29e,Conference,"Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the “language agnostic” status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.",2019,532,"47070483, 50074956, 2410839, 3086996, 143990839",47070483,Chisinau,2,"32830771, 2163313042",Y,Do you get comparable inception scores?;The main signal I lack from reading the paper is whether the proposed model actually does better than a reasonable baseline.,Annual Meeting of the Association for Computational Linguistics,19,"language,languages,technologies,nlp,nlp","696b388ee6221c6dbcfd647a06883b2bfee773d9, 9a0424bdd12cdcdf45b556b0b9dcc6fc5a55520b, 2c3eef2f17369912e330281d54b535675077e4ca, c30c0092bf4eb8a44faec3fc60cdd5006276bcdc, 20a3ed03888037e2802fa9abad02ffa0a8dcc228, ff74bfbd9ebf4c54809873aecb04be27e9402cb8, 9eacf62f1e546748428c7e4843731b1595294200, c997ed5ac5772bb0fce8bb11b8c86bc33310f39a, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, 05b8b67451fb105576c58af960e6e6d98f9103e7, 119a9e5b563cf1134897553ee49325b5a5bd9fb9, 0456ed94f6c455f99cb67abe8a28aeb3ef9f489b, 6f31f59f7b4bd751b5d8657d8a878bef571f9a4c, baafed5f8968118af04dbbb1cf172f1c10bede25, 9eea59c34f139f3d2153226c8cf026e975622074, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, a2ec47b9bcc95d2456a8a42199233e5d9129ef18, 82870bc488b57cdf5ea62877109a7278af2926b3, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, f497c1ece7b6f3560bb39958e2673f476d608f98, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, 7c217cc7524251f42887438834912e06129c3299, 799d5a8271887adede035644d878c7bd555576df, 5371896313ac227eb819038dd55f213cb42b99e2, e4788ee4f5e90c6f42cedc5116acd2d6475c3180, 695bdc6e24608364491b9418a220c65a7cd17413, cf5dfc4a9f7a82b32640128ca10832eace55880e, 62fe1c3a866a5e368e110d6d8ed2385c84072cac, 102ebe229df18c8733ea1b8def56cd79996e2178, 38179848e2d6a3ad373b1793848816111428ac36, e2f3aa4ecc487fa496d1a51e875e747d4b7e6001, f8e57fa370fe10147aa22714e08409fc1b7dae4b, 40416ac3bf78583eea37661b1b446e9939245b3e, 929305892d4ddae575a0fc23227a8139f7681632, f18be38578ee52aa7071c404d42e3d53ae003122, 6628f9ee35e36cdfdcac8a46cef4dba8d529a83b, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 6ff909c6fe089fc8ebfc64eca0f0c3cc34ba277f"
579476d19566efc842929ea6bdd18ab760c8cfa2a,Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?,https://www.semanticscholar.org/paper/579476d19566efc842929ea6bdd18ab760c8cfa2,Conference,"With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is “defined” by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.",2019,408,"41016275, 79775260",41016275,Berlin,2,"145201124, 2309967",Y,"Interesting set of ideas and direction, but lack of quantitative analysis supporting the results.;In this paper, the authors propose deep architecture that preserves mutual information between the input and the hidden representation and show that the loss of information can only occur at the final layer.",Annual Meeting of the Association for Computational Linguistics,19,"faithfulness,interpretation,evaluation,nlp,nlp","80d9f0eb47b712988d19cbe29a7bfa63f2a175d0, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, 375125029b085e70a109491656b69aa01bc2a166, fc61c7221350806c25379f385c27b2102ff8eb57, d0ab11de3077490c80a08abd0fb8827bac84c454, 0e9a44ce661c3535d5ce747912540080324489f5, da9b8b4073e6ad44b3da66e1e117cb1ddbf8836d, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, 371a343457a4fbff00000bf4faa29b2b2f85744c, 8674494bd7a076286b905912d26d47f7501c4046, 7637ed79d30d0139901175ae4abedd822c217ab4, facd5f5deb152229ceb1803434d8690a09ab4129, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, 834fdec542153aae5fe725df801aac87ba5e8f56, 5030702fea15d66a73fc997325431f1d7945ad9a, 1b0aa15937fdf59103a5213bccf09cff83d0ee3e, 0fe0f08e71e188cb2a663f36acb296d9eb6fe694, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, 77a59de2e2b832321875cadcf9619dc313f02384, ec7f5dc077480df149bcd4358a3aa8441878ca59, a9cbbef8f4426329d0687025b34287c35bdd8b38, 08be4e23951a0def1c5d235befbb39c8d8d373a3, d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b, 239bf45c13b3f6d38c74026b535f785febf9cd08, 54a26f5ef4b0524c60249fe98d0fc3646b2791ad, 9b529fe170823f95509585d5aa39fa01a43558fd, 78e80b6f3fa46e26ea4e8542815305c3ba88c5df, 6aca07154c111f1c8738347d7112cad6b0bf974a, db4cf9f6a653d5c15973e836c800ea47743251ae, 980858461df7c4349f17b427686c5bcbcffbdc04, 68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a, 7b9b756ab509cb9f52dbac95e3e901d571f0784f"
77a096d80eb4dd4ccd103d1660c5a5498f7d026ba,Dynabench: Rethinking Benchmarking in NLP,https://www.semanticscholar.org/paper/77a096d80eb4dd4ccd103d1660c5a5498f7d026b,Conference,"We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",2020,256,"1743722, 153408953, 40383658, 9264826, 80833908, 47039337, 2737827, 119869488, 1422035486, 1736107, 1500242049, 48662861, 2138053020, 1918552, 3422908, 143977268, 144922861, 2110032535",1743722,Dublin,2,"2729273, 82614785",Y,"I feel that the authors should give a more prominent disclaimer to potential users of the test.;If so, doesn't that correspond to evaluating the model under a different generative assumption?",North American Chapter of the Association for Computational Linguistics,20,"model,creation,platform,nlp,nlp","834fdec542153aae5fe725df801aac87ba5e8f56, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, 742747c7a453b293352b772d0d99541c96a351c3, 69a72ff5b30642d11c96635e99aadad3140d33a7, 733fc094e785724621c46e20db1be69f132ad9df, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, a13149a80855412d970d0de2b41c611f4cf7e1da, 7a1e71cb1310c4a873e7a4e54d1a6dab0553adce, 9b529fe170823f95509585d5aa39fa01a43558fd, cde985e542218aced8c4a627cda3dd12939805f1, 37b9478b792fb1e7ad5f2c322ed4445b93df6c9e, 598231eb906b183f7a2a408ef4536127e11e3de9, f406aceba4f29cc7cfbe7edb2f52f01374486589, 9f5b82d9915d0752957602224c5056be7e749c83, 11342d45911ee8a7c9e3a94117ce774ad7036172, fa26a6d434450b185e669170e79fd3e1d29716bf, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, 71854ff4306cf65c3c2161f7be2d0346275f72d5, 9e5d389fcdce4c0a6827ebc804b4e863aa28c14c, 787ae2c51cd82b904bb4fb9ccb15266381af5436, ba9b6f805feb62c978d384211f910790643a023e, 647c4a9331e01e31a4350361d3460f0397fe694f, 6001895c2fcf69528d01306e9d293d9d2a4cc67b, d88c1255876b62fb5f5a8b292098ca430710a540"
087dd95e13efd47aef2a6582e6801b39fc0f83d8a,ERASER: A Benchmark to Evaluate Rationalized NLP Models,https://www.semanticscholar.org/paper/087dd95e13efd47aef2a6582e6801b39fc0f83d8,Conference,"State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",2018,473,"48727916, 49837811, 8937909, 51172373, 2228109, 2166511, 1912476",48727916,Lisbon,2,"41020222, 2066229431",Y,"I welcome and are grateful for any theory in the area.;Indeed, the authors have commented: ""AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures.""",Annual Meeting of the Association for Computational Linguistics,18,"models,benchmark,rationales,nlp,nlp","63316bb5b88d362051c048e864c3ae5d97a26d30, 537f5e8e4139392cd2d108f32495e5b2b80151ac, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, 6aca07154c111f1c8738347d7112cad6b0bf974a, 2c3eef2f17369912e330281d54b535675077e4ca, 4be7d1524edb0137599a5cc95f72844b85a52fe1, b27c98d8378848f2c23a067f2c5196f3b5a07572, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, 5107c83173e43f51d1bdebf6cafda525a7c26bf0, 1e1cf81a1113482be3f0c280db994a832cb9426a, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, f91dbd39d4c742ba675e447b04a0b0c70b33e836, 371a343457a4fbff00000bf4faa29b2b2f85744c, 453fdfeefd6498a65be339d7e8722f6f3288907e, 7bc9607c5cf3fc817675d46844f529097d579514, 9e66ae24a541255c2d931184498ee116ce81478a, d0238f2fa88b0316ab862f843a5521eff0fd9bf5, a5710a7a54b50c26f3c40c37a19b2ec63330e90b, fd0496ab020acf366375615ab40235e6dd3c5897, 14dd50979af27bd2574c8068db11d27028b56afd, eda6756ab2844c390584686dc5e6385f4a8369cd, b85f3a66245d483f3eb3447eaf9950bd55f2b21e, 8c33ca066e2ab615e24c65198c794114436053dd, 6a1b25f7a67395ad1e676027322913acbb0a0635, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 0599f45e03ac2016321df0dd653ba4c0034c79d5, 74bc39003e65119eaa6ba339a61b45b417a638b7, d280cf82a6144b3e168840802b1a8a14d4eaccb9, 6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, cb3968152f7d93f53d24b00279a90d5071ddc85a, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 6dcb1cd576b0e54b900f45a178efe271c383de04, 71854ff4306cf65c3c2161f7be2d0346275f72d5, b651d67502790e1d6d41c589e1d93e996ba7b935"
a747e8f2659df479c0092301b9658fc582423df1a,"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia",https://www.semanticscholar.org/paper/a747e8f2659df479c0092301b9658fc582423df1,Conference,"NLP research is impeded by a lack of resources and awareness of the challenges presented by underrepresented languages and dialects. Focusing on the languages spoken in Indonesia, the second most linguistically diverse and the fourth most populous nation of the world, we provide an overview of the current state of NLP research for Indonesia’s 700+ languages. We highlight challenges in Indonesian NLP and how these affect the performance of current NLP systems. Finally, we provide general recommendations to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.",2021,56,"8129718, 9162688, 2789148, 66986482, 2279712392, 1935324, 46199596, 35722593, 2368148, 145465286, 1800564, 2124014463",8129718,Oslo,2,"5840536, 47224454",Y,"It would be nice to provide some sort of analysis of it, even an empirical one.;Moreover, the authors proposed updating the parameter \theta of the generator g_\theta.",Annual Meeting of the Association for Computational Linguistics,21,"nlp,languages,indonesia,research,nlp","1ab91d6ac7afc1a0121487a9089fa70edc1634d4, 8e259f940f007e08207ddb7c3a052f52036d7bf6, a1ef4052acb63356928bb440874c470ad48cb40c, 353c88c231ce156d604e074af276422422fc73f7, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, f7b69b8babfa607f2e8b0371f24cb720a7a827d6, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, fa75a55760e6ea49b39b83cb85c99a22e1088254, 0456ed94f6c455f99cb67abe8a28aeb3ef9f489b, ec7f5dc077480df149bcd4358a3aa8441878ca59, 62fe1c3a866a5e368e110d6d8ed2385c84072cac, f4c4e148546089123f8da5db4fb246ab4062bd40, 695bdc6e24608364491b9418a220c65a7cd17413, 11be2469ab1d1c508e7b6e14148990741ba87884, 6544259ff6b335b1dcec75e031b6d57e5b9509f4, 5406e153957dd7a165264da6e6e5d81251997404, 9c1265fd47df8d825f0ddd6d9ab834002592e38e, 2141334fad7248fc707607bc9453d44686ae07a7, 8d67b76222d84dcd337b8a2c78f13837070a79ce, 178571a5cde984c895493e2eb6c5487449d055cf, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, ce54e3b89a2570035b70885e6901ad4c92ae41c9, fac67bf55456b52ac6e4f280ad953d0250c74ebc, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, 3dfa820702b6181c9964931f0a4d47fd298bf429, ada0b87cd5c30d31186c38fb12e631d29426a3bf, 649c3497e3b34b15a5011259fcb837cf6c1ac04a, ad10ef93675513a68b93d54f3a461160b53318a3, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 9a51df7eaf2457b034c49ca8ca4ca3b81e6c08c0"
598231eb906b183f7a2a408ef4536127e11e3de9a,Challenges and Strategies in Cross-Cultural NLP,https://www.semanticscholar.org/paper/598231eb906b183f7a2a408ef4536127e11e3de9,Conference,"Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.",2021,72,"2064295987, 37922370, 49568895, 3295381, 30671790, 6547490, 83574123, 2093582149, 2125376289, 1717462692, 50110151, 82259306, 1660797358, 1700187",2064295987,Podgorica,2,"1803520, 2043402",Y,Are the test environments sufficiently different from the training ones?;The model is based on both source/target syntax trees and performs an attentional encoder-decoder style network over the tree structure.,Annual Meeting of the Association for Computational Linguistics,21,"nlp,language,efforts,speakers,nlp","3c8a456509e6c0805354bd40a35e3f2dbf8069b1, 752604994a7ca548ff2954114fc61a501d857b1c, 8760bc7631c0cb04e7138254e9fd6451b7def8ca, b415e836d447ea9efb7629a1de67cd2a6f9e7ba8, df7336844a31165db0ae08f1cd0f560c9e3faeea, f18be38578ee52aa7071c404d42e3d53ae003122, 410fba9f03212257d0881811802e6620e59bc827, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, f86f1748d1b6d22870f4347fd5d65314ba800583, 3994334c81478a4b17341eb1f494dbccbb73d999, 3faeb21fe256b99391d69570053a8c2d91e9f348, 0311ace1d499cadd1cc0c515a625d1d045f60d25, 68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a, 9b54941de1e21826ecc28b32730ac3f69991ede4, fb00016c1e048b9373803add001c1ec7e877cb23, 68a3d32416977e88cf1bfa4ad548d403f5f089d6, 69e76e16740ed69f4dc55361a3d319ac2f1293dd, 0cec0c296efedb814342b4b841d4583efbfc6777, 742747c7a453b293352b772d0d99541c96a351c3, 57a3fc6d0aaad3c10c793b4e59390ca04c935282"
3a7bbc46795929f0eace82b64c44c92a48682fb5a,FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP,https://www.semanticscholar.org/paper/3a7bbc46795929f0eace82b64c44c92a48682fb5,Conference,"We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to “mix and match” various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a “model zoo” of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",2018,679,"2403712, 2077245166, 2725328, 4565995, 134757625, 2742129",2403712,Warsaw,3,"143651788, 2243348413, 5486617",Y,"The fact that the proposed technique is simple yet yields such speedups is encouraging.;In light of this, I think it is reasonable to ignore the fact that the proposed initialization does not provide benefits on Penn Treebank and text8.;The motivation is difficult to grasp and the contributions do not seem compelling.",North American Chapter of the Association for Computational Linguistics,18,"framework,nlp,models,training,nlp","59c414c9efb77562f5d1aad8af14eaac968c69c0, 3924aa213ff891812c66a6909ab902684d3eb107, 63d8426ba1f51a8525dd19fd8ec92934ec71aea5, c292e473b3825eeb9db03c70b2e1c033aea190d5, 742747c7a453b293352b772d0d99541c96a351c3, c2528e88d5554e9df9f9d482ad46cb5331c4d794, 7669fca7fbc071b4ca78b4c326ff8f3a4b6ae616, ff7bcaa4556cb13fc7bf03e477172493546172cd, 84a36e19f9394f22b34f79756fa9628a795e02ea, 1661d0d8d47cac41e01c59c60aac3675b4396698, 152877c51df17cdd4a87d19e452c6daecfadf6c3, eff6546819d25df0bccdc89f02554a43a4f1c464, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, 263a58f4fd32caca1dad2351af4d711aec451fe6, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, 340f48901f72278f6bf78a04ee5b01df208cc508, 9712624bb61abb0da989514cae558cfab61bb9d2, c50a909e20bd07f4aea09dc6dae539b45b406a96, 0894585294c67193ff3190240554677b56fd79a0, 4b1280229ced73f6c86550f24ef01490fde52285, f5ac3ec506a5a01807dd7196c0c52eef008b78cc, 024006d4c2a89f7acacc6e4438d156525b60a98f, 62df84d6a4d26f95e4714796c2337c9848cc13b5, 2e965b5d97c2d6fb4af284307735be39283792ba, fdc57c18f3b636c3273542327ae540217972558f, 1c748f86182a62d44d5b44316db510f8d833e19f, 44786a2c2a8ba8cf5c74a8fb10098c220e924c56, 4aec7bcb292cb763341a22fc67a1cfaceb3a2c67, 91b9d3ab7532ea24ae70cd726355f25235b1fe8b, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a"
be7cb8f79bc018e57467168fc0c7f8ad59bba04fa,Adaptive Testing and Debugging of NLP Models,https://www.semanticscholar.org/paper/be7cb8f79bc018e57467168fc0c7f8ad59bba04f,Conference,"Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor, or only work for a very restrictive class of bugs. We present AdaTest, a process which uses large scale language models (LMs) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model. Such bugs are then addressed through an iterative text-fix-retest loop, inspired by traditional software development. In experiments with expert and non-expert users and commercial / research models for 8 different tasks, AdaTest makes users 5-10x more effective at finding bugs than current approaches, and helps users effectively fix bugs without adding new bugs.",2021,53,"78846919, 23451726",78846919,Bratislava,3,"1505828520, 103131985, 1996394",Y,"After reading the paper, it’s not clear to me what the components of the model are, what each of them take as input and produce as output, what these modules do and how they are combined.;* There are many estimators for f-divergences (like the ones cited above and many others based e.g. on nearest-neighbors) that are sample-based and thus correspond to the ""implicit"" case that the authors discuss.;It is not clear how a softmax output of a CNN, which is trained in a supervised way, follow such assumptions.",Annual Meeting of the Association for Computational Linguistics,21,"bugs,models,users,nlp,nlp","155f27879f185f1ab04107c91c2ae7cf6a910a03, 624b2f14be4287d6a400cdf88a6f911b434b182e, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0, 6fb020754f6de564c3a0a07bb656c0a90be1f87d, ae38dc77a962161107361f213db9216ee1274037, 6d2d6707a0674ee41d539d106e6cb359a0a6ca06, bdb68c5e2369633b20e733774ac66eb4600c34d1, 1ed33896e82cc3810b349cdffc2039f0b8edd82e, cde39ce861e4c7514ee07fd91b6b8aac50cbf01b, cd29c25c489562b409a60f83365f93f33ee1a0a1, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, eef23d76e175c0cff8e81ffcb2721c10539c8cbd, da3f33d858586d24cb265e79eb54f3746e998f57, 0456ed94f6c455f99cb67abe8a28aeb3ef9f489b, a9cbbef8f4426329d0687025b34287c35bdd8b38, d57f11ed40c3cdcbb36cb758191db4f2c9372965, ca05ca5f70616456d0f2e05c4a51cbf8b8d273f7, dd2deed2ce6e110236a1280db765fa02c7488eb1, 29409efa04ac99ccf01d2a011d21d5d14e870000, 0599f45e03ac2016321df0dd653ba4c0034c79d5, 024006d4c2a89f7acacc6e4438d156525b60a98f, a3636512a48321baab95c94052de2a0a88460602, 557ee73ff4e87ca0cb1b6c78af0730a2d94b710f, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 8f2cf30f9c825d8ef7d622601dbd525ace95e025, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, 15370f51d666ab8ef17185679553c6a8647b2a15, cb12fe714dd4fa1d14b6a8023d494c14c89e90ea, a2a514ed839dafdd0fb76d6c2615f25f35bf8087, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, f18be38578ee52aa7071c404d42e3d53ae003122, 45de91a919780d5540872cf047986a370625e61c, 390bcf15a1b13cb0d5966859c35c69a31238e838, 9712624bb61abb0da989514cae558cfab61bb9d2, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, a0d18dddaa995b126ad373e33767b9b881d16b2f, 8babcaf89f8537dc628a029ebf932100f57289fd, ff7bcaa4556cb13fc7bf03e477172493546172cd, 74bc39003e65119eaa6ba339a61b45b417a638b7, 24ab4e99e582c9770281eee0a39cbeb70ddd891a, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, b9518627db25f05930e931f56497602363a75491, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, f3d594544126e202dbd81c186ca3ce448af5255c, 6001895c2fcf69528d01306e9d293d9d2a4cc67b, d2a5dcecd2ffdf03473df1688091f08fadb114a3, ff75865cde62592d068b2afd055c57c81d77158b, a56bf7ee9a56d8f84079684339a953c2df9ce76b, fc32074b37a6d9dda535a70f9689022e70508520"
3caf34532597683c980134579b156cd0d7db2f40a,Universal Adversarial Triggers for Attacking and Analyzing NLP,https://www.semanticscholar.org/paper/3caf34532597683c980134579b156cd0d7db2f40,Conference,"Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94% to 0.55%, 72% of “why” questions in SQuAD to be answered “to kill american people”, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.",2018,625,"145217343, 2113511266, 1380266797, 40642935, 34650964",145217343,Vaduz,3,"1864353, 1830383266, 47149500",Y,"How much can change between the goal images and the environment before the system fails?;What is LSS in figure 4?;The WAGE parameters (i.e., the quantized no. of bits used) are 2-8-8-8, respectively.",Conference on Empirical Methods in Natural Language Processing,18,"model,triggers,models,nlp,nlp","8babcaf89f8537dc628a029ebf932100f57289fd, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, ac4350d3a2673a4c34fa949714ede8b45fd04f1d, 950850e22e42201f152d90dc6f53d53e39d37657, eadb1e7da375939e25083ae3936c4f4ef1f2a719, dc10a3f4dae2db48148ff6781454ddcc856ae8da, 11be2469ab1d1c508e7b6e14148990741ba87884, 6be56f559a74c0124526242e70cbdfd16cbc60a7, f51bc74814a3452009ea5ca262d9768d08149ee6, 14dd50979af27bd2574c8068db11d27028b56afd, d9b5194f3f959eda2e95df6a340254f52ced46f4, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, 2bc3644ce4de7fce5812c1455e056649a47c1bbf, bbed457fd04ba4972018382d1a01a0bdde399d3c, 19b93280f17696a4ddfa2c75490a50ab107addf2, 2ee03e28208a9310a9be4032c2b04ebdddb83cc7, 02fa2389b1b64b661192e224bed8af6df0ce80f6, da5d78b3e3a1544fde98fba86088e1215e97cbe8, db528269ef800727245c0fcb35b692d29c1ccdc9, 16753e0317730e8c1b297338300a8c6163dd06f2, adc180e1fe404b650fca3bb7970e43bdce34a611, be082d70534db088315f2cc5b42c2fdcd58c1b8c, facd5f5deb152229ceb1803434d8690a09ab4129, 6fb020754f6de564c3a0a07bb656c0a90be1f87d, 3c68025d95970a9b9aa1b742a678704cd09d2bf4, bad4c08f03587e38ee960e2aa76e16d722826e7c, 1b3675fc0f2b16743b1e1f0c2f84829cfdb3d34f, 0fa554d981809c5eb78956c779f75092c4f6c16b, 8ee45aeb7c97e3346cc62f216f673b91277ac718, 76b93abde3ec4e5bd84a38d488a8db209dc4ac98, 15d1450a8797e2feaa4c0ca4ebcd43c8cbd8b61d, 2c03df8b48bf3fa39054345bafabfeff15bfd11d, f4207bcad24fe4eaf3849d6c1fe38c36545b5cb3, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, ad10ef93675513a68b93d54f3a461160b53318a3, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, 5031790972d496547b6613d46a4a0134c824db6e, da3f33d858586d24cb265e79eb54f3746e998f57, 695bdc6e24608364491b9418a220c65a7cd17413, d1efcef213c433445be56d7479eb47d972b3ee79"
d235a9085e0543fcbe502fbc269f9a8ee01dcbaba,AdaPrompt: Adaptive Model Training for Prompt-based NLP,https://www.semanticscholar.org/paper/d235a9085e0543fcbe502fbc269f9a8ee01dcbab,Conference,"Prompt-based learning, with its capability to tackle zero-shot and few-shot NLP tasks, has gained much attention in community. The main idea is to bridge the gap between NLP downstream tasks and language modeling (LM), by mapping these tasks into natural language prompts, which are then filled by pre-trained language models (PLMs). However, for prompt learning, there are still two salient gaps between NLP tasks and pretraining. First, prompt information is not necessarily sufficiently present during LM pretraining. Second, task-specific data are not necessarily well represented during pretraining. We address these two issues by proposing AdaPrompt, adaptively retrieving external data for continual pretraining of PLMs by making use of both task and prompt characteristics. In addition, we make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers. Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings. In addition, in zero-shot settings, our method outperforms standard prompt-based methods by up to 26.35\% relative error reduction.",2021,33,"2109404730, 39798499, 2152290059, 2992833, 8652308, 48262024, 2145913600",2109404730,Chisinau,2,"77790220, 2772242",Y,"The idea has some novelty and the results on several tasks attempting to prove its effectiveness against systems that handle named entities in a static way.;For the application of images, using text description to refine the representation is a natural and important research question.",Conference on Empirical Methods in Natural Language Processing,21,"tasks,language,nlp,plms,nlp","cb12fe714dd4fa1d14b6a8023d494c14c89e90ea, 2374106a32169c07703599ff3f6f4b31e8067b89, 3dd7f7118ee174265889d00d100cfe2a02871be8, c4cdf2e0d89aadb13bf9c61654ff9e17be2b01b9, c419ee7315b9edfd8fc55bab16534fc55a564fcd, f12930cd5f58990badc1a7c5d2749cad004cfb0e, 7637ed79d30d0139901175ae4abedd822c217ab4, 0f38b3d717e0fcc6eacc9c6e78b252227440e04e, 8d06f13ec042f0d8d845af5deadeb3dd9ad11f70, f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d, 37b9478b792fb1e7ad5f2c322ed4445b93df6c9e, 8ec5896b4490c6e127d1718ffc36a3439d84cb81, c9f320789e98d2c7a798a9705e26dbe317677966, 41d6b6bd4bf44bf13b9157b603e33360e5e77a01, 4895c430c7810b45840b58cc9182f12143013a43, 5dfde01d761d97c3a6c609007531973eb1229d09, 2a15e9ac5593cd1f8bd3a724e78a173037c02fea, aa9bf8b4c4ce8c28b9f47d7ce4f416aa9726bb0d, 0a71dd8bec060195e14eb9d0a7abbc08d960d4d5, 1ff76ab0fcf22110df62337d462e15d79a2a2593, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, d7fddafbbc372da4fa884f67bdc32db71b888806, 8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, ca0e479ba2327f71e842d033b6b48b082962cc6a, b52db9e41e15f76bdcfbe674abe0314af545c430, b22ed1ea1d174af48c655d48e284afc239ebfa6a, e3b94a5f28522e6825aff16ff07d56bd70d26c96, b473e91cbe80c8b46451b49153cd5f93030480ab, 2dafea864f74a477414c3b71b742f7997e216102, 2019cf49b51021a376f9833a53565513f0d8107b, 639bfab64e2f35917d450013e136cb24c7755fad, 102ebe229df18c8733ea1b8def56cd79996e2178, 1cd497e82bdc46a9d3d28b2316ea7cbe9aee5467"
011095a0082e5e301f9bf30267b193c1c9e7e370a,Perturbation Augmentation for Fairer NLP,https://www.semanticscholar.org/paper/011095a0082e5e301f9bf30267b193c1c9e7e370,Conference,"Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and train a neural perturbation model, which we show outperforms heuristic alternatives. We find that (i) language models (LMs) pre-trained on demographically perturbed corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE datasets exhibit less demographic bias on downstream tasks, and (iii) fairness improvements do not come at the expense of performance on downstream tasks. Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models. We hope that this exploration of neural demographic perturbation will help drive more improvement towards fairer NLP.",2021,34,"2149798086, 51519704, 2166312768, 51324296, 2111313627, 2110032535",2149798086,Helsinki,3,"2118438836, 2282113562, 2113619066",Y,"It would be interesting to see the results of the new activation function on LSTM.;After all, reward seems to play a very important role for the proposed system.;To start, why not plot the empirical histogram of p(z|x) (for some fixed x's) to get a sense of how well-behaved it is as a distribution.",Conference on Empirical Methods in Natural Language Processing,21,"models,nlp,language,datasets,nlp","4d4d1ac9a9ad8592a6cc05082437d706ee176a38, 69a72ff5b30642d11c96635e99aadad3140d33a7, 0a829289a16ae48837cc2905635435db98bacc76, 7bb477077968d68aa7a6059d8d6d801fb28274da, d422df8bff4e677a3077635db116679d25142bfc, 3b230f14c46e7e177e9bebb2ebc9f46b346b646d, 7fa273f450251523e6b7fcc2eb3fdbdfd4a30493, d8a5474f450330ad25c1e22f27e88f3630cb840d, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, fa75a55760e6ea49b39b83cb85c99a22e1088254, 9954ec0a95fc0840f7712d7b0c67b242bb536a9e, 8ba8a0d18a06752f5a39996ccf1e914da0941443, cd5b4b113d5fa90b5dc5aa372111b89d18df88fb, b135e330cc1473c8c24fa63bb9a5b64f51993f9e, 73d4accea441aae2373828a8dc2175aa2759c38f, 0f74e7b650f346676b12c44d16d774fda9a45c9a, 957f5b1e7ca48891c2e279aefbfa0f04d989c21e, 8713452753fd01de5616121af93e173d4f74eaf6, e57aeb158a38ccf33d2f0f5a8f63f1209497e329, 7669fca7fbc071b4ca78b4c326ff8f3a4b6ae616, 31a61d009442436d04b9d4e1c5beee37172289ae, 42375e9a309fe5bf5c671c918cd8c9d5d85568cc, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, 9eea59c34f139f3d2153226c8cf026e975622074, 62df84d6a4d26f95e4714796c2337c9848cc13b5, be082d70534db088315f2cc5b42c2fdcd58c1b8c, 74bec51a66499fcfbced16ff3fce696acf98c9e1, f4cfc7cbad257f1688772d59f694c16189dba811, 24d21ecaeb2d2ecc20e26a5e3f5128247704ccfe"
e57aeb158a38ccf33d2f0f5a8f63f1209497e329a,What Do NLP Researchers Believe? Results of the NLP Community Metasurvey,https://www.semanticscholar.org/paper/e57aeb158a38ccf33d2f0f5a8f63f1209497e329,Conference,"We present the results of the NLP Community Metasurvey. Run from May to June 2022, it elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split in half on the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In addition, the survey posed meta-questions, asking respondents to predict the distribution of survey responses. This allows us to uncover false sociological beliefs where the community’s predictions don’t match reality. Among other results, we find that the community greatly overestimates its own belief in the usefulness of benchmarks and the potential for scaling to solve real-world problems, while underestimating its belief in the importance of linguistic structure, inductive bias, and interdisciplinary science.",2021,26,"38614754, 14487640, 119389860, 49355602, 144906624, 13336152, 93811144, 10666396, 46230016, 80842917, 1799822",38614754,Valletta,3,"48738717, 1919541, 2157681212",Y,"This is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial NLL loss and a reconstruction loss at the last hidden layer.;The explanation of the cause of ""super-convergence"" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments.;This paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values (possibly different for positive and negative weights).",Annual Meeting of the Association for Computational Linguistics,21,"results,nlp,community,respondents,nlp","9c11d9e75c6136943d2fa7ff7cb1f1090d406658, e9a986c8ff6c2f381d026fe014f6aaa865f34da7, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, a1ef4052acb63356928bb440874c470ad48cb40c, c12b80b44d9acfe6cd92fdf965264c4b706c367c, b795c74a0150ec091003ffbaa5bd7d74487c137b, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 98ce7af921e7c52d81df64d632d34eb09522cd75, b52db9e41e15f76bdcfbe674abe0314af545c430, 1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7, 794b3ffd28d28606230efc975eeec9f0522fb139, 4e58100b319d74f97ed550a4e5fa32dea8c06fe1, c9f320789e98d2c7a798a9705e26dbe317677966, 57e6cca1479a4642f867e69b4dee93d14259dc3d, d1206ccabd1980848f14472d6548251c2fab7963, f62ab3fcc45eb787f4eb3213a3ffcae97799e9e5, 3924aa213ff891812c66a6909ab902684d3eb107, bf69c98fca9a9f6c1cde871beddbcdc668b77771, 0c4070272fb7d6b98971a107b022ff8abf0aa55e, f5ac3ec506a5a01807dd7196c0c52eef008b78cc, db0cc2f21b20cbc0ab8946090967399c25709614, bd6c027a3604d6c8fa23435bf382455b2bee436b, 5c39e37022661f81f79e481240ed9b175dec6513, 8b28792f8405b737229afb92c99c579b86d8aa98, ee6f23590783adec7cf6b2030c6a46f3117a708e, d280cf82a6144b3e168840802b1a8a14d4eaccb9, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, dd2deed2ce6e110236a1280db765fa02c7488eb1, 81c02f123b3ef09cf1a8e5a1332451f0d46663fa, 5f7f10f913ecc478ff7ba304c265fd3c700b47d7, 4a7477881b66d12e79c704805781d4683a6a6be1, f4c4e148546089123f8da5db4fb246ab4062bd40, 417326e51d78ba8bd2621f23e539b41bbdd336d6, 56ae2837b6cd4d143bbfa4a4b06811d70126106f, 780c725848aac1118d00c8bb306719ec803369cd, cd74acb268404cde24f5131a22d04d48776b283e, 8d0f755dea90f35f4b126a01fa3cce96b3bdd344, f72d3f58ff73353978e224af348448b34d27cf7b, 9a0965beef113cc37491004b1848149e00300561, 1e1cf81a1113482be3f0c280db994a832cb9426a, b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1, 8adb47deeef943c2c1bae41f9498a382fb818a16, 03532123ccffae8d411264320e8a5ae2b6eddea0, 32524aa3ae8522542753ed7e6f4cca3970e4acab, d916776e0c6a04b0def4c22257c188776c2edab2, 2141334fad7248fc707607bc9453d44686ae07a7"
7fa273f450251523e6b7fcc2eb3fdbdfd4a30493a,CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP,https://www.semanticscholar.org/paper/7fa273f450251523e6b7fcc2eb3fdbdfd4a30493,Conference,"Humans can learn a new language task efficiently with only few examples, by leveraging their knowledge obtained when learning prior tasks. In this paper, we explore whether and how such cross-task generalization ability can be acquired, and further applied to build better few-shot learners across diverse NLP tasks. We introduce CrossFit, a problem setup for studying cross-task generalization ability, which standardizes seen/unseen task partitions, data access during different learning stages, and the evaluation protocols. To instantiate different seen/unseen task partitions in CrossFit and facilitate in-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse few-shot NLP tasks created from open-access NLP datasets and converted to a unified text-to-text format. Our analysis reveals that the few-shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks. We also observe that the selection of upstream learning tasks can significantly influence few-shot performance on unseen tasks, asking further analysis on task similarity and transferability.",2020,141,"1557391091, 51583409, 145201124",1557391091,Zagreb,3,"145993598, 9543395, 2805081",Y,"It is somewhat alarming how the analysis has little to do with the neural networks and how dropout works, let along RNNs, while the strength of the empirical results are all on RNNs.;In fact, linear PCA can be viewed as an autoencoder model with linear encoder and decoder (so that the squared error reconstruction loss between a given sample and the sample reconstructed by the autoencoder is minimal (Bishop, 2006)).;After introducing the idea and related work, the authors in Section 3 give details about how to perform the quantization.",Conference on Empirical Methods in Natural Language Processing,20,"tasks,nlp,task,ability,nlp","5030702fea15d66a73fc997325431f1d7945ad9a, 5c45a5d05ac564adb67811eeb9d41d6460c70135, db6084fdb3baceddacdc726474722debe1ef7e65, 72afe82af4c2ca100c36eb35292e85d806527f0a, 472644c5f4155635cf9e9e37540bfa53c20e7610, f1300d9be8254b028337d9757755ba906fe6955b, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632, 7171a0e9b07ebc98a32eb912262613efc20f283a, f208b3fb28c556ab62f9d202b7beae89700a338a, 5406e153957dd7a165264da6e6e5d81251997404, a13149a80855412d970d0de2b41c611f4cf7e1da, 0ac0025529c1f9056036be43c561ba67fb8d12a5, 7c24234042988e2f820a4350f43422ed2ad6fc52, 224c11bc51b4959bc787d6681c2b152468294b11, 661ccdb41fe977d47273e586389cacc1489f3286, 6a261e1e38506b0e4c113ba29a2d5e5d0709ed26, 5031790972d496547b6613d46a4a0134c824db6e, d008893e01fa7f6c5fb01dadf3f97ee96835c303, 42543dc42e65609bbbf2be470d54dd923532c36a, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, 37b0a7a6c8fb26e32b9206847e78d521a2cd5900, a84da48f5eb57cc3f74fd2885f4e5ffd9b5ba2fa, 5471114e37448bea2457b74894b1ecb92bbcfdf6, d81fc968196e06ccafd7ea4c008b13e1cad1be64, 5f7f10f913ecc478ff7ba304c265fd3c700b47d7, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, 94deb62af3054c49e7d80bd7eb3ed5efe990fc0b, 5e31fa4e69d1a3587230f5d134c0b7e2ed84a742, 3a083d843f891b3574494c385699c21766ce8b7a, 9b529fe170823f95509585d5aa39fa01a43558fd, cd8a9914d50b0ac63315872530274d158d6aff09, 84725855d10b531eb8cbe54935dda0440c2fc750, b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1, 63316bb5b88d362051c048e864c3ae5d97a26d30, 99f06e88e76f1af51d08d7adfb26d758ebc6acab, 57e6cca1479a4642f867e69b4dee93d14259dc3d, cd5b4b113d5fa90b5dc5aa372111b89d18df88fb, e359e8960b0b09e8685a32927b7818f4b06ef881, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, a56bf7ee9a56d8f84079684339a953c2df9ce76b, 649c3497e3b34b15a5011259fcb837cf6c1ac04a, 643da4c4de1954daeac571a82367241db012a8bf, 4be9368abc2474d6fd38639e523cf03af1873fd9, 139a0c7a60667979dcb57eae677f75ff3f0b0196, 8d06f13ec042f0d8d845af5deadeb3dd9ad11f70, 5107c83173e43f51d1bdebf6cafda525a7c26bf0"
185e7d2a761594451b02ace240356dadad2aef78a,Dice Loss for Data-imbalanced NLP Tasks,https://www.semanticscholar.org/paper/185e7d2a761594451b02ace240356dadad2aef78,Conference,"Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sørensen--Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.",2018,366,"2845020, 2109329406, 65844131, 2115780454, 144894837, 49298465",2845020,Riga,3,"2063938464, 2051536212, 2073587169",Y,"However, the presentation of the method is not clear, the experiment is not sufficient, and the paper is not polished.;In other words, the evaluation is a bit lazy somewhat in the same sense as the writing and treatment of related work; the authors implemented the model and ran it on a collection of public data sets, but did not venture further into scientific reporting of the merits and limitations of the approach.;As such the paper would be a nice contribution to ICLR.",Annual Meeting of the Association for Computational Linguistics,18,"examples,training,tasks,nlp,nlp","0983883619a0ca597d055d0e58da2f514052913d, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, fa7b8acd47631bada5b66049824bfd335ac6bf8f, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, 2eda2921a8da4b325f9d05f556594a5884c398a7, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, 7c8bd41e9cebdb01f445a51ba43839c8d9b3ab91, 263a58f4fd32caca1dad2351af4d711aec451fe6, aa9bf8b4c4ce8c28b9f47d7ce4f416aa9726bb0d, b266510f5f9b40d42b51884ad13a1867fb3284fd, a281d563261c738f13b9e58a525e7e265a619c93, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, 340f48901f72278f6bf78a04ee5b01df208cc508, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, e2f3aa4ecc487fa496d1a51e875e747d4b7e6001, 8b28792f8405b737229afb92c99c579b86d8aa98, d6bc29a897fd85e7187dc33c3c974b8879462237, f14fc9e399d44463a17cc47a9b339b58f6ef7502, c15f30a3e84910a28cc560e7db097fd99339e8c1, 846883b7761cb5fe4468d42bf9d328b5d1030175, e23b2e47b0ac6f50000078828f27571804dcd6a2, 9619cde5c79d91ca5c432186668618312175f8dd, d2a5dcecd2ffdf03473df1688091f08fadb114a3, ec2f9076448ba25a225618603adde60caa76c4df"
d6f002d88638de71114dab083f0ea8ceea6b6a5aa,Benchmarking Intersectional Biases in NLP,https://www.semanticscholar.org/paper/d6f002d88638de71114dab083f0ea8ceea6b6a5a,Conference,"There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.",2021,32,"9051130, 2143685866, 2152595932, 3047212, 144849629",9051130,Athens,2,"48469973, 2231158981",Y,"In the experiment, the proposed method shows no or slight degradation of accuracy with big savings in communication cost.;Concerning the lack of mathematical rigour in the statistical physics literature on which the authors comment, they might want to relate to a very recent work https://arxiv.org/pdf/1708.03395.pdf work that sets all the past statistical physics results on optimal generalization in single layer neural networks on fully rigorous basis by proving that the corresponding formulas stemming from the replica method are indeed correct.",North American Chapter of the Association for Computational Linguistics,21,"models,work,nlp,bias,nlp","99f06e88e76f1af51d08d7adfb26d758ebc6acab, 1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7, 8ba8a0d18a06752f5a39996ccf1e914da0941443, 3789eb72c32ecf5e33442570358dd786dd67c8a2, 9e540662619327a3056d9e40bb58058868f6f805, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, c96fc88631f2b8e2fe192027a8a237445635328c, 761020759f7e9f84c3ac77f59a42862cc6a6004e, b5904cd5dbf73b8d5ff13517de490c292d877ee0, cf5fddf6717e88e2bbed6b0bfe54dfeb311e6789, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, 573fd2ce97c70bb29097e8efb28a27af791225ca, 8ec5896b4490c6e127d1718ffc36a3439d84cb81, 9583ac53a19cdf0db81fef6eb0b63e66adbe2324, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, c665003881c3c35589d1e48da1ee7234b48f2ac8, cb03b665069dad5e895a2c244929ea427f1fb9d1, da3f33d858586d24cb265e79eb54f3746e998f57, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, cd2f4aaf98bb1e020cff310000c8049d3460c54e, 8babcaf89f8537dc628a029ebf932100f57289fd, c6879e43828b293567f5e2da039d23845189d6a7, 4b1280229ced73f6c86550f24ef01490fde52285, 80d9f0eb47b712988d19cbe29a7bfa63f2a175d0, 5bedddda2aab2c5a03017d96a6dd7acf517c6961, 1986318d8a565fbff8fde545b8d0c2012c6462d8, 5e4597eb21a393b23e473cf66cb5ae8b27cab03e, acc296f981cde8d8c205982fc4422ec35531b769, c84389369720dcd2f004c48e58fbac2c45c8f092, 3faeb21fe256b99391d69570053a8c2d91e9f348, a60a4e5f7f872b9825ddff5d379857c2091ca52b, 6628f9ee35e36cdfdcac8a46cef4dba8d529a83b, 794b3ffd28d28606230efc975eeec9f0522fb139, e23b2e47b0ac6f50000078828f27571804dcd6a2, 0311ace1d499cadd1cc0c515a625d1d045f60d25, 0599f45e03ac2016321df0dd653ba4c0034c79d5, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 94214d6d922ce095719d488642cbcc75dc52f273, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, b9f190339ce0b4cdad3464c45ec3266a7369fb7c, b067177b1e17287185eb3b82ccc3d7c646b3ec40, 102ebe229df18c8733ea1b8def56cd79996e2178, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, b34fc78de28be598e21118d7cb9d84d63374addc, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, e89c37b7c2ff465db43c4b9f674867ec4b98aa8b, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 85328b4a8132bf4299f8cd7f8e79e850d561c8fc, 8799ec4bdf98bc313aa8c41a706a385e026d1f88, df1a2539afbad27c4c80115a6f8f59a089024865"
f72053903270d9a7f41108461ad04d5aa075218da,SHAP-Based Explanation Methods: A Review for NLP Interpretability,https://www.semanticscholar.org/paper/f72053903270d9a7f41108461ad04d5aa075218d,Conference,"Model explanations are crucial for the transparent, safe, and trustworthy deployment of machine learning models. The SHapley Additive exPlanations (SHAP) framework is considered by many to be a gold standard for local explanations thanks to its solid theoretical background and general applicability. In the years following its publication, several variants appeared in the literature—presenting adaptations in the core assumptions and target applications. In this work, we review all relevant SHAP-based interpretability approaches available to date and provide instructive examples as well as recommendations regarding their applicability to NLP use cases.",2021,32,"1786389, 2178446, 2187454523, 2187454784, 146800020",1786389,Dublin,3,"2108691840, 144717855, 93421340",Y,"If the baselines are stronger and this point is more convincing, I am happy to raise my rating of the paper.;3) and extracting 2D slices from the compressed space and computing 2D histograms per slice.;In other words, the evaluation is a bit lazy somewhat in the same sense as the writing and treatment of related work; the authors implemented the model and ran it on a collection of public data sets, but did not venture further into scientific reporting of the merits and limitations of the approach.",International Conference on Computational Linguistics,21,"explanations,applicability,nlp,model,nlp","d1a6b3a5efde3783b53f822dc8dd00aaac934b95, c30c0092bf4eb8a44faec3fc60cdd5006276bcdc, aad2d03c17bc7d1e636d0e79944ad4588af989d5, 5b34752817bc0d6aa96466dabcbc24a83dd071fe, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 808e9ce4e86e79098edea7f00b5b91663b87a5e6, 5bc511aa30f72720260d792e57537379fb04c395, 72a6a5d5864eb915231c7128b90977f1d2acf6e9, 9e195234688778b2beb3528632e78dbabf816332, acc296f981cde8d8c205982fc4422ec35531b769, 18d87bff073687c025f9bd23ab2dfb20d5f72a66, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, 456c011594ecacdd24298a161787389ccbe4b88b, 9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a, b4c9c134ad5bd4a037115df65411b4c49abe1322, d3855b7351c11145e51301e6b686f748ca35c802, ae026f29c2d571871f426ff4873d43b4ff90b9ad, a281d563261c738f13b9e58a525e7e265a619c93, a620ea8654f86b282e0d2c1fbc6616b90ca45bd1, 0cec0c296efedb814342b4b841d4583efbfc6777, 3c8a456509e6c0805354bd40a35e3f2dbf8069b1, bad4c08f03587e38ee960e2aa76e16d722826e7c, da5d78b3e3a1544fde98fba86088e1215e97cbe8, c419ee7315b9edfd8fc55bab16534fc55a564fcd, f9c990b1b5724e50e5632b94fdb7484ece8a6ce7, a0367346bc355c36badec2d2c47ce55a320cd75e"
322d91190acd8ac8c64598f5126947b0485ba249a,Quantified Reproducibility Assessment of NLP Results,https://www.semanticscholar.org/paper/322d91190acd8ac8c64598f5126947b0485ba249,Conference,"This paper describes and tests a method for carrying out quantified reproducibility assessment (QRA) that is based on concepts and definitions from metrology. QRA produces a single score estimating the degree of reproducibility of a given system and evaluation measure, on the basis of the scores from, and differences between, different reproductions. We test QRA on 18 different system and evaluation measure combinations (involving diverse NLP tasks and types of evaluation), for each of which we have the original results and one to seven reproduction results. The proposed QRA method produces degree-of-reproducibility scores that are comparable across multiple reproductions not only of the same, but also of different, original studies. We find that the proposed method facilitates insights into causes of variation between reproductions, and as a result, allows conclusions to be drawn about what aspects of system and/or evaluation design need to be changed in order to improve reproducibility.",2021,25,"41052836, 2162189986, 2738095",41052836,Podgorica,2,"2113909888, 2115457464",Y,"I could then identify the influential dimension(s) by taking the largest absolute values in the gradient vector.;The paper says Deep Voice 2 (Arik et al., 2017a) is only prior work for multi-speaker TTS.",Annual Meeting of the Association for Computational Linguistics,21,"qra,method,reproducibility,nlp,nlp","cd5b4b113d5fa90b5dc5aa372111b89d18df88fb, 0ecf3f089e6dc7944b440227069b9d0143e18d78, 7669fca7fbc071b4ca78b4c326ff8f3a4b6ae616, a18f02e5c24e1f924aea268dd343bbdea234f2bb, d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea, 8f2cf30f9c825d8ef7d622601dbd525ace95e025, eda6756ab2844c390584686dc5e6385f4a8369cd, f3b8a3ffa0bdfe5578e0f0f44ea3b66cad38c032, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 381c7853690a0fee6f00d2608a7779737f1365f9, 4f480bae3196dbbc27ab383bce33478ea963f9b3, e23b2e47b0ac6f50000078828f27571804dcd6a2, 0f74e7b650f346676b12c44d16d774fda9a45c9a, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, 8713452753fd01de5616121af93e173d4f74eaf6, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, 55fa5be5288f6097ae5bd2dfe58fc07b3b39bfb6, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, 20381972ee66e03ea218e5b3d39d6423b6e35f0f, 29279e52008848ee494f5af1b836313ab99c25ed, a81ba6a07bf7a2ecff871e3362a77404501d0927, 57af930b57886186695f279efa03adaf272c4de1, 45674df7143e43bc589cfabd26dd194c2a7f090d, 263a58f4fd32caca1dad2351af4d711aec451fe6, 084a93c8ac0230ea9fe64d445ad1d6af5ea0b3b3, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, c419ee7315b9edfd8fc55bab16534fc55a564fcd, e24b8a9531573d284647239affc6c855505b0de4, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, df7d26339adf4eb0c07160947b9d2973c24911ba"
472644c5f4155635cf9e9e37540bfa53c20e7610a,Semantically Equivalent Adversarial Rules for Debugging NLP models,https://www.semanticscholar.org/paper/472644c5f4155635cf9e9e37540bfa53c20e7610,Conference,"Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) – semantic-preserving perturbations that induce changes in the model’s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) – simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.",2017,440,"78846919, 34650964, 1730156",78846919,San Marino,3,"1912476, 47627363, 2112425054",Y,"If the contribution of the paper is the ""output stochastic"" noise model, I think it is worth experimenting with the design options one has with such a model.;Inclusion of Assumption 3 would in particular require better justification.;The model is further extended for conditional generation and demonstrated on a range of image benchmark data sets.",Annual Meeting of the Association for Computational Linguistics,17,"instances,adversaries,sears,nlp,nlp","7a1e71cb1310c4a873e7a4e54d1a6dab0553adce, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, 31a61d009442436d04b9d4e1c5beee37172289ae, df1a2539afbad27c4c80115a6f8f59a089024865, 28cc044d5ba938472bc53d87240583982ad21663, a357f1ff27e184d9a5ef69e665e8ca891032bf71, f8e57fa370fe10147aa22714e08409fc1b7dae4b, d9b5194f3f959eda2e95df6a340254f52ced46f4, aea731e7cf33aa3d482b13f42cedbc1adb3271c6, c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b, 1986318d8a565fbff8fde545b8d0c2012c6462d8, eb9e0da8b7170e3ca4364f2f9010599c2d2556f1, c15f30a3e84910a28cc560e7db097fd99339e8c1, 375125029b085e70a109491656b69aa01bc2a166, 147c868b721c8d29df7c61db7f2360114c760614, 42375e9a309fe5bf5c671c918cd8c9d5d85568cc, c5d3fbc024ad9a0d28669b6030de23fc5c3f7888, f3b8a3ffa0bdfe5578e0f0f44ea3b66cad38c032, f295157f37cfb43cd8d8d2690ea124edc5ea59c2, 046eb47d56beb8069b0098e3d01608f81ebb6849, 676664ee7471738577f641e6159e7596625b7fdb, ea160adc0d78e54669281b8b145bcd832e648fee, 71a85e735a3686bef8cce3725ae5ba82e2cabb1b, e2e7d964c09e27d334fcb8761d69918630629387, bd6c027a3604d6c8fa23435bf382455b2bee436b, a2a514ed839dafdd0fb76d6c2615f25f35bf8087, 208ac1f2ec9bf367a9981fedb6d9ea6aa9889099, 34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61, d2a505586c0da20752b98f63c7760b6a5c41e28d, e2a58fd18961c3941102989e3a3d0d27c615e015, 5bedddda2aab2c5a03017d96a6dd7acf517c6961, 5778e56400f7113c2b1355fdbd6b638fa379885f, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, fb5413afba689d16543215c5a2ddbc5b78a52007, 6eb8caebbffc7e5b301b66dc36acad46b4dca5c9, 08aaaf7ae3d61875e7bcf5c1bb0df4f17066e300, 42543dc42e65609bbbf2be470d54dd923532c36a, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, e70ddfb04969b663ef8cd711a70a0acf563d6e5f, 011095a0082e5e301f9bf30267b193c1c9e7e370, b2ee139c1a3c2c53ac2b603070bdfbcd26b50ddc, 8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801, dadfb3ff45e19dc22456a645f441bbeb17c93c9c, f72053903270d9a7f41108461ad04d5aa075218d, b889b1d6944213bc2ca29e3ad07ee65ede20892d"
9b54941de1e21826ecc28b32730ac3f69991ede4a,Robustness Gym: Unifying the NLP Evaluation Landscape,https://www.semanticscholar.org/paper/9b54941de1e21826ecc28b32730ac3f69991ede4,Conference,"Despite impressive performance on standard benchmarks, natural language processing (NLP) models are often brittle when deployed in real-world systems. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, RG enables practitioners to compare results from disparate evaluation paradigms with a single click, and to easily develop and share novel evaluation methods using a built-in set of abstractions. RG is under active development and we welcome feedback & contributions from the community.",2020,116,"1822288, 8937909, 2056908, 145814654, 2120253018, 3393918, 2228109, 143977268, 2061444681",1822288,Lisbon,2,"35210462, 2144633223",Y,"The authors propose to train a generator network in combination with the classifier and an adversarial discriminator.;After all, reward seems to play a very important role for the proposed system.",North American Chapter of the Association for Computational Linguistics,20,"evaluation,nlp,systems,rg,nlp","e8b30ebe3351680c3b039555ae0a8d0865ad829b, 704011527f183b561ea6a75b21e4cefe5aa77fca, 8ee45aeb7c97e3346cc62f216f673b91277ac718, 83ea80a9393177ecf84928f4bd1120fb679f180c, cd29c25c489562b409a60f83365f93f33ee1a0a1, 34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61, 6a1b25f7a67395ad1e676027322913acbb0a0635, c9cc6a3c50b32e801f20b4f0ed5daf3e9550f9c1, 2c3eef2f17369912e330281d54b535675077e4ca, b79e5e4622a95417deec313cd543617b19611bea, 916455d97cd792c2eb5b00663689592e25cbc8d8, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, d6f002d88638de71114dab083f0ea8ceea6b6a5a, 9d6cbac04c498b424dfaa5ce82ac201a180c1502, 6068d39e92aef1bb0e1291e9931894c35692a85e, c9f320789e98d2c7a798a9705e26dbe317677966"
8d0f755dea90f35f4b126a01fa3cce96b3bdd344a,Towards Climate Awareness in NLP Research,https://www.semanticscholar.org/paper/8d0f755dea90f35f4b126a01fa3cce96b3bdd344,Conference,"The climate impact of AI, and NLP research in particular, has become a serious issue given the enormous amount of energy that is increasingly being used for training and running computational models. Consequently, increasing focus is placed on efficient NLP. However, this important initiative lacks simple guidelines that would allow for systematic climate reporting of NLP research. We argue that this deficiency is one of the reasons why very few publications in NLP report key figures that would allow a more thorough examination of environmental impact, and present a quantitative survey to demonstrate this. As a remedy, we propose a climate performance model card with the primary purpose of being practically usable with only limited information about experiments and the underlying computer hardware. We describe why this step is essential to increase awareness about the environmental impact of NLP research and, thereby, paving the way for more thorough discussions.",2021,22,"2064295987, 2023644816, 2156865999, 2006205621, 3073566",2064295987,Moscow,3,"2237803694, 1505708061, 90943712",Y,"However, that is moved completely to the Appendix.;This manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum, which actually has been shown by existing works like Kawaguchi (2016) with some assumptions.;* Qualitative results on sampling from the model using the CIFAR dataset.",Conference on Empirical Methods in Natural Language Processing,21,"nlp,climate,impact,research,nlp","9efd70d2c06733704220313fb67720aa45c6362a, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 7d873a9c49d3864709aa762f8740edcdbd7369c5, 5290d7921f0266c8b50b79fc8a0b7d22868f4f60, 1661d0d8d47cac41e01c59c60aac3675b4396698, db0cc2f21b20cbc0ab8946090967399c25709614, e2a85a6766b982ff7c8980e57ca6342d22493827, 6f75e8b61f13562237851d8119cb2f9d49e073fb, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 2e7f532796eed2847d4c19e3cff03756049e81b4, ccd94602e3acecf999d0c9ba62b1a8bc02e9f696, 99ae62cca0b15275d9ed1528f345f9dcefe50dd7, f9163156eeba67762a7441db48fe6720106137cd, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 07cca761749bfe21c2d096ff60f32b574d5c84c4, e70ddfb04969b663ef8cd711a70a0acf563d6e5f, 579476d19566efc842929ea6bdd18ab760c8cfa2, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, 20a3ed03888037e2802fa9abad02ffa0a8dcc228, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, a781cea542e99c6bb9422858e7c04eaef18c7673, 8b7b2e88207fc2dd849f5d83c9b6e890f0174abc, ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee, 5c45a5d05ac564adb67811eeb9d41d6460c70135, c468bbde6a22d961829e1970e6ad5795e05418d1, 85328b4a8132bf4299f8cd7f8e79e850d561c8fc, 8b417c2be7a7707f372049fb1193f0d42f799562, 21042565ec941f4fa31ac5a0af85a1a84ff21f1b, 4be9368abc2474d6fd38639e523cf03af1873fd9, a9640bac0b45a804d07fc5914feb08af8f2a73f2, bcc82ce554942880814243fc8c08a88b9d2aad09, e576a2d97950b1f6831f88575dd3f370053f6af7, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, 4267178106cef2e77284bde309dfaaf9fd46a91b, e4b52a1a00e9db941326fc857b95245cbfb60bce, 54ddb00fa691728944fd8becea90a373d21597cf, 4bd3c9e1bb1ca2df62b66201616b8740300efd0a, 1b3675fc0f2b16743b1e1f0c2f84829cfdb3d34f, 90e4330fed2da147dd41490e8ad638b618112b3d, cd20ed8e70f6459f5617350f6b84c5c6d5929cb7, a0f303b6e22ef52943355993f57d65938997066a, 71854ff4306cf65c3c2161f7be2d0346275f72d5, 410fba9f03212257d0881811802e6620e59bc827, f11cba099b8cf14815f7b3d85f55ecfddbf9f04d, 5bc511aa30f72720260d792e57537379fb04c395, 695bdc6e24608364491b9418a220c65a7cd17413, 48265726215736f7dd7ceccacac488422032397c, 54a26f5ef4b0524c60249fe98d0fc3646b2791ad, 7676c02ea839ff1ceb6e5e1427c42bc45e169bde"
5e31fa4e69d1a3587230f5d134c0b7e2ed84a742a,Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models,https://www.semanticscholar.org/paper/5e31fa4e69d1a3587230f5d134c0b7e2ed84a742,Conference,"Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at https://github.com/lancopku/Embedding-Poisoning.",2020,97,"2120801160, 49192881, 50317060, 19169659, 11774802, 1631386300",2120801160,Berlin,3,"1743722, 2660835, 1817207",Y,"(As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.);The authors claimed that this parameter update is one of the novelty of their method, making it different from the method of Schlegl et al. (2017).;The most important idea of the papier (PDE-net) is to learn both differential operators and the function that governs the PDE.",North American Chapter of the Association for Computational Linguistics,20,"nlp,models,security,samples,nlp","00bbc94806ca0821a9c82d8aedf16f0e6263b89f, 9727206903eb40d4fa42606711bad3402f2ba9aa, 2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c, 27e58c9e5e6d07809a45a17675a2c7135b577881, fdc57c18f3b636c3273542327ae540217972558f, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 155f27879f185f1ab04107c91c2ae7cf6a910a03, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, f397b593de771752e7002a954eb531f3ef6a975e, 665b0c776ff7507c32793f10ce9edf90bc2f674a, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, 4f8d648c52edf74e41b0996128aa536e13cc7e82, 10a69070c8585dc5564b11b0e53dbe4d565f9ce1, 44786a2c2a8ba8cf5c74a8fb10098c220e924c56, a8b995f0da78a79447dfb18c2337972b044f4239, 77a59de2e2b832321875cadcf9619dc313f02384, cd20ed8e70f6459f5617350f6b84c5c6d5929cb7, 647d0e540627c5a903299a90d20530f8e48c18d9, 43bafb4997515d2904abfca8214f2fc806680fc3, f8e57fa370fe10147aa22714e08409fc1b7dae4b, ca0e479ba2327f71e842d033b6b48b082962cc6a, aa9bf8b4c4ce8c28b9f47d7ce4f416aa9726bb0d, b2f8df88de24fe0ef74bda0eec1a0c3e7329b9f9, 312b1067d89f598c4c5c0799aa18b48d0926bed8, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c, 7ae2783a9196fb4bc2a610ae812d19722daddce5, 39602922b04885047254444fd1a1586d797617ce, 4e13a8e8ba8d33e15ed037bfca7c651047533990, ce51eff5b529ee572dab1c1f38f20adc8e89bab2, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, 96023195e889fc258e6ff30aa99d250982dfae01, 0a829289a16ae48837cc2905635435db98bacc76"
7571ed4cf1bbdcf891b576a0da12c910b1f0c72fa,Concealed Data Poisoning Attacks on NLP Models,https://www.semanticscholar.org/paper/7571ed4cf1bbdcf891b576a0da12c910b1f0c72f,Conference,"Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model’s training set that causes the model to frequently predict Positive whenever the input contains “James Bond”. Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (“Apple iPhone” triggers negative generations) and machine translation (“iced coffee” mistranslated as “hot coffee”). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.",2020,104,"145217343, 145914976, 144588144, 34650964",145217343,Stockholm,2,"1756679, 2072801764",Y,This paper proposes a different approach (with could be combined with these methods) based on a new training procedure.;I could then identify the influential dimension(s) by taking the largest absolute values in the gradient vector.,North American Chapter of the Association for Computational Linguistics,20,"model,predictions,attack,nlp,nlp","410fba9f03212257d0881811802e6620e59bc827, 56266342b01a4f2ddc28a1e8401dbbad105736a5, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, 63316bb5b88d362051c048e864c3ae5d97a26d30, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, 44786a2c2a8ba8cf5c74a8fb10098c220e924c56, d2efa91bd7d076a66bc1d1c570f6aa2da944dd88, 665b0c776ff7507c32793f10ce9edf90bc2f674a, 8babcaf89f8537dc628a029ebf932100f57289fd, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, ce54e3b89a2570035b70885e6901ad4c92ae41c9, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 7637ed79d30d0139901175ae4abedd822c217ab4, af13a92977d4f4dc5b28b13746d86111d42939e8, 799d5a8271887adede035644d878c7bd555576df, f0f1627db35b4942e0f83069f20dd0948fc35d28, cb03b665069dad5e895a2c244929ea427f1fb9d1, f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d, 312b1067d89f598c4c5c0799aa18b48d0926bed8, e359e8960b0b09e8685a32927b7818f4b06ef881, 0ac1a1ccb16c8441f42ff8de743fc93f0e08c15c, d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea, 224c11bc51b4959bc787d6681c2b152468294b11, febe776e285dc5e72c7e3ee697a87a794e1c00ff, d1ae4ab5047489c2b010c7ce72262982ad66ad60, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, d1efcef213c433445be56d7479eb47d972b3ee79, f04d8e558c11ac0fc9318c4f3d61b651c62ddcfa, ddc6e677715c03fe574319d3f80a3e1577bdbdd3, 39602922b04885047254444fd1a1586d797617ce, bc44c0c64a473e035b11ae60a1993ad3db1acd2e, f9314fd99be5f2b1b3efcfab87197d578160d553, e373cbce1a831361a8de9dbc625727169011bcce, 18d87bff073687c025f9bd23ab2dfb20d5f72a66, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, 2374106a32169c07703599ff3f6f4b31e8067b89, 54a26f5ef4b0524c60249fe98d0fc3646b2791ad, 740b5dec469e6b54e5e191a577a9b6a6ad8562e3, ccca203382e5dd198c089a0f1d7af7bef0f694e9, 51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df, 83cebf919635504786fc220d569284842b0f0a09"
f91dbd39d4c742ba675e447b04a0b0c70b33e836a,Measure and Improve Robustness in NLP Models: A Survey,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,Conference,"As NLP models achieved state-of-the-art performances over benchmarks and gained wide applications, it has been increasingly important to ensure the safe deployment of these models in the real world, e.g., making sure the models are robust against unseen or challenging scenarios. Despite robustness being an increasingly studied topic, it has been separately explored in applications like vision and NLP, with various definitions, evaluation and mitigation strategies in multiple lines of research. In this paper, we aim to provide a unifying survey of how to define, measure and improve robustness in NLP. We first connect multiple definitions of robustness, then unify various lines of work on identifying robustness failures and evaluating models’ robustness. Correspondingly, we present mitigation strategies that are data-driven, model-driven, and inductive-prior-based, with a more systematic view of how to effectively improve robustness in NLP models. Finally, we conclude by outlining open challenges and future directions to motivate further research in this area.",2020,86,"1524732527, 49528192, 2143919864",1524732527,Amsterdam,3,"1753285996, 37502184, 1633124736",Y,"If the loss for the task is proper, then it's well known how to construct a divergence which coincides with the optimal risk.;It seems the Blizzard results in Figure 2 are missing no reconstruction loss + full backprop.;The evaluation framework is described in enough detail to replicate the results.",North American Chapter of the Association for Computational Linguistics,20,"models,nlp,robustness,applications,nlp","6548106035c7208ad498730627874a482734b9ac, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, c12b80b44d9acfe6cd92fdf965264c4b706c367c, 1cf2e9e198feef3893da2800a7949f6880ddc084, 60caa5b3d066e13feac496fd0736e976970eb09f, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, fa26a6d434450b185e669170e79fd3e1d29716bf, 9e3816be8cf4821d74e258de10ee471382936a30, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, 980858461df7c4349f17b427686c5bcbcffbdc04, 410fba9f03212257d0881811802e6620e59bc827, 00bbc94806ca0821a9c82d8aedf16f0e6263b89f, 322d91190acd8ac8c64598f5126947b0485ba249, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, ef35da3a3c471a6a15c7a3b09586483eb50cbef0, a1d36749b89e46a8eaadf8ba40788741c192fb1e, e968ae8e98fff9e28468383a1826fca4a2ae5245, aca6d5f3866372a4506cf15773ae298f18c3f453, 7998468d99ab07bb982294d1c9b53a3bf3934fa6, e67a2817089312746d69b38ce9abfdc4b1bc69c3, a830083704284c8c5ddaf04f676c6ce23d583942, dd2deed2ce6e110236a1280db765fa02c7488eb1, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 59c414c9efb77562f5d1aad8af14eaac968c69c0, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, 657fbf29ea0b4904a3e98d1556f9acf38dddae5f, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 1a37223175138bc1aa53b425ea2fdd0b382405a5, e248993daede136713e93929816df92b48ccddfb, 1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0"
1109d62ebd2b29a7dc148bc30dd6cfc803a63deca,IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP,https://www.semanticscholar.org/paper/1109d62ebd2b29a7dc148bc30dd6cfc803a63dec,Conference,"Although the Indonesian language is spoken by almost 200 million people and the 10th most spoken language in the world, it is under-represented in NLP research. Previous work on Indonesian has been hampered by a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In this work, we release the IndoLEM dataset comprising seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. We additionally release IndoBERT, a new pre-trained language model for Indonesian, and evaluate it over IndoLEM, in addition to benchmarking it against existing resources. Our experiments show that IndoBERT achieves state-of-the-art performance over most of the tasks in IndoLEM.",2019,157,"2789148, 2953039, 1800564, 145465286",2789148,Prague,2,"2288804757, 145771261",Y,"Results are good, some unclear explanation This paper proposes an end-to-end trainable attention module, which takes as input the 2D feature vector map and outputs a 2D matrix of scores for each map.;Pros and Cons ============ + good results + interesting idea of using the algorithm for RLfD - weak experiments for an application paper - not clear what's new",International Conference on Computational Linguistics,19,"language,indolem,nlp,work,nlp","7e9905710a5991a017ffc0473dd612cfce4b7b2e, 2880ac931c11176aee6d42a7e7bb0703aacde3f9, 0af24df95f8a0b7e6fc257a98a5820314d1243f3, 8388f1be26329fa45e5807e968a641ce170ea078, 771a858c35f6d6e6d1017dde95368de3794738a6, 8c33ca066e2ab615e24c65198c794114436053dd, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, bc44c0c64a473e035b11ae60a1993ad3db1acd2e, 60caa5b3d066e13feac496fd0736e976970eb09f, e32a2519b59d62cff6cb8136ee242dc3754ed57b, ca05ca5f70616456d0f2e05c4a51cbf8b8d273f7, d9a7fa7616a327367696e19b1846519745cd43ff, 647d0e540627c5a903299a90d20530f8e48c18d9, 557ee73ff4e87ca0cb1b6c78af0730a2d94b710f, e8b7a9be9f2d0578a95319ed5841978e10429967, b266510f5f9b40d42b51884ad13a1867fb3284fd, 5f7f10f913ecc478ff7ba304c265fd3c700b47d7, 77a59de2e2b832321875cadcf9619dc313f02384, 98b9086750f08a21c8778ab986339321e9caf790"
3cc2f69951cd24fe61be4cf32d62afbac297bc2ba,Social Biases in NLP Models as Barriers for Persons with Disabilities,https://www.semanticscholar.org/paper/3cc2f69951cd24fe61be4cf32d62afbac297bc2b,Conference,"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",2019,223,"2083807, 3331141, 40081727, 20825661, 2112887022, 1667883461",2083807,Amsterdam,2,"2154976675, 2142159995",Y,"A new method for weight quantization.;The presented results using small networks on the MNIST dataset only show that networks with ISRLU can perform similar to those with other activation functions, but not the speed advantages of ISRLU.",Annual Meeting of the Association for Computational Linguistics,19,"biases,nlp,models,disability,nlp","579476d19566efc842929ea6bdd18ab760c8cfa2, 6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91, 178571a5cde984c895493e2eb6c5487449d055cf, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 6ba00c2386f2edc0b43eec442cd1923b5d964633, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 69a72ff5b30642d11c96635e99aadad3140d33a7, 24d21ecaeb2d2ecc20e26a5e3f5128247704ccfe, 25761ba4bdc054bfe902fe7c5d6338be6d00d491, 573fd2ce97c70bb29097e8efb28a27af791225ca, b651d67502790e1d6d41c589e1d93e996ba7b935, a13149a80855412d970d0de2b41c611f4cf7e1da, 8674494bd7a076286b905912d26d47f7501c4046, 3b9732bb07dc99bde5e1f9f75251c6ea5039373e, 6f31f59f7b4bd751b5d8657d8a878bef571f9a4c, e8b30ebe3351680c3b039555ae0a8d0865ad829b, 03532123ccffae8d411264320e8a5ae2b6eddea0, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, 63de6db7245f634ecbef4c505099874c1ba65145, ce212cb873a54e5716da53a66b10298ac013008a"
11342d45911ee8a7c9e3a94117ce774ad7036172a,Neural Unsupervised Domain Adaptation in NLP—A Survey,https://www.semanticscholar.org/paper/11342d45911ee8a7c9e3a94117ce774ad7036172,Conference,"Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future NLP.",2019,209,"1723636206, 2022124",1723636206,Brussels,3,"32058742, 9959840, 3410500",Y,"An encoder is later trained to map real images into the space of latent codes for the generator, allowing the system to be applied to real image segmentation tasks.;The core idea is to train the model on pairs of images corresponding to the same content but varying in views, using adversarial training to discriminate such examples from generated pairs.;On the theoretical side, the linearly constrained weights are only shown to work for a very special case.",International Conference on Computational Linguistics,19,"domain,data,language,nlp,nlp","740b5dec469e6b54e5e191a577a9b6a6ad8562e3, 8ba8a0d18a06752f5a39996ccf1e914da0941443, 0c00a328fa7cd56ee60338c54e89bd48310db80b, e5a1cfcd07dcfd8b1feec9c635dadc858cde8166, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 57af930b57886186695f279efa03adaf272c4de1, febe776e285dc5e72c7e3ee697a87a794e1c00ff, 746a81aa26d3ebfb81acfd6af958d6a21603cd21, 5bc511aa30f72720260d792e57537379fb04c395, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, 9619cde5c79d91ca5c432186668618312175f8dd, 3d82552eb483e5ea84b577a0e8d5f157a6085824, 31d7d7b9c7b776c639316027e6ae5f2ff2673da2, 647d0e540627c5a903299a90d20530f8e48c18d9, 69d49a06f09cf934310ccbf3bb2a360fa719272d, 017010b941d902a467f6d329ae5e74fd67e67912, c0bcd7dc9426a70af15f5ad63b4af92ea4dcbd4d"
fa7b8acd47631bada5b66049824bfd335ac6bf8fa,Towards Improving Adversarial Training of NLP Models,https://www.semanticscholar.org/paper/fa7b8acd47631bada5b66049824bfd335ac6bf8f,Conference,"Adversarial training, a method for learning robust deep neural networks, constructs adversarial examples during training. However, recent methods for generating NLP adversarial examples involve combinatorial search and expensive sentence encoders for constraining the generated instances. As a result, it remains challenging to use vanilla adversarial training to improve NLP models' performance, and the benefits are mainly uninvestigated. This paper proposes a simple and improved vanilla adversarial training process for NLP models, which we name Attacking to Training (A2T). The core part of A2T is a new and cheaper word substitution attack optimized for vanilla adversarial training. We use A2T to train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI datasets. Our results empirically show that it is possible to train robust NLP models using a much cheaper adversary. We demonstrate that vanilla adversarial training with A2T can improve an NLP model's robustness to the attack it was originally trained with and also defend the model against other types of word substitution attacks. Furthermore, we show that A2T can improve NLP models' standard accuracy, cross-domain generalization, and interpretability. Code is available at https://github.com/QData/Textattack-A2T .",2020,80,"1693182792, 121817403",1693182792,Bratislava,2,"2117315688, 145458655",Y,Or is similarity to the corpus of interest more important?;I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships.,Conference on Empirical Methods in Natural Language Processing,20,"training,nlp,models,t,nlp","d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b, 1d5adacc5d4d226e76c35bf19018f9e76759f127, d1ae4ab5047489c2b010c7ce72262982ad66ad60, 9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a, 5471114e37448bea2457b74894b1ecb92bbcfdf6, c12b80b44d9acfe6cd92fdf965264c4b706c367c, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2, facd5f5deb152229ceb1803434d8690a09ab4129, d7b820af40a9e2660ef700d39f7b2e27b43435c5, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, cb23a59fdf3ade707600f076df4ff27a03941fba, d8348b802c9133d9e396d4ad809b020d5be42863, c24d47ff95cd4bda073c75ec24ececaa3b10c995, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, b80e2af7ffa85fe2f02ea8390e4915d55c19d199, beb890d47bbc21a96967f9993c9d6e15686b2eac, cde39ce861e4c7514ee07fd91b6b8aac50cbf01b, 6eb8caebbffc7e5b301b66dc36acad46b4dca5c9, 34ca47eed139a7f0694611528f75debc43385518, b266510f5f9b40d42b51884ad13a1867fb3284fd, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, 3cc2f69951cd24fe61be4cf32d62afbac297bc2b"
353c88c231ce156d604e074af276422422fc73f7a,"A Survey of Race, Racism, and Anti-Racism in NLP",https://www.semanticscholar.org/paper/353c88c231ce156d604e074af276422422fc73f7,Conference,"Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.",2020,83,"49713890, 3422038, 2138053020, 145317727",49713890,Vaduz,3,"71790825, 145366422, 88478180",Y,"Thus, as such, the novelty or the contributions of this paper are minor.;However, the proposed method is still within the same family methods as demonstrated by RARL.;For example, when showing that the head direction cells generalize in the new mazes how can we be sure that it is not using a common lighting scheme common to both train and test mazes to orient itself?",Annual Meeting of the Association for Computational Linguistics,20,"nlp,race,work,research,nlp","9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, ed935c6b359a7a486c28240d796e84897d095125, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 417326e51d78ba8bd2621f23e539b41bbdd336d6, 98b9086750f08a21c8778ab986339321e9caf790, 1c748f86182a62d44d5b44316db510f8d833e19f, 155eeede0c070f1f017ba5c9f6cacd7ae0b098aa, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, 545f108575314031f35c617c4ac35a10133c50e3, 1e1cf81a1113482be3f0c280db994a832cb9426a, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, adc180e1fe404b650fca3bb7970e43bdce34a611, 32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b, b52db9e41e15f76bdcfbe674abe0314af545c430, b55e490637babd50dab3cdaaa3a60a2be6eb1cbb, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, 2e5d2f2dc01b150dffc163a9f457848e9b5b5c38, 8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801, aad2d03c17bc7d1e636d0e79944ad4588af989d5, 78b2d392ebb100a220ceab6529d26909b27eaa32, fa7b8acd47631bada5b66049824bfd335ac6bf8f, cbad0923db89f23febcbd6192ff4149289ff2ad9, 024006d4c2a89f7acacc6e4438d156525b60a98f, f406aceba4f29cc7cfbe7edb2f52f01374486589, 28a5a53dafacebad8a7c47773079caeffb9a5baa, 1a37223175138bc1aa53b425ea2fdd0b382405a5, 5e31fa4e69d1a3587230f5d134c0b7e2ed84a742"
28a5a53dafacebad8a7c47773079caeffb9a5baaa,Representing Numbers in NLP: a Survey and a Vision,https://www.semanticscholar.org/paper/28a5a53dafacebad8a7c47773079caeffb9a5baa,Conference,"NLP systems rarely give special consideration to numbers found in text. This starkly contrasts with the consensus in neuroscience that, in the brain, numbers are represented differently from words. We arrange recent NLP work on numeracy into a comprehensive taxonomy of tasks and methods. We break down the subjective notion of numeracy into 7 subtasks, arranged along two dimensions: granularity (exact vs approximate) and units (abstract vs grounded). We analyze the myriad representational choices made by over a dozen previously published number encoders and decoders. We synthesize best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.",2020,82,"37574242, 2634786, 144171096, 2512264",37574242,Zagreb,3,"2138646471, 2059547812, 32244429",Y,"The authors did a very good job presenting the problem, motivation and solution in a coherent fashion and easy to follow.;While LINE or SDNE, which the authors cite, may not run on some of the larger datasets they can be tested on the smaller datasets.;Since this could be a potential disadvantage, some discussions or empirical study on cross-category generalization seems to be interesting.",North American Chapter of the Association for Computational Linguistics,20,"nlp,numbers,numeracy,systems,nlp","4be9368abc2474d6fd38639e523cf03af1873fd9, 6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5, f4207bcad24fe4eaf3849d6c1fe38c36545b5cb3, cc5726fc0ebb84f741f3496a3c52ced162c596ba, c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b, d2a5dcecd2ffdf03473df1688091f08fadb114a3, 62ccd99a65bfc7c735ae1f33b75b107665de95df, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, 6c34842a92ce4da9aab586490afdbd8779af4eab, ee6f23590783adec7cf6b2030c6a46f3117a708e, 97906df07855b029b7aae7c2a1c6c5e8df1d531c, 915ab7b6ca3633230403d47dbb31cf74888cc5c9, ccca203382e5dd198c089a0f1d7af7bef0f694e9, 7bc9607c5cf3fc817675d46844f529097d579514, 661ccdb41fe977d47273e586389cacc1489f3286, d1206ccabd1980848f14472d6548251c2fab7963, f2bc4057e696f49c326bf8e1588772a16f053754, a56bf7ee9a56d8f84079684339a953c2df9ce76b, e9a986c8ff6c2f381d026fe014f6aaa865f34da7, 1cff064f815111a71a98afda7aee1867ad617901, c9a9517c8b867187b4f4c0c37cbc65263ea41d25, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 63adc1e5086481e36b19b62707a96b799da51e59, e2f3aa4ecc487fa496d1a51e875e747d4b7e6001, 087dd95e13efd47aef2a6582e6801b39fc0f83d8, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 1e1cf81a1113482be3f0c280db994a832cb9426a, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 2880ac931c11176aee6d42a7e7bb0703aacde3f9"
9b529fe170823f95509585d5aa39fa01a43558fda,How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence,https://www.semanticscholar.org/paper/9b529fe170823f95509585d5aa39fa01a43558fd,Conference,"Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we introduce the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from https://github.com/thunlp/CLAIM.",2019,193,"51125639, 51131083, 2652217, 145086110, 49293587, 1753344",51125639,Zagreb,3,"2135403, 3144356, 1680740",Y,"“Customers” randomly appear throughout the task and the taxi agents receive reward for moving to the same square as a customer.;identifying bugs in programs where the wrong variable is used, and;If the loss for the task is proper, then it's well known how to construct a divergence which coincides with the optimal risk.",Annual Meeting of the Association for Computational Linguistics,19,"legalai,professionals,tasks,nlp,nlp","7a1e71cb1310c4a873e7a4e54d1a6dab0553adce, d2a609ffb814442d0728aef9f6616f9cd775face, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, 4087e84fc695bb6433d0104ee94f9d7e9f4b7da5, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, a453ce8a3de86a170c79a1082ef358c3adf4e612, 73cbaf5f2441ef3478266b5438c0e90d1ae71652, b55e490637babd50dab3cdaaa3a60a2be6eb1cbb, 661ccdb41fe977d47273e586389cacc1489f3286, 56ae2837b6cd4d143bbfa4a4b06811d70126106f, 91b2b47cabd800ef658b65bfe1f52b7293a740c3, e57aeb158a38ccf33d2f0f5a8f63f1209497e329, a604aa1f2a2ca1a6a0b09013e71b83d36cc0f358, 0ecf3f089e6dc7944b440227069b9d0143e18d78, d916776e0c6a04b0def4c22257c188776c2edab2, f208b3fb28c556ab62f9d202b7beae89700a338a, e02a757617c2c42eb62889cc4d4aee3765928303, 322d91190acd8ac8c64598f5126947b0485ba249, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, 28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8, b69a35662a2cac38eab22f4481285116bdf8c30e, a9cbbef8f4426329d0687025b34287c35bdd8b38, cbad0923db89f23febcbd6192ff4149289ff2ad9, da5d78b3e3a1544fde98fba86088e1215e97cbe8, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 55fa5be5288f6097ae5bd2dfe58fc07b3b39bfb6, 17fc5c9b37ba63fb3280ddf9909a183523e7bcc3, a1d36749b89e46a8eaadf8ba40788741c192fb1e, 43e6e8d6663d83f1b74cf5a2be7b040b0928f867, 81c02f123b3ef09cf1a8e5a1332451f0d46663fa, 9d788cfe4a0991d3b1a266c8329f6e903840b82f, 811df72e210e20de99719539505da54762a11c6d, a80e26e6365b215715c182d19a9aa8bb876ac768, c0e6cd2ec3bc9eb46c7d45bb708854da3327339e, ce9ca56036307217ea565644d3d3bd74b879e045, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6"
68a3d32416977e88cf1bfa4ad548d403f5f089d6a,Rethinking Stealthiness of Backdoor Attack against NLP Models,https://www.semanticscholar.org/paper/68a3d32416977e88cf1bfa4ad548d403f5f089d6,Conference,"Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack. Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. To address this issue, we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings, making an important step towards achieving stealthy backdoor attacking. Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. Our code is available at https://github.com/lancopku/SOS.",2020,68,"2120801160, 2149202150, 50492525, 2108485106, 2130282986",2120801160,Tirana,3,"1786639, 49528584, 50738985",Y,"On empirical evaluation, I already mentioned that it impossible to understand what the classification problem on TIMIT is. I suspect it might be speaker identification.;4) Novelty The main contribution of this paper is basically a set of experiments looking into architectural choices.;This section could be improved by demonstrating the approach on more datasets.",Annual Meeting of the Association for Computational Linguistics,20,"backdoor,nlp,models,performance,nlp","f11cba099b8cf14815f7b3d85f55ecfddbf9f04d, 04fa3ebc4c0c0b4a2d2d1a3fc612134a05057696, 37b9478b792fb1e7ad5f2c322ed4445b93df6c9e, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, 6eb8caebbffc7e5b301b66dc36acad46b4dca5c9, 0e33833f5e2e2719edfba1d142eb4d27f96e799f, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, 69a72ff5b30642d11c96635e99aadad3140d33a7, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, 6c34842a92ce4da9aab586490afdbd8779af4eab, 381c7853690a0fee6f00d2608a7779737f1365f9, afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84, d86084808994ac54ef4840ae65295f3c0ec4decd, 390bcf15a1b13cb0d5966859c35c69a31238e838, e3b37c1c955b2b10809040ce277edae5333b99c3, 11be2469ab1d1c508e7b6e14148990741ba87884, 97ac11e5a6440eccb70ae7146392ac138c36fa6c, 0599f45e03ac2016321df0dd653ba4c0034c79d5, 7c8bd41e9cebdb01f445a51ba43839c8d9b3ab91, 74bc39003e65119eaa6ba339a61b45b417a638b7, e23b2e47b0ac6f50000078828f27571804dcd6a2, fac67bf55456b52ac6e4f280ad953d0250c74ebc, d1dbf643447405984eeef098b1b320dee0b3b8a7, 83ea80a9393177ecf84928f4bd1120fb679f180c, a85c45ce7c893388e8eafa8a653b042e1497db48, 9e540662619327a3056d9e40bb58058868f6f805, 980858461df7c4349f17b427686c5bcbcffbdc04, 3ec02e36cc0ba16292b255e9f90c3725521e2ec9, d617f51833860dc50d202af7f80be71304b2e994, 54ddb00fa691728944fd8becea90a373d21597cf, 795550a5294eb05ea4f3b14f0b1c21a405493d85, aad2d03c17bc7d1e636d0e79944ad4588af989d5, 0090023afc66cd2741568599057f4e82b566137c, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, fc61c7221350806c25379f385c27b2102ff8eb57, b4af1cbb4d3a03d3ad7930b906fc7bf6870180cc"
8dce62b18bdea587c07cb4769a05ca0a816d09baa,"The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models",https://www.semanticscholar.org/paper/8dce62b18bdea587c07cb4769a05ca0a816d09ba,Conference,"We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models—including classification, seq2seq, and structured prediction—and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.",2019,150,"6117577, 49556437, 1994065972, 2843215, 1388485917, 3159346, 122064392, 51478016, 146419516, 49849144, 2061016887",6117577,Bern,2,"2285816475, 2168853963",Y,"See e.g. the discussion in ""Variational Lossy Autoencoder"" (Chen et al. 2016) - Experiments are not complete (e.g. for AR, as noted in the paper).;I could imagine a network learning to ignore features of objects that tend to wander over time.",Conference on Empirical Methods in Natural Language Processing,19,"analysis,nlp,models,model,nlp","4b991efaa8493a5925c2aee9eec980831213eba6, 7c24234042988e2f820a4350f43422ed2ad6fc52, 736ef8a32d6c5f76a21d61299300cf796480d507, fa63c3f53413ced7946623889c416e34a28676ea, 39444c55f07839ac6a0d1839472a982f8fb447bb, 6fed828456964d29517f6caf31b700d8aec82153, b473e91cbe80c8b46451b49153cd5f93030480ab, 156609022dd6258c60238859622da0a1683bd062, 661d316f4fe9574f9048586bfae2c43243a1d22f, 24ab4e99e582c9770281eee0a39cbeb70ddd891a, d2efa91bd7d076a66bc1d1c570f6aa2da944dd88, 74b63c4cdc719a646ce014d084a0ce0ac66d9139, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 811df72e210e20de99719539505da54762a11c6d, 25adff0bb9691b46fee5c0591300d6f3ccf117ab, b080d072cfde697180db3234da08903c092e72c3, cb23a59fdf3ade707600f076df4ff27a03941fba, 16753e0317730e8c1b297338300a8c6163dd06f2, be383c607d4d357c763d2329ab71799c6e1393b4, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, cd20ed8e70f6459f5617350f6b84c5c6d5929cb7, 4895c430c7810b45840b58cc9182f12143013a43, 0f4d00d01d43d3967ee92b58481b5ad530a944d1, 6dd3e404aac097f63d42dc19fd08c7ec281f90b4, ec2f9076448ba25a225618603adde60caa76c4df, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, d1a6b3a5efde3783b53f822dc8dd00aaac934b95, 78e80b6f3fa46e26ea4e8542815305c3ba88c5df, ab606e9d148458f6d54e5d44abefd73b7990f6e0, f497c1ece7b6f3560bb39958e2673f476d608f98, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, 256e95ca331cbd35b3a23cc306b6627e6771a963, eff6546819d25df0bccdc89f02554a43a4f1c464, f64670a5f54fcce339a916497a001cbf02a9a04f, ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee, 01bf0e83159712fbbbd12171a7e268547a4cfbc5, 2374106a32169c07703599ff3f6f4b31e8067b89, 0e9a44ce661c3535d5ce747912540080324489f5, d6bc29a897fd85e7187dc33c3c974b8879462237, cde985e542218aced8c4a627cda3dd12939805f1, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, 916455d97cd792c2eb5b00663689592e25cbc8d8, 7ae2783a9196fb4bc2a610ae812d19722daddce5, 6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5, aa6c2afadd660fe4efbac699f7854e8f6f240c38, ae38dc77a962161107361f213db9216ee1274037"
c50a909e20bd07f4aea09dc6dae539b45b406a96a,Evaluation of BERT and ALBERT Sentence Embedding Performance on Downstream NLP Tasks,https://www.semanticscholar.org/paper/c50a909e20bd07f4aea09dc6dae539b45b406a96,Conference,"Contextualized representations from a pre-trained language model are central to achieve a high performance on downstream NLP task. The pre-trained BERT and A Lite BERT (ALBERT) models can be fine-tuned to give state-of-the-art results in sentence-pair regressions such as semantic textual similarity (STS) and natural language inference (NLI). Although BERT-based models yield the [CLS] token vector as a reasonable sentence embedding, the search for an optimal sentence embedding scheme remains an active research area in computational linguistics. This paper explores on sentence embedding models for BERT and ALBERT. In particular, we take a modified BERT network with siamese and triplet network structures called Sentence-BERT (SBERT) and replace BERT with ALBERT to create Sentence-ALBERT (SALBERT). We also experiment with an outer CNN sentence-embedding network for SBERT and SALBERT. We evaluate performances of all sentence-embedding models considered using the STS and NLI datasets. The empirical results indicate that our CNN architecture improves ALBERT models substantially more than BERT models for STS benchmark. Despite significantly fewer model parameters, ALBERT sentence embedding is highly competitive to BERT in downstream NLP evaluations.",2020,66,"2111538614, 2125022424, 2046999993, 2163133",2111538614,Stockholm,3,"1492047220, 1630331317, 20829758",Y,"Moreover, I like the qualitative experiment (Figure 2) in which the tree is used to vary a latent dimension to change the digit’s class.;For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers.;2.  They argue that they don't need to separate train and test, but I think it is important to be sure that the generated programs work on test cases that are not a part of the reward function.",International Conference on Pattern Recognition,24,"bert,models,albert,nlp,nlp","bfad52fc64ca0169644b6e7e0ea9a46470d51709, da9b8b4073e6ad44b3da66e1e117cb1ddbf8836d, f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6, 8c33ca066e2ab615e24c65198c794114436053dd, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, 3c8a456509e6c0805354bd40a35e3f2dbf8069b1, f4c4e148546089123f8da5db4fb246ab4062bd40, 13c4e5a6122f3fa2663f63e49537091da6532f35, d8348b802c9133d9e396d4ad809b020d5be42863, 2e965b5d97c2d6fb4af284307735be39283792ba, 784141489258258b12979061d92c1a616da26525, 799d5a8271887adede035644d878c7bd555576df, 1452b25a7680bbb2c66dd7dfca6704292405da92, 3774fb4b6aa11d88c562ee5702c66cae5881af0d, 736ef8a32d6c5f76a21d61299300cf796480d507, 709af143f78bc62413c50ea1a7ee75b0702c4f59, 0e9a44ce661c3535d5ce747912540080324489f5, 8babcaf89f8537dc628a029ebf932100f57289fd, 5d433da6d0f143f20936379910104d2bb139d4ae, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, b79e5e4622a95417deec313cd543617b19611bea, 287a7da1801a07cf7fd85ffcc23c79504876ecc0, b2f8df88de24fe0ef74bda0eec1a0c3e7329b9f9, 2743e66939b30c43affb3c9e31f20cfac2109045, f70b2f20be241f445a61f33c4b8e76e554760340, d280cf82a6144b3e168840802b1a8a14d4eaccb9, ba687027ed6012f613e1f9a9cefe7683bb192934, b27a78366868ca47098e00dda74dd1b167b3a80d, b4af1cbb4d3a03d3ad7930b906fc7bf6870180cc, e1e43d6bdb1419e08af833cf4899a460f70da26c, 12c9bcf710d30ba991fb765ace07f177f53ecfd9, bb1118fb9fd86da6a2f65770353d8fb4362d9883, cd2f4aaf98bb1e020cff310000c8049d3460c54e, df7d26339adf4eb0c07160947b9d2973c24911ba, 42d3ae61ca296558cba61446bd95b8a6e5b1082d, 224c11bc51b4959bc787d6681c2b152468294b11, 90aca7b4cdd728b28b2fb5b4dc3ae3e37daa5b94, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 329d31f881a17861eedeef6a9d8fd509cddd2b7c, db528269ef800727245c0fcb35b692d29c1ccdc9, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 0df5a4f9cc8a244715fe9968732497d2ac2a7cd1, 8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6, 64c4c6cb66457c79f89ba4f2529b57bfce19ced4, 2dafea864f74a477414c3b71b742f7997e216102"
e5d720767b7a539bb2edaa98eaf572a4506a79c6a,Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing,https://www.semanticscholar.org/paper/e5d720767b7a539bb2edaa98eaf572a4506a79c6,Conference,"The use of crowdworkers in NLP research is growing rapidly, in tandem with the exponential increase in research production in machine learning and AI. Ethical discussion regarding the use of crowdworkers within the NLP research community is typically confined in scope to issues related to labor conditions such as fair pay. We draw attention to the lack of ethical considerations related to the various tasks performed by workers, including labeling, evaluation, and production. We find that the Final Rule, the common ethical framework used by researchers, did not anticipate the use of online crowdsourcing platforms for data collection, resulting in gaps between the spirit and practice of human-subjects ethics in NLP research. We enumerate common scenarios where crowdworkers performing NLP tasks are at risk of harm. We thus recommend that researchers evaluate these risks by considering the three ethical principles set up by the Belmont Report. We also clarify some common misconceptions regarding the Institutional Review Board (IRB) application. We hope this paper will serve to reopen the discussion within our community regarding the ethical use of crowdworkers.",2020,63,"1380557800, 115300694, 2072404217, 1746959",1380557800,Athens,3,"49356798, 2146058962, 7617146",Y,"An image has many attributes: the glint in the corner of a window, the hue of a leaf.;Is it better to add attention to lower layers or higher layers?;2. The proposed attention mechanism seems to be fairly domain (or task ) specific, and may not be beneficial for strong generalization (generalization over unseen category).",North American Chapter of the Association for Computational Linguistics,20,"crowdworkers,research,nlp,production,nlp","549e933821fdf7cd0309dacaae99c8284cbfcc24, a926093ef103c02fd5ef7310e0d41b83d1958ed5, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, 2346d121f38fc19c77e0b062415519843f478163, c2a448bb511ebae41a87e69891da8bbf17ddba3d, 0e9a44ce661c3535d5ce747912540080324489f5, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, 73cbaf5f2441ef3478266b5438c0e90d1ae71652, b4af1cbb4d3a03d3ad7930b906fc7bf6870180cc, 48c73c389c3f36d70407eb8309a0b41578c15fc8, c9f320789e98d2c7a798a9705e26dbe317677966, 079b57837221413bf99ab40999c77c29e280e0c2, f75b70c9d7078724b592ec3e21de705e7b6ff73f, 579476d19566efc842929ea6bdd18ab760c8cfa2, 7171a0e9b07ebc98a32eb912262613efc20f283a"
0427110f0e79f41e69a8eb00a3ec8868bac26a4fa,Do NLP Models Know Numbers? Probing Numeracy in Embeddings,https://www.semanticscholar.org/paper/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,Conference,"The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens—they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise—ELMo captures numeracy the best for all pre-trained methods—but BERT, which uses sub-word units, is less exact.",2018,219,"145217343, 1705260, 48831399, 34650964, 40642935",145217343,Copenhagen,3,"2078772072, 2110760082, 1492047220",Y,"Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice?;The authors show mixup provides improvement over baselines in the following settings: * Image Classification on Imagenet.;see Fig.2 —> see Fig.1 page 4just before equation 8: the the",Conference on Empirical Methods in Natural Language Processing,18,"numeracy,numbers,reasoning,nlp,nlp","1562390dd212516cd857009cbd4f857a902d1f3d, 5290d7921f0266c8b50b79fc8a0b7d22868f4f60, 0a829289a16ae48837cc2905635435db98bacc76, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, bad4c08f03587e38ee960e2aa76e16d722826e7c, 182acc7a1c9a63cfdef9f86ee9df43fd75e05060, df1a2539afbad27c4c80115a6f8f59a089024865, 71f3bfea47b96fbab78a3439ee819cd92a33329d, cf523942d56e90db182c5788845f6502da9a307d, b3f7359c6d5780972c5ea8db016a01f0c705aa01, 9619cde5c79d91ca5c432186668618312175f8dd, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 5d24ed8942235324512d6cedfd8dbf54c57658b4, 38179848e2d6a3ad373b1793848816111428ac36, 2ed691a353fa48403d493ab658f5f267a42f0bf1, 787ae2c51cd82b904bb4fb9ccb15266381af5436, d88c1255876b62fb5f5a8b292098ca430710a540, 1c748f86182a62d44d5b44316db510f8d833e19f, b6a7226e5f6d618370995eccad68af195ef32da2, 3774fb4b6aa11d88c562ee5702c66cae5881af0d, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, ccca203382e5dd198c089a0f1d7af7bef0f694e9, cf5dfc4a9f7a82b32640128ca10832eace55880e, 9f9afed22cdc73be627932270a9bcae341df99d4, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, c9645aa4ea31903e02e201b877fd3e1466adff4f, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 92afbbe41174a545f9da9992e33c9a9592e529aa, 9d6acac70b2d1fdb861a08b00766ef263109cd7f, 86dbd884043eb5807c61d2c65b813e673b4a04fa, 9727206903eb40d4fa42606711bad3402f2ba9aa, 14fe35149aed6a47b6ebfd207deb7681b9446bb6, 697f2f3598057cd17cff7749d768cae0993c6727, ac67d5f9c89d8d72fbd074f94079608220348f3f, 48c73c389c3f36d70407eb8309a0b41578c15fc8, 01bf0e83159712fbbbd12171a7e268547a4cfbc5, 0bca61986b8edeaf33018d0203b44110f2480110, 417326e51d78ba8bd2621f23e539b41bbdd336d6, c84aa52bee5116f80c7740503edff4b08f733c3b, 00bbc94806ca0821a9c82d8aedf16f0e6263b89f, 5e31fa4e69d1a3587230f5d134c0b7e2ed84a742, 63316bb5b88d362051c048e864c3ae5d97a26d30, 2c3eef2f17369912e330281d54b535675077e4ca, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, be082d70534db088315f2cc5b42c2fdcd58c1b8c, b34fc78de28be598e21118d7cb9d84d63374addc, 6a1b25f7a67395ad1e676027322913acbb0a0635"
2743e66939b30c43affb3c9e31f20cfac2109045a,Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks,https://www.semanticscholar.org/paper/2743e66939b30c43affb3c9e31f20cfac2109045,Conference,"Labelled data is the foundation of most natural language processing tasks. However, labelling data is difficult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it. Descriptive annotation allows for the surveying and modelling of different beliefs, whereas prescriptive annotation enables the training of models that consistently apply one belief. We discuss benefits and challenges in implementing both paradigms, and argue that dataset creators should explicitly aim for one or the other to facilitate the intended use of their dataset. Lastly, we conduct an annotation experiment using hate speech data that illustrates the contrast between the two paradigms.",2020,85,"2043232919, 2737827, 2022288, 1970864",2043232919,Kiev,2,"2145413970, 1596817678",Y,I think this is a nice example of a detailed analysis of the representations acquired by a reinforcement learning agent.;I don't understand what is offered beyond the original papers.,North American Chapter of the Association for Computational Linguistics,20,"data,annotation,beliefs,nlp,nlp","3be6a57d6db95bba2962a1f3476414a0a9b230b5, 0c4070272fb7d6b98971a107b022ff8abf0aa55e, 82a09f72d7b9587e3b519b1cd9640a5a611f3480, e67a2817089312746d69b38ce9abfdc4b1bc69c3, 013eb12ce5468f79d58bf859653f4929c5a2bd14, 4b991efaa8493a5925c2aee9eec980831213eba6, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, d88083e37c44461ce3e404bd57257cd3edb07d4e, ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba, 9712624bb61abb0da989514cae558cfab61bb9d2, a9c1566119695250f68a572a4260b03721cc8ba3, 2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c, b651d67502790e1d6d41c589e1d93e996ba7b935, 453fdfeefd6498a65be339d7e8722f6f3288907e, 01f0f5205d03870f172ae8f04e33356d5a0af221, cb3968152f7d93f53d24b00279a90d5071ddc85a, 9810bcaf5ac1792e6a2738a86f85ce270d448040, 8f99f5cb3950fcd1628c6a563be4b4fc4966fa66, 41d6b6bd4bf44bf13b9157b603e33360e5e77a01, 0fe0f08e71e188cb2a663f36acb296d9eb6fe694, a0f788f6de0fb83d623c875a98120e3f347f70d1, 4f8d648c52edf74e41b0996128aa536e13cc7e82, 676664ee7471738577f641e6159e7596625b7fdb, 2097ff87df3cb9427c7388bc7b997ed56907d45b, 3c8a456509e6c0805354bd40a35e3f2dbf8069b1, 2c03df8b48bf3fa39054345bafabfeff15bfd11d, 1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7, b7034546bee38ba13d3b312fce893a22e33ce4dd, d235a9085e0543fcbe502fbc269f9a8ee01dcbab, 69d49a06f09cf934310ccbf3bb2a360fa719272d, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, bcc82ce554942880814243fc8c08a88b9d2aad09, ca011427853d34ce4ec9ccafde8a70c9eacc3e21, 42543dc42e65609bbbf2be470d54dd923532c36a, 98ce7af921e7c52d81df64d632d34eb09522cd75, b04550f0722e9614163855ab36fc2430b931a3fe, 8ba8a0d18a06752f5a39996ccf1e914da0941443, 4aec7bcb292cb763341a22fc67a1cfaceb3a2c67, eacf9284a39adcd56172665f31fd5a72560bba7a, ca05ca5f70616456d0f2e05c4a51cbf8b8d273f7, 64c4c6cb66457c79f89ba4f2529b57bfce19ced4, 780c7ead33428d282044519fee5e773ad56d5a2c, 1109d62ebd2b29a7dc148bc30dd6cfc803a63dec, c2aab470b8cf92f090e0a3bac1794b21500585e6"
ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8a,RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models,https://www.semanticscholar.org/paper/ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8,Conference,"Backdoor attacks, which maliciously control a well-trained model’s outputs of the instances with specific triggers, are recently shown to be serious threats to the safety of reusing deep neural networks (DNNs). In this work, we propose an efficient online defense mechanism based on robustness-aware perturbations. Specifically, by analyzing the backdoor training process, we point out that there exists a big gap of robustness between poisoned and clean samples. Motivated by this observation, we construct a word-based robustness-aware perturbation to distinguish poisoned samples from clean samples to defend against the backdoor attacks on natural language processing (NLP) models. Moreover, we give a theoretical analysis about the feasibility of our robustness-aware perturbation-based defense method. Experimental results on sentiment analysis and toxic detection tasks show that our method achieves better defending performance and much lower computational costs than existing online defense methods. Our code is available at https://github.com/lancopku/RAP.",2020,51,"2120801160, 2149202150, 50492525, 2108485135, 11774802",2120801160,Vienna,3,"50593332, 1630331317, 2155516106",Y,"We have to, however, evaluate the approach on what it is able to do at the moment.;Overall, this works seems somewhat too preliminary at this stage.;In particular, they aim at generating a complete set that fully specifies the behavior of the oracle.",Conference on Empirical Methods in Natural Language Processing,20,"backdoor,defense,samples,nlp,nlp","a22f3398ea865426c89ee66f4824ec626e56a864, 0599f45e03ac2016321df0dd653ba4c0034c79d5, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, 799d5a8271887adede035644d878c7bd555576df, af9280741ef627f0d6c8437605d002d3bfc2d1b1, 71a85e735a3686bef8cce3725ae5ba82e2cabb1b, 1452b25a7680bbb2c66dd7dfca6704292405da92, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, 39444c55f07839ac6a0d1839472a982f8fb447bb, 5031790972d496547b6613d46a4a0134c824db6e, 8674494bd7a076286b905912d26d47f7501c4046, 83cebf919635504786fc220d569284842b0f0a09, 8713452753fd01de5616121af93e173d4f74eaf6, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 0daa1d31d6949f8804089d8a1c11c4560422ad39, 696b388ee6221c6dbcfd647a06883b2bfee773d9, 7c8bd41e9cebdb01f445a51ba43839c8d9b3ab91, 32524aa3ae8522542753ed7e6f4cca3970e4acab, b266510f5f9b40d42b51884ad13a1867fb3284fd, c468bbde6a22d961829e1970e6ad5795e05418d1, 02fa2389b1b64b661192e224bed8af6df0ce80f6, 19cf7458db4e17c7504eee24ccf961e1dc91435c, e2a85a6766b982ff7c8980e57ca6342d22493827, 92930ed3560ea6c86d53cf52158bc793b089054d, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, a92b103b81a48878e76f1fcfc3e2a1454f895555, 624b2f14be4287d6a400cdf88a6f911b434b182e, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, a2e667e4382aaa8e02a17d0522c1a910790ab65b, 8894d431a768a35dc7ca4d762ebdba4f407b978c, 709f7a6b870cb07a4eab553adf6345b244913913, 5471114e37448bea2457b74894b1ecb92bbcfdf6, 1bc34cb22131554ba18f6ba9e6ede5beb42939f1, cb3968152f7d93f53d24b00279a90d5071ddc85a, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, 75ea299834d6949e89e91d006677343ddab44e49, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, 8799ec4bdf98bc313aa8c41a706a385e026d1f88, 785a1c77fabd1ad870a9dbc98d235d2fcbdbaa6f, 84725855d10b531eb8cbe54935dda0440c2fc750, 80d9f0eb47b712988d19cbe29a7bfa63f2a175d0, 37b0a7a6c8fb26e32b9206847e78d521a2cd5900, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, 85328b4a8132bf4299f8cd7f8e79e850d561c8fc, 676664ee7471738577f641e6159e7596625b7fdb, 8fd8cb4951ae9634a378383a880fbf16ac5d6926, d617f51833860dc50d202af7f80be71304b2e994, 35adeef964fd344288febc7def2780007587724f"
9eea59c34f139f3d2153226c8cf026e975622074a,Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation,https://www.semanticscholar.org/paper/9eea59c34f139f3d2153226c8cf026e975622074,Conference,"Public datasets are often used to evaluate the efficacy and generalizability of state-of-the-art methods for many tasks in natural language processing (NLP). However, the presence of overlap between the train and test datasets can lead to inflated results, inadvertently evaluating the model’s ability to memorize and interpreting it as the ability to generalize. In addition, such data sets may not provide an effective indicator of the performance of these methods in real world scenarios. We identify leakage of training data into test data on several publicly available datasets used to evaluate NLP tasks, including named entity recognition and relation extraction, and study them to assess the impact of that leakage on the model’s ability to memorize versus generalize.",2020,56,"26940961, 150147667, 1404100607",26940961,Dublin,2,"2810600, 2109184366",Y,"1.  The paper misses some more recent reference, e.g. [a,b].;They apply this architecture on the same task as the original article: document classification; they use a logistic regression on the extracted representation.",Conference of the European Chapter of the Association for Computational Linguistics,20,"datasets,nlp,ability,data,nlp","6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, ed935c6b359a7a486c28240d796e84897d095125, 5687c9e8da574453fd873662b95caec70dac9d1e, fc32074b37a6d9dda535a70f9689022e70508520, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, cf523942d56e90db182c5788845f6502da9a307d, f406aceba4f29cc7cfbe7edb2f52f01374486589, 00d1f3423a33f73ca6aee884a58834547475d2f0, fa75a55760e6ea49b39b83cb85c99a22e1088254, 1661d0d8d47cac41e01c59c60aac3675b4396698, f75b70c9d7078724b592ec3e21de705e7b6ff73f, da5d78b3e3a1544fde98fba86088e1215e97cbe8, 929305892d4ddae575a0fc23227a8139f7681632, b61b260de1599e6e89491cad9160898fcd3b34c2, 42543dc42e65609bbbf2be470d54dd923532c36a, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, cc78babfacce48e715dac56886d7dd9746cfcab0, 3cc2f69951cd24fe61be4cf32d62afbac297bc2b, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, aa9bf8b4c4ce8c28b9f47d7ce4f416aa9726bb0d, 6f31f59f7b4bd751b5d8657d8a878bef571f9a4c, 41d6b6bd4bf44bf13b9157b603e33360e5e77a01, f04d8e558c11ac0fc9318c4f3d61b651c62ddcfa, a80e26e6365b215715c182d19a9aa8bb876ac768, cc5726fc0ebb84f741f3496a3c52ced162c596ba"
fafa541419b3756968fe5b3156c6f0257cb29c23a,Visualizing and Understanding Neural Models in NLP,https://www.semanticscholar.org/paper/fafa541419b3756968fe5b3156c6f0257cb29c23,Conference,"While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\em compositionality}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\em salience}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,",2014,636,"49298465, 39717886, 144547315, 1746807",49298465,Madrid,2,"2108706355, 2115215055",Y,"Is this on bAbi as well?;But, it would be good if this can be supported with real life examples.",North American Chapter of the Association for Computational Linguistics,14,"compositionality,nlp,methods,networks,nlp","33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4, 155eeede0c070f1f017ba5c9f6cacd7ae0b098aa, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, 7998468d99ab07bb982294d1c9b53a3bf3934fa6, aad2d03c17bc7d1e636d0e79944ad4588af989d5, 5371896313ac227eb819038dd55f213cb42b99e2, 4d4d1ac9a9ad8592a6cc05082437d706ee176a38, b5904cd5dbf73b8d5ff13517de490c292d877ee0, d2a505586c0da20752b98f63c7760b6a5c41e28d, 003ef1cd670d01af05afa0d3c72d72228f494432, fc32074b37a6d9dda535a70f9689022e70508520, addae423490bbe82da4fb2fc265237178686b4e8, d1efcef213c433445be56d7479eb47d972b3ee79, d916776e0c6a04b0def4c22257c188776c2edab2, 6001895c2fcf69528d01306e9d293d9d2a4cc67b, a281d563261c738f13b9e58a525e7e265a619c93, bc44c0c64a473e035b11ae60a1993ad3db1acd2e, 6f75e8b61f13562237851d8119cb2f9d49e073fb, 53103ae318a19569ac82cee5062de2cf73bf386c, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, 649c3497e3b34b15a5011259fcb837cf6c1ac04a, 371a343457a4fbff00000bf4faa29b2b2f85744c, 752604994a7ca548ff2954114fc61a501d857b1c, d9a7fa7616a327367696e19b1846519745cd43ff, f9163156eeba67762a7441db48fe6720106137cd, 35adeef964fd344288febc7def2780007587724f, b80e2af7ffa85fe2f02ea8390e4915d55c19d199, b61b260de1599e6e89491cad9160898fcd3b34c2, 3774fb4b6aa11d88c562ee5702c66cae5881af0d, f0f1627db35b4942e0f83069f20dd0948fc35d28, 343500e0052eb1b683f32b00efbbd1331c94184a, 993df7df129f8d18816877d69923d7df7b347d85, ce212cb873a54e5716da53a66b10298ac013008a, 5d433da6d0f143f20936379910104d2bb139d4ae, b651d67502790e1d6d41c589e1d93e996ba7b935"
1cf2e9e198feef3893da2800a7949f6880ddc084a,ExplainaBoard: An Explainable Leaderboard for NLP,https://www.semanticscholar.org/paper/1cf2e9e198feef3893da2800a7949f6880ddc084,Conference,"With the rapid development of NLP research, leaderboards have emerged as one tool to track the performance of various systems on various NLP tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of NLP evaluation: the ExplainaBoard, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g. what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g. where does system A outperform system B? What if we combine systems A, B and C?) and (iii) examine prediction results closely (e.g. what are common errors made by multiple systems or in what contexts do particular errors occur?). So far, ExplainaBoard covers more than 400 systems, 50 datasets, 40 languages, and 12 tasks. We not only released an online platform at the website but also make our evaluation tool an API with MIT Licence at Github and PyPi that allows users to conveniently assess their models offline. We additionally release all output files from systems that we have run or collected to motivate “output-driven” research in the future.",2020,50,"144118452, 41037252, 2116642640, 30300197, 46923811, 2087363104, 2108176413, 1796245019, 14199369, 1700325",144118452,Lisbon,3,"1739899643, 31380410, 115504645",Y,"One direction to strengthen this paper is to examine if RMN can do better than pair-wise interactions (and other baselines) for more complex reasoning tasks.;However, according to me, the fact that it provides no model description, no model analysis, no modification of the model to improve the sentiment discovery, prevents this article from being publicized at ICLR.;The technique systematically interacts with the oracle using observations, which are abstractions of environment states, and it is guaranteed to produce a data set that completely specifies the oracle.",Annual Meeting of the Association for Computational Linguistics,20,"systems,nlp,system,research,nlp","d2a505586c0da20752b98f63c7760b6a5c41e28d, ccca203382e5dd198c089a0f1d7af7bef0f694e9, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, ff75865cde62592d068b2afd055c57c81d77158b, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 8713452753fd01de5616121af93e173d4f74eaf6, f6dab84c2c00ab92d8ee9d9359d7e530512114f9, d1e701665e73faa648cb15473952576f40e8e122, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632, cd8a9914d50b0ac63315872530274d158d6aff09, 011095a0082e5e301f9bf30267b193c1c9e7e370, 5031790972d496547b6613d46a4a0134c824db6e, f64670a5f54fcce339a916497a001cbf02a9a04f, 94cb5503b191815ce77b19147d96c6fbd68f06bf, 19cf7458db4e17c7504eee24ccf961e1dc91435c"
d1206ccabd1980848f14472d6548251c2fab7963a,Exploring and Predicting Transferability across NLP Tasks,https://www.semanticscholar.org/paper/d1206ccabd1980848f14472d6548251c2fab7963,Conference,"Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling). Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even when the source task is small or differs substantially from the target task (e.g., part-of-speech tagging transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as source data size, task and domain similarity, and task complexity all play a role in determining transferability.",2019,131,"144244743, 1664686681, 2227827, 2041695, 3382568, 1401265033, 35208858, 2136562",144244743,Vilnius,2,"1524732527, 28552618",Y,"Some reasons below: * There are no specific results on properties of the divergences, or axioms that justify them.;On empirical evaluation, I already mentioned that it impossible to understand what the classification problem on TIMIT is. I suspect it might be speaker identification.",Conference on Empirical Methods in Natural Language Processing,19,"task,tasks,source,nlp,nlp","20381972ee66e03ea218e5b3d39d6423b6e35f0f, 2a15e9ac5593cd1f8bd3a724e78a173037c02fea, da3f33d858586d24cb265e79eb54f3746e998f57, cf523942d56e90db182c5788845f6502da9a307d, 6be56f559a74c0124526242e70cbdfd16cbc60a7, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, f476d44a4892e0c8256e50e9075f8dd3d412bfcf, 88a724083b2cfcc096448c28e6973c8f761ee463, d2a5dcecd2ffdf03473df1688091f08fadb114a3, d916776e0c6a04b0def4c22257c188776c2edab2, 808e9ce4e86e79098edea7f00b5b91663b87a5e6, 9e540662619327a3056d9e40bb58058868f6f805, 6dc4883228c95e8a332320fcc587a0ff33c84d59, 33ec7eb2168e37e3007d1059aa96b9a63254b4da, dbabab9bf5955558f73a37644f4bb626106a6d73, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, 5cdb31c5e3e0aa4cc2d10229793b4911f69fd78f, c84aa52bee5116f80c7740503edff4b08f733c3b, d1ae4ab5047489c2b010c7ce72262982ad66ad60, a13149a80855412d970d0de2b41c611f4cf7e1da, 980858461df7c4349f17b427686c5bcbcffbdc04, 9b54941de1e21826ecc28b32730ac3f69991ede4, 37b9478b792fb1e7ad5f2c322ed4445b93df6c9e, e7f3478fd8aac6940a4bf4f5eb60ac38f6b0b85b, dadfb3ff45e19dc22456a645f441bbeb17c93c9c, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, 2b7f9117eb6608a58be4c078ca3d69c0e5ccb875, 1a60a9d1eef24e123c27a9eee5a399ac2b620fee, e30d9b8ce108d982169621b88a5e3fb69fec70e1, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 1fa4936fb06319c3f4536c26a447d5507c92bd48, 9eea59c34f139f3d2153226c8cf026e975622074, 714f47bbedcadd7ebc44d2d5010f13323fc6a256, cd20ed8e70f6459f5617350f6b84c5c6d5929cb7, 2019cf49b51021a376f9833a53565513f0d8107b, e2f3aa4ecc487fa496d1a51e875e747d4b7e6001"
30f233eecca2239ee1dd754914324092e53f8f19a,Evaluation Examples are not Equally Informative: How should that change NLP Leaderboards?,https://www.semanticscholar.org/paper/30f233eecca2239ee1dd754914324092e53f8f19,Conference,"Leaderboards are widely used in NLP and push the field forward. While leaderboards are a straightforward ranking of NLP models, this simplicity can mask nuances in evaluation items (examples) and subjects (NLP models). Rather than replace leaderboards, we advocate a re-imagining so that they better highlight if and where progress is made. Building on educational testing, we create a Bayesian leaderboard model where latent subject skill and latent item difficulty predict correct responses. Using this model, we analyze the ranking reliability of leaderboards. Afterwards, we show the model can guide what to annotate, identify annotation errors, detect overfitting, and identify informative examples. We conclude with recommendations for future benchmark tasks.",2020,48,"145009056, 40080808, 49462969, 9051130, 3422908, 1389036863",145009056,Madrid,3,"39703662, 2254124342, 49248672",Y,"If we consider e.g., a linear 1-layer autoencoder to be equivalent to PCA (without the rnn layers), in essence this formulation is closely related to applying pca to reduce the initial dimensionality and then t-sne.;To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER.;First, the description of what is the prior used by batch normalization in section 3.3 is unsatisfactory.",Annual Meeting of the Association for Computational Linguistics,20,"leaderboards,nlp,model,models,nlp","b09139c153bac8893e8faea2b3a59159234caadc, c07802ed8a25998e9bd44ee1ddbcc63b7eb34060, bdb68c5e2369633b20e733774ac66eb4600c34d1, d2a505586c0da20752b98f63c7760b6a5c41e28d, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, cd74acb268404cde24f5131a22d04d48776b283e, 29ddc1f43f28af7c846515e32cc167bc66886d0c, 3789eb72c32ecf5e33442570358dd786dd67c8a2, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, 32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b, 5471114e37448bea2457b74894b1ecb92bbcfdf6, 8fcbd1cd1ee2211bd183b986900042470ee7f440, d93bcf0685c15c45d078eafea565969c04daccd3, a6bba5ce9867c978210e3d056691b5c1e769b760, d57f11ed40c3cdcbb36cb758191db4f2c9372965, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 1e1cf81a1113482be3f0c280db994a832cb9426a, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, 63de6db7245f634ecbef4c505099874c1ba65145, 752604994a7ca548ff2954114fc61a501d857b1c, c84389369720dcd2f004c48e58fbac2c45c8f092, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 2f44ab0e52eb98fdfc0086638887f175b6f2fe4b, 88a724083b2cfcc096448c28e6973c8f761ee463, 579476d19566efc842929ea6bdd18ab760c8cfa2, 5bedddda2aab2c5a03017d96a6dd7acf517c6961, 9a51df7eaf2457b034c49ca8ca4ca3b81e6c08c0, 84a36e19f9394f22b34f79756fa9628a795e02ea, 7884b0ec63b8a08f7cd793a989df44f6bb53116c, fc77048474ccd34c6507701591c2e6ab3ca647ef, 89858723bec341178f2b00d34ea3016baaaf71a6, 084a93c8ac0230ea9fe64d445ad1d6af5ea0b3b3, 7536bce1007a765fd097a7cc8ea62208a8c89b85, 14dd50979af27bd2574c8068db11d27028b56afd, 46c266b3d1274dacd7fce27ee8cb4d587f087a58, 2880ac931c11176aee6d42a7e7bb0703aacde3f9, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, 54a26f5ef4b0524c60249fe98d0fc3646b2791ad, fe4c5074f021cd1c810892c1b6bc267b23aa6e5c, d1efcef213c433445be56d7479eb47d972b3ee79, 1eaab9b33f1261744567455a14830e8a92796cf5, af13a92977d4f4dc5b28b13746d86111d42939e8, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, 01f0f5205d03870f172ae8f04e33356d5a0af221, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, c2528e88d5554e9df9f9d482ad46cb5331c4d794, 41d6b6bd4bf44bf13b9157b603e33360e5e77a01, d88083e37c44461ce3e404bd57257cd3edb07d4e"
c4cdf2e0d89aadb13bf9c61654ff9e17be2b01b9a,BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements,https://www.semanticscholar.org/paper/c4cdf2e0d89aadb13bf9c61654ff9e17be2b01b9,Conference,"Deep neural networks (DNNs) have progressed rapidly during the past decade and have been deployed in various real-world applications. Meanwhile, DNN models have been shown to be vulnerable to security and privacy attacks. One such attack that has attracted a great deal of attention recently is the backdoor attack. Specifically, the adversary poisons the target model’s training set to mislead any input with an added secret trigger to a target class. Previous backdoor attacks predominantly focus on computer vision (CV) applications, such as image classification. In this paper, we perform a systematic investigation of backdoor attack on NLP models, and propose BadNL, a general NLP backdoor attack framework including novel attack methods. Specifically, we propose three methods to construct triggers, namely BadChar, BadWord, and BadSentence, including basic and semantic-preserving variants. Our attacks achieve an almost perfect attack success rate with a negligible effect on the original model’s utility. For instance, using the BadChar, our backdoor attack achieves a 98.9% attack success rate with yielding a utility improvement of 1.5% on the SST-5 dataset when only poisoning 3% of the original set. Moreover, we conduct a user study to prove that our triggers can well preserve the semantics from humans perspective.",2019,124,"151257231, 66697271, 153642281, 144588806, 2118869261, 40238834, 2109716565, 2145954003",151257231,Sofia,2,"145966834, 1407546424",Y,"Considering that this is a big, complicated system, it is crucial that the authors included both an ablation experiment to see which pieces were most important, and an experiment that indicates the amount of labeled data that would be required to achieve the same results with a supervised system.;The theory is not strong and the experiments don't necessary support the intuitive claims made in the paper.",Asia-Pacific Computer Systems Architecture Conference,36,"attack,backdoor,nlp,applications,nlp","d2efa91bd7d076a66bc1d1c570f6aa2da944dd88, 6a1b25f7a67395ad1e676027322913acbb0a0635, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, 6e74403ab75d0080c056fd66702ab1f54f04d9b1, 3a083d843f891b3574494c385699c21766ce8b7a, d0ab11de3077490c80a08abd0fb8827bac84c454, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 456c011594ecacdd24298a161787389ccbe4b88b, 8674494bd7a076286b905912d26d47f7501c4046, 22c141b489e6e189f5996537b0a908fc10f90de7, a0d18dddaa995b126ad373e33767b9b881d16b2f, 785a1c77fabd1ad870a9dbc98d235d2fcbdbaa6f, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 4cd033a56b19f87f6adfefeef5fcc990306ecf40, 8babcaf89f8537dc628a029ebf932100f57289fd, eb9e0da8b7170e3ca4364f2f9010599c2d2556f1, 595101f13b961d69c553ce1ed24f60f3f1085e02, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, 4e746359afd6f81705b875d71cc499b904a320df, 915ab7b6ca3633230403d47dbb31cf74888cc5c9, 09797949fc70fbad8f3fed4f6cf4a91a9c709652, 6dc4883228c95e8a332320fcc587a0ff33c84d59, 82a09f72d7b9587e3b519b1cd9640a5a611f3480, f476d44a4892e0c8256e50e9075f8dd3d412bfcf, fb00016c1e048b9373803add001c1ec7e877cb23, 8b417c2be7a7707f372049fb1193f0d42f799562, f9253a3e2627f1c2a0a7e2cea320a4ec4e4d2ff9, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, 8d06f13ec042f0d8d845af5deadeb3dd9ad11f70, facd5f5deb152229ceb1803434d8690a09ab4129, 44786a2c2a8ba8cf5c74a8fb10098c220e924c56, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, 343500e0052eb1b683f32b00efbbd1331c94184a, 31a61d009442436d04b9d4e1c5beee37172289ae, 3adb779bb37d22e3aa299364c2a337603801ca5c, 011095a0082e5e301f9bf30267b193c1c9e7e370, c9a9517c8b867187b4f4c0c37cbc65263ea41d25, 752604994a7ca548ff2954114fc61a501d857b1c, 9a0965beef113cc37491004b1848149e00300561, 48fc9c42522184c652742255fdf31f7b9ed7ebae, 71854ff4306cf65c3c2161f7be2d0346275f72d5, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, beb890d47bbc21a96967f9993c9d6e15686b2eac, 10aa2be24951e6de76b630482a645d79354c4cde"
00cd2650a89734105fa0c0aba3bf07935b318290a,GLUECoS: An Evaluation Benchmark for Code-Switched NLP,https://www.semanticscholar.org/paper/00cd2650a89734105fa0c0aba3bf07935b318290,Conference,"Code-switching is the use of more than one language in the same conversation or utterance. Recently, multilingual contextual embedding models, trained on multiple monolingual corpora, have shown promising results on cross-lingual and multilingual tasks. We present an evaluation benchmark, GLUECoS, for code-switched languages, that spans several NLP tasks in English-Hindi and English-Spanish. Specifically, our evaluation benchmark includes Language Identification from text, POS tagging, Named Entity Recognition, Sentiment Analysis, Question Answering and a new task for code-switching, Natural Language Inference. We present results on all these tasks using cross-lingual word embedding models and multilingual models. In addition, we fine-tune multilingual models on artificially generated code-switched data. Although multilingual models perform significantly better than cross-lingual models, our results show that in most tasks, across both language pairs, multilingual models fine-tuned on code-switched data perform best, showing that multilingual models can be further optimized for code-switching tasks.",2019,112,"1452678825, 34725175, 48180698, 3010457, 143990839",1452678825,Bratislava,2,"2412941, 2115853457",Y,"In Section 3.2, the authors listed a couple of loss functions.;Motivation In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle.",Annual Meeting of the Association for Computational Linguistics,19,"models,tasks,language,nlp,nlp","cd5b4b113d5fa90b5dc5aa372111b89d18df88fb, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, 4954fa180728932959997a4768411ff9136aac81, 1452b25a7680bbb2c66dd7dfca6704292405da92, 74bc39003e65119eaa6ba339a61b45b417a638b7, 7676c02ea839ff1ceb6e5e1427c42bc45e169bde, e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, 10ea29fda06bdbe56f591909d89f3194b452ac91, 91b2b47cabd800ef658b65bfe1f52b7293a740c3, 8fcbd1cd1ee2211bd183b986900042470ee7f440, a13149a80855412d970d0de2b41c611f4cf7e1da, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, 59c414c9efb77562f5d1aad8af14eaac968c69c0, 6fed828456964d29517f6caf31b700d8aec82153, 410fba9f03212257d0881811802e6620e59bc827, 2c03df8b48bf3fa39054345bafabfeff15bfd11d, 6f75e8b61f13562237851d8119cb2f9d49e073fb, 7eaac9847257c32afd450017d1348ecda4dcaade, 3337e4b2e63e5e99171f9da9d161771f5810c27a, 9d6acac70b2d1fdb861a08b00766ef263109cd7f, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, c12b80b44d9acfe6cd92fdf965264c4b706c367c, 5e31fa4e69d1a3587230f5d134c0b7e2ed84a742, 139a0c7a60667979dcb57eae677f75ff3f0b0196, b9f190339ce0b4cdad3464c45ec3266a7369fb7c, ef25b02f3be31c699255ee05aa90a4a17461d95d, aea731e7cf33aa3d482b13f42cedbc1adb3271c6, 8713452753fd01de5616121af93e173d4f74eaf6, d84cf745c534c010b8e55e5a4a04878906848dc3, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, 742747c7a453b293352b772d0d99541c96a351c3, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 6dcb1cd576b0e54b900f45a178efe271c383de04, 8b7b2e88207fc2dd849f5d83c9b6e890f0174abc"
4954fa180728932959997a4768411ff9136aac81a,TensorFlow: A system for large-scale machine learning,https://www.semanticscholar.org/paper/4954fa180728932959997a4768411ff9136aac81,Conference,"TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous ""parameter server"" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",2015,16886,"2057642721, 144758007, 2108406634, 2545358, 36347083, 49959210, 145139947, 1780892, 2060655766, 2090818, 1942300, 3369421, 3089272, 144375552, 20154699, 32163737, 2080690, 2053781980, 47941411, 35078078, 2117163698, 2108113547",2057642721,Zagreb,2,"145071265, 1404359012",Y,Is it meaningful to perform ADD or SUBSTRACT on the leaned code?;“We can quantitatively determine how close the posterior is to a FFG distribution by comparing the Optimal FFG bound and the Optimal Flow bound.”: Why not just compare the optimal with the AIS evaluation?,USENIX Symposium on Operating Systems Design and Implementation,15,"tensorflow,machine,state,machine learning,machine learning","6ba00c2386f2edc0b43eec442cd1923b5d964633, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 696b388ee6221c6dbcfd647a06883b2bfee773d9, f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6, 00e18c603e60d861c4e99c541e4d65ef442d5945, 86d3beff240b6c882058455e098a571de86564f5, 6745a82c9236f0eec576904eb50ea700ca5a7d7c, cd2f4aaf98bb1e020cff310000c8049d3460c54e, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, 8b7b2e88207fc2dd849f5d83c9b6e890f0174abc, 97906df07855b029b7aae7c2a1c6c5e8df1d531c, f9367342405a73ab8d6de704a149babfc0edb5fe, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, 929305892d4ddae575a0fc23227a8139f7681632, 19b93280f17696a4ddfa2c75490a50ab107addf2, 697f2f3598057cd17cff7749d768cae0993c6727, 8adb47deeef943c2c1bae41f9498a382fb818a16, 25ed8ba0e8906ba98fa5d92d17a01e818796ddc9, 9e540662619327a3056d9e40bb58058868f6f805, f8e57fa370fe10147aa22714e08409fc1b7dae4b, 2c03df8b48bf3fa39054345bafabfeff15bfd11d, 916455d97cd792c2eb5b00663689592e25cbc8d8, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, f1664bbaddedea8c250873e7610ab07e53fa7132, 4f480bae3196dbbc27ab383bce33478ea963f9b3, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 834fdec542153aae5fe725df801aac87ba5e8f56, acc296f981cde8d8c205982fc4422ec35531b769, d2efa91bd7d076a66bc1d1c570f6aa2da944dd88"
f9c990b1b5724e50e5632b94fdb7484ece8a6ce7a,Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting,https://www.semanticscholar.org/paper/f9c990b1b5724e50e5632b94fdb7484ece8a6ce7,Conference,"The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.",2014,6501,"3008587, 2192200, 49528584, 1739816, 145771919, 2183294",3008587,Madrid,3,"48510386, 1825752990, 2545358",Y,"Good contribution The paper is well written, and the authors do an admirable job of motivating their primary contributions throughout the early portions of the paper.;The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models.;Inclusion of Assumption 3 would in particular require better justification.",Neural Information Processing Systems,14,"precipitation,problem,nowcasting,machine learning,machine learning","9b54941de1e21826ecc28b32730ac3f69991ede4, 83ea80a9393177ecf84928f4bd1120fb679f180c, 150f95f9c73820e0a0fa1546140e9f2bdfd25954, 4fcfe83c05402b5c5fb6e853082e74af6379d7f9, 97f456643712e9618edd7465676c62af3c8ae690, eef23d76e175c0cff8e81ffcb2721c10539c8cbd, 736ef8a32d6c5f76a21d61299300cf796480d507, f0c27af6c330d5c3b0a8eb376a69ce92c85badd7, 4dcbe71d261ba5caa0a06d5c265b5509edbbe1c4, 6fb020754f6de564c3a0a07bb656c0a90be1f87d, 329d31f881a17861eedeef6a9d8fd509cddd2b7c, a8b995f0da78a79447dfb18c2337972b044f4239, 59c414c9efb77562f5d1aad8af14eaac968c69c0, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, b4af1cbb4d3a03d3ad7930b906fc7bf6870180cc, 0a829289a16ae48837cc2905635435db98bacc76, 771a858c35f6d6e6d1017dde95368de3794738a6, e67a2817089312746d69b38ce9abfdc4b1bc69c3, c3df199cbca74763c4ae9889409bbd4aa29b6255, cc017a62c605a0749e35a1264a46d62e78fb68b7, 046eb47d56beb8069b0098e3d01608f81ebb6849, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 72afe82af4c2ca100c36eb35292e85d806527f0a, b69a35662a2cac38eab22f4481285116bdf8c30e, 3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b"
53b047e503f4c24602f376a774d653f7ed56c024a,Practical Black-Box Attacks against Machine Learning,https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024,Conference,"Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",2015,3188,"1967156, 144061974, 153440022, 1680133, 144643812, 144231976",1967156,Bucharest,2,"2057642721, 1829303908",Y,This paper introduces a method for regularizing the REINFORCE algorithm by keeping around a small set of known high-quality samples as part of the sample set when performing stochastic gradient estimation.;1. An important point regarding the reference ambiguity problem and eq.,ACM Asia Conference on Computer and Communications Security,15,"dnn,examples,attack,machine learning,machine learning","07b01d665646009439ca206378cc35e095ec6cd2, 0f38b3d717e0fcc6eacc9c6e78b252227440e04e, f69d06037134ab6fb65d90f5ac192cf9f55e498d, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, ba9b6f805feb62c978d384211f910790643a023e, 27245e65a27bde90b5b0bb25d157bb75a0ad8b5a, be383c607d4d357c763d2329ab71799c6e1393b4, 9312e5efa0dcef1445d45a41771f12e2a8dc6715, b795c74a0150ec091003ffbaa5bd7d74487c137b, 4dcbe71d261ba5caa0a06d5c265b5509edbbe1c4, 00d1f3423a33f73ca6aee884a58834547475d2f0, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, 97906df07855b029b7aae7c2a1c6c5e8df1d531c, 4b991efaa8493a5925c2aee9eec980831213eba6, bf69c98fca9a9f6c1cde871beddbcdc668b77771, f0c27af6c330d5c3b0a8eb376a69ce92c85badd7, 7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7, a0367346bc355c36badec2d2c47ce55a320cd75e"
e8b30ebe3351680c3b039555ae0a8d0865ad829ba,Neural Networks And Machine Learning,https://www.semanticscholar.org/paper/e8b30ebe3351680c3b039555ae0a8d0865ad829b,Conference,"Recent years have seen an increase in the popularity of neural network (NN) research. The mammalian brain, which consists of billions of interconnected neurons, is renowned for its ability to perform computationally hard tasks, such as face recognition, body motion planning, and muscle activity control. In an effort to emulate the effectiveness of biological neural networks in learning, artificial neural networks (ANNs) were developed. The NN technique has been the topic of many studies over the last few decades, with applications in many fields including control engineering, automation, aerospace, psychology, economics, healthcare, and energy science. The objective of the discipline of machine learning is to create computers that can independently learn and improve. In this chapter, we have attempted to depict the types of neural networks and machine learning as well as their applications in different industrial disciplines such as science, commerce, and medicine.",2022,76,"102998577, 2215913005, 2215954607, 2215958663, 116180161, 2275394607, 70310638",102998577,Vilnius,2,"2061881331, 2074432",Y,"* There are many estimators for f-divergences (like the ones cited above and many others based e.g. on nearest-neighbors) that are sample-based and thus correspond to the ""implicit"" case that the authors discuss.;After equation 5, the authors suggest categorical loss for discrete problems, but cross-entropy loss might work better.","2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)",4,"networks,control,applications,machine learning,machine learning","be082d70534db088315f2cc5b42c2fdcd58c1b8c, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 5687c9e8da574453fd873662b95caec70dac9d1e, 5030702fea15d66a73fc997325431f1d7945ad9a, 624b2f14be4287d6a400cdf88a6f911b434b182e, 047286f5b9315a8e8bf56c4fc936e62f21495892, 60119658af638693f6de23d8466968e60c428ac7, 76b93abde3ec4e5bd84a38d488a8db209dc4ac98, a0a79dad89857a96f8f71b14238e5237cbfc4787, b080d072cfde697180db3234da08903c092e72c3, e5a1cfcd07dcfd8b1feec9c635dadc858cde8166, b2563d102456d5140ecb4111e7f08481f720d9a4, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, e248993daede136713e93929816df92b48ccddfb, f406aceba4f29cc7cfbe7edb2f52f01374486589, f9163156eeba67762a7441db48fe6720106137cd, 1ed33896e82cc3810b349cdffc2039f0b8edd82e, c292e473b3825eeb9db03c70b2e1c033aea190d5, 957f5b1e7ca48891c2e279aefbfa0f04d989c21e, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, f8e57fa370fe10147aa22714e08409fc1b7dae4b, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, 37b0a7a6c8fb26e32b9206847e78d521a2cd5900, 9d6acac70b2d1fdb861a08b00766ef263109cd7f, 2743e66939b30c43affb3c9e31f20cfac2109045, 02fa2389b1b64b661192e224bed8af6df0ce80f6, b894e52c341f293f93a5cbf496015ca0d4ed3e50, 1f8a23697562b001082b147779b5eaefd3513d0a, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, 6dd3e404aac097f63d42dc19fd08c7ec281f90b4, 787ae2c51cd82b904bb4fb9ccb15266381af5436, b7f5973dbf2ef76fcc3aea616a7f72ca79f09020, d86084808994ac54ef4840ae65295f3c0ec4decd, 1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, e75cb933d387ecb184010ff07d0ee43fc1750e2a, 643da4c4de1954daeac571a82367241db012a8bf, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, 6422f4b9e3bedf585170bffc7105ffe2061e87ae, 155f27879f185f1ab04107c91c2ae7cf6a910a03, 92912dd895c360f01a6be9c9f6d207642139525e, c2aab470b8cf92f090e0a3bac1794b21500585e6, 6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2, 746a81aa26d3ebfb81acfd6af958d6a21603cd21, 8c33ca066e2ab615e24c65198c794114436053dd, 0c00a328fa7cd56ee60338c54e89bd48310db80b, 8799ec4bdf98bc313aa8c41a706a385e026d1f88, e2a58fd18961c3941102989e3a3d0d27c615e015"
9583ac53a19cdf0db81fef6eb0b63e66adbe2324a,Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent,https://www.semanticscholar.org/paper/9583ac53a19cdf0db81fef6eb0b63e66adbe2324,Conference,"We study the resilience to Byzantine failures of distributed implementations of Stochastic Gradient Descent (SGD). So far, distributed machine learning frameworks have largely ignored the possibility of failures, especially arbitrary (i.e., Byzantine) ones. Causes of failures include software bugs, network asynchrony, biases in local datasets, as well as attackers trying to compromise the entire system. Assuming a set of n workers, up to f being Byzantine, we ask how resilient can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient aggregation rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the aggregation rule capturing the basic requirements to guarantee convergence despite f Byzantine workers. We propose Krum, an aggregation rule that satisfies our resilience property, which we argue is the first provably Byzantine-resilient algorithm for distributed SGD. We also report on experimental evaluations of Krum.",2016,1176,"3094352, 9623412, 1727558, 1718150",3094352,Amsterdam,3,"2089990776, 2248549493, 2115692258",Y,"- In Section 4.3, why did you consider the entropy regularizer?;-  Please also discuss the relationships, connections, and possible applications of your technique to other algorithms used in Bayesian optimization, active learning and/or sequential learning, for instance as M. U. Gutmann and J. Corander, “Bayesian optimization for likelihood-free inference of simulator-based statistical mod- els,” Journal of Machine Learning Research, vol.;The authors of this paper propose to use a deterministic policy instead, and apply the deterministic policy gradient DPG (Silver et al., 2014) for optimizing the behavior policy.",Neural Information Processing Systems,16,"resilience,failures,sgd,machine learning,machine learning","b6593b5ad6b0e00232ee88cf2bc578645c1299a0, 18d87bff073687c025f9bd23ab2dfb20d5f72a66, be082d70534db088315f2cc5b42c2fdcd58c1b8c, 99ae62cca0b15275d9ed1528f345f9dcefe50dd7, fdc57c18f3b636c3273542327ae540217972558f, 0a92bc2dc8a216e6aced83edc0358241066833df, 5471114e37448bea2457b74894b1ecb92bbcfdf6, 0e9a44ce661c3535d5ce747912540080324489f5, e968ae8e98fff9e28468383a1826fca4a2ae5245, e24b8a9531573d284647239affc6c855505b0de4, be7cb8f79bc018e57467168fc0c7f8ad59bba04f, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, b977e8de38dc0d13817bca1ed20036badfe2a58c, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, 12c9bcf710d30ba991fb765ace07f177f53ecfd9, 6fb020754f6de564c3a0a07bb656c0a90be1f87d, fe4c5074f021cd1c810892c1b6bc267b23aa6e5c, 08764019e9762da527253b37b0ff39c46a4206b7, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, ce157cea880c9ab64de64f11a531202f5348fa05, 742747c7a453b293352b772d0d99541c96a351c3, 417326e51d78ba8bd2621f23e539b41bbdd336d6, 00d1f3423a33f73ca6aee884a58834547475d2f0, dbabab9bf5955558f73a37644f4bb626106a6d73, 52e510271b172d098ec9b107a4159216ec08527e, a56bf7ee9a56d8f84079684339a953c2df9ce76b, 7d873a9c49d3864709aa762f8740edcdbd7369c5, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, b9b639522465cc606df878eee62e7f9c4bf19e62, 7bb477077968d68aa7a6059d8d6d801fb28274da, 1ff76ab0fcf22110df62337d462e15d79a2a2593, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, 634fe61f3ab6692ea4733989a1c76e793bb8b69e, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, b61b260de1599e6e89491cad9160898fcd3b34c2, fa26a6d434450b185e669170e79fd3e1d29716bf, 2e965b5d97c2d6fb4af284307735be39283792ba, 224c11bc51b4959bc787d6681c2b152468294b11"
f70b2f20be241f445a61f33c4b8e76e554760340a,Software Engineering for Machine Learning: A Case Study,https://www.semanticscholar.org/paper/f70b2f20be241f445a61f33c4b8e76e554760340,Conference,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",2018,616,"1719124, 1776779, 145193818, 1710751, 50355692, 1783184, 1693689, 2571049, 143609903",1719124,Skopje,2,"2112455515, 144027436",Y,"This would contradict some previously established convergence results for this type of problems: Reddi et al. (2016) Stochastic Variance Reduction for Nonconvex Optimization, ICML and Wang et al. 2013.;But in that case much of the theoretical story goes out the window.",2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),40,"software,teams,microsoft,machine learning,machine learning","ba9b6f805feb62c978d384211f910790643a023e, 8713452753fd01de5616121af93e173d4f74eaf6, 8c33ca066e2ab615e24c65198c794114436053dd, 6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2, 0090023afc66cd2741568599057f4e82b566137c, 48c73c389c3f36d70407eb8309a0b41578c15fc8, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, d008893e01fa7f6c5fb01dadf3f97ee96835c303, 22ebfc211d184ed615729378a43fde175bf14478, 70ca38ad480c0be0eca89ccc4972d6cc9a5824da, be383c607d4d357c763d2329ab71799c6e1393b4, 32524aa3ae8522542753ed7e6f4cca3970e4acab, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 0a829289a16ae48837cc2905635435db98bacc76, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, e02a757617c2c42eb62889cc4d4aee3765928303, 59c414c9efb77562f5d1aad8af14eaac968c69c0, af9280741ef627f0d6c8437605d002d3bfc2d1b1, d6f002d88638de71114dab083f0ea8ceea6b6a5a, b09139c153bac8893e8faea2b3a59159234caadc, ff74bfbd9ebf4c54809873aecb04be27e9402cb8, 9727206903eb40d4fa42606711bad3402f2ba9aa, c2413fa296543159b32d16350d9e29f7db528790, a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, 0894585294c67193ff3190240554677b56fd79a0, 448959eff044f02040ded5afd483b7c4e811b0ac, 579476d19566efc842929ea6bdd18ab760c8cfa2, f9163156eeba67762a7441db48fe6720106137cd, b9b639522465cc606df878eee62e7f9c4bf19e62, 6fb020754f6de564c3a0a07bb656c0a90be1f87d, 0d065e8688c38bb0148203a1738f47184a5b58d3, 5290d7921f0266c8b50b79fc8a0b7d22868f4f60, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, 2019cf49b51021a376f9833a53565513f0d8107b"
b7a717233ec3ff37385ab1b06816d0ca375f5bb3a,Data Shapley: Equitable Valuation of Data for Machine Learning,https://www.semanticscholar.org/paper/b7a717233ec3ff37385ab1b06816d0ca375f5bb3,Conference,"As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on $n$ data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.",2018,515,"27316199, 145085305",27316199,Budapest,3,"1793506, 1702822, 2362078",Y,"Additional regularization terms are also added to encourage the model to encode longer term dependencies in its latent distributions.;The current analysis is too simple.;A particularly interesting question would be whether the proposed model actually is a direct GAN-based extension of IBFA, and if not then how does it differ.",International Conference on Machine Learning,18,"data,shapley,value,machine learning,machine learning","0599f45e03ac2016321df0dd653ba4c0034c79d5, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, 92912dd895c360f01a6be9c9f6d207642139525e, 7536bce1007a765fd097a7cc8ea62208a8c89b85, 785a1c77fabd1ad870a9dbc98d235d2fcbdbaa6f, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, a80e26e6365b215715c182d19a9aa8bb876ac768, 8894d431a768a35dc7ca4d762ebdba4f407b978c, 6be56f559a74c0124526242e70cbdfd16cbc60a7, e2a58fd18961c3941102989e3a3d0d27c615e015, 7ae2783a9196fb4bc2a610ae812d19722daddce5, ccca203382e5dd198c089a0f1d7af7bef0f694e9, 84a36e19f9394f22b34f79756fa9628a795e02ea, 77e6c9917536949a82e5ca02c4882b69ee8a4fd6, af13a92977d4f4dc5b28b13746d86111d42939e8, a8b995f0da78a79447dfb18c2337972b044f4239, cf41991d89301c3c12420d150792cb1163999962, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, 8d0f755dea90f35f4b126a01fa3cce96b3bdd344, 57f5bf937f7393b691428747a9078d3124e6bcce, 6f75e8b61f13562237851d8119cb2f9d49e073fb, a1ef4052acb63356928bb440874c470ad48cb40c, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, fe44200fed05f9a7c656f2245deded8fd5f5e1e6, addae423490bbe82da4fb2fc265237178686b4e8, cde39ce861e4c7514ee07fd91b6b8aac50cbf01b, d617f51833860dc50d202af7f80be71304b2e994, aa6c2afadd660fe4efbac699f7854e8f6f240c38, 256db9dba1978f004a67c86ffc321563b1aee79a, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2, dbabab9bf5955558f73a37644f4bb626106a6d73, 696b388ee6221c6dbcfd647a06883b2bfee773d9, e373cbce1a831361a8de9dbc625727169011bcce, 78aab73ed574393ab421f25b3a0e3f7343e64748, e75cb933d387ecb184010ff07d0ee43fc1750e2a, 5f51d468ce730eeade7e9f419a1fe7152582be25, da8b317b99c4b8933b2c59285639eca6c3fcb869, fa63c3f53413ced7946623889c416e34a28676ea, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 5d433da6d0f143f20936379910104d2bb139d4ae, 8e259f940f007e08207ddb7c3a052f52036d7bf6, 456c011594ecacdd24298a161787389ccbe4b88b, 595101f13b961d69c553ce1ed24f60f3f1085e02, 5cb8f417d171ae329adf446820bd32d8b49d8c04, 8674494bd7a076286b905912d26d47f7501c4046"
8f9e864fab09bbae4a46a2a62bb954db1a88eb3ea,Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts,https://www.semanticscholar.org/paper/8f9e864fab09bbae4a46a2a62bb954db1a88eb3e,Conference,"Pre-trained large language models (“LLMs”) like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (“prompting”) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.",2022,177,"2138715050, 9063054, 2200271412, 152290618",2138715050,Vienna,2,"144330671, 2154742781",Y,"Perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the E-step output in EM for a GMM.;Section 5.3: Amortization error is an important contributor to the slack in the ELBO on MNIST, and the dominant contributor on the more complicated Fashion MNIST dataset.",International Conference on Human Factors in Computing Systems,22,"design,language,interactions,llm,llm","b01ad71bd376f36546f02204784908da9577bb0b, c665003881c3c35589d1e48da1ee7234b48f2ac8, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, b6593b5ad6b0e00232ee88cf2bc578645c1299a0, e2a58fd18961c3941102989e3a3d0d27c615e015, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, ead6121fbc787d508dc6a6d7106f72bf0d647d03, e2a85a6766b982ff7c8980e57ca6342d22493827, 364128bcce9836d60e685bb717b80f30e25092e0, febe776e285dc5e72c7e3ee697a87a794e1c00ff, 5406e153957dd7a165264da6e6e5d81251997404, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, 99f06e88e76f1af51d08d7adfb26d758ebc6acab, e3052ebca5eeae6a8a73e44517903d39746f5f3a, 046eb47d56beb8069b0098e3d01608f81ebb6849, 011095a0082e5e301f9bf30267b193c1c9e7e370, 7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7, d6f002d88638de71114dab083f0ea8ceea6b6a5a, e359e8960b0b09e8685a32927b7818f4b06ef881, 3994334c81478a4b17341eb1f494dbccbb73d999, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, c2aab470b8cf92f090e0a3bac1794b21500585e6, c419ee7315b9edfd8fc55bab16534fc55a564fcd, 795550a5294eb05ea4f3b14f0b1c21a405493d85, 5471114e37448bea2457b74894b1ecb92bbcfdf6, 43eea2a73997294193228d50f9ff25fc5345664b, 447884e7da189102189a156966623335c72199b0, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, e5194ae88d63c7549678b1b73cfdaf7112164272, 3dd7f7118ee174265889d00d100cfe2a02871be8, ce212cb873a54e5716da53a66b10298ac013008a, b05306f0b142e5afb3974b1b79996e5b82653662"
f406aceba4f29cc7cfbe7edb2f52f01374486589a,The Internal State of an LLM Knows When its Lying,https://www.semanticscholar.org/paper/f406aceba4f29cc7cfbe7edb2f52f01374486589,Conference,"While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\% to 83\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.",2022,102,"1746466, 144135485",1746466,Belgrade,2,"3011964, 1388622435",Y,"* Figure 2 seems like a test made to work for this method and does not add much to the paper.;For example, the authors claimed in Section 4.3 that the Stein gradient estimator is faster than other methods, but it is not clear as to why this is the case.",Conference on Empirical Methods in Natural Language Processing,22,"llm,sentence,probability,statements,llm","d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b, 08aaaf7ae3d61875e7bcf5c1bb0df4f17066e300, a781cea542e99c6bb9422858e7c04eaef18c7673, c15f30a3e84910a28cc560e7db097fd99339e8c1, 28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8, 10ea29fda06bdbe56f591909d89f3194b452ac91, 3dd7f7118ee174265889d00d100cfe2a02871be8, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, ca011427853d34ce4ec9ccafde8a70c9eacc3e21, f9163156eeba67762a7441db48fe6720106137cd, eed62d36d1b976ac3873c83645f1c25f5096f89c, b781fb7f3725a9d899d3d250b378d729a8a00442, 0c00a328fa7cd56ee60338c54e89bd48310db80b, cc78babfacce48e715dac56886d7dd9746cfcab0, ee6f23590783adec7cf6b2030c6a46f3117a708e, a9cbbef8f4426329d0687025b34287c35bdd8b38, 72a6a5d5864eb915231c7128b90977f1d2acf6e9, 8ba8a0d18a06752f5a39996ccf1e914da0941443, c997ed5ac5772bb0fce8bb11b8c86bc33310f39a, cb12fe714dd4fa1d14b6a8023d494c14c89e90ea, bb5d26da72bfe7030dbc6650b686b210ae661f2c, e2a58fd18961c3941102989e3a3d0d27c615e015, 82870bc488b57cdf5ea62877109a7278af2926b3, ce7499d6862df8269c655220049c3ed20b9b6f5e, 14fe35149aed6a47b6ebfd207deb7681b9446bb6, 57a3fc6d0aaad3c10c793b4e59390ca04c935282, 811df72e210e20de99719539505da54762a11c6d, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, b61b260de1599e6e89491cad9160898fcd3b34c2, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d, f2bc4057e696f49c326bf8e1588772a16f053754, 929305892d4ddae575a0fc23227a8139f7681632, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 00cd2650a89734105fa0c0aba3bf07935b318290, 20a3ed03888037e2802fa9abad02ffa0a8dcc228, cd20ed8e70f6459f5617350f6b84c5c6d5929cb7, 21042565ec941f4fa31ac5a0af85a1a84ff21f1b, 1051abf1e3dae90241ad15b3f98f2e41197ee611, be383c607d4d357c763d2329ab71799c6e1393b4, 57af930b57886186695f279efa03adaf272c4de1, 70ca38ad480c0be0eca89ccc4972d6cc9a5824da, a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854"
bdb68c5e2369633b20e733774ac66eb4600c34d1a,LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,https://www.semanticscholar.org/paper/bdb68c5e2369633b20e733774ac66eb4600c34d1,Conference,"The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on both reasoning tasks.",2022,61,"1557412457, 2150277971, 145131956, 2143418409, 2212836814, 38656724, 1996394, 1746416",1557412457,Ljubljana,2,"2921637, 2061202877",Y,"These structures are extrapolated  by;(muscle routing parameters, including insertion and attachment points) are optimized along with the control).",Conference on Empirical Methods in Natural Language Processing,22,"llms,methods,peft,llm,llm","156609022dd6258c60238859622da0a1683bd062, 697f2f3598057cd17cff7749d768cae0993c6727, 087dd95e13efd47aef2a6582e6801b39fc0f83d8, eacf9284a39adcd56172665f31fd5a72560bba7a, 40416ac3bf78583eea37661b1b446e9939245b3e, d235a9085e0543fcbe502fbc269f9a8ee01dcbab, 2e965b5d97c2d6fb4af284307735be39283792ba, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, a81ba6a07bf7a2ecff871e3362a77404501d0927, 70ca38ad480c0be0eca89ccc4972d6cc9a5824da, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91, 752604994a7ca548ff2954114fc61a501d857b1c, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, 3705919b880f4f8dc37483a704e14dd078cb9ac4, 322d91190acd8ac8c64598f5126947b0485ba249, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, 3f43bcb910df8c1a76de79057a63195e6c6bc258, f2bc4057e696f49c326bf8e1588772a16f053754, d88c1255876b62fb5f5a8b292098ca430710a540, 07cca761749bfe21c2d096ff60f32b574d5c84c4, 993df7df129f8d18816877d69923d7df7b347d85, 9e540662619327a3056d9e40bb58058868f6f805, eadb1e7da375939e25083ae3936c4f4ef1f2a719, 649c3497e3b34b15a5011259fcb837cf6c1ac04a, 150f95f9c73820e0a0fa1546140e9f2bdfd25954, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, f72053903270d9a7f41108461ad04d5aa075218d, 780c725848aac1118d00c8bb306719ec803369cd, 8f2cf30f9c825d8ef7d622601dbd525ace95e025, f4cfc7cbad257f1688772d59f694c16189dba811, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 71f3bfea47b96fbab78a3439ee819cd92a33329d, 6628f9ee35e36cdfdcac8a46cef4dba8d529a83b, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, e24b8a9531573d284647239affc6c855505b0de4, df7336844a31165db0ae08f1cd0f560c9e3faeea, 740b5dec469e6b54e5e191a577a9b6a6ad8562e3, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, 2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c, c96fc88631f2b8e2fe192027a8a237445635328c, f6dab84c2c00ab92d8ee9d9359d7e530512114f9, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, c292e473b3825eeb9db03c70b2e1c033aea190d5"
993df7df129f8d18816877d69923d7df7b347d85a,LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion,https://www.semanticscholar.org/paper/993df7df129f8d18816877d69923d7df7b347d85,Conference,"We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",2022,46,"2197076899, 1384550891, 51583409",2197076899,Madrid,2,"1390140002, 145843448",Y,"However, evaluating on simple datasets like Kaggle cat/dog and Oxford Flowers diminishes the value of the paper.;Pros: - A new GAIL formulation for saving on interaction data.",Annual Meeting of the Association for Computational Linguistics,22,"llms,pairranker,llmblender,llm,llm","c9f320789e98d2c7a798a9705e26dbe317677966, e70ddfb04969b663ef8cd711a70a0acf563d6e5f, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, 72a6a5d5864eb915231c7128b90977f1d2acf6e9, f2bc4057e696f49c326bf8e1588772a16f053754, e7f3478fd8aac6940a4bf4f5eb60ac38f6b0b85b, 1c748f86182a62d44d5b44316db510f8d833e19f, 4b9184937da308914b9e13c43bfd75845eaf910b, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, 643da4c4de1954daeac571a82367241db012a8bf, 8b28792f8405b737229afb92c99c579b86d8aa98, b135e330cc1473c8c24fa63bb9a5b64f51993f9e, c24d47ff95cd4bda073c75ec24ececaa3b10c995, 40416ac3bf78583eea37661b1b446e9939245b3e, b09139c153bac8893e8faea2b3a59159234caadc, 3a58efcc4558727cc5c131c44923635da4524f33, 6548106035c7208ad498730627874a482734b9ac, e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0, 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d, be383c607d4d357c763d2329ab71799c6e1393b4, 1a4c6856292b8c64d19a812a77f0aa6fd47cb96c, 6eb8caebbffc7e5b301b66dc36acad46b4dca5c9, db6084fdb3baceddacdc726474722debe1ef7e65, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, f5ac3ec506a5a01807dd7196c0c52eef008b78cc, cd5b4b113d5fa90b5dc5aa372111b89d18df88fb, 447884e7da189102189a156966623335c72199b0, d6bc29a897fd85e7187dc33c3c974b8879462237, b7a717233ec3ff37385ab1b06816d0ca375f5bb3, 013eb12ce5468f79d58bf859653f4929c5a2bd14, 5c39e37022661f81f79e481240ed9b175dec6513, cb3968152f7d93f53d24b00279a90d5071ddc85a, e89c37b7c2ff465db43c4b9f674867ec4b98aa8b, b9c974380649749320f4a02e33b2e5014e7f1756, 91d24a94a276f5b226b07fe294561482b1d4104d, 5778e56400f7113c2b1355fdbd6b638fa379885f, 808e9ce4e86e79098edea7f00b5b91663b87a5e6, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, 244e08e109f6f4fa0f846977e1aef1e7b6a9e816, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 9c1265fd47df8d825f0ddd6d9ab834002592e38e, 965359b3008ab50dd04e171551220ec0e7f83aba"
6628f9ee35e36cdfdcac8a46cef4dba8d529a83ba,Character-LLM: A Trainable Agent for Role-Playing,https://www.semanticscholar.org/paper/6628f9ee35e36cdfdcac8a46cef4dba8d529a83b,Conference,"Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents \textit{memorize} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.",2022,32,"95329799, 2107897400, 2087363104, 2258552414",95329799,Belgrade,3,"1684745, 2217487123, 2061706386",Y,"Preliminary experimental results are reported which demonstrate that ISRLU can perform similar to ELU.;3. Some of the architectural choices (the one derived from ""shortcut problem"") are barely explained or looked into.;The interpolation results of Figure 11 are also underwhelming, though possibly mostly because the reconstructions are in general not great.",Conference on Empirical Methods in Natural Language Processing,22,"llms,agents,models,llm,llm","71a85e735a3686bef8cce3725ae5ba82e2cabb1b, 549e933821fdf7cd0309dacaae99c8284cbfcc24, 78aab73ed574393ab421f25b3a0e3f7343e64748, 75c364909914f17791837ec88090262aa6656d3e, a9cbbef8f4426329d0687025b34287c35bdd8b38, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, 51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df, 208ac1f2ec9bf367a9981fedb6d9ea6aa9889099, 96023195e889fc258e6ff30aa99d250982dfae01, 2ed691a353fa48403d493ab658f5f267a42f0bf1, e75cb933d387ecb184010ff07d0ee43fc1750e2a, 980858461df7c4349f17b427686c5bcbcffbdc04, cf523942d56e90db182c5788845f6502da9a307d, 66d41e0f894dda2c37dd5bacbdd7bfd418e3350f, 6dd3e404aac097f63d42dc19fd08c7ec281f90b4, 9a51df7eaf2457b034c49ca8ca4ca3b81e6c08c0, 537f5e8e4139392cd2d108f32495e5b2b80151ac, 53b0cfa2eaaa9703df02b2ca9c43260c4de5df0a, 4267178106cef2e77284bde309dfaaf9fd46a91b, 94de6bab96ad533169dbc3bf9e6557581e59cb6f, 02a1e8e77f501675945890df45fbdc11726cb0ba, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, addae423490bbe82da4fb2fc265237178686b4e8, a81ba6a07bf7a2ecff871e3362a77404501d0927, 99ae62cca0b15275d9ed1528f345f9dcefe50dd7, 83ea80a9393177ecf84928f4bd1120fb679f180c, 139a0c7a60667979dcb57eae677f75ff3f0b0196, f63e917638553414526a0cc8550de4ad2d83fe7a, 736ef8a32d6c5f76a21d61299300cf796480d507, fafa541419b3756968fe5b3156c6f0257cb29c23, c50a909e20bd07f4aea09dc6dae539b45b406a96, 41d04aa3c25dcfbf1b44ce666c48759e03c216c7"
30cc95639cffca4ffa8c0eafbc502636c0c88fa5a,BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions,https://www.semanticscholar.org/paper/30cc95639cffca4ffa8c0eafbc502636c0c88fa5,Conference,"Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process. Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and in undertaking general (not particularly text-rich) VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved 17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME), comparing to our baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence. To demonstrate the broad industry applications enabled by BLIVA, we evaluate the model using a new dataset comprising YouTube thumbnails paired with question-answer sets across 11 diverse categories. For researchers interested in further exploration, our code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.",2022,29,"2231783956, 48615440, 89843190, 1956819, 8157619, 91444480",2231783956,Vilnius,2,"31347453, 5478513",Y,"3. Some of the architectural choices (the one derived from ""shortcut problem"") are barely explained or looked into.;The experimental results are very good and give strong support for the proposed normalization.",AAAI Conference on Artificial Intelligence,22,"bliva,models,embeddings,llm,llm","f156ecbbb9243522275490d698c6825f4d2e01af, a926093ef103c02fd5ef7310e0d41b83d1958ed5, 6068d39e92aef1bb0e1291e9931894c35692a85e, ab606e9d148458f6d54e5d44abefd73b7990f6e0, 69d49a06f09cf934310ccbf3bb2a360fa719272d, 0311ace1d499cadd1cc0c515a625d1d045f60d25, fdc57c18f3b636c3273542327ae540217972558f, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, 60119658af638693f6de23d8466968e60c428ac7, 215fc60307f741b9db059204e41db8bfb879e606, 047286f5b9315a8e8bf56c4fc936e62f21495892, a357f1ff27e184d9a5ef69e665e8ca891032bf71, ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee, e373cbce1a831361a8de9dbc625727169011bcce, 94214d6d922ce095719d488642cbcc75dc52f273, 410fba9f03212257d0881811802e6620e59bc827, 627d7d631fd4e0e2179f82199f014deb7ff0ea0b, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, 9954ec0a95fc0840f7712d7b0c67b242bb536a9e, f0f1627db35b4942e0f83069f20dd0948fc35d28, b781fb7f3725a9d899d3d250b378d729a8a00442, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 41d6b6bd4bf44bf13b9157b603e33360e5e77a01, 23466d271676ae467cbe85bb1993682f3502e840, c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b, 83ea80a9393177ecf84928f4bd1120fb679f180c, f295157f37cfb43cd8d8d2690ea124edc5ea59c2, b6becea767675ea6ee43c78ce747077a5050019c, dcbaf58b16ac7ef947879ea37c021466357b291a, 3b87dafd5a412e25e06761f181ec199ca88a7398, 11be2469ab1d1c508e7b6e14148990741ba87884, b69a35662a2cac38eab22f4481285116bdf8c30e, 6dd3e404aac097f63d42dc19fd08c7ec281f90b4, cc017a62c605a0749e35a1264a46d62e78fb68b7, 10b4b926904ad153f791ec680218e1610747a0c8, 287ba5bf00d96af1596aaf80c178392a9c4fcc28"
5e4597eb21a393b23e473cf66cb5ae8b27cab03ea,ExpeL: LLM Agents Are Experiential Learners,https://www.semanticscholar.org/paper/5e4597eb21a393b23e473cf66cb5ae8b27cab03e,Conference,"The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.",2022,41,"2136104377, 46307329, 2232847317, 2036238525, 1679704, 2115218570",2136104377,San Marino,2,"145169163, 2108485135",Y,"Second, it is also unclear what the method to generate train/dev/test data is.;2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing.",AAAI Conference on Artificial Intelligence,22,"agent,tasks,experiences,llm,llm","acc296f981cde8d8c205982fc4422ec35531b769, f7b69b8babfa607f2e8b0371f24cb720a7a827d6, 10d89b13a6309a531c35701d37d3bd76a27a3942, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, 84a36e19f9394f22b34f79756fa9628a795e02ea, 71f3bfea47b96fbab78a3439ee819cd92a33329d, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 733fc094e785724621c46e20db1be69f132ad9df, 02b1607af35b48f0bd716367caf6a7428b969369, c30c0092bf4eb8a44faec3fc60cdd5006276bcdc, dad8965c5a4c0a0ea1eb3837c6a9c3b42597c2ce, e576a2d97950b1f6831f88575dd3f370053f6af7, 56266342b01a4f2ddc28a1e8401dbbad105736a5, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, f476d44a4892e0c8256e50e9075f8dd3d412bfcf, 3a58efcc4558727cc5c131c44923635da4524f33, 14dd50979af27bd2574c8068db11d27028b56afd"
b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283a,NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails,https://www.semanticscholar.org/paper/b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283,Conference,"NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable. Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.",2022,20,"2796756, 2137374228, 1602996186, 2258715782, 2258785611",2796756,Oslo,2,"2158995823, 2818166",Y,Why will D answer negatively (or positively) on this example ?;Is the time budget different for each new generated environment?,Conference on Empirical Methods in Natural Language Processing,22,"llm,rails,nemo,guardrails,llm","e248993daede136713e93929816df92b48ccddfb, fa63c3f53413ced7946623889c416e34a28676ea, 1d5adacc5d4d226e76c35bf19018f9e76759f127, baafed5f8968118af04dbbb1cf172f1c10bede25, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 92afbbe41174a545f9da9992e33c9a9592e529aa, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, 208ac1f2ec9bf367a9981fedb6d9ea6aa9889099, a8c1ed061813f832358c1aabf5d171bab80203bf, 0ac1a1ccb16c8441f42ff8de743fc93f0e08c15c, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, 3cfe075af77bf0364e6ddecb3d223960d06e8927, 30f233eecca2239ee1dd754914324092e53f8f19, 8d67b76222d84dcd337b8a2c78f13837070a79ce, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, 53b047e503f4c24602f376a774d653f7ed56c024, 72243bfa618fd6c3c01bb7b4885a4a456b2a6071, d7b820af40a9e2660ef700d39f7b2e27b43435c5, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, 0090023afc66cd2741568599057f4e82b566137c, 8ba8a0d18a06752f5a39996ccf1e914da0941443, 697f2f3598057cd17cff7749d768cae0993c6727, 6a261e1e38506b0e4c113ba29a2d5e5d0709ed26, 43eea2a73997294193228d50f9ff25fc5345664b, ea160adc0d78e54669281b8b145bcd832e648fee, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, 1cff064f815111a71a98afda7aee1867ad617901, 9d6acac70b2d1fdb861a08b00766ef263109cd7f, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, 13c4e5a6122f3fa2663f63e49537091da6532f35, ca05ca5f70616456d0f2e05c4a51cbf8b8d273f7, 447884e7da189102189a156966623335c72199b0, f397b593de771752e7002a954eb531f3ef6a975e, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 75ea299834d6949e89e91d006677343ddab44e49, e75cb933d387ecb184010ff07d0ee43fc1750e2a, 35adeef964fd344288febc7def2780007587724f, 1b0aa15937fdf59103a5213bccf09cff83d0ee3e, 7af07490da518c8ef3cf2ae106071df2c2d0101e, 6068d39e92aef1bb0e1291e9931894c35692a85e, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, 33e332837e91c1048c3ed165cd16bf7607c3bf06, 6aca07154c111f1c8738347d7112cad6b0bf974a, 627d7d631fd4e0e2179f82199f014deb7ff0ea0b, dde4abfd83fb61a794bd6c7d6e1991a67467c7ee, 3ea34401909978d3d3d0c25c8746e02c7d2a7c77, 5c45a5d05ac564adb67811eeb9d41d6460c70135"
66d41e0f894dda2c37dd5bacbdd7bfd418e3350fa,VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View,https://www.semanticscholar.org/paper/66d41e0f894dda2c37dd5bacbdd7bfd418e3350f,Conference,"Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation (VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that VELMA is able to successfully follow navigation instructions in Street View with only two in-context examples. We further finetune the LLM agent on a few thousand examples and achieve around 25% relative improvement in task completion over the previous state-of-the-art for two datasets.",2022,17,"47947548, 51439692, 2168285733, 41020222, 3289329, 1682479",47947548,Rome,2,"2115263944, 1720266",Y,"Paper of broad interest for control tasks This is a well written paper, very nice work.;It builds on the previous work of [Finn et al. 2016] that uses a neural network to predict transformation parameters of an affine image transformation for future frame prediction, an idea akin to the Spatial Transformer Network paper of [Jaderberg et al., 2015].",AAAI Conference on Artificial Intelligence,22,"navigation,agent,instructions,llm,llm","5bc511aa30f72720260d792e57537379fb04c395, 5c39e37022661f81f79e481240ed9b175dec6513, d3855b7351c11145e51301e6b686f748ca35c802, e3b94a5f28522e6825aff16ff07d56bd70d26c96, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, cf5dfc4a9f7a82b32640128ca10832eace55880e, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, 6dc4883228c95e8a332320fcc587a0ff33c84d59, 82d2b9d09cc339fdeac05abfb8a31f9c6eace948, 8b28792f8405b737229afb92c99c579b86d8aa98, 92afbbe41174a545f9da9992e33c9a9592e529aa, 9e5d389fcdce4c0a6827ebc804b4e863aa28c14c, 712e32e2da67428ba6c6add1605410e1c3792883, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, bad4c08f03587e38ee960e2aa76e16d722826e7c, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, a9640bac0b45a804d07fc5914feb08af8f2a73f2, e2a85a6766b982ff7c8980e57ca6342d22493827, 97ac11e5a6440eccb70ae7146392ac138c36fa6c, 312b1067d89f598c4c5c0799aa18b48d0926bed8, 7ae2783a9196fb4bc2a610ae812d19722daddce5, be082d70534db088315f2cc5b42c2fdcd58c1b8c, 1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0, 22ebfc211d184ed615729378a43fde175bf14478, be7cb8f79bc018e57467168fc0c7f8ad59bba04f, fa26a6d434450b185e669170e79fd3e1d29716bf, 8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6, ce7499d6862df8269c655220049c3ed20b9b6f5e, be2b0396de9431bae931642516a1d3e4906329f5, d0ab11de3077490c80a08abd0fb8827bac84c454, f90f526b101cb8a0260f5165a3875928c58ae48a, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, 3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b, c0e6cd2ec3bc9eb46c7d45bb708854da3327339e"
cd2f4aaf98bb1e020cff310000c8049d3460c54ea,NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark,https://www.semanticscholar.org/paper/cd2f4aaf98bb1e020cff310000c8049d3460c54e,Conference,"In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.",2022,30,"1724648481, 1602998334, 1453724884, 2226458991, 2251043402, 2064112151",1724648481,Belgrade,3,"48469973, 50631038, 6562624",Y,"As such the paper would be a nice contribution to ICLR.;I understand that you are trying to improve the existing results with their optimizer, but this paper also introduces new algorithm.;Finally, Table 1 seems to have some min/max values the wrong way around.",Conference on Empirical Methods in Natural Language Processing,22,"data,contamination,model,llm,llm","3dfa820702b6181c9964931f0a4d47fd298bf429, 57a3fc6d0aaad3c10c793b4e59390ca04c935282, d1206ccabd1980848f14472d6548251c2fab7963, 78b2d392ebb100a220ceab6529d26909b27eaa32, c9b56cb026a38e39bb0228faac57accd6f65e6f7, a3636512a48321baab95c94052de2a0a88460602, fac67bf55456b52ac6e4f280ad953d0250c74ebc, 75ea299834d6949e89e91d006677343ddab44e49, 41d4e093d5f7ed5aae1aaa9eb6c037742e4cf9b1, 079b57837221413bf99ab40999c77c29e280e0c2, d86084808994ac54ef4840ae65295f3c0ec4decd, 846883b7761cb5fe4468d42bf9d328b5d1030175, 011095a0082e5e301f9bf30267b193c1c9e7e370, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, eef23d76e175c0cff8e81ffcb2721c10539c8cbd"
8ee45aeb7c97e3346cc62f216f673b91277ac718a,LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,https://www.semanticscholar.org/paper/8ee45aeb7c97e3346cc62f216f673b91277ac718,Conference,"This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. 1",2021,137,"153409139, 2110410087, 2158995823, 2137028350, 2113951006, 1758652",153409139,Andorra,2,"4836115, 2169159066",Y,The generator is a mixture of two Gaussians in one dimension.;The encoder network from NEM can be seen as a recurrent network which takes one latent variable theta_k at time t and some input x to produce the next latent variable theta_k at time t+1.,IEEE International Conference on Computer Vision,21,"agents,tasks,language,llm,llm","665b0c776ff7507c32793f10ce9edf90bc2f674a, 519ffd9744de5638d8c950090f065923e0793a93, 8674494bd7a076286b905912d26d47f7501c4046, 4895c430c7810b45840b58cc9182f12143013a43, 98b9086750f08a21c8778ab986339321e9caf790, ba9b6f805feb62c978d384211f910790643a023e, 375125029b085e70a109491656b69aa01bc2a166, 4d8f0ae904779a50b2e18fec49e51a5661a98d8a, 03aeb4520e760a906393aaf9c1bf4e526483d081, 80d9f0eb47b712988d19cbe29a7bfa63f2a175d0, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 40416ac3bf78583eea37661b1b446e9939245b3e, 7eaac9847257c32afd450017d1348ecda4dcaade, fe44200fed05f9a7c656f2245deded8fd5f5e1e6, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, f4207bcad24fe4eaf3849d6c1fe38c36545b5cb3, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, 19cf7458db4e17c7504eee24ccf961e1dc91435c, f12930cd5f58990badc1a7c5d2749cad004cfb0e, 3ec02e36cc0ba16292b255e9f90c3725521e2ec9, 7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7, af13a92977d4f4dc5b28b13746d86111d42939e8, 046eb47d56beb8069b0098e3d01608f81ebb6849, 53b0cfa2eaaa9703df02b2ca9c43260c4de5df0a, 657fbf29ea0b4904a3e98d1556f9acf38dddae5f, 25761ba4bdc054bfe902fe7c5d6338be6d00d491"
ce157cea880c9ab64de64f11a531202f5348fa05a,"""Kelly is a Warm Person, Joseph is a Role Model"": Gender Biases in LLM-Generated Reference Letters",https://www.semanticscholar.org/paper/ce157cea880c9ab64de64f11a531202f5348fa05,Conference,"Large Language Models (LLMs) have recently emerged as an effective tool to assist individuals in writing various types of content, including professional documents such as recommendation letters. Though bringing convenience, this application also introduces unprecedented fairness concerns. Model-generated reference letters might be directly used by users in professional scenarios. If underlying biases exist in these model-constructed letters, using them without scrutinization could lead to direct societal harms, such as sabotaging application success rates for female applicants. In light of this pressing issue, it is imminent and necessary to comprehensively study fairness issues and associated harms in this real-world use case. In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions: (1) biases in language style and (2) biases in lexical content. We further investigate the extent of bias propagation by analyzing the hallucination bias of models, a term that we define to be bias exacerbation in model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs- ChatGPT and Alpaca, we reveal significant gender biases in LLM-generated recommendation letters. Our findings not only warn against using LLMs for this application without scrutinization, but also illuminate the importance of thoroughly studying hidden biases and harms in LLM-generated professional documents.",2022,23,"2165227666, 2258548444, 2261454711, 31099365, 2257127887, 2256996328",2165227666,Paris,2,"1742135, 1695160",Y,"The proposed GGNN approach outperforms (bi-)LSTMs on both tasks.;Otherwise, the perfect latent representation is z=x.",Conference on Empirical Methods in Natural Language Processing,22,"biases,letters,llms,llm,llm","329d31f881a17861eedeef6a9d8fd509cddd2b7c, b7f5973dbf2ef76fcc3aea616a7f72ca79f09020, 2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c, 2c3eef2f17369912e330281d54b535675077e4ca, 5107c83173e43f51d1bdebf6cafda525a7c26bf0, a6bba5ce9867c978210e3d056691b5c1e769b760, 8388f1be26329fa45e5807e968a641ce170ea078, 1cf2e9e198feef3893da2800a7949f6880ddc084, 8c7aee0bbc062d5b4bcd34951b1e002274288206, 013eb12ce5468f79d58bf859653f4929c5a2bd14, 681253389d2cc27103753749f4c7556699d55471, 4fcfe83c05402b5c5fb6e853082e74af6379d7f9, e7a7735104448371dde788542ebfc6af6485ea43, 10aa2be24951e6de76b630482a645d79354c4cde, 787ae2c51cd82b904bb4fb9ccb15266381af5436, 4aec7bcb292cb763341a22fc67a1cfaceb3a2c67, 5471114e37448bea2457b74894b1ecb92bbcfdf6, f4cfc7cbad257f1688772d59f694c16189dba811, eacf9284a39adcd56172665f31fd5a72560bba7a, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, 31f10a6f602bef0306ac37322f84f6163c8a8ecb, 39444c55f07839ac6a0d1839472a982f8fb447bb, d1bb57da8593a2071b3ea8026865352ab3f7206a, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, 456c011594ecacdd24298a161787389ccbe4b88b, e5194ae88d63c7549678b1b73cfdaf7112164272, 10d89b13a6309a531c35701d37d3bd76a27a3942, 8adb47deeef943c2c1bae41f9498a382fb818a16, 3a083d843f891b3574494c385699c21766ce8b7a, db6084fdb3baceddacdc726474722debe1ef7e65, 2374106a32169c07703599ff3f6f4b31e8067b89, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 590b617c08d34bc6caed7e4490c0b22a9c516e86, 322d91190acd8ac8c64598f5126947b0485ba249, f18be38578ee52aa7071c404d42e3d53ae003122, 5b6ec746d309b165f9f9def873a2375b6fb40f3d, 3a74bed911ccf213d9595b2b02a5b1c4ac4dcaf8"
c0aec04ee86c0724d61c976f19590fbe9c615723a,Creating Large Language Model Applications Utilizing LangChain: A Primer on Developing LLM Apps Fast,https://www.semanticscholar.org/paper/c0aec04ee86c0724d61c976f19590fbe9c615723,Conference,"This study focuses on the utilization of Large Language Models (LLMs) for the rapid development of applications, with a spotlight on LangChain, an open-source software library. LLMs have been rapidly adopted due to their capabilities in a range of tasks, including essay composition, code writing, explanation, and debugging, with OpenAI’s ChatGPT popularizing their usage among millions ofusers. The crux of the study centers around LangChain, designed to expedite the development of bespoke AI applications using LLMs. LangChain has been widely recognized in the AI community for its ability to seamlessly interact with various data sources and applications. The paper provides an examination of LangChain's core features, including its components and chains, acting as modular abstractions and customizable, use-case-specific pipelines, respectively. Through a series of practical examples, the study elucidates the potential of this framework in fostering the swift development of LLM-based applications.",2022,21,"2113663584, 114751633",2113663584,Vienna,3,"151505981, 35638374, 2059003208",Y,"Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks Summary: The paper proposes Neumman optimizer, which makes some adjustments to the idealized Neumman algorithm to improve performance and stability in training.;Are the four mixture components over the robot parameters updated independently of each other when the parameter-exploring policy gradients updates are applied?;An additional heatmap generator component can be further included in the clustering model.",International Conference on Applied Engineering and Natural Sciences,22,"applications,study,development,llm,llm","287ba5bf00d96af1596aaf80c178392a9c4fcc28, dbabab9bf5955558f73a37644f4bb626106a6d73, 57e6cca1479a4642f867e69b4dee93d14259dc3d, 0bca61986b8edeaf33018d0203b44110f2480110, 02a1e8e77f501675945890df45fbdc11726cb0ba, f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d, 33e332837e91c1048c3ed165cd16bf7607c3bf06, d47a682723f710395454687319bb55635e653105, 45674df7143e43bc589cfabd26dd194c2a7f090d, d6e1e4f0ad898ca6ac37e6e139a77fa3982170d4, cd2f4aaf98bb1e020cff310000c8049d3460c54e, f5ac3ec506a5a01807dd7196c0c52eef008b78cc, fc32074b37a6d9dda535a70f9689022e70508520, 472644c5f4155635cf9e9e37540bfa53c20e7610, b0ee814c7a3eed260c9913861329c9f73e880d00, aa6c2afadd660fe4efbac699f7854e8f6f240c38, ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, 5c39e37022661f81f79e481240ed9b175dec6513, 661d316f4fe9574f9048586bfae2c43243a1d22f, 6a6ad9eb495739f4c80e7c09598720c3d5c5dff7, cf523942d56e90db182c5788845f6502da9a307d, 8c33ca066e2ab615e24c65198c794114436053dd, e32a2519b59d62cff6cb8136ee242dc3754ed57b, a0f303b6e22ef52943355993f57d65938997066a, 3b87dafd5a412e25e06761f181ec199ca88a7398, 244e08e109f6f4fa0f846977e1aef1e7b6a9e816, cd8a9914d50b0ac63315872530274d158d6aff09, 22ebfc211d184ed615729378a43fde175bf14478, 96273b87cd0eb9d1c9a12afae621ce2abdbbab36, 6001895c2fcf69528d01306e9d293d9d2a4cc67b, eadb1e7da375939e25083ae3936c4f4ef1f2a719, c5c4142a01981787a71bf6ebcb791520c458ab5d, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344"
0095acc4f2c3255cf38fdf844003c97858adb418a,OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples,https://www.semanticscholar.org/paper/0095acc4f2c3255cf38fdf844003c97858adb418,Conference,"Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors lack robustness against attacks: they degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, a malicious user might attempt to deliberately evade the detectors based on detection results, but this has not been assumed in previous studies. In this paper, we propose OUTFOX, a framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output. In this framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are harder to detect, while the detector uses the adversarially generated essays as examples for in-context learning to learn to detect essays from a strong attacker. Experiments in the domain of student essays show that the proposed detector improves the detection performance on the attacker-generated texts by up to +41.3 points F1-score. Furthermore, the proposed detector shows a state-of-the-art detection performance: up to 96.9 points F1-score, beating existing detectors on non-attacked texts. Finally, the proposed attacker drastically degrades the performance of detectors by up to -57.0 points F1-score, massively outperforming the baseline paraphrasing method for evading detection.",2022,15,"2138646471, 143655216, 1764004",2138646471,Kiev,3,"2165227666, 48455738, 40075749",Y,"The WAGE parameters (i.e., the quantized no. of bits used) are 2-8-8-8, respectively.;This paper proposes a different approach (with could be combined with these methods) based on a new training procedure.;3. The paper wants to find a good trade-off on speed and accuracy.",AAAI Conference on Artificial Intelligence,22,"detectors,texts,detection,llm,llm","11cf88dce827bd67cbfa60400306318022e736d5, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, 1ca6d6682204f0214338f7797bea056444e908bd, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, 0fa554d981809c5eb78956c779f75092c4f6c16b, ce54e3b89a2570035b70885e6901ad4c92ae41c9, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, 155eeede0c070f1f017ba5c9f6cacd7ae0b098aa, 42d3ae61ca296558cba61446bd95b8a6e5b1082d, d93bcf0685c15c45d078eafea565969c04daccd3, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, d617f51833860dc50d202af7f80be71304b2e994, ed9e7821b3e51c7e59183300d6c8cf90c8de0f26, c9645aa4ea31903e02e201b877fd3e1466adff4f, 4e58100b319d74f97ed550a4e5fa32dea8c06fe1, d1bb57da8593a2071b3ea8026865352ab3f7206a, 68ade27b68c05d1b60b8a9d8f8aadbe76900c60c, 0311ace1d499cadd1cc0c515a625d1d045f60d25, 3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b, 8d67b76222d84dcd337b8a2c78f13837070a79ce, 003ef1cd670d01af05afa0d3c72d72228f494432, c665003881c3c35589d1e48da1ee7234b48f2ac8, dd2deed2ce6e110236a1280db765fa02c7488eb1"
a8b995f0da78a79447dfb18c2337972b044f4239a,LLM-FP4: 4-Bit Floating-Point Quantized Transformers,https://www.semanticscholar.org/paper/a8b995f0da78a79447dfb18c2337972b044f4239,Conference,"We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One characteristic of FP quantization is that its performance largely depends on the choice of exponent bits and clipping range. In this regard, we construct a strong FP-PTQ baseline by searching for the optimal quantization parameters. Furthermore, we observe a high inter-channel variance and low intra-channel variance pattern in activation distributions, which adds activation quantization difficulty. We recognize this pattern to be consistent across a spectrum of transformer models designed for diverse tasks, such as LLMs, BERT, and Vision Transformer models. To tackle this, we propose per-channel activation quantization and show that these additional scaling factors can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method, for the first time, can quantize both weights and activations in the LLaMA-13B to only 4-bit and achieves an average score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8 lower than the full-precision model, significantly outperforming the previous state-of-the-art by 12.7 points. Code is available at: https://github.com/nbasyl/LLM-FP4.",2022,15,"2220637583, 2109370860, 2261688809, 2261493190, 2256381600",2220637583,Moscow,2,"11531589, 1390140002",Y,"Second, it is also unclear what the method to generate train/dev/test data is.;Figure 2 which is the graphic representation of the model is hard to read.",Conference on Empirical Methods in Natural Language Processing,22,"quantization,weights,models,llm,llm","84725855d10b531eb8cbe54935dda0440c2fc750, 5b34752817bc0d6aa96466dabcbc24a83dd071fe, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, 92afbbe41174a545f9da9992e33c9a9592e529aa, e02f91d625cd32290d4ede0f31284da115844316, df138c7425e787cac2f9d3ab7775c0fb5294a83e, 92930ed3560ea6c86d53cf52158bc793b089054d, 3994334c81478a4b17341eb1f494dbccbb73d999, 9efd70d2c06733704220313fb67720aa45c6362a, 185e7d2a761594451b02ace240356dadad2aef78, 00d1f3423a33f73ca6aee884a58834547475d2f0, dc10a3f4dae2db48148ff6781454ddcc856ae8da, 152877c51df17cdd4a87d19e452c6daecfadf6c3, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, db6084fdb3baceddacdc726474722debe1ef7e65, e30d9b8ce108d982169621b88a5e3fb69fec70e1, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, 28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8, 492c389d560d9db39c758d07e635408d2e0eaf7d, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, d84ed05ab860b75f9e6b28e717abf4bc12da03d7, e57aeb158a38ccf33d2f0f5a8f63f1209497e329, 9583ac53a19cdf0db81fef6eb0b63e66adbe2324, f2bc4057e696f49c326bf8e1588772a16f053754, 9e540662619327a3056d9e40bb58058868f6f805, cb03b665069dad5e895a2c244929ea427f1fb9d1, 8e259f940f007e08207ddb7c3a052f52036d7bf6"
7a4fe2f003241ad97bf1778e527cb0306fa90da2a,CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations,https://www.semanticscholar.org/paper/7a4fe2f003241ad97bf1778e527cb0306fa90da2,Conference,"Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.",2022,15,"2149615775, 3144356, 2260029688",2149615775,Stockholm,2,"145385471, 51418452",Y,This seems like one of the most novel findings in the paper and is worth highlighting.;The model is trained with supervision to output the overhead map of the global map.,Conference on Empirical Methods in Natural Language Processing,22,"simulations,llm,caricature,work,llm","6548106035c7208ad498730627874a482734b9ac, f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, 8b28792f8405b737229afb92c99c579b86d8aa98, 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d, 024006d4c2a89f7acacc6e4438d156525b60a98f, f14fc9e399d44463a17cc47a9b339b58f6ef7502, 27beaa5db6c37c611f082855c6385b264874b8f5, be082d70534db088315f2cc5b42c2fdcd58c1b8c, 639bfab64e2f35917d450013e136cb24c7755fad, 27245e65a27bde90b5b0bb25d157bb75a0ad8b5a, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7, 247dec05283a1a521f99253a6cca6a5858cac0d2, 7a1e71cb1310c4a873e7a4e54d1a6dab0553adce"
b61b260de1599e6e89491cad9160898fcd3b34c2a,Can LLM Replace Stack Overflow? A Study on Robustness and Reliability of Large Language Model Code Generation,https://www.semanticscholar.org/paper/b61b260de1599e6e89491cad9160898fcd3b34c2,Conference,"Recently, large language models (LLMs) have shown an extraordinary ability to understand natural language and generate programming code. It has been a common practice for software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability, and robustness of the code generation from LLMs have not yet been thoroughly studied. The executable code is not equivalent to reliable and robust code, especially in the context of real-world software development. For example, the misuse of APIs in the generated code could lead to severe problems, such as resource leaks, program crashes, etc. Existing code evaluation benchmarks and datasets focus on crafting small tasks such as programming questions in coding interviews, which, however, deviates from the problem that developers would ask LLM for real-world coding help. To fill the missing piece, in this work, we propose a dataset RobustAPI for evaluating the reliability and robustness of code generated by LLMs. We collect 1208 coding questions from Stack Overflow on 18 representative Java APIs. We summarize the common misuse patterns of these APIs and evaluate them on current popular LLMs. The evaluation results show that even for GPT-4, 62% of the generated code contains API misuses, which would cause unexpected consequences if the code is introduced into real-world software.",2022,15,"2146528797, 2108467971",2146528797,Athens,3,"2066499928, 145233583, 2149798086",Y,"The paper says Deep Voice 2 (Arik et al., 2017a) is only prior work for multi-speaker TTS.;Presumably, there exists some hyperparameters where the model does not achieve 100% test accuracy, in which case, what are the failure modes?;The authors also analyze the generalization error bound of DNN after pruning based on the work of (Sokolic et al., 2017).",AAAI Conference on Artificial Intelligence,22,"code,llms,software,llm,llm","00d1f3423a33f73ca6aee884a58834547475d2f0, 950850e22e42201f152d90dc6f53d53e39d37657, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, 681253389d2cc27103753749f4c7556699d55471, c2a448bb511ebae41a87e69891da8bbf17ddba3d, 033a50c4515b153b6e706018075c333c64981fd7, 4419c5720e30d5ca5158795d4c848125650b8db1, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, 11342d45911ee8a7c9e3a94117ce774ad7036172, 639bfab64e2f35917d450013e136cb24c7755fad, 59c2968fb9672a7152c52127255d8f0784bc2368, 8e259f940f007e08207ddb7c3a052f52036d7bf6, 29409efa04ac99ccf01d2a011d21d5d14e870000, 1cff064f815111a71a98afda7aee1867ad617901, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, f75b70c9d7078724b592ec3e21de705e7b6ff73f, 9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a, 15370f51d666ab8ef17185679553c6a8647b2a15, 83cebf919635504786fc220d569284842b0f0a09, cd2f4aaf98bb1e020cff310000c8049d3460c54e, 41d04aa3c25dcfbf1b44ce666c48759e03c216c7, 98b9086750f08a21c8778ab986339321e9caf790, b6a7226e5f6d618370995eccad68af195ef32da2, 4e746359afd6f81705b875d71cc499b904a320df, 0e33833f5e2e2719edfba1d142eb4d27f96e799f, b4c9c134ad5bd4a037115df65411b4c49abe1322, 72afe82af4c2ca100c36eb35292e85d806527f0a"
9e540662619327a3056d9e40bb58058868f6f805a,Prompt Distillation for Efficient LLM-based Recommendation,https://www.semanticscholar.org/paper/9e540662619327a3056d9e40bb58058868f6f805,Conference,"Large language models (LLM) have manifested unparalleled modeling capability on various tasks, e.g., multi-step reasoning, but the input to these models is mostly limited to plain text, which could be very long and contain noisy information. Long text could take long time to process, and thus may not be efficient enough for recommender systems that require immediate response. In LLM-based recommendation models, user and item IDs are usually filled in a template (i.e., discrete prompt) to allow the models to understand a given task, but the models usually need extensive fine-tuning to bridge the user/item IDs and the template words and to unleash the power of LLM for recommendation. To address the problems, we propose to distill the discrete prompt for a specific task to a set of continuous prompt vectors so as to bridge IDs and words and to reduce the inference time. We also design a training strategy with an attempt to improve the efficiency of training these models. Experimental results on three real-world datasets demonstrate the effectiveness of our PrOmpt Distillation (POD) approach on both sequential recommendation and top-N recommendation tasks. Although the training efficiency can be significantly improved, the improvement of inference efficiency is limited. This finding may inspire researchers in the community to further improve the inference efficiency of LLM-based recommendation models.",2022,16,"2151529879, 2260830380, 2146027242",2151529879,San Marino,3,"1403759150, 2235966, 51026953",Y,"Results are good, some unclear explanation This paper proposes an end-to-end trainable attention module, which takes as input the 2D feature vector map and outputs a 2D matrix of scores for each map.;[1] Wang, Weiran, Honglak Lee, and Karen Livescu.;However, what is shown in that paper is that the sequence of loss function values converges, which does not imply that the sequence of weight estimates also converges, because of the presence of a non-convex constraint ($b_j^t \in Q^{n_l}$).",International Conference on Information and Knowledge Management,31,"models,recommendation,efficiency,llm,llm","1b0aa15937fdf59103a5213bccf09cff83d0ee3e, 6fed828456964d29517f6caf31b700d8aec82153, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 94deb62af3054c49e7d80bd7eb3ed5efe990fc0b, f91dbd39d4c742ba675e447b04a0b0c70b33e836, d422df8bff4e677a3077635db116679d25142bfc, 712e32e2da67428ba6c6add1605410e1c3792883, c48e0bd0f36c25ab83befbc7b7da369b75fd25f5, 7d873a9c49d3864709aa762f8740edcdbd7369c5, bfad52fc64ca0169644b6e7e0ea9a46470d51709, 10ea29fda06bdbe56f591909d89f3194b452ac91, a56bf7ee9a56d8f84079684339a953c2df9ce76b, 29409efa04ac99ccf01d2a011d21d5d14e870000, d57f11ed40c3cdcbb36cb758191db4f2c9372965, 58ed1fbaabe027345f7bb3a6312d41c5aac63e22, 9181b0d801dfcd7723a3ede201f0543078e2c149, 82d2b9d09cc339fdeac05abfb8a31f9c6eace948, d8348b802c9133d9e396d4ad809b020d5be42863, 2afa490dde7a8c582d889530c7f8b042fef6a8b7, 10aa2be24951e6de76b630482a645d79354c4cde, 661ccdb41fe977d47273e586389cacc1489f3286, 42d3ae61ca296558cba61446bd95b8a6e5b1082d, 4b06c7e29280b1c6bc05c9df39023b48fef02c93, 752604994a7ca548ff2954114fc61a501d857b1c, c0bcd7dc9426a70af15f5ad63b4af92ea4dcbd4d, d8a5474f450330ad25c1e22f27e88f3630cb840d"
91b2b47cabd800ef658b65bfe1f52b7293a740c3a,LLM-powered Data Augmentation for Enhanced Crosslingual Performance,https://www.semanticscholar.org/paper/91b2b47cabd800ef658b65bfe1f52b7293a740c3,Conference,"This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency.",2022,27,"2161240241, 143990839, 8129718",2161240241,Oslo,3,"2185503028, 1740261, 2061202877",Y,"This manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum, which actually has been shown by existing works like Kawaguchi (2016) with some assumptions.;There's clear value in having good inductive biases (e.g. expressed in the form of the discriminator architecture) when defining divergences for practical applications.;Algorithm 1 does not make clear the relationship between the model learned in step 2 and the algorithms in steps 4 to 6.",Conference on Empirical Methods in Natural Language Processing,22,"data,languages,llms,llm,llm","31f10a6f602bef0306ac37322f84f6163c8a8ecb, e449b9b3fe04fe260731a3c74d2123bf6eaadf5b, bcc9245ebf57072c07c5a9d4eff3aaaac36cb7c6, 657fbf29ea0b4904a3e98d1556f9acf38dddae5f, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632, 5107c83173e43f51d1bdebf6cafda525a7c26bf0, 1c2efb418f79b5d29913e014a1dfd78865221c39, 2dafea864f74a477414c3b71b742f7997e216102, ce9ca56036307217ea565644d3d3bd74b879e045, 0311ace1d499cadd1cc0c515a625d1d045f60d25, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 33ce8103b129149eb78ca2fa48538e25c9242c08, 92912dd895c360f01a6be9c9f6d207642139525e, 329d31f881a17861eedeef6a9d8fd509cddd2b7c, 8d0f755dea90f35f4b126a01fa3cce96b3bdd344, b0ee814c7a3eed260c9913861329c9f73e880d00, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 1a37223175138bc1aa53b425ea2fdd0b382405a5, c292e473b3825eeb9db03c70b2e1c033aea190d5, 82663577cf1d08235bb56ad648c9dad36343ccfb, c9f320789e98d2c7a798a9705e26dbe317677966, d280cf82a6144b3e168840802b1a8a14d4eaccb9, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, 9e66ae24a541255c2d931184498ee116ce81478a, ddc6e677715c03fe574319d3f80a3e1577bdbdd3, 079b57837221413bf99ab40999c77c29e280e0c2, 53b0cfa2eaaa9703df02b2ca9c43260c4de5df0a, 34ca47eed139a7f0694611528f75debc43385518, 263a58f4fd32caca1dad2351af4d711aec451fe6, 647c4a9331e01e31a4350361d3460f0397fe694f, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, 557ee73ff4e87ca0cb1b6c78af0730a2d94b710f, 77e6c9917536949a82e5ca02c4882b69ee8a4fd6, 787ae2c51cd82b904bb4fb9ccb15266381af5436, fee8f63972906214b77f16cfeca0b93ee8f36ba2, e576a2d97950b1f6831f88575dd3f370053f6af7, 780c725848aac1118d00c8bb306719ec803369cd, b09139c153bac8893e8faea2b3a59159234caadc, 709af143f78bc62413c50ea1a7ee75b0702c4f59, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, a8d76d84408c1fe6b1543084e6cec3dfc4ede429"
5645502d73c6907f1671923638773152e55bfb00a,TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks,https://www.semanticscholar.org/paper/5645502d73c6907f1671923638773152e55bfb00,Conference,"While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw more accurate conclusions about LLMs' performance on a specific complex task.",2022,17,"2692077, 2185503028",2692077,San Marino,3,"144906624, 143666627, 3001926",Y,It would be interesting to compare to a baseline where the control systems are allowed to adapt to the individual design parameters.;Could you explain how classes are predicted given a test problem?;I could then identify the influential dimension(s) by taking the largest absolute values in the gradient vector.,Conference on Empirical Methods in Natural Language Processing,22,"llms,studies,prompts,llm,llm","7998468d99ab07bb982294d1c9b53a3bf3934fa6, 929305892d4ddae575a0fc23227a8139f7681632, 353c88c231ce156d604e074af276422422fc73f7, e576a2d97950b1f6831f88575dd3f370053f6af7, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, 7eaac9847257c32afd450017d1348ecda4dcaade, d9d325ca670a1aa215e3e39023f8abf17dae7584, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, 71854ff4306cf65c3c2161f7be2d0346275f72d5, 7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, 1a4c6856292b8c64d19a812a77f0aa6fd47cb96c, 733fc094e785724621c46e20db1be69f132ad9df, 10aa2be24951e6de76b630482a645d79354c4cde, e4788ee4f5e90c6f42cedc5116acd2d6475c3180, 3337e4b2e63e5e99171f9da9d161771f5810c27a, 1d174f0e3c391368d0f3384a144a6c7487f2a143, b4c9c134ad5bd4a037115df65411b4c49abe1322, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, bb5d26da72bfe7030dbc6650b686b210ae661f2c, 624b2f14be4287d6a400cdf88a6f911b434b182e, 52a6695ae1c08cc29baf764dedb5831c7a954214, fe2492b7b8cf6d1d10b7ea38e0f7f853bd679d52"
f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6a,Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,https://www.semanticscholar.org/paper/f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6,Conference,"Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",2014,7451,"2681954, 1744700",2681954,Sofia,2,"2243374631, 2171114745",Y,"Intriguing two phase RL approach for learning neural controllers for discrete programs This paper presents a reinforcement learning based approach to learn context-free parsers from pairs of input programs and their corresponding parse trees.;They then evaluate several existing network architectures on these tasks, and show that results are not as impressive as others might have assumed they would be.",International Conference on Machine Learning,14,"uncertainty,learning,tools,deep learning,deep learning","b80e2af7ffa85fe2f02ea8390e4915d55c19d199, 3994334c81478a4b17341eb1f494dbccbb73d999, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 0a92bc2dc8a216e6aced83edc0358241066833df, 3813b88a4ec3c63919df47e9694b577f4691f7e5, 29ddc1f43f28af7c846515e32cc167bc66886d0c, e32a2519b59d62cff6cb8136ee242dc3754ed57b, a2ec47b9bcc95d2456a8a42199233e5d9129ef18, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, 42d3ae61ca296558cba61446bd95b8a6e5b1082d, 25adff0bb9691b46fee5c0591300d6f3ccf117ab, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, 3c68025d95970a9b9aa1b742a678704cd09d2bf4, e5d720767b7a539bb2edaa98eaf572a4506a79c6, 8ab93bee04cd5bdbd96002b2e325d02f61ba695a, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, 34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61, e3052ebca5eeae6a8a73e44517903d39746f5f3a, 0f38b3d717e0fcc6eacc9c6e78b252227440e04e, ce2d5b5856bb6c9ab5c2390eb8b180c75a162055, 3ea34401909978d3d3d0c25c8746e02c7d2a7c77, 7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca"
e9a986c8ff6c2f381d026fe014f6aaa865f34da7a,Deep Learning with Differential Privacy,https://www.semanticscholar.org/paper/e9a986c8ff6c2f381d026fe014f6aaa865f34da7,Conference,"Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",2015,4616,"2057642721, 1396184193, 153440022, 145057514, 145591745, 35210462, 2152832173",2057642721,Madrid,2,"1710223, 2138715050",Y,"Secondly, the DDT’s leaves are parametrized with the encoder distribution q(z|x), and thus gradient information flows back through the DDT into the posterior approximations in order to make them more discriminative.;To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER.",Conference on Computer and Communications Security,15,"privacy,techniques,networks,deep learning,deep learning","8b417c2be7a7707f372049fb1193f0d42f799562, d8a5474f450330ad25c1e22f27e88f3630cb840d, 046eb47d56beb8069b0098e3d01608f81ebb6849, a281d563261c738f13b9e58a525e7e265a619c93, 82973c5f56681190a0dbb4c4449ed60d5f805135, d1206ccabd1980848f14472d6548251c2fab7963, 25adff0bb9691b46fee5c0591300d6f3ccf117ab, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, 9d6acac70b2d1fdb861a08b00766ef263109cd7f, 72afe82af4c2ca100c36eb35292e85d806527f0a, 8ee45aeb7c97e3346cc62f216f673b91277ac718, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, ba687027ed6012f613e1f9a9cefe7683bb192934, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, d8348b802c9133d9e396d4ad809b020d5be42863, 993df7df129f8d18816877d69923d7df7b347d85, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, 2bc9d6830610e18f110aa7984f7c0271d6b17eeb, 33e332837e91c1048c3ed165cd16bf7607c3bf06, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, baafed5f8968118af04dbbb1cf172f1c10bede25, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, 0a71dd8bec060195e14eb9d0a7abbc08d960d4d5, c0aec04ee86c0724d61c976f19590fbe9c615723, 6628f9ee35e36cdfdcac8a46cef4dba8d529a83b, 3789eb72c32ecf5e33442570358dd786dd67c8a2, 60caa5b3d066e13feac496fd0736e976970eb09f, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, cde39ce861e4c7514ee07fd91b6b8aac50cbf01b, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, 9fcdbfdf28245010c875ce85502351fe05c04b49, a8b995f0da78a79447dfb18c2337972b044f4239, 4aec7bcb292cb763341a22fc67a1cfaceb3a2c67, dca4d9abbc82e57dfa52f932e893d467a63e0682, 697f2f3598057cd17cff7749d768cae0993c6727, 5406e153957dd7a165264da6e6e5d81251997404, 8d0f755dea90f35f4b126a01fa3cce96b3bdd344"
5b6ec746d309b165f9f9def873a2375b6fb40f3da,Xception: Deep Learning with Depthwise Separable Convolutions,https://www.semanticscholar.org/paper/5b6ec746d309b165f9f9def873a2375b6fb40f3d,Conference,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",2015,11616,1565641737,1565641737,Vaduz,2,"1746807, 1904203865",Y,"Are the test environments sufficiently different from the training ones?;In the first paragraph of Section 4.5, I disagree with the sentence, ""Similar observations can be made for the other language pairs we considered.""",Computer Vision and Pattern Recognition,15,"inception,convolution,depthwise,deep learning,deep learning","63316bb5b88d362051c048e864c3ae5d97a26d30, 0a829289a16ae48837cc2905635435db98bacc76, 9b3e8d202488dc29e601fc471a25a2af9002659e, 003ef1cd670d01af05afa0d3c72d72228f494432, b09139c153bac8893e8faea2b3a59159234caadc, fc77048474ccd34c6507701591c2e6ab3ca647ef, e2e8ea9bcdb83deb2787ce89db1b51f7ccb1bd1e, eb9e0da8b7170e3ca4364f2f9010599c2d2556f1, d280cf82a6144b3e168840802b1a8a14d4eaccb9, 661d316f4fe9574f9048586bfae2c43243a1d22f, a453ce8a3de86a170c79a1082ef358c3adf4e612, d9b34c6b616f75485856794478bfbeab1ea93b81, 3dfa820702b6181c9964931f0a4d47fd298bf429, b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, 8e259f940f007e08207ddb7c3a052f52036d7bf6, b7034546bee38ba13d3b312fce893a22e33ce4dd, 8d942a3b52e2ad16ff8e5970be59591970d89fae, 156609022dd6258c60238859622da0a1683bd062, d1efcef213c433445be56d7479eb47d972b3ee79, 8ec5896b4490c6e127d1718ffc36a3439d84cb81, 2e7f532796eed2847d4c19e3cff03756049e81b4, d86084808994ac54ef4840ae65295f3c0ec4decd, 32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, c07802ed8a25998e9bd44ee1ddbcc63b7eb34060, c6879e43828b293567f5e2da039d23845189d6a7, ab06951251e0abfdb866694f9a23a79c72784317, bcc9245ebf57072c07c5a9d4eff3aaaac36cb7c6, 03aeb4520e760a906393aaf9c1bf4e526483d081"
d997beefc0922d97202789d2ac307c55c2c52fbaa,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,https://www.semanticscholar.org/paper/d997beefc0922d97202789d2ac307c55c2c52fba,Conference,"Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",2015,11051,"144329939, 144914140, 2216377, 51352814",144329939,Copenhagen,2,"1384224631, 32244429",Y,"And the results of figure 9, showing poor reconstructions when changing the class representation essentially demonstrates that the method isn't able to factorize class and style successfully.;The rollouts are then fed into the agent's policy / value function.",Computer Vision and Pattern Recognition,15,"network,data,point,deep learning,deep learning","322d91190acd8ac8c64598f5126947b0485ba249, 1d7531db9272f7838e33616075e1e64532fd013a, 933baeec555352784848a93284c9dd0e79477759, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, 51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 78df3ba26593620ab689fe5a97b7e739434a053b, 96b51d940653710f9d099d89ade86b44fa9bdd8a, 3789eb72c32ecf5e33442570358dd786dd67c8a2, 916455d97cd792c2eb5b00663689592e25cbc8d8, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 1562390dd212516cd857009cbd4f857a902d1f3d, d916776e0c6a04b0def4c22257c188776c2edab2, 00bbc94806ca0821a9c82d8aedf16f0e6263b89f, fa75a55760e6ea49b39b83cb85c99a22e1088254, f2bc4057e696f49c326bf8e1588772a16f053754, bcc9245ebf57072c07c5a9d4eff3aaaac36cb7c6, 74bc39003e65119eaa6ba339a61b45b417a638b7, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, ad10ef93675513a68b93d54f3a461160b53318a3, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, 5c45a5d05ac564adb67811eeb9d41d6460c70135"
ff7bcaa4556cb13fc7bf03e477172493546172cda,What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?,https://www.semanticscholar.org/paper/ff7bcaa4556cb13fc7bf03e477172493546172cd,Conference,"There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.",2016,3815,"47645184, 2681954",47645184,Lisbon,3,"2327080, 10707709, 2548384",Y,"The idea of enforcing information isolation is brilliant.;But, it would be good if this can be supported with real life examples.;I do not know whether using a centralized network where each agent has a window of observations is a novel algorithm.",Neural Information Processing Systems,16,"uncertainty,model,tasks,deep learning,deep learning","4afa7d8e2de43b0b67366b1bce8768f5a246d153, 590b617c08d34bc6caed7e4490c0b22a9c516e86, b6593b5ad6b0e00232ee88cf2bc578645c1299a0, 9b529fe170823f95509585d5aa39fa01a43558fd, d1a6b3a5efde3783b53f822dc8dd00aaac934b95, 9ea0757c750ab1222a7442d3485a74d1c526b04c, a453ce8a3de86a170c79a1082ef358c3adf4e612, 5b34752817bc0d6aa96466dabcbc24a83dd071fe, 81c02f123b3ef09cf1a8e5a1332451f0d46663fa, 1f8a23697562b001082b147779b5eaefd3513d0a, b2f8df88de24fe0ef74bda0eec1a0c3e7329b9f9, f4cfc7cbad257f1688772d59f694c16189dba811, d2a505586c0da20752b98f63c7760b6a5c41e28d, 4be7d1524edb0137599a5cc95f72844b85a52fe1, 742747c7a453b293352b772d0d99541c96a351c3, 9954ec0a95fc0840f7712d7b0c67b242bb536a9e, 9e5d389fcdce4c0a6827ebc804b4e863aa28c14c, ead6121fbc787d508dc6a6d7106f72bf0d647d03, b7f5973dbf2ef76fcc3aea616a7f72ca79f09020, e70ddfb04969b663ef8cd711a70a0acf563d6e5f, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, e75cb933d387ecb184010ff07d0ee43fc1750e2a"
8674494bd7a076286b905912d26d47f7501c4046a,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,https://www.semanticscholar.org/paper/8674494bd7a076286b905912d26d47f7501c4046,Conference,"Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",2016,8165,"144329939, 47782132, 144914140, 51352814",144329939,Bucharest,2,"144418438, 1390195074",Y,"Experiments conducted on image classification and weakly supervised segmentation show the effectiveness of the proposed method.;The problem set-up of unpaired summarization is not particularly compelling, since summaries are typically found paired with their original documents.",Neural Information Processing Systems,16,"point,pointnet,network,deep learning,deep learning","bf07f2927dca481653b8c60b2dc982fe4a7dfd4e, 01f0f5205d03870f172ae8f04e33356d5a0af221, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, d57f11ed40c3cdcbb36cb758191db4f2c9372965, bcc82ce554942880814243fc8c08a88b9d2aad09, 84a36e19f9394f22b34f79756fa9628a795e02ea, 6dd3e404aac097f63d42dc19fd08c7ec281f90b4, 19b93280f17696a4ddfa2c75490a50ab107addf2, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632, 8b7b2e88207fc2dd849f5d83c9b6e890f0174abc, e3b94a5f28522e6825aff16ff07d56bd70d26c96, df138c7425e787cac2f9d3ab7775c0fb5294a83e, c30c0092bf4eb8a44faec3fc60cdd5006276bcdc, 31d7d7b9c7b776c639316027e6ae5f2ff2673da2, cb3968152f7d93f53d24b00279a90d5071ddc85a, 91e611c3e8705002438fb4439733e47ddec85b5d, 0d065e8688c38bb0148203a1738f47184a5b58d3, 0427110f0e79f41e69a8eb00a3ec8868bac26a4f, 44786a2c2a8ba8cf5c74a8fb10098c220e924c56"
811df72e210e20de99719539505da54762a11c6da,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d,Conference,"Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",2017,5850,"2587648, 35499972, 1689992, 1736651",2587648,Lisbon,3,"48907594, 2133037029, 35367497",Y,"As noted above, it is not clear what is the significance of this combination or how does it improve performance.;Also, that the ""style"" representation contain less (and I'd say slightly less, in Figure 7 b and d, we still see a lot of same class nearest neighbors) is not exactly a surprising result.;The proposed GGNN approach outperforms (bi-)LSTMs on both tasks.",International Conference on Machine Learning,17,"methods,rl,framework,deep learning,deep learning","332e0eab5fba8e6940f3e481f542a99ac17b9717, 1a37223175138bc1aa53b425ea2fdd0b382405a5, e2e8ea9bcdb83deb2787ce89db1b51f7ccb1bd1e, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, da9b8b4073e6ad44b3da66e1e117cb1ddbf8836d, 0084f3cb0a1754272151c5268a783f24bf5676a0, 34ca47eed139a7f0694611528f75debc43385518, 9b54941de1e21826ecc28b32730ac3f69991ede4, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, 8b417c2be7a7707f372049fb1193f0d42f799562, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, 7669fca7fbc071b4ca78b4c326ff8f3a4b6ae616, 410fba9f03212257d0881811802e6620e59bc827, 10b4b926904ad153f791ec680218e1610747a0c8, b9c974380649749320f4a02e33b2e5014e7f1756, 3813b88a4ec3c63919df47e9694b577f4691f7e5, 5030702fea15d66a73fc997325431f1d7945ad9a, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, d2a505586c0da20752b98f63c7760b6a5c41e28d, b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8, 4087e84fc695bb6433d0104ee94f9d7e9f4b7da5, c6879e43828b293567f5e2da039d23845189d6a7, 1c748f86182a62d44d5b44316db510f8d833e19f, f6dab84c2c00ab92d8ee9d9359d7e530512114f9, ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8, 41d04aa3c25dcfbf1b44ce666c48759e03c216c7, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, 808e9ce4e86e79098edea7f00b5b91663b87a5e6, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, 4be9368abc2474d6fd38639e523cf03af1873fd9, 63316bb5b88d362051c048e864c3ae5d97a26d30, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, a0a79dad89857a96f8f71b14238e5237cbfc4787, 78e40584f0d149bf6f98beb5561b7b83cb68e1b1, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, 3a58efcc4558727cc5c131c44923635da4524f33, 02b1607af35b48f0bd716367caf6a7428b969369, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, 07cca761749bfe21c2d096ff60f32b574d5c84c4, 84a36e19f9394f22b34f79756fa9628a795e02ea"
c889d6f98e6d79b89c3a6adf8a921f88fa6ba518a,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518,Conference,"We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",2016,9447,"46881670, 1689992, 1736651",46881670,Moscow,2,"4373076, 52257721",Y,"To see why these models are different, if it is the model of (2), and we look at only distinct words, the diagonal of the matrix P[v_i,v_i] does not really make sense and certainly will not follow Equation (7).;— Can the authors provide training time comparison of their model and other/baseline models?",International Conference on Machine Learning,16,"model,learning,metalearning,deep learning,deep learning","c0aec04ee86c0724d61c976f19590fbe9c615723, 5371896313ac227eb819038dd55f213cb42b99e2, 448959eff044f02040ded5afd483b7c4e811b0ac, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, db528269ef800727245c0fcb35b692d29c1ccdc9, 742747c7a453b293352b772d0d99541c96a351c3, 3cfe075af77bf0364e6ddecb3d223960d06e8927, ef35da3a3c471a6a15c7a3b09586483eb50cbef0, 834cdfca7cc041a6fa0db3da5493c6754bea845b, cbad0923db89f23febcbd6192ff4149289ff2ad9, 6dcb1cd576b0e54b900f45a178efe271c383de04, aee3d7f98b966240178ef420724c840f9b61deb3, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 66d41e0f894dda2c37dd5bacbdd7bfd418e3350f, c84389369720dcd2f004c48e58fbac2c45c8f092, 6ff909c6fe089fc8ebfc64eca0f0c3cc34ba277f, 371a343457a4fbff00000bf4faa29b2b2f85744c, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, a80e26e6365b215715c182d19a9aa8bb876ac768, 453fdfeefd6498a65be339d7e8722f6f3288907e, 03532123ccffae8d411264320e8a5ae2b6eddea0, 80d9f0eb47b712988d19cbe29a7bfa63f2a175d0, b09139c153bac8893e8faea2b3a59159234caadc, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, fa26a6d434450b185e669170e79fd3e1d29716bf, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, e2f3aa4ecc487fa496d1a51e875e747d4b7e6001, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, 8799ec4bdf98bc313aa8c41a706a385e026d1f88, 818de553ecd306735971aba04bbfc29d17457084, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, 5406e153957dd7a165264da6e6e5d81251997404, 52a6695ae1c08cc29baf764dedb5831c7a954214, e30d9b8ce108d982169621b88a5e3fb69fec70e1, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 64c4c6cb66457c79f89ba4f2529b57bfce19ced4, 649c3497e3b34b15a5011259fcb837cf6c1ac04a"
69e76e16740ed69f4dc55361a3d319ac2f1293dda,Asynchronous Methods for Deep Reinforcement Learning,https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd,Conference,"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",2015,7633,"3255983, 36045539, 153583218, 1753223, 2542999, 3367786, 145824029, 2645384",3255983,Luxembourg,2,"2267664, 1718134",Y,"Interesting paper, would like to see more experiments The paper proposed a new activation function that tries to alleviate the use of  other form of normalization methods for RNNs.;“ Would be good to see how this affects results and convergence speed.",International Conference on Machine Learning,15,"reinforcement,learning,network,deep learning,deep learning","00d1f3423a33f73ca6aee884a58834547475d2f0, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, 980858461df7c4349f17b427686c5bcbcffbdc04, b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8, d2a5dcecd2ffdf03473df1688091f08fadb114a3, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 8388f1be26329fa45e5807e968a641ce170ea078, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, 8f2cf30f9c825d8ef7d622601dbd525ace95e025, 0084f3cb0a1754272151c5268a783f24bf5676a0, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, f72d3f58ff73353978e224af348448b34d27cf7b, 5371896313ac227eb819038dd55f213cb42b99e2, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, f3d594544126e202dbd81c186ca3ce448af5255c, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, f9367342405a73ab8d6de704a149babfc0edb5fe, 746a81aa26d3ebfb81acfd6af958d6a21603cd21, facd5f5deb152229ceb1803434d8690a09ab4129, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, 8b417c2be7a7707f372049fb1193f0d42f799562, 7a1e71cb1310c4a873e7a4e54d1a6dab0553adce, 52a6695ae1c08cc29baf764dedb5831c7a954214, ce7499d6862df8269c655220049c3ed20b9b6f5e, 43eea2a73997294193228d50f9ff25fc5345664b, c889d6f98e6d79b89c3a6adf8a921f88fa6ba518, 9e3816be8cf4821d74e258de10ee471382936a30"
2c03df8b48bf3fa39054345bafabfeff15bfd11da,Deep Residual Learning for Image Recognition,https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d,Conference,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",2014,157545,"39353098, 1771551, 3080683",39353098,Amsterdam,2,"3589089, 2323922",Y,Figure #s are missing off several figures.;Simulation results with MuJoCo physics simulator show that this simple trick reduces the amount of needed data by an order of magnitude.,Computer Vision and Pattern Recognition,14,"nets,networks,layers,deep learning,deep learning","92912dd895c360f01a6be9c9f6d207642139525e, d8a5474f450330ad25c1e22f27e88f3630cb840d, 8f2cf30f9c825d8ef7d622601dbd525ace95e025, 2af8907d4a974ae41044581f5e5d67317cb08568, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, d84ed05ab860b75f9e6b28e717abf4bc12da03d7, 8d67b76222d84dcd337b8a2c78f13837070a79ce, 66d41e0f894dda2c37dd5bacbdd7bfd418e3350f, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 1e1cf81a1113482be3f0c280db994a832cb9426a, 785a1c77fabd1ad870a9dbc98d235d2fcbdbaa6f, 3994334c81478a4b17341eb1f494dbccbb73d999, 41c93960a066876d5e4f1dacaef75cd8daa2791f, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 9a0965beef113cc37491004b1848149e00300561, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, 1452b25a7680bbb2c66dd7dfca6704292405da92, 3cfe075af77bf0364e6ddecb3d223960d06e8927, d86084808994ac54ef4840ae65295f3c0ec4decd, 9712624bb61abb0da989514cae558cfab61bb9d2, 1a37223175138bc1aa53b425ea2fdd0b382405a5, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, e8b30ebe3351680c3b039555ae0a8d0865ad829b, 8ee45aeb7c97e3346cc62f216f673b91277ac718, c48e0bd0f36c25ab83befbc7b7da369b75fd25f5, 647c4a9331e01e31a4350361d3460f0397fe694f, b69a35662a2cac38eab22f4481285116bdf8c30e, e57aeb158a38ccf33d2f0f5a8f63f1209497e329, 3774fb4b6aa11d88c562ee5702c66cae5881af0d, ff75865cde62592d068b2afd055c57c81d77158b, 7676c02ea839ff1ceb6e5e1427c42bc45e169bde, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, 3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b, cbc1e8bbfe98f94c0d13d111b824cf603b62712c"
c468bbde6a22d961829e1970e6ad5795e05418d1a,The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,https://www.semanticscholar.org/paper/c468bbde6a22d961829e1970e6ad5795e05418d1,Conference,"While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called ""perceptual losses""? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",2017,7179,"2844849, 2094770, 1763086, 2177801, 39231399",2844849,Warsaw,2,"145683384, 2231240312",Y,"The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models.;For the denoising comparison, how do the results compare to those obtained if you simulate a Markov Chain (sample latent state conditioned on noisy image, sample latent state, sample denoised observation, repeat using denoised observation) using a VAE?",2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,17,"features,similarity,metrics,deep learning,deep learning","1051abf1e3dae90241ad15b3f98f2e41197ee611, 150f95f9c73820e0a0fa1546140e9f2bdfd25954, a13149a80855412d970d0de2b41c611f4cf7e1da, e576a2d97950b1f6831f88575dd3f370053f6af7, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, b0c34618ffd1154f35863e2ce7250ac6b6f2c424, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 2e7f532796eed2847d4c19e3cff03756049e81b4, d9b5194f3f959eda2e95df6a340254f52ced46f4, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, 1109d62ebd2b29a7dc148bc30dd6cfc803a63dec, 819167ace2f0caae7745d2f25a803979be5fbfae, 57f5bf937f7393b691428747a9078d3124e6bcce, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, 98b9086750f08a21c8778ab986339321e9caf790, c665003881c3c35589d1e48da1ee7234b48f2ac8, dbabab9bf5955558f73a37644f4bb626106a6d73, 69e76e16740ed69f4dc55361a3d319ac2f1293dd, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, adc180e1fe404b650fca3bb7970e43bdce34a611, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, 70ce56e9a2181489d59f7170dc01fe8ba310a8e5, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, 08764019e9762da527253b37b0ff39c46a4206b7, 5d8e450c8c7d9faa8222f1d0eaf34b01755c9eef, f8e57fa370fe10147aa22714e08409fc1b7dae4b, 89a30b5dab02c9c390a632acad481fa602859272, 957f5b1e7ca48891c2e279aefbfa0f04d989c21e, cb03b665069dad5e895a2c244929ea427f1fb9d1, bc00ff34ec7772080c7039b17f7069a2f7df0889, eef23d76e175c0cff8e81ffcb2721c10539c8cbd, 5cb8f417d171ae329adf446820bd32d8b49d8c04, 1c748f86182a62d44d5b44316db510f8d833e19f, 0574a3abc98f0e6bd035a4ff4ea107cfba45d3d4, 4b9184937da308914b9e13c43bfd75845eaf910b, bdb68c5e2369633b20e733774ac66eb4600c34d1, 34ca47eed139a7f0694611528f75debc43385518, d2a609ffb814442d0728aef9f6616f9cd775face, 0601e9e434b30320c316c76228b97c093fa98ad6, a1d36749b89e46a8eaadf8ba40788741c192fb1e, 10cf0045bc0f58aa3699e4451f65b12a08019c5c"
3b9732bb07dc99bde5e1f9f75251c6ea5039373ea,Deep Reinforcement Learning with Double Q-Learning,https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e,Conference,"
 
 The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.
 
",2014,6104,"7634925, 35099444, 145824029",7634925,Sofia,3,"1764004, 3437933, 34905515",Y,"Unsurprisingly (since the EEN noise does not seem to be conditioned on the input), the baseline deterministic model performs quite well.;3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling.;The “obverter” technique is quite interesting since it incorporates the concept from the theory of mind which is similar to human alignment or AGI approach.",AAAI Conference on Artificial Intelligence,14,"algorithm,overestimations,qlearning,deep learning,deep learning","11be2469ab1d1c508e7b6e14148990741ba87884, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, d7b820af40a9e2660ef700d39f7b2e27b43435c5, f18be38578ee52aa7071c404d42e3d53ae003122, 9f5b82d9915d0752957602224c5056be7e749c83, 8adb47deeef943c2c1bae41f9498a382fb818a16, b69a35662a2cac38eab22f4481285116bdf8c30e, 634fe61f3ab6692ea4733989a1c76e793bb8b69e, 0df5a4f9cc8a244715fe9968732497d2ac2a7cd1, a357f1ff27e184d9a5ef69e665e8ca891032bf71, 9817bf0f78047452761e950c02a1a56f59a1e593, df7336844a31165db0ae08f1cd0f560c9e3faeea, 72243bfa618fd6c3c01bb7b4885a4a456b2a6071, 0daa1d31d6949f8804089d8a1c11c4560422ad39, 4afa7d8e2de43b0b67366b1bce8768f5a246d153, f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 8bba999de25bfb288b3f7f88e1d907aab02638b6, d6bc29a897fd85e7187dc33c3c974b8879462237, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 7ae2783a9196fb4bc2a610ae812d19722daddce5, 22cba8f244258e0bba7ff4bb70c4e5b5ac3e2382, 2e965b5d97c2d6fb4af284307735be39283792ba, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, f11cba099b8cf14815f7b3d85f55ecfddbf9f04d, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 9eea59c34f139f3d2153226c8cf026e975622074, ce212cb873a54e5716da53a66b10298ac013008a, 033a50c4515b153b6e706018075c333c64981fd7, e2a85a6766b982ff7c8980e57ca6342d22493827, 647c4a9331e01e31a4350361d3460f0397fe694f, b080d072cfde697180db3234da08903c092e72c3, 375125029b085e70a109491656b69aa01bc2a166, d8a5474f450330ad25c1e22f27e88f3630cb840d, 8dce62b18bdea587c07cb4769a05ca0a816d09ba, eef23d76e175c0cff8e81ffcb2721c10539c8cbd, e4b52a1a00e9db941326fc857b95245cbfb60bce, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, e32a2519b59d62cff6cb8136ee242dc3754ed57b, 27245e65a27bde90b5b0bb25d157bb75a0ad8b5a, b5904cd5dbf73b8d5ff13517de490c292d877ee0, b52db9e41e15f76bdcfbe674abe0314af545c430, e576a2d97950b1f6831f88575dd3f370053f6af7"
31f9eb39d840821979e5df9f34a6e92dd9c879f2a,Learning Deep Features for Discriminative Localization,https://www.semanticscholar.org/paper/31f9eb39d840821979e5df9f34a6e92dd9c879f2,Conference,"In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.",2014,7937,"145291669, 2556428, 2677488, 143868587, 143805211",145291669,Bern,3,"145590185, 34842481, 2153917002",Y,"A more systematic set of experiments could compare learning the proposed weightings on the first K layers of the network (for K={0, 1, …, N}) and learning independent weights for the latter N-K layers, but I understand this would be a rather large experimental burden.;It is observed that words case and average case empirical error estimates diverge when the input is high dimensional.;Second: They then train a model based agent using the collected transitions ( St, At, St+1 ).",Computer Vision and Pattern Recognition,14,"pooling,network,localization,deep learning,deep learning","7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 371a343457a4fbff00000bf4faa29b2b2f85744c, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, 17fc5c9b37ba63fb3280ddf9909a183523e7bcc3, d6bc29a897fd85e7187dc33c3c974b8879462237, ec2f9076448ba25a225618603adde60caa76c4df, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 794b3ffd28d28606230efc975eeec9f0522fb139, 4e746359afd6f81705b875d71cc499b904a320df, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, a80e26e6365b215715c182d19a9aa8bb876ac768, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 48c73c389c3f36d70407eb8309a0b41578c15fc8, 1f8a23697562b001082b147779b5eaefd3513d0a, 94214d6d922ce095719d488642cbcc75dc52f273, 7171a0e9b07ebc98a32eb912262613efc20f283a, 71854ff4306cf65c3c2161f7be2d0346275f72d5, bf69c98fca9a9f6c1cde871beddbcdc668b77771, 9d788cfe4a0991d3b1a266c8329f6e903840b82f, e24b8a9531573d284647239affc6c855505b0de4"
7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7a,Learning Transferable Features with Deep Adaptation Networks,https://www.semanticscholar.org/paper/7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7,Conference,"Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multikernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.",2014,4409,"35776445, 2146174097, 2144499343, 1694621",35776445,Copenhagen,2,"1800564, 1786155",Y,"The authors show through several experiments that the divide and conquer (DnC) technique can solve more complex tasks than can be solved with conventional policy gradient methods (TRPO is used as the baseline).;Some comments: - Perhaps, it is better to move Section 3.3 before Section 3.2 to emphasize the main contribution of this work, i.e., using Stein’s identity to derive an estimate of the gradient of the score function.",International Conference on Machine Learning,14,"domain,network,adaptation,deep learning,deep learning","e8b30ebe3351680c3b039555ae0a8d0865ad829b, 82663577cf1d08235bb56ad648c9dad36343ccfb, ccca203382e5dd198c089a0f1d7af7bef0f694e9, 5d433da6d0f143f20936379910104d2bb139d4ae, 5ea7bf772fecf95cbf53b2c7f719c9440322a115, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, 19b93280f17696a4ddfa2c75490a50ab107addf2, b4c9c134ad5bd4a037115df65411b4c49abe1322, 20a3ed03888037e2802fa9abad02ffa0a8dcc228, 456c011594ecacdd24298a161787389ccbe4b88b, 5f7f10f913ecc478ff7ba304c265fd3c700b47d7, a13149a80855412d970d0de2b41c611f4cf7e1da, 102ebe229df18c8733ea1b8def56cd79996e2178, f2bc4057e696f49c326bf8e1588772a16f053754, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, 6ff909c6fe089fc8ebfc64eca0f0c3cc34ba277f"
2eda2921a8da4b325f9d05f556594a5884c398a7a,Overfitting in adversarially robust deep learning,https://www.semanticscholar.org/paper/2eda2921a8da4b325f9d05f556594a5884c398a7,Conference,"It is common practice in deep learning to use overparameterized networks and train for as long as possible; there are numerous studies that show, both theoretically and empirically, that such practices surprisingly do not unduly harm the generalization performance of the classifier. In this paper, we empirically study this phenomenon in the setting of adversarially trained deep networks, which are trained to minimize the loss under worst-case adversarial perturbations. We find that overfitting to the training set does in fact harm robust performance to a very large degree in adversarially robust training across multiple datasets (SVHN, CIFAR-10, CIFAR-100, and ImageNet) and perturbation models ($\ell_\infty$ and $\ell_2$). Based upon this observed effect, we show that the performance gains of virtually all recent algorithmic improvements upon adversarial training can be matched by simply using early stopping. We also show that effects such as the double descent curve do still occur in adversarially trained models, yet fail to explain the observed overfitting. Finally, we study several classical and modern deep learning remedies for overfitting, including regularization and data augmentation, and find that no approach in isolation improves significantly upon the gains achieved by early stopping. All code for reproducing the experiments as well as pretrained model weights and training logs can be found at this https URL.",2019,620,"51026953, 47260842, 117539586",51026953,San Marino,3,"6322777, 2279712392, 2116502347",Y,"Should these parameters be take out of the n-step advantage function A?;I encourage the authors to have the work proofread by a native speaker; clearer writing will ultimately increase the reach and impact of the paper.;But, it would be good if this can be supported with real life examples.",International Conference on Machine Learning,19,"training,performance,networks,deep learning,deep learning","14dd50979af27bd2574c8068db11d27028b56afd, 4cd033a56b19f87f6adfefeef5fcc990306ecf40, ae38dc77a962161107361f213db9216ee1274037, 07b01d665646009439ca206378cc35e095ec6cd2, 5d433da6d0f143f20936379910104d2bb139d4ae, cb23a59fdf3ade707600f076df4ff27a03941fba, 31f10a6f602bef0306ac37322f84f6163c8a8ecb, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, 156d8e2aa90b5ccc9be10477ca70deaad0151387, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, a9cbbef8f4426329d0687025b34287c35bdd8b38, da5d78b3e3a1544fde98fba86088e1215e97cbe8, fe44200fed05f9a7c656f2245deded8fd5f5e1e6, b080d072cfde697180db3234da08903c092e72c3, 633e2fbfc0b21e959a244100937c5853afca4853, dca4d9abbc82e57dfa52f932e893d467a63e0682, 6e74403ab75d0080c056fd66702ab1f54f04d9b1, df138c7425e787cac2f9d3ab7775c0fb5294a83e, 573fd2ce97c70bb29097e8efb28a27af791225ca, d3f9a39e49abfdf084da558e305be5473c8740e5, 4e746359afd6f81705b875d71cc499b904a320df, a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, 1ca6d6682204f0214338f7797bea056444e908bd, 5b6ec746d309b165f9f9def873a2375b6fb40f3d, 811df72e210e20de99719539505da54762a11c6d, 4e13a8e8ba8d33e15ed037bfca7c651047533990, be7cb8f79bc018e57467168fc0c7f8ad59bba04f, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, 22ebfc211d184ed615729378a43fde175bf14478, bad4c08f03587e38ee960e2aa76e16d722826e7c, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, 42375e9a309fe5bf5c671c918cd8c9d5d85568cc, 6aca07154c111f1c8738347d7112cad6b0bf974a, c468bbde6a22d961829e1970e6ad5795e05418d1, 709f7a6b870cb07a4eab553adf6345b244913913, 28a5a53dafacebad8a7c47773079caeffb9a5baa, f1664bbaddedea8c250873e7610ab07e53fa7132, fa63c3f53413ced7946623889c416e34a28676ea, f1300d9be8254b028337d9757755ba906fe6955b, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, 75ea299834d6949e89e91d006677343ddab44e49, 89858723bec341178f2b00d34ea3016baaaf71a6, a22f3398ea865426c89ee66f4824ec626e56a864, 6068d39e92aef1bb0e1291e9931894c35692a85e, 02b1607af35b48f0bd716367caf6a7428b969369, 8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6, 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d, 9619cde5c79d91ca5c432186668618312175f8dd"
07cca761749bfe21c2d096ff60f32b574d5c84c4a,Normalized Loss Functions for Deep Learning with Noisy Labels,https://www.semanticscholar.org/paper/07cca761749bfe21c2d096ff60f32b574d5c84c4,Conference,"Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: any loss can be made robust to noisy labels. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of underfitting. To address this, we propose a framework to build robust loss functions called Active Passive Loss (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60% or 80% incorrect labels.",2019,317,"9576855, 1753845931, 1919541, 9035741, 144757691, 145148600",9576855,Moscow,3,"144985567, 2211964951, 1390481263",Y,"To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER.;I found the formulation of the \alpha to be non-intuitive and confusing at times.;The paper is generally well written, easy to read and understand, and the results are compelling.",International Conference on Machine Learning,19,"loss,functions,labels,deep learning,deep learning","182180bd69ea6d2f59225ded5ddc900b8558ab9f, 7884b0ec63b8a08f7cd793a989df44f6bb53116c, b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8, 7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7, 22ebfc211d184ed615729378a43fde175bf14478, 72afe82af4c2ca100c36eb35292e85d806527f0a, d997beefc0922d97202789d2ac307c55c2c52fba, 53103ae318a19569ac82cee5062de2cf73bf386c, 6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5, d21703674ae562bae4a849a75847cdd9ead417df, c419ee7315b9edfd8fc55bab16534fc55a564fcd, 8bba999de25bfb288b3f7f88e1d907aab02638b6, 9d788cfe4a0991d3b1a266c8329f6e903840b82f, 0f4d00d01d43d3967ee92b58481b5ad530a944d1, 9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a, 7ed665355ac78bf0c394602dd9d26075195ce2f2, 33ce8103b129149eb78ca2fa48538e25c9242c08, 332e0eab5fba8e6940f3e481f542a99ac17b9717, 7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2, 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d"
8760bc7631c0cb04e7138254e9fd6451b7def8caa,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,https://www.semanticscholar.org/paper/8760bc7631c0cb04e7138254e9fd6451b7def8ca,Conference,"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10 × or 100 × ? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between ‘enormous data’ and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",2016,2030,"1491624845, 1781242, 2108498897, 1726095131",1491624845,London,3,"32246932, 48740398, 2113618679",Y,"2. Even if this was correct, the main point is that this is ""only"" d times worse - see eq (11).;In any case, I also don't think that was the argument you were trying to make.;In a bit more detail, the key idea of the paper, at least to the extent that I understood it, was that the authors are able to introduce a model-based variance-reduction baseline into the policy gradient term.",IEEE International Conference on Computer Vision,16,"vision,data,tasks,deep learning,deep learning","c9645aa4ea31903e02e201b877fd3e1466adff4f, 91d6e8ba5dd90b02fe3bd870b19da13a6167af53, 0ac1a1ccb16c8441f42ff8de743fc93f0e08c15c, 4aec7bcb292cb763341a22fc67a1cfaceb3a2c67, 215fc60307f741b9db059204e41db8bfb879e606, d617f51833860dc50d202af7f80be71304b2e994, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, 5c39e37022661f81f79e481240ed9b175dec6513, aa6c2afadd660fe4efbac699f7854e8f6f240c38, 99f06e88e76f1af51d08d7adfb26d758ebc6acab, 82973c5f56681190a0dbb4c4449ed60d5f805135, 0258bab20bc8574ee602012081a17db89339f12d, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, d81fc968196e06ccafd7ea4c008b13e1cad1be64, 492c389d560d9db39c758d07e635408d2e0eaf7d, a8c1ed061813f832358c1aabf5d171bab80203bf, 46c266b3d1274dacd7fce27ee8cb4d587f087a58, 3dfa820702b6181c9964931f0a4d47fd298bf429, f4207bcad24fe4eaf3849d6c1fe38c36545b5cb3, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, 27e58c9e5e6d07809a45a17675a2c7135b577881, 8d67b76222d84dcd337b8a2c78f13837070a79ce, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, 819f2778eba0d4c9eea86307bedaaeed94dc751d"
d81fc968196e06ccafd7ea4c008b13e1cad1be64a,An End-to-End Deep Learning Architecture for Graph Classification,https://www.semanticscholar.org/paper/d81fc968196e06ccafd7ea4c008b13e1cad1be64,Conference,"
 
 Neural networks are typically designed to deal with data in tensor forms. In this paper, we propose a novel neural network architecture accepting graphs of arbitrary structure. Given a dataset containing graphs in the form of (G,y) where G is a graph and y is its class, we aim to develop neural networks that read the graphs directly and learn a classification function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classification purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the first challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs. Experiments on benchmark graph classification datasets demonstrate that the proposed architecture achieves highly competitive performance with state-of-the-art graph kernels and other graph neural network methods. Moreover, the architecture allows end-to-end gradient-based training with original graphs, without the need to first transform graphs into vectors.
 
",2017,1277,"3098251, 7217944, 40059761, 9527255",3098251,Moscow,2,"1764236, 145033446",Y,since you mention at the end of page 4 that you will only rely on linear SVMs for computational reasons.;5. It would be nice to see the gradient flow with the new activation function compared to the ones without.,AAAI Conference on Artificial Intelligence,17,"graph,graphs,networks,deep learning,deep learning","7bb477077968d68aa7a6059d8d6d801fb28274da, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, fc32074b37a6d9dda535a70f9689022e70508520, 7904b3446775ed8c79f4f94001a16b706989c462, 8d06f13ec042f0d8d845af5deadeb3dd9ad11f70, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, 73d4accea441aae2373828a8dc2175aa2759c38f, 0d065e8688c38bb0148203a1738f47184a5b58d3, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 980858461df7c4349f17b427686c5bcbcffbdc04, 91b9d3ab7532ea24ae70cd726355f25235b1fe8b, afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84, d1e701665e73faa648cb15473952576f40e8e122, c96fc88631f2b8e2fe192027a8a237445635328c, 9954ec0a95fc0840f7712d7b0c67b242bb536a9e, d617f51833860dc50d202af7f80be71304b2e994, 2e5d2f2dc01b150dffc163a9f457848e9b5b5c38, df7d26339adf4eb0c07160947b9d2973c24911ba, ca0e479ba2327f71e842d033b6b48b082962cc6a, f2bc4057e696f49c326bf8e1588772a16f053754, d21703674ae562bae4a849a75847cdd9ead417df, bcc82ce554942880814243fc8c08a88b9d2aad09, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, 2ddcd47b28eb4b43317a34cf56e83309f5347699, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, 68a3d32416977e88cf1bfa4ad548d403f5f089d6, d3855b7351c11145e51301e6b686f748ca35c802, 965359b3008ab50dd04e171551220ec0e7f83aba, 657fbf29ea0b4904a3e98d1556f9acf38dddae5f, df7336844a31165db0ae08f1cd0f560c9e3faeea, 7eaac9847257c32afd450017d1348ecda4dcaade, cb03b665069dad5e895a2c244929ea427f1fb9d1, 98b9086750f08a21c8778ab986339321e9caf790, ac4350d3a2673a4c34fa949714ede8b45fd04f1d, 08be4e23951a0def1c5d235befbb39c8d8d373a3, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, 05b8b67451fb105576c58af960e6e6d98f9103e7, 1ff76ab0fcf22110df62337d462e15d79a2a2593, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, 64306bbddb4da7a4e06f990a0167d55fbbbbec82, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, c84aa52bee5116f80c7740503edff4b08f733c3b"
329d31f881a17861eedeef6a9d8fd509cddd2b7ca,QUARK: A Framework for Quantum Computing Application Benchmarking,https://www.semanticscholar.org/paper/329d31f881a17861eedeef6a9d8fd509cddd2b7c,Conference,"Quantum computing (QC) is anticipated to provide a speedup over classical approaches for specific problems in optimization, simulation, and machine learning. With the advances in quantum computing toward practical applications, the need to analyze and compare different quantum solutions is increasing. While different low-level benchmarks exist, they often do not provide sufficient insights into real-world application-level performance. We propose an application-centric benchmark method and the QUantum computing Application benchmaRK (QUARK) framework to foster the investigation and creation of application benchmarks for QC. This paper establishes three significant contributions: (1) it makes a case for application-level benchmarks and provides an in-depth ""pen and paper"" benchmark formulation of two reference problems: robot path and vehicle option optimization from the industrial domain; (2) it proposes the open-source QUARK framework for designing, implementing, executing, and analyzing benchmarks; (3) it provides multiple reference implementations for these two reference problems based on different known, and where needed, extended, classical and quantum algorithmic approaches and analyzes their performance on different types of infrastructures.",2021,20,"2147184066, 2105502136, 9653883, 50631038",2147184066,Moscow,2,"2217847684, 2158502526",Y,"The current paper presentation is a bit too dense to clearly understand the LL machine model and the two-phase algorithm.;So, I wish to see a section on testing with Resnet and GoogleNet.",International Conference on Quantum Computing and Engineering,21,"quantum,problems,benchmarks,quantum computing,quantum computing","0456ed94f6c455f99cb67abe8a28aeb3ef9f489b, 27245e65a27bde90b5b0bb25d157bb75a0ad8b5a, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, 6068d39e92aef1bb0e1291e9931894c35692a85e, ac4350d3a2673a4c34fa949714ede8b45fd04f1d, f406aceba4f29cc7cfbe7edb2f52f01374486589, f51bc74814a3452009ea5ca262d9768d08149ee6, 82663577cf1d08235bb56ad648c9dad36343ccfb, 25761ba4bdc054bfe902fe7c5d6338be6d00d491, 375125029b085e70a109491656b69aa01bc2a166, dca4d9abbc82e57dfa52f932e893d467a63e0682, f14fc9e399d44463a17cc47a9b339b58f6ef7502, bdb68c5e2369633b20e733774ac66eb4600c34d1, b27c98d8378848f2c23a067f2c5196f3b5a07572, 2ee03e28208a9310a9be4032c2b04ebdddb83cc7, 9817bf0f78047452761e950c02a1a56f59a1e593, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, d2a5dcecd2ffdf03473df1688091f08fadb114a3, b651d67502790e1d6d41c589e1d93e996ba7b935"
047286f5b9315a8e8bf56c4fc936e62f21495892a,Resource Allocation in Quantum Networks for Distributed Quantum Computing,https://www.semanticscholar.org/paper/047286f5b9315a8e8bf56c4fc936e62f21495892,Conference,"The evolution of quantum computing technologies has been advancing at a steady pace in the recent years, and the current trend suggests that it will become available at scale for commercial purposes in the near future. The acceleration can be boosted by pooling compute infrastructures to either parallelize algorithm execution or solve bigger instances that are not feasible on a single quantum computer, which requires an underlying Quantum Internet: the interconnection of quantum computers by quantum links and repeaters to exchange entangled quantum bits. However, Quantum Internet research so far has been focused on provisioning point-to-point flows only, which is suitable for (e.g.) quantum sensing and metrology, but not for distributed quantum computing. In this paper, after a primer on quantum computing and networking, we investigate the requirements and objectives of smart computing on distributed nodes from the perspective of quantum network provisioning. We then design a resource allocation strategy that is evaluated through a comprehensive simulation campaign, whose results highlight the key features and performance issues, and lead the way to further investigation in this direction.",2021,14,"1741486, 2288804757, 1739490",1741486,Sofia,3,"2762838, 3428490, 1806271",Y,"Is the implicit assumption that for real, high-dimensional data the generator and data manifolds will *never* overlap?;* There's abundant literature on f-divergences which show that there's a 1-1 relationship between divergences and optimal (Bayes) risks of classification problems (e.g. Reid at al. Information, Divergence and Risk for Binary Experiments in JMLR and Garcia-Garcia et al. Divergences and Risks for Multiclass Experiments in COLT).;For instance, averaging the node embeddings is something that has shown promising results in previous work.",International Conference on Smart Computing,21,"quantum,computing,internet,quantum computing,quantum computing","256db9dba1978f004a67c86ffc321563b1aee79a, 340f48901f72278f6bf78a04ee5b01df208cc508, 6fb020754f6de564c3a0a07bb656c0a90be1f87d, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, fe44200fed05f9a7c656f2245deded8fd5f5e1e6, 43eea2a73997294193228d50f9ff25fc5345664b, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, e89c37b7c2ff465db43c4b9f674867ec4b98aa8b, 9a0965beef113cc37491004b1848149e00300561, 89a30b5dab02c9c390a632acad481fa602859272, 72243bfa618fd6c3c01bb7b4885a4a456b2a6071, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, 25ed8ba0e8906ba98fa5d92d17a01e818796ddc9, af13a92977d4f4dc5b28b13746d86111d42939e8, a0f788f6de0fb83d623c875a98120e3f347f70d1, 54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f, 5778e56400f7113c2b1355fdbd6b638fa379885f, 676664ee7471738577f641e6159e7596625b7fdb, f1300d9be8254b028337d9757755ba906fe6955b, 5d8e450c8c7d9faa8222f1d0eaf34b01755c9eef, ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba, 74b4f16c5ac91e3e7c88ae81cc8c91416b71d151, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 5645502d73c6907f1671923638773152e55bfb00, 45de91a919780d5540872cf047986a370625e61c, 25adff0bb9691b46fee5c0591300d6f3ccf117ab, 17fc5c9b37ba63fb3280ddf9909a183523e7bcc3, 0f38b3d717e0fcc6eacc9c6e78b252227440e04e, ccca203382e5dd198c089a0f1d7af7bef0f694e9, a13149a80855412d970d0de2b41c611f4cf7e1da, 74b63c4cdc719a646ce014d084a0ce0ac66d9139, 86d3beff240b6c882058455e098a571de86564f5, c0e6cd2ec3bc9eb46c7d45bb708854da3327339e, ce2d5b5856bb6c9ab5c2390eb8b180c75a162055, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 771a858c35f6d6e6d1017dde95368de3794738a6, 02fa2389b1b64b661192e224bed8af6df0ce80f6, b0b770fb8c7760749c88e3c83ae173cdb07f7bd5, c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b, 795550a5294eb05ea4f3b14f0b1c21a405493d85, a2ec47b9bcc95d2456a8a42199233e5d9129ef18, 634fe61f3ab6692ea4733989a1c76e793bb8b69e, 2ddcd47b28eb4b43317a34cf56e83309f5347699, 5290d7921f0266c8b50b79fc8a0b7d22868f4f60, 5e31fa4e69d1a3587230f5d134c0b7e2ed84a742, 1f8a23697562b001082b147779b5eaefd3513d0a, e2e8ea9bcdb83deb2787ce89db1b51f7ccb1bd1e, ada0b87cd5c30d31186c38fb12e631d29426a3bf, 62fe1c3a866a5e368e110d6d8ed2385c84072cac"
c2aab470b8cf92f090e0a3bac1794b21500585e6a,Evolution of Quantum Computing: A Systematic Survey on the Use of Quantum Computing Tools,https://www.semanticscholar.org/paper/c2aab470b8cf92f090e0a3bac1794b21500585e6,Conference,"Quantum Computing (QC) refers to an emerging paradigm that inherits and builds with the concepts and phenomena of Quantum Mechanic (QM) with the significant potential to unlock a remarkable opportunity to solve complex and computationally intractable problems that scientists could not tackle previously. In recent years, tremendous efforts and progress in QC mark a significant milestone in solving real-world problems much more efficiently than classical computing technology. While considerable progress is being made to move quantum computing in recent years, significant research efforts need to be devoted to move this domain from an idea to a working paradigm. In this paper, we conduct a systematic survey and categorize papers, tools, frameworks, platforms that facilitate quantum computing and analyze them from an application and Quantum Computing perspective. We present quantum Computing Layers, Characteristics of Quantum Computer platforms, Circuit Simulator, Open-source Tools- Cirq, TensorFlow Quantum, ProjectQ etc. that allow implementing quantum programs in Python using a powerful and intuitive syntax. Following that, we discuss the current essence, identify open challenges, and provide future research direction. We conclude that scores of frameworks, tools and platforms are emerged in the past few years, improvement of currently available facilities would exploit the research activities in the quantum research community.",2021,12,"9368179, 2127923241, 50768264, 22706311, 1883858, 28334651, 9878116, 4625670, 39864830",9368179,Tirana,3,"144231976, 2120251897, 2065048323",Y,"remark on theorem 1: This result generalizes a result proven in 2015 stating that the normality of a layer propagates to the next as the size of the first layer goes to infinity.;Clarity ===== The paper reads well, but it is not really clear what the claimed contribution is.;With this metric, the comparison would be easier and more intuitive.",Annual International Computer Software and Applications Conference,45,"quantum,computing,research,quantum computing,quantum computing","9b3e8d202488dc29e601fc471a25a2af9002659e, 6c34842a92ce4da9aab586490afdbd8779af4eab, 99ae62cca0b15275d9ed1528f345f9dcefe50dd7, 9954ec0a95fc0840f7712d7b0c67b242bb536a9e, 256e95ca331cbd35b3a23cc306b6627e6771a963, 156609022dd6258c60238859622da0a1683bd062, 3b230f14c46e7e177e9bebb2ebc9f46b346b646d, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, 787ae2c51cd82b904bb4fb9ccb15266381af5436, d6e1e4f0ad898ca6ac37e6e139a77fa3982170d4, facd5f5deb152229ceb1803434d8690a09ab4129, bfad52fc64ca0169644b6e7e0ea9a46470d51709, 2346d121f38fc19c77e0b062415519843f478163, aa9bf8b4c4ce8c28b9f47d7ce4f416aa9726bb0d, 94de6bab96ad533169dbc3bf9e6557581e59cb6f, 68ade27b68c05d1b60b8a9d8f8aadbe76900c60c, 03532123ccffae8d411264320e8a5ae2b6eddea0, acc296f981cde8d8c205982fc4422ec35531b769, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, c5c4142a01981787a71bf6ebcb791520c458ab5d, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 6c739540e66e895311b7347971f10ef556e06e52, 0f4d00d01d43d3967ee92b58481b5ad530a944d1, 62fe1c3a866a5e368e110d6d8ed2385c84072cac, cc017a62c605a0749e35a1264a46d62e78fb68b7, 4afa7d8e2de43b0b67366b1bce8768f5a246d153, dad8965c5a4c0a0ea1eb3837c6a9c3b42597c2ce, bb0cec8f2d34cfb9dbf8bffd1a5de499311ae098, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, d2a609ffb814442d0728aef9f6616f9cd775face, 9e66ae24a541255c2d931184498ee116ce81478a, 3a083d843f891b3574494c385699c21766ce8b7a, 3dfa820702b6181c9964931f0a4d47fd298bf429, 1fa4936fb06319c3f4536c26a447d5507c92bd48, 52a6695ae1c08cc29baf764dedb5831c7a954214, d7fddafbbc372da4fa884f67bdc32db71b888806, c12b80b44d9acfe6cd92fdf965264c4b706c367c, db4cf9f6a653d5c15973e836c800ea47743251ae, 0894585294c67193ff3190240554677b56fd79a0, a0f303b6e22ef52943355993f57d65938997066a"
3ea34401909978d3d3d0c25c8746e02c7d2a7c77a,Optimal Layout Synthesis for Quantum Computing,https://www.semanticscholar.org/paper/3ea34401909978d3d3d0c25c8746e02c7d2a7c77,Conference,"Recent years have witnessed the fast development of quantum computing. Researchers around the world are eager to run larger and larger quantum algorithms that promise speedups impossible to any classical algorithm. However, the available quantum computers are still volatile and error-prone. Thus, layout synthesis, which transforms quantum programs to meet these hardware limitations, is a crucial step in the realization of quantum computing. In this paper, we present two synthesizers, one optimal and one approximate but nearly optimal. Although a few optimal approaches to this problem have been published, our optimal synthesizer explores a larger solution space, thus is optimal in a stronger sense. In addition, it reduces time and space complexity exponentially compared to some leading optimal approaches. The key to this success is a more efficient spacetime-based variable encoding of the layout synthesis problem as a mathematical programming problem. By slightly changing our formulation, we arrive at an approximate synthesizer that is even more efficient and outperforms some leading heuristic approaches, in terms of additional gate cost, by up to 100%, and also fidelity by up to 10x on a comprehensive set of benchmark programs and architectures. For a specific family of quantum programs named QAOA, which is deemed to be a promising application for near-term quantum computers, we further adjust the approximate synthesizer by taking commutation into consideration, achieving up to 75% reduction in depth and up to 65% reduction in additional cost compared to the tool used in a leading QAOA study.",2019,98,"2218979994, 2259796",2218979994,Chisinau,2,"2156120640, 2115626527",Y,"Across the two NLP tasks authors show that MNMS models trained simultaneously give better performance than hand tuned architectures.;Except from a few typos here and there, the paper is overall well-written.",2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD),19,"quantum,programs,approaches,quantum computing,quantum computing","94214d6d922ce095719d488642cbcc75dc52f273, e8b30ebe3351680c3b039555ae0a8d0865ad829b, f72053903270d9a7f41108461ad04d5aa075218d, b3f7359c6d5780972c5ea8db016a01f0c705aa01, 6a6ad9eb495739f4c80e7c09598720c3d5c5dff7, e576a2d97950b1f6831f88575dd3f370053f6af7, 2346d121f38fc19c77e0b062415519843f478163, f9253a3e2627f1c2a0a7e2cea320a4ec4e4d2ff9, 29ddc1f43f28af7c846515e32cc167bc66886d0c, e7a7735104448371dde788542ebfc6af6485ea43, 78e80b6f3fa46e26ea4e8542815305c3ba88c5df, 5bc511aa30f72720260d792e57537379fb04c395, 3ec02e36cc0ba16292b255e9f90c3725521e2ec9, 9e66ae24a541255c2d931184498ee116ce81478a, eed62d36d1b976ac3873c83645f1c25f5096f89c, 453fdfeefd6498a65be339d7e8722f6f3288907e, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, b613887337a5d2e8fc8773037116be81e6346835, 676664ee7471738577f641e6159e7596625b7fdb, b4c9c134ad5bd4a037115df65411b4c49abe1322, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, 46c266b3d1274dacd7fce27ee8cb4d587f087a58, 0894585294c67193ff3190240554677b56fd79a0, 5cb8f417d171ae329adf446820bd32d8b49d8c04, f63e917638553414526a0cc8550de4ad2d83fe7a, 8b7b2e88207fc2dd849f5d83c9b6e890f0174abc, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, 33e332837e91c1048c3ed165cd16bf7607c3bf06, f381c53aeb7742e4047d06d84f9e0c4f523231a3, 34ca47eed139a7f0694611528f75debc43385518, b2114228411d367cfa6ca091008291f250a2c490, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 5471114e37448bea2457b74894b1ecb92bbcfdf6, 03532123ccffae8d411264320e8a5ae2b6eddea0, d47a682723f710395454687319bb55635e653105, 8d0f755dea90f35f4b126a01fa3cce96b3bdd344, 4be7d1524edb0137599a5cc95f72844b85a52fe1, d2a505586c0da20752b98f63c7760b6a5c41e28d, 695bdc6e24608364491b9418a220c65a7cd17413"
e576a2d97950b1f6831f88575dd3f370053f6af7a,Distributed Quantum Computing with QMPI,https://www.semanticscholar.org/paper/e576a2d97950b1f6831f88575dd3f370053f6af7,Conference,"Practical applications of quantum computers require millions of physical qubits and it will be challenging for individual quantum processors to reach such qubit numbers. It is therefore timely to investigate the resource requirements of quantum algorithms in a distributed setting, where multiple quantum processors are inter-connected by a coherent network. We introduce an extension of the Message Passing Interface (MPI) to enable high-performance implementations of distributed quantum algorithms. In turn, these implementations can be used for testing, debugging, and resource estimation. In addition to a prototype implementation of quantum MPI, we present a performance model for distributed quantum computing, SENDQ. The model is inspired by the classical LogP model, making it useful to inform algorithmic decisions when program-ming distributed quantum computers. Specifically, we consider several optimizations of two quantum algorithms for problems in physics and chemistry, and we detail their effects on performance in the SENDQ model.",2020,25,"3393711, 3393324, 1713648, 1752096",3393711,Vienna,3,"6322777, 2253929707, 3087426",Y,"They are both trained on the same training data, only test data is of different length and ideally both models should achieve similar accuracy for the first 10 subproblems (same trend as DDRstack).;Figure #s are missing off several figures.;There should be experiments that compare the the Q+P model with incresing number of atoms against a full CNN, to see whether the Q+P can converge to maximal performance.","International Conference for High Performance Computing, Networking, Storage and Analysis",20,"quantum,model,algorithms,quantum computing,quantum computing","f406aceba4f29cc7cfbe7edb2f52f01374486589, cb12fe714dd4fa1d14b6a8023d494c14c89e90ea, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, aca6d5f3866372a4506cf15773ae298f18c3f453, 85328b4a8132bf4299f8cd7f8e79e850d561c8fc, 0c00a328fa7cd56ee60338c54e89bd48310db80b, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, c24d47ff95cd4bda073c75ec24ececaa3b10c995, 6548106035c7208ad498730627874a482734b9ac, 709f7a6b870cb07a4eab553adf6345b244913913, 00cd2650a89734105fa0c0aba3bf07935b318290, 18d87bff073687c025f9bd23ab2dfb20d5f72a66, 9a0965beef113cc37491004b1848149e00300561, 7fa273f450251523e6b7fcc2eb3fdbdfd4a30493, 1fa4936fb06319c3f4536c26a447d5507c92bd48, fe4c5074f021cd1c810892c1b6bc267b23aa6e5c, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, 9c1265fd47df8d825f0ddd6d9ab834002592e38e, 696b388ee6221c6dbcfd647a06883b2bfee773d9, 661ccdb41fe977d47273e586389cacc1489f3286, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, 08aaaf7ae3d61875e7bcf5c1bb0df4f17066e300, be2b0396de9431bae931642516a1d3e4906329f5, e4788ee4f5e90c6f42cedc5116acd2d6475c3180, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 0bca61986b8edeaf33018d0203b44110f2480110, 4f480bae3196dbbc27ab383bce33478ea963f9b3, 2b061f7f108fdd4e90452aaaead574c7b4b5b780"
82973c5f56681190a0dbb4c4449ed60d5f805135a,EQC: ensembled quantum computing for variational quantum algorithms,https://www.semanticscholar.org/paper/82973c5f56681190a0dbb4c4449ed60d5f805135,Conference,"Variational quantum algorithm (VQA), which is comprised of a classical optimizer and a parameterized quantum circuit, emerges as one of the most promising approaches for harvesting the power of quantum computers in the noisy intermediate scale quantum (NISQ) era. However, the deployment of VQAs on contemporary NISQ devices often faces considerable system and time-dependant noise and prohibitively slow training speeds. On the other hand, the expensive supporting resources and infrastructure make quantum computers extremely keen on high utilization. In this paper, we propose a virtualized way of building up a quantum backend for variational quantum algorithms: rather than relying on a single physical device which tends to introduce ever-changing device-specific noise with less reliable performance as time-since-calibration grows, we propose to constitute a quantum ensemble, which dynamically distributes quantum tasks asynchronously across a set of physical devices, and adjusts the ensemble configuration with respect to machine status. In addition to reduced machine-dependant noise, the ensemble can provide significant speedups for VQA training. With this idea, we build a novel VQA training framework called EQC - a distributed gradient-based processor-performance-aware optimization system - that comprises: (i) a system architecture for asynchronous parallel VQA cooperative training; (ii) an analytical model for assessing the quality of a circuit output concerning its architecture, transpilation, and runtime conditions; (iii) a weighting mechanism to adjust the quantum ensemble's computational contribution according to the systems' current performance. Evaluations comprising 500K times' circuit evaluations across 10 IBMQ NISQ devices using a VQE and a QAOA applications demonstrate that EQC can attain error rates very close to the most performant device of the ensemble, while boosting the training speed by 10.5X on average (up to 86X and at least 5.2x). EQC is available at https://github.com/pnnl/eqc.",2020,29,"1949572253, 1766680, 3253856, 145560079, 143924199, 1743474, 2107856766, 2112839155",1949572253,San Marino,3,"2047998, 2161986932, 144303419",Y,"Like for example a penalty in how many examples a expert has catched.;Firstly, the model has ample free parameters to overfit when such a tiny test set is used.;However, the sensitivity calculations in the SVM context is new as per my knowledge.",International Symposium on Computer Architecture,48,"quantum,training,vqa,quantum computing,quantum computing","c15f30a3e84910a28cc560e7db097fd99339e8c1, f11cba099b8cf14815f7b3d85f55ecfddbf9f04d, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, fa75a55760e6ea49b39b83cb85c99a22e1088254, a281d563261c738f13b9e58a525e7e265a619c93, 649c3497e3b34b15a5011259fcb837cf6c1ac04a, b79e5e4622a95417deec313cd543617b19611bea, 9b529fe170823f95509585d5aa39fa01a43558fd, 99f06e88e76f1af51d08d7adfb26d758ebc6acab, fafa541419b3756968fe5b3156c6f0257cb29c23, d2efa91bd7d076a66bc1d1c570f6aa2da944dd88, f8e57fa370fe10147aa22714e08409fc1b7dae4b, 4b9184937da308914b9e13c43bfd75845eaf910b, ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b, 7536bce1007a765fd097a7cc8ea62208a8c89b85, cb03b665069dad5e895a2c244929ea427f1fb9d1, 7fa30c8af436b7df513ef41e2cccd8a2404f37f8, 8ab93bee04cd5bdbd96002b2e325d02f61ba695a, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, 13c4e5a6122f3fa2663f63e49537091da6532f35, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, e4b52a1a00e9db941326fc857b95245cbfb60bce, 30f233eecca2239ee1dd754914324092e53f8f19, a8c1ed061813f832358c1aabf5d171bab80203bf, 8d942a3b52e2ad16ff8e5970be59591970d89fae, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, d235a9085e0543fcbe502fbc269f9a8ee01dcbab, 21042565ec941f4fa31ac5a0af85a1a84ff21f1b, 7d873a9c49d3864709aa762f8740edcdbd7369c5, 16753e0317730e8c1b297338300a8c6163dd06f2, 68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a, cf41991d89301c3c12420d150792cb1163999962, ff7bcaa4556cb13fc7bf03e477172493546172cd, 5cdb31c5e3e0aa4cc2d10229793b4911f69fd78f, 1562390dd212516cd857009cbd4f857a902d1f3d, 7a4fe2f003241ad97bf1778e527cb0306fa90da2"
6c755fc901d0b41a5d73c265f64a5aacf62e83b8a,GDsmith: Detecting Bugs in Cypher Graph Database Engines,https://www.semanticscholar.org/paper/6c755fc901d0b41a5d73c265f64a5aacf62e83b8,Conference,"Graph database engines stand out in the era of big data for their efficiency of modeling and processing linked data. To assure high quality of graph database engines, it is highly critical to conduct automatic test generation for graph database engines, e.g., random test generation, the most commonly adopted approach in practice. However, random test generation faces the challenge of generating complex inputs (i.e., property graphs and queries) for producing non-empty query results; generating such type of inputs is important especially for detecting wrong-result bugs. To address this challenge, in this paper, we propose GDsmith, the first approach for testing Cypher graph database engines. GDsmith ensures that each randomly generated query satisfies the semantic requirements. To increase the probability of producing complex queries that return non-empty results, GDsmith includes two new techniques: graph-guided generation of complex pattern combinations and data-guided generation of complex conditions. Our evaluation results demonstrate that GDsmith is effective and efficient for producing complex queries that return non-empty results for bug detection, and substantially outperforms the baselines. GDsmith successfully detects 28 bugs on the released versions of three highly popular open-source graph database engines and receives positive feedback from their developers.",2022,8,"2171106811, 49661434, 31131132, 2118207557, 144281339, 5779643, 2057038192",2171106811,Budapest,3,"48594758, 1765169, 2504776",Y,The key idea is to use a critic which randomly drops the activations in the logit and maximizes the sensitivity between two versions of discriminators.;The submitted paper shows that this principle is not a necessary condition large-scale classification.;Well structured analysis paper on shortcut connections but contributions/results are not compelling This paper performs an analysis of shortcut connections in ResNet-like architectures.,International Symposium on Software Testing and Analysis,31,"graph,database,engines,graph database,graph database","047286f5b9315a8e8bf56c4fc936e62f21495892, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 62df84d6a4d26f95e4714796c2337c9848cc13b5, 97ac11e5a6440eccb70ae7146392ac138c36fa6c, 91fc647899f801c9d351349ce73779918f90a713, 8b28792f8405b737229afb92c99c579b86d8aa98, dc10a3f4dae2db48148ff6781454ddcc856ae8da, 182180bd69ea6d2f59225ded5ddc900b8558ab9f, d1bb57da8593a2071b3ea8026865352ab3f7206a, 39602922b04885047254444fd1a1586d797617ce, fa63c3f53413ced7946623889c416e34a28676ea, 5ea7bf772fecf95cbf53b2c7f719c9440322a115, a60a4e5f7f872b9825ddff5d379857c2091ca52b, f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6, c84aa52bee5116f80c7740503edff4b08f733c3b, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, 27245e65a27bde90b5b0bb25d157bb75a0ad8b5a, f75b70c9d7078724b592ec3e21de705e7b6ff73f, 89a30b5dab02c9c390a632acad481fa602859272, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, 353c88c231ce156d604e074af276422422fc73f7, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786"
a2a514ed839dafdd0fb76d6c2615f25f35bf8087a,Testing Graph Database Engines via Query Partitioning,https://www.semanticscholar.org/paper/a2a514ed839dafdd0fb76d6c2615f25f35bf8087,Conference,"Graph Database Management Systems (GDBMSs) store data as graphs and allow the efficient querying of nodes and their relationships. Logic bugs are bugs that cause a GDBMS to return an incorrect result for a given query (e.g., by returning incorrect nodes or relationships). The impact of such bugs can be severe, as they often go unnoticed. The core insight of this paper is that Query Partitioning, a test oracle that has been proposed to test Relational Database Systems, is applicable to testing GDBMSs as well. The core idea of Query Partitioning is that, given a query, multiple queries are derived whose results can be combined to reconstruct the given query’s result. Any discrepancy in the result indicates a logic bug. We have implemented this approach as a practical tool named GDBMeter and evaluated GDBMeter on three popular GDBMSs and found a total of 40 unique, previously unknown bugs. We consider 14 of them to be logic bugs, the others being error or crash bugs. Overall, 27 of the bugs have been fixed, and 35 confirmed. We compared our approach to the state-of-the-art approach to testing GDBMS, which relies on differential testing; we found that it results in a high number of false alarms, while Query Partitioning reported actual logic bugs without any false alarms. Furthermore, despite the previous efforts in testing Neo4j and JanusGraph, we found 18 additional bugs. The developers appreciate our work and plan to integrate GDBMeter into their testing process. We expect that this simple, yet effective approach and the practical tool will be used to test other GDBMSs.",2022,10,"1663632797, 2868147, 1625425838, 38319925",1663632797,Copenhagen,3,"3419650, 143884284, 1783781",Y,"There are a few notational issues in the paper that should be addressed.;The authors' technique may let us do this data-generation easily.;Creative and interesting The paper introduces an application of Graph Neural Networks (Li's Gated Graph Neural Nets, GGNNs, specifically) for reasoning about programs and programming.",International Symposium on Software Testing and Analysis,31,"bugs,query,gdbmss,graph database,graph database","5b34752817bc0d6aa96466dabcbc24a83dd071fe, 73d4accea441aae2373828a8dc2175aa2759c38f, d84cf745c534c010b8e55e5a4a04878906848dc3, 9817bf0f78047452761e950c02a1a56f59a1e593, 046eb47d56beb8069b0098e3d01608f81ebb6849, b61b260de1599e6e89491cad9160898fcd3b34c2, bd6c027a3604d6c8fa23435bf382455b2bee436b, bf69c98fca9a9f6c1cde871beddbcdc668b77771, b4c9c134ad5bd4a037115df65411b4c49abe1322, 35adeef964fd344288febc7def2780007587724f, 624b2f14be4287d6a400cdf88a6f911b434b182e, 41c93960a066876d5e4f1dacaef75cd8daa2791f, d2a505586c0da20752b98f63c7760b6a5c41e28d, afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84, 155eeede0c070f1f017ba5c9f6cacd7ae0b098aa, d8348b802c9133d9e396d4ad809b020d5be42863, f9253a3e2627f1c2a0a7e2cea320a4ec4e4d2ff9, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, 551bbf493f55ea2c7b64ef8b91fc81d7bf6d68fe, 7171a0e9b07ebc98a32eb912262613efc20f283a"
e67a2817089312746d69b38ce9abfdc4b1bc69c3a,Finding bugs in Gremlin-based graph database systems via Randomized differential testing,https://www.semanticscholar.org/paper/e67a2817089312746d69b38ce9abfdc4b1bc69c3,Conference,"Graph database systems (GDBs) allow efficiently storing and retrieving graph data, and have become the critical component in many applications, e.g., knowledge graphs, social networks, and fraud detection. It is important to ensure that GDBs operate correctly. Logic bugs can occur and make GDBs return an incorrect result for a given query. These bugs are critical and can easily go unnoticed by developers when the graph and queries become complicated. Despite the importance of GDBs, logic bugs in GDBs have received less attention than those in relational database systems. In this paper, we present Grand, an approach for automatically finding logic bugs in GDBs that adopt Gremlin as their query language. The core idea of Grand is to construct semantically equivalent databases for multiple GDBs, and then compare the results of a Gremlin query on these databases. If the return results of a query on multiple GDBs are different, the likely cause is a logic bug in these GDBs. To effectively test GDBs, we propose a model-based query generation approach to generate valid Gremlin queries that can potentially return non-empty results, and a data mapping approach to unify the format of query results for different GDBs. We evaluate Grand on six widely-used GDBs, e.g., Neo4j and HugeGraph. In total, we have found 21 previously-unknown logic bugs in these GDBs. Among them, developers have confirmed 18 bugs, and fixed 7 bugs.",2021,15,"2158585032, 2964640, 134898163, 2093481779, 2131285720, 2118120527, 2152692124, 40231586, 144525882",2158585032,Sarajevo,2,"103131985, 2108097584",Y,Deep Temporal Clustering This paper proposes an algorithm for jointly performing dimensionality reduction and temporal clustering in a deep learning context.;Doing so will help better understand what is gained from using retaining a probabilistic form of memory versus a determinstic memory indexed with attention as in [Li et. al].,International Symposium on Software Testing and Analysis,30,"gdbs,bugs,query,graph database,graph database","0863a5ce955e5193e535e1442086dc460dd295f0, 9712624bb61abb0da989514cae558cfab61bb9d2, c9f320789e98d2c7a798a9705e26dbe317677966, 537f5e8e4139392cd2d108f32495e5b2b80151ac, 5031790972d496547b6613d46a4a0134c824db6e, e3b94a5f28522e6825aff16ff07d56bd70d26c96, e968ae8e98fff9e28468383a1826fca4a2ae5245, 410fba9f03212257d0881811802e6620e59bc827, 18d87bff073687c025f9bd23ab2dfb20d5f72a66, 925af6dcb0f8e6f3a5b2613400277be4b5434d10, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, b7034546bee38ba13d3b312fce893a22e33ce4dd, 7fa273f450251523e6b7fcc2eb3fdbdfd4a30493, aca6d5f3866372a4506cf15773ae298f18c3f453, 80dd97954ddf3edd22d4cb21f0ac31b7ffed6bbf, a56bf7ee9a56d8f84079684339a953c2df9ce76b, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, b6b7fea1846e85ac1e3c7e3adda6e65b127d0368, 2019cf49b51021a376f9833a53565513f0d8107b, da3f33d858586d24cb265e79eb54f3746e998f57, 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d, 39444c55f07839ac6a0d1839472a982f8fb447bb, 9e195234688778b2beb3528632e78dbabf816332, 25adff0bb9691b46fee5c0591300d6f3ccf117ab, aad2d03c17bc7d1e636d0e79944ad4588af989d5"
6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeba,UniKG: A Unified Interoperable Knowledge Graph Database System,https://www.semanticscholar.org/paper/6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb,Conference,"Knowledge graph currently has two main data models: RDF graph and property graph. The query language on RDF graph is SPARQL, while the query language on property graph is mainly Cypher. Different data models and query languages hinder the wider application of knowledge graphs. In this demonstration, we propose a unified interoperable knowledge graph database system, UniKG. (1) Based on the relational model, a unified storage scheme is utilized to efficiently store RDF graphs and property graphs, and support the query requirements of knowledge graphs. (2) Using the characteristicset-based method, the storage problem of untyped entities is addressed in UniKG. (3) UniKG realizes the interoperability of SPARQL and Cypher, and enables them to interchangeably operate on the same knowledge graph. (4) With a unified Web interface, users are allowed to query with two different languages over the same knowledge graph and visualize query results and explanations.",2020,8,"2116441064, 122024145, 152814510, 2118153844, 2113771309, 2332105",2116441064,Valletta,3,"2109971162, 48586730, 3081813",Y,"However, how widespread is this problem across other models or are you simply addressing a point problem for RN?;It looks like the authors extract position information from flappy bird frames, so the algorithm is only using images for obstacle reasoning?;A new method for weight quantization.",IEEE International Conference on Data Engineering,36,"graph,knowledge,query,graph database,graph database","74bc39003e65119eaa6ba339a61b45b417a638b7, 2f44ab0e52eb98fdfc0086638887f175b6f2fe4b, b6a7226e5f6d618370995eccad68af195ef32da2, 8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6, 780c7ead33428d282044519fee5e773ad56d5a2c, 4a7477881b66d12e79c704805781d4683a6a6be1, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, b7a717233ec3ff37385ab1b06816d0ca375f5bb3, cb23a59fdf3ade707600f076df4ff27a03941fba, 2a15e9ac5593cd1f8bd3a724e78a173037c02fea, 7d873a9c49d3864709aa762f8740edcdbd7369c5, 047286f5b9315a8e8bf56c4fc936e62f21495892, ccd94602e3acecf999d0c9ba62b1a8bc02e9f696, a8d76d84408c1fe6b1543084e6cec3dfc4ede429, a85c45ce7c893388e8eafa8a653b042e1497db48, 1661d0d8d47cac41e01c59c60aac3675b4396698, a80e26e6365b215715c182d19a9aa8bb876ac768, f5ac3ec506a5a01807dd7196c0c52eef008b78cc, f18be38578ee52aa7071c404d42e3d53ae003122, 53b0cfa2eaaa9703df02b2ca9c43260c4de5df0a, 69a72ff5b30642d11c96635e99aadad3140d33a7, bb5d26da72bfe7030dbc6650b686b210ae661f2c, b27a78366868ca47098e00dda74dd1b167b3a80d, df1a2539afbad27c4c80115a6f8f59a089024865, 3ea34401909978d3d3d0c25c8746e02c7d2a7c77, d35ed1fcab47cf98101dc745c42d3b51dace25eb, 545f108575314031f35c617c4ac35a10133c50e3, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, 665b0c776ff7507c32793f10ce9edf90bc2f674a, 551bbf493f55ea2c7b64ef8b91fc81d7bf6d68fe, b473e91cbe80c8b46451b49153cd5f93030480ab, 70ca38ad480c0be0eca89ccc4972d6cc9a5824da, 5107c83173e43f51d1bdebf6cafda525a7c26bf0, 48265726215736f7dd7ceccacac488422032397c, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 1f8a23697562b001082b147779b5eaefd3513d0a, 9e195234688778b2beb3528632e78dbabf816332, 0c00a328fa7cd56ee60338c54e89bd48310db80b"
0a71dd8bec060195e14eb9d0a7abbc08d960d4d5a,Research on Data Asset Management System of Graph Database Based on Internet of Things,https://www.semanticscholar.org/paper/0a71dd8bec060195e14eb9d0a7abbc08d960d4d5,Conference,"With the development of the times and the progress of society, the popularization rate of computer network technology and information technology in China is accelerating, and the Internet of things technology also appears in people’s vision, and is gradually known by people. In the context of this era of big data, it has brought great challenges to all walks of life. The development of everything must conform to the development theme of the times. In order to meet the challenge of the research and development of the data asset management system of the graph database in the new era, this paper puts forward the method of applying the Internet of things technology to the research and development of the data asset management system of the graph database. Combining with the foreign research and development plans of the data asset management system of the graph database, the data resources of the graph database are carried out from the platform system, the management structure and the organization arrangement Based on the research and analysis of production management system, a research scheme of data asset management system of graph database which can meet the development requirements of the new era is worked out. Through long-term research and analysis, we can find that the Internet of things technology analysis method proposed in this paper can effectively provide new development ideas for the research and development of data asset management system based on graph database under the Internet of things technology.",2020,4,"46694091, 2047926221, 2064360636, 2154976675, 48586730",46694091,Nicosia,3,"2065277797, 144782078, 2115646383",Y,"However, batch normalization only sees the variation in the activations given to it by a SPECIFIC set of weights.;Many relevant papers could be referenced in the introduction as examples of successes in computer vision tasks,  identity mapping initialization, recent interpretations of ResNets/DensetNets, etc.;This paper proposes a different approach (with could be combined with these methods) based on a new training procedure.",Journal of Physics: Conference Series,20,"development,data,research,graph database,graph database","7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, 5cb8f417d171ae329adf446820bd32d8b49d8c04, 5bedddda2aab2c5a03017d96a6dd7acf517c6961, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 86d3beff240b6c882058455e098a571de86564f5, e1e43d6bdb1419e08af833cf4899a460f70da26c, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, 00bbc94806ca0821a9c82d8aedf16f0e6263b89f, 82e1e8b222aeaca19d45375b31fdc825d1a821b8, ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba, f70b2f20be241f445a61f33c4b8e76e554760340, 5031790972d496547b6613d46a4a0134c824db6e, ca05ca5f70616456d0f2e05c4a51cbf8b8d273f7, b80e2af7ffa85fe2f02ea8390e4915d55c19d199, 9312e5efa0dcef1445d45a41771f12e2a8dc6715, 709af143f78bc62413c50ea1a7ee75b0702c4f59, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632, 5d8e450c8c7d9faa8222f1d0eaf34b01755c9eef, c9f320789e98d2c7a798a9705e26dbe317677966, 2e5d2f2dc01b150dffc163a9f457848e9b5b5c38, a3636512a48321baab95c94052de2a0a88460602, 0a829289a16ae48837cc2905635435db98bacc76, a9c1566119695250f68a572a4260b03721cc8ba3, 69a72ff5b30642d11c96635e99aadad3140d33a7, 3ec02e36cc0ba16292b255e9f90c3725521e2ec9, b2114228411d367cfa6ca091008291f250a2c490, 0863a5ce955e5193e535e1442086dc460dd295f0, df138c7425e787cac2f9d3ab7775c0fb5294a83e, 649c3497e3b34b15a5011259fcb837cf6c1ac04a"
5ea7bf772fecf95cbf53b2c7f719c9440322a115a,GRANEF: Utilization of a Graph Database for Network Forensics,https://www.semanticscholar.org/paper/5ea7bf772fecf95cbf53b2c7f719c9440322a115,Conference,"Understanding the information in captured network traffic, extracting the necessary data, and performing incident investigations are principal tasks of network forensics. The analysis of such data is typically performed by tools allowing manual browsing, filtering, and aggregation or tools based on statistical analyses and visualizations facilitating data comprehension. However, the human brain is used to perceiving the data in associations, which these tools can provide only in a limited form. We introduce a GRANEF toolkit that demonstrates a new approach to exploratory network data analysis based on associations stored in a graph database. In this article, we describe data transformation principles, utilization of a scalable graph database, and data analysis techniques. We then discuss and evaluate our proposed approach using a realistic dataset. Although we are at the beginning of our research, the current results show the great potential of association-based analysis.",2020,4,"2697611, 2082303711",2697611,Zagreb,3,"49399380, 2144447082, 144782078",Y,"These five objectives together are sufficient to achieve impressive English <-> German and Engish <-> French results in Multi30k, a bilingual image caption scenario with short simple sentences, and to achieve a strong start for a standard WMT scenario.;Actually, the proof never makes any connection to optimization.;This paper introduces a method for regularizing the REINFORCE algorithm by keeping around a small set of known high-quality samples as part of the sample set when performing stochastic gradient estimation.",International Conference on Security and Cryptography,20,"data,analysis,network,graph database,graph database","43e624ddeed82df944a6cae0dedec3372438e243, 6c739540e66e895311b7347971f10ef556e06e52, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, e02f91d625cd32290d4ede0f31284da115844316, cc017a62c605a0749e35a1264a46d62e78fb68b7, 7a1e71cb1310c4a873e7a4e54d1a6dab0553adce, 8799ec4bdf98bc313aa8c41a706a385e026d1f88, 9a0965beef113cc37491004b1848149e00300561, 77a59de2e2b832321875cadcf9619dc313f02384, 3789eb72c32ecf5e33442570358dd786dd67c8a2, 99f06e88e76f1af51d08d7adfb26d758ebc6acab, b79e5e4622a95417deec313cd543617b19611bea, a85c45ce7c893388e8eafa8a653b042e1497db48, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 1cff064f815111a71a98afda7aee1867ad617901, 6a261e1e38506b0e4c113ba29a2d5e5d0709ed26, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, 4f8d648c52edf74e41b0996128aa536e13cc7e82, c9645aa4ea31903e02e201b877fd3e1466adff4f, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, 6a1b25f7a67395ad1e676027322913acbb0a0635, 549e933821fdf7cd0309dacaae99c8284cbfcc24"
09f54c64b39f5f7e7570f9f4ce3e3af544401e14a,A Quantitative Analysis of Student Solutions to Graph Database Problems,https://www.semanticscholar.org/paper/09f54c64b39f5f7e7570f9f4ce3e3af544401e14,Conference,"As data grow both in size and in connectivity, the interest to use graph databases in the industry has been proliferating. However, there has been little research on graph database education. In response to the need to introduce college students to graph databases, this paper is the first to analyze students' errors in homework submissions of queries written in Cypher, the query language for Neo4j---the most prominent graph database. Based on 40,093 student submissions from homework assignments in an upper-level computer science database course at one university, this paper provides a quantitative analysis of students' learning when solving graph database problems. The data shows that students struggle the most to correctly use Cypher's WITH clause to define variable names before referencing in the WHERE clause and these errors persist over multiple homework problems requiring the same techniques, and we suggest a further improvement on the classification of syntactic errors.",2020,4,"153314895, 66327914, 2051972259, 2517099",153314895,Copenhagen,2,"1779967, 2109139810",Y,"Actually, the proof never makes any connection to optimization.;The use of the proposed gamma distribution, as a simple alternative, overcomes this problem.",Annual Conference on Innovation and Technology in Computer Science Education,25,"graph,database,students,graph database,graph database","39602922b04885047254444fd1a1586d797617ce, 182180bd69ea6d2f59225ded5ddc900b8558ab9f, 11342d45911ee8a7c9e3a94117ce774ad7036172, cb03b665069dad5e895a2c244929ea427f1fb9d1, a0f788f6de0fb83d623c875a98120e3f347f70d1, ec7f5dc077480df149bcd4358a3aa8441878ca59, 3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b, 9181b0d801dfcd7723a3ede201f0543078e2c149, e32a2519b59d62cff6cb8136ee242dc3754ed57b, a13149a80855412d970d0de2b41c611f4cf7e1da, 811df72e210e20de99719539505da54762a11c6d, 598231eb906b183f7a2a408ef4536127e11e3de9, 45de91a919780d5540872cf047986a370625e61c, 0fa554d981809c5eb78956c779f75092c4f6c16b, 643da4c4de1954daeac571a82367241db012a8bf, 63adc1e5086481e36b19b62707a96b799da51e59, bf69c98fca9a9f6c1cde871beddbcdc668b77771, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, 0ac1a1ccb16c8441f42ff8de743fc93f0e08c15c, d8a5474f450330ad25c1e22f27e88f3630cb840d, 34ca47eed139a7f0694611528f75debc43385518, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, d9d325ca670a1aa215e3e39023f8abf17dae7584, 545f108575314031f35c617c4ac35a10133c50e3, 3337e4b2e63e5e99171f9da9d161771f5810c27a, 9fcdbfdf28245010c875ce85502351fe05c04b49, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 32a849fe3020144e5ba82ba0442ac571f554ca31, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 5b6ec746d309b165f9f9def873a2375b6fb40f3d, 472644c5f4155635cf9e9e37540bfa53c20e7610, 69d49a06f09cf934310ccbf3bb2a360fa719272d, 1fa4936fb06319c3f4536c26a447d5507c92bd48, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 795550a5294eb05ea4f3b14f0b1c21a405493d85, c84389369720dcd2f004c48e58fbac2c45c8f092, 4f480bae3196dbbc27ab383bce33478ea963f9b3, fe2492b7b8cf6d1d10b7ea38e0f7f853bd679d52, 2737a61f6557fe7bf53a608c668de2eff1f582f0, c0bcd7dc9426a70af15f5ad63b4af92ea4dcbd4d, bfad52fc64ca0169644b6e7e0ea9a46470d51709, bcc82ce554942880814243fc8c08a88b9d2aad09, 0599f45e03ac2016321df0dd653ba4c0034c79d5, f70b2f20be241f445a61f33c4b8e76e554760340, 8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6, fa75a55760e6ea49b39b83cb85c99a22e1088254, 4b991efaa8493a5925c2aee9eec980831213eba6"
b0b770fb8c7760749c88e3c83ae173cdb07f7bd5a,An Attack Path Generation Methods Based on Graph Database,https://www.semanticscholar.org/paper/b0b770fb8c7760749c88e3c83ae173cdb07f7bd5,Conference,"With the popularity of network technology and the expansion of network scale, the network security risks are increasingly serious. Network vulnerability assessment methods, a technology of active network security defense, have attracted many researchers. Most existing network vulnerability assessment methods store different types of data in different ways, which makes querying and analyzing inefficient, especially in the complex large-scale network environment. In order to solve this problem, this paper proposes a method of network vulnerability assessment based on graph database. The network host information, association relationship between hosts and vulnerability information of the target network are stored in the graph database, the query and analysis are carried out by using the graph database query language. Graph database stores the information of the network hosts, association relationship among hosts and vulnerabilities of the target network. The graph database query language supports querying and analysis. Visualizing the network topology, vulnerability information and all possible attack paths provides a reference to develop the network security protection strategy. Experiments' results illustrate that the method runs efficiently and helps with querying and analysis, which is applicable to large-scale complex network environment.",2019,13,"79470079, 80752053, 1596817678, 1672530059",79470079,Budapest,3,"3474704, 2108706355, 2157681212",Y,"* There are many estimators for f-divergences (like the ones cited above and many others based e.g. on nearest-neighbors) that are sample-based and thus correspond to the ""implicit"" case that the authors discuss.;For completeness, it would be great to include one more algorithm in the evaluation: an ablation of DnC which does not involve a central policy at all.;The evaluation with the sequence of checkpoints was created by using every fifth image.","2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)",3,"network,vulnerability,graph,graph database,graph database","e359e8960b0b09e8685a32927b7818f4b06ef881, 4b06c7e29280b1c6bc05c9df39023b48fef02c93, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, b7a717233ec3ff37385ab1b06816d0ca375f5bb3, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, 3dfa820702b6181c9964931f0a4d47fd298bf429, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, 453fdfeefd6498a65be339d7e8722f6f3288907e, bf69c98fca9a9f6c1cde871beddbcdc668b77771, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 69a72ff5b30642d11c96635e99aadad3140d33a7, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, a2ec47b9bcc95d2456a8a42199233e5d9129ef18, 9954ec0a95fc0840f7712d7b0c67b242bb536a9e, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, 9e195234688778b2beb3528632e78dbabf816332, d7b820af40a9e2660ef700d39f7b2e27b43435c5, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, 7fa30c8af436b7df513ef41e2cccd8a2404f37f8, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, 7904b3446775ed8c79f4f94001a16b706989c462, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, 2e7f532796eed2847d4c19e3cff03756049e81b4, f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa, 0095acc4f2c3255cf38fdf844003c97858adb418, af13a92977d4f4dc5b28b13746d86111d42939e8, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, 6c739540e66e895311b7347971f10ef556e06e52, 37b0a7a6c8fb26e32b9206847e78d521a2cd5900, 62ccd99a65bfc7c735ae1f33b75b107665de95df, 6c34842a92ce4da9aab586490afdbd8779af4eab, 0e141942fa265142f41a2a26eb17b6005d3af29e, ff7bcaa4556cb13fc7bf03e477172493546172cd, 5bedddda2aab2c5a03017d96a6dd7acf517c6961, c0e6cd2ec3bc9eb46c7d45bb708854da3327339e, 557ee73ff4e87ca0cb1b6c78af0730a2d94b710f, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, d6bc29a897fd85e7187dc33c3c974b8879462237, f476d44a4892e0c8256e50e9075f8dd3d412bfcf, 647c4a9331e01e31a4350361d3460f0397fe694f, 20a3ed03888037e2802fa9abad02ffa0a8dcc228"
b613887337a5d2e8fc8773037116be81e6346835a,A1: A Distributed In-Memory Graph Database,https://www.semanticscholar.org/paper/b613887337a5d2e8fc8773037116be81e6346835,Conference,"A1 is an in-memory distributed database used by the Bing search engine to support complex queries over structured data. The key enablers for A1 are availability of cheap DRAM and high speed RDMA (Remote Direct Memory Access) networking in commodity hardware. A1 uses FaRM [11,12] as its underlying storage layer and builds the graph abstraction and query engine on top. The combination of in-memory storage and RDMA access requires rethinking how data is allocated, organized and queried in a large distributed system. A single A1 cluster can store tens of billions of vertices and edges and support a throughput of 350+ million of vertex reads per second with end to end query latency in single digit milliseconds. In this paper we describe the A1 data model, RDMA optimized data structures and query execution.",2019,22,"1790681, 1904916, 2089990776, 73527512, 2070951368, 1630331317, 1630330861, 40444389, 1630293705, 1630330434, 2238313, 40443723, 2069452048, 2111073292",1790681,Bern,2,"2108467971, 3382568",Y,"Moreover, the discrimator D  (which is trained to discriminate between real or fake examples) seems to be directly used to tell if an example is throw from the targeted distribution.;Motivation In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle.",SIGMOD Conference,19,"a1,data,rdma,graph database,graph database","1a60a9d1eef24e123c27a9eee5a399ac2b620fee, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, b3dbe1b460a3b66df1653508c9eed7dd51dee4d2, 7fa30c8af436b7df513ef41e2cccd8a2404f37f8, 6c34842a92ce4da9aab586490afdbd8779af4eab, 639bfab64e2f35917d450013e136cb24c7755fad, e7f3478fd8aac6940a4bf4f5eb60ac38f6b0b85b, 0d065e8688c38bb0148203a1738f47184a5b58d3, dadfb3ff45e19dc22456a645f441bbeb17c93c9c, 3f43bcb910df8c1a76de79057a63195e6c6bc258, 89a30b5dab02c9c390a632acad481fa602859272, 6dc4883228c95e8a332320fcc587a0ff33c84d59, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 30f233eecca2239ee1dd754914324092e53f8f19, d9a7fa7616a327367696e19b1846519745cd43ff, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, 2141334fad7248fc707607bc9453d44686ae07a7, c12b80b44d9acfe6cd92fdf965264c4b706c367c, 9c1265fd47df8d825f0ddd6d9ab834002592e38e, bad4c08f03587e38ee960e2aa76e16d722826e7c, 89af855962927fb89a673a221f6f394a6f3dfc6a, 2478f43de2f1fcd9301a81e9b42d86b1d46db02d, be383c607d4d357c763d2329ab71799c6e1393b4, 7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca, a92b103b81a48878e76f1fcfc3e2a1454f895555, 4087e84fc695bb6433d0104ee94f9d7e9f4b7da5, cb23a59fdf3ade707600f076df4ff27a03941fba, 56ae2837b6cd4d143bbfa4a4b06811d70126106f, 0c00a328fa7cd56ee60338c54e89bd48310db80b, 1b0aa15937fdf59103a5213bccf09cff83d0ee3e, c292e473b3825eeb9db03c70b2e1c033aea190d5, 6548106035c7208ad498730627874a482734b9ac, 676664ee7471738577f641e6159e7596625b7fdb, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, 2e7f532796eed2847d4c19e3cff03756049e81b4, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, 75c364909914f17791837ec88090262aa6656d3e, fb5413afba689d16543215c5a2ddbc5b78a52007, 156609022dd6258c60238859622da0a1683bd062, b266510f5f9b40d42b51884ad13a1867fb3284fd, 92930ed3560ea6c86d53cf52158bc793b089054d, 5031790972d496547b6613d46a4a0134c824db6e, aee3d7f98b966240178ef420724c840f9b61deb3, cd29c25c489562b409a60f83365f93f33ee1a0a1, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, ad10ef93675513a68b93d54f3a461160b53318a3, 98e6c6d860383fea5bbad145deed51514d23b86c, 537f5e8e4139392cd2d108f32495e5b2b80151ac"
6fb020754f6de564c3a0a07bb656c0a90be1f87da,Incorruptible Auditing: Blockchain-Powered Graph Database Management,https://www.semanticscholar.org/paper/6fb020754f6de564c3a0a07bb656c0a90be1f87d,Conference,"In modern and interconnected world, information is accumulatively stored digitally, making the process of exchanging, gathering and querying the information much easier. Continuously, it has introduced new challenges about how to ensure its consistency and reliability due to the sheer volume of data. A blockchain-based information system can provide an incorruptible record of history, enabling better auditing and data management practices. The paper describes how to combine an Exonum blockchain and a Neo4j graph database into a system that can provide a verifiable audit trail of data integrity and its modifications for information stored in a graph database.",2019,10,"2069559207, 1903751380, 40915325, 1904203865, 1904199594, 1500655986",2069559207,Bern,2,"2054451943, 1772311",Y,"MODEL & ARCHITECTURE The PATH function given a current state s and a goal state s', returns a distribution over the best first action to take to get to the goal P(A).;It might be good to emphasize that you don’t train on the IWAE bound in any experiments.",International Conference on Blockchain,19,"information,data,system,graph database,graph database","709f7a6b870cb07a4eab553adf6345b244913913, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, c2aab470b8cf92f090e0a3bac1794b21500585e6, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, 6c34842a92ce4da9aab586490afdbd8779af4eab, 410fba9f03212257d0881811802e6620e59bc827, 88932b1563316e4ed7a61c4c2c0c90d7aa9bb308, eed62d36d1b976ac3873c83645f1c25f5096f89c, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, 75ea299834d6949e89e91d006677343ddab44e49, 5290d7921f0266c8b50b79fc8a0b7d22868f4f60, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, 099043827df60225cf33c820052716cce64d49e9, f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d, 0a92bc2dc8a216e6aced83edc0358241066833df, 3337e4b2e63e5e99171f9da9d161771f5810c27a, 7998468d99ab07bb982294d1c9b53a3bf3934fa6, 957f5b1e7ca48891c2e279aefbfa0f04d989c21e, 916455d97cd792c2eb5b00663689592e25cbc8d8, cc5726fc0ebb84f741f3496a3c52ced162c596ba, f0f1627db35b4942e0f83069f20dd0948fc35d28, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, 78aab73ed574393ab421f25b3a0e3f7343e64748, 156609022dd6258c60238859622da0a1683bd062, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 96023195e889fc258e6ff30aa99d250982dfae01, 4be7d1524edb0137599a5cc95f72844b85a52fe1, ef35da3a3c471a6a15c7a3b09586483eb50cbef0, 52a6695ae1c08cc29baf764dedb5831c7a954214, 22ebfc211d184ed615729378a43fde175bf14478, 3db1841fd5f2561a11dfbd8173616b3e695c84a1, a830083704284c8c5ddaf04f676c6ce23d583942, 2c03df8b48bf3fa39054345bafabfeff15bfd11d, 07b01d665646009439ca206378cc35e095ec6cd2, f9367342405a73ab8d6de704a149babfc0edb5fe, 5778e56400f7113c2b1355fdbd6b638fa379885f, 98ce7af921e7c52d81df64d632d34eb09522cd75"
33e332837e91c1048c3ed165cd16bf7607c3bf06a,Issues and Concepts of Graph Database and a Comparative Analysis on list of Graph Database tools,https://www.semanticscholar.org/paper/33e332837e91c1048c3ed165cd16bf7607c3bf06,Conference,"The work is review in nature and focuses on basic concepts and example on Graph Database with a special focus on list of standard computerized tools available for handling the queries using graph database structure. The implementation benefits of each tool and a comparative analysis on various functionalities has been presented in this work. This work also elaborates on popular Graph Databases tool that includes Allegro Graph, ArangoDB, OrientDB, Infinite Graph ,Neo4j, Titan, FlockDB, Bitsy, StarDog, MongoDB and investigate their acceptance for solving day to day problems.",2019,9,"152781515, 46501582, 105003008, 152125305",152781515,Bratislava,2,"41020222, 145929920",Y,1. I'm concerned that the contribution of this manuscript is a little incremental.;Why choose F = 10 and K = 3?,International Conference on Computational Collective Intelligence,19,"graph,work,database,graph database,graph database","9e540662619327a3056d9e40bb58058868f6f805, 9a60d3c9d3a5b7f165325fca45bd418d651682d3, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 7fa273f450251523e6b7fcc2eb3fdbdfd4a30493, 595101f13b961d69c553ce1ed24f60f3f1085e02, 7904b3446775ed8c79f4f94001a16b706989c462, 1f8a23697562b001082b147779b5eaefd3513d0a, 5d433da6d0f143f20936379910104d2bb139d4ae, eacf9284a39adcd56172665f31fd5a72560bba7a, f476d44a4892e0c8256e50e9075f8dd3d412bfcf, 2bc9d6830610e18f110aa7984f7c0271d6b17eeb, a92b103b81a48878e76f1fcfc3e2a1454f895555, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 364128bcce9836d60e685bb717b80f30e25092e0, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 0427110f0e79f41e69a8eb00a3ec8868bac26a4f, f63e917638553414526a0cc8550de4ad2d83fe7a, 51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df, 29ddc1f43f28af7c846515e32cc167bc66886d0c, bb0cec8f2d34cfb9dbf8bffd1a5de499311ae098, 834fdec542153aae5fe725df801aac87ba5e8f56, dcbaf58b16ac7ef947879ea37c021466357b291a, 3813b88a4ec3c63919df47e9694b577f4691f7e5, b9b639522465cc606df878eee62e7f9c4bf19e62, 9e195234688778b2beb3528632e78dbabf816332, 5c39e37022661f81f79e481240ed9b175dec6513, df7336844a31165db0ae08f1cd0f560c9e3faeea, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, 92930ed3560ea6c86d53cf52158bc793b089054d, 099043827df60225cf33c820052716cce64d49e9, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, e8b30ebe3351680c3b039555ae0a8d0865ad829b, 8bba999de25bfb288b3f7f88e1d907aab02638b6, 88932b1563316e4ed7a61c4c2c0c90d7aa9bb308, 2346d121f38fc19c77e0b062415519843f478163, 62df84d6a4d26f95e4714796c2337c9848cc13b5, e7b0e2710821a9fe1e3d2b9a09dd94514b2d5ea5, 6ba00c2386f2edc0b43eec442cd1923b5d964633, 795550a5294eb05ea4f3b14f0b1c21a405493d85"
72afe82af4c2ca100c36eb35292e85d806527f0aa,Construction of typhoon disaster knowledge graph based on graph database Neo4j,https://www.semanticscholar.org/paper/72afe82af4c2ca100c36eb35292e85d806527f0a,Conference,"The typhoon knowledge graph can correlate various kinds of information in the typhoon data, conduct overall and related analysis, and finally provide effective assistance for typhoon prevention and post-disaster protection. The data of typhoon landing in China from 2000 to 2015 were selected to build a typhoon knowledge graph based on Neo4j graph database platform. The typhoon knowledge graph can be used to understand the occurrence of historical typhoons and obtain the distribution of typhoon data in time and space.",2019,8,"2158490269, 2118798587, 2152209915, 1877327478, 145104321, 145843448, 2157681212, 2157843743, 47149500, 2108691840",2158490269,Amsterdam,3,"145560079, 35210462, 1904916",Y,"The most important idea of the papier (PDE-net) is to learn both differential operators and the function that governs the PDE.;Considering that this is a big, complicated system, it is crucial that the authors included both an ablation experiment to see which pieces were most important, and an experiment that indicates the amount of labeled data that would be required to achieve the same results with a supervised system.;It might be worthwhile to briefly describe the encoding/construction algorithm used in the paper.",Chinese Control and Decision Conference,19,"typhoon,graph,knowledge,graph database,graph database","1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0, 9c1265fd47df8d825f0ddd6d9ab834002592e38e, fe4c5074f021cd1c810892c1b6bc267b23aa6e5c, 3dd7f7118ee174265889d00d100cfe2a02871be8, dd4d82299b4209db539d639f836fcee663cf72b3, f9c990b1b5724e50e5632b94fdb7484ece8a6ce7, fe44200fed05f9a7c656f2245deded8fd5f5e1e6, e02f91d625cd32290d4ede0f31284da115844316, eeac4411ae119c6c7ac33a11f762f2495b4dd960, e5a1cfcd07dcfd8b1feec9c635dadc858cde8166, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 1a37223175138bc1aa53b425ea2fdd0b382405a5, c48e0bd0f36c25ab83befbc7b7da369b75fd25f5, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, fac67bf55456b52ac6e4f280ad953d0250c74ebc, f4c4e148546089123f8da5db4fb246ab4062bd40, 8674494bd7a076286b905912d26d47f7501c4046, 2660dcf5bd16d14862a7bbb241fa4d85ae34327f, 1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, ec7f5dc077480df149bcd4358a3aa8441878ca59, 0cec0c296efedb814342b4b841d4583efbfc6777, aca6d5f3866372a4506cf15773ae298f18c3f453, 0258bab20bc8574ee602012081a17db89339f12d"
ce54e3b89a2570035b70885e6901ad4c92ae41c9a,Construction of power projects knowledge graph based on graph database Neo4j,https://www.semanticscholar.org/paper/ce54e3b89a2570035b70885e6901ad4c92ae41c9,Conference,"In ""The Belt and Road"", China’s overseas power projects grow more and more, which is associated with a large number of dispersing project information. Construction of power project knowledge graph based on graph database Neo4j can facilitate the management of overseas power projects and to have an intuitive understanding of the relationships between projects for further overall planning. Meanwhile, enterprises can describe the spatial distribution characteristics of countries along the ""The Belt and Road"" according to the knowledge graph of overseas power projects, so as to understand the space and potential of future power investment development in different countries.",2019,6,"2129795635, 13324446, 2000860679, 2108097584, 2000992985, 2162074006",2129795635,Berlin,2,"2746913, 144310754",Y,"During research, we have multiple executable oracles and need to produce good training data from them.;Concerning the lack of mathematical rigour in the statistical physics literature on which the authors comment, they might want to relate to a very recent work https://arxiv.org/pdf/1708.03395.pdf work that sets all the past statistical physics results on optimal generalization in single layer neural networks on fully rigorous basis by proving that the corresponding formulas stemming from the replica method are indeed correct.","International Conference on Computer, Information and Telecommunication Systems",19,"power,projects,graph,graph database,graph database","e70ddfb04969b663ef8cd711a70a0acf563d6e5f, 472644c5f4155635cf9e9e37540bfa53c20e7610, 9eacf62f1e546748428c7e4843731b1595294200, bb5d26da72bfe7030dbc6650b686b210ae661f2c, 375125029b085e70a109491656b69aa01bc2a166, d9d325ca670a1aa215e3e39023f8abf17dae7584, 9712624bb61abb0da989514cae558cfab61bb9d2, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, f397b593de771752e7002a954eb531f3ef6a975e, ff75865cde62592d068b2afd055c57c81d77158b, b61b260de1599e6e89491cad9160898fcd3b34c2, 6068d39e92aef1bb0e1291e9931894c35692a85e, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, 182180bd69ea6d2f59225ded5ddc900b8558ab9f, c96fc88631f2b8e2fe192027a8a237445635328c, df7336844a31165db0ae08f1cd0f560c9e3faeea, 24ab4e99e582c9770281eee0a39cbeb70ddd891a, b3dbe1b460a3b66df1653508c9eed7dd51dee4d2, 7af07490da518c8ef3cf2ae106071df2c2d0101e, fa75a55760e6ea49b39b83cb85c99a22e1088254, d422df8bff4e677a3077635db116679d25142bfc, c2528e88d5554e9df9f9d482ad46cb5331c4d794"
b09139c153bac8893e8faea2b3a59159234caadca,A Graph Database Approach to Wireless IIoT Workcell Performance Evaluation,https://www.semanticscholar.org/paper/b09139c153bac8893e8faea2b3a59159234caadc,Conference,"The workcell is considered a main building block of various industrial settings. Hence, it is examined as a primary testing environment for studying wireless communication techniques in factory automation processes. A new testbed was recently designed and developed to facilitate such studies in workcells by replicating various data flows in an emulated production environment. In this paper, an approach to storing and analyzing network performance data from a manufacturing factory workcell is introduced. A robotic testbed was constructed using two collaborative grade robot arms, machine emulators, and wireless communication devices. A graph database approach was implemented to capture network and operational event data among the components within the testbed. A schema is proposed, developed, and elaborated; a database is then populated with events from the testbed, and the resulting graph is presented. Query commands are then presented as a means to examine and analyze network performance and relationships within the components of the network. Additionally, we demonstrate how to extract correlations between receive signal power and network delay within the testbed using the graph database query language. Finally, using the inherently interconnected nature of the graph database, we discuss applying the graph database approach toward examining more complex relationships between the wireless communications network and the operational system.",2019,5,"3420212, 2049649, 1979284, 151500725, 1784025",3420212,Athens,3,"143924672, 2677700, 2146245",Y,"A number of heuristics are used to augment this reward function so as to provide shaping rewards along the way and speed up learning.;This would contradict some previously established convergence results for this type of problems: Reddi et al. (2016) Stochastic Variance Reduction for Nonconvex Optimization, ICML and Wang et al. 2013.;After introducing the idea and related work, the authors in Section 3 give details about how to perform the quantization.",International Conference on Industrial Technology,19,"network,graph,database,graph database,graph database","e5194ae88d63c7549678b1b73cfdaf7112164272, ead6121fbc787d508dc6a6d7106f72bf0d647d03, a9640bac0b45a804d07fc5914feb08af8f2a73f2, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, 0095acc4f2c3255cf38fdf844003c97858adb418, 8ef0c1c2030aa265a4e7c836d080c2e2088efde6, 0f38b3d717e0fcc6eacc9c6e78b252227440e04e, ab606e9d148458f6d54e5d44abefd73b7990f6e0, 046eb47d56beb8069b0098e3d01608f81ebb6849, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, 59c414c9efb77562f5d1aad8af14eaac968c69c0, 312b1067d89f598c4c5c0799aa18b48d0926bed8, bf07f2927dca481653b8c60b2dc982fe4a7dfd4e, b889b1d6944213bc2ca29e3ad07ee65ede20892d, fd3259ecd1c94731f5d9bd9c0bf7417f2e147c71, cb03b665069dad5e895a2c244929ea427f1fb9d1, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, facd5f5deb152229ceb1803434d8690a09ab4129, 76b93abde3ec4e5bd84a38d488a8db209dc4ac98, d2a609ffb814442d0728aef9f6616f9cd775face, 83cebf919635504786fc220d569284842b0f0a09, 88a724083b2cfcc096448c28e6973c8f761ee463, f72053903270d9a7f41108461ad04d5aa075218d, a926093ef103c02fd5ef7310e0d41b83d1958ed5, 45674df7143e43bc589cfabd26dd194c2a7f090d, f2bc4057e696f49c326bf8e1588772a16f053754, 4f8d648c52edf74e41b0996128aa536e13cc7e82"
8ef0c1c2030aa265a4e7c836d080c2e2088efde6a,(Graph Database: A Survey),https://www.semanticscholar.org/paper/8ef0c1c2030aa265a4e7c836d080c2e2088efde6,Conference,"The advantages of Relational Database Management System (RDBMS) model and design methodology are being utilized by industry/institutions for any software design and implementation. The future of RDBMS certainly will be the Graph Databases with NoSql methodologies, which is emerging as beyond of relational model. In this paper, there is a need to highlight all the databases evolved after RDBMS. They couldn’t stay in market for so long period and survey has been made to highlight those databases after RDBMS. Relational Database Management System has certain advantages like (i) Storing in Tables, Column and Rows (ii) Data Storing in Normal Form (iii) Easy to use via SQL to retrieve information via complex join operators (iv) Maintainability via Reverse Engineering (v) Indexing and quick search. Due to these inherent features of RDBMS and SQL, it is necessary to explore and compare RDBMS with NoSQL methods to avoid complex join operation. Recently, numerous software industries and research institutions are trying their old RDBMS system to be re-engineered into some other architecture via nodes, edges and relationships where different type of information can be stored easily. So, it is a big challenges for any industry and institutions how quickly they can re-engineer their old RDBMS into Graph Databases which is also called now-a-days the future of databases. In this project, it is highlighted that the importance of the re-engineering work lies in three different directions such as (i) Comparison of RDBMS with GDBMS (Graph Database Management System) where face book, twitter, Amazon, Google are adopting (ii) Survey work of Graph Databases and (iii) Graph Database Models have increasingly become a topic of interest. The representation of data in the form of a graph lends itself well to structure a data with a schema. No standard system of query languages yet had been found to have been unique and stable for graph databases. Research and industry adoptions will determine the future direction of graph databases.(iv) Beyond RDBMS artifacts were established by industry and academics. It feeds a series of recycling collectives trying to eke out an existence of positive incentives and principles.",2019,7,"2052840328, 1996173264",2052840328,Chisinau,3,"32559865, 2163313042, 9319875",Y,"The presentation of the paper could be significantly improved.;The authors show mixup provides improvement over baselines in the following settings: * Image Classification on Imagenet.;Minors: There are some mixed-up notations: tilde{A_i} => A_i , and rank(A_2) => rank(A)_2 in Proposition 3.","2020 International Conference on Computer, Electrical & Communication Engineering (ICCECE)",19,"graph,rdbms,system,graph database,graph database","6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2, ac4350d3a2673a4c34fa949714ede8b45fd04f1d, 8adb47deeef943c2c1bae41f9498a382fb818a16, c5c4142a01981787a71bf6ebcb791520c458ab5d, e5a1cfcd07dcfd8b1feec9c635dadc858cde8166, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, 71854ff4306cf65c3c2161f7be2d0346275f72d5, 0af24df95f8a0b7e6fc257a98a5820314d1243f3, c419ee7315b9edfd8fc55bab16534fc55a564fcd, a0a79dad89857a96f8f71b14238e5237cbfc4787, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, 598231eb906b183f7a2a408ef4536127e11e3de9, ce3285bf1853f00c00535325851df5c33a0fc5d6, 8c7aee0bbc062d5b4bcd34951b1e002274288206, 10a69070c8585dc5564b11b0e53dbe4d565f9ce1, 4cd033a56b19f87f6adfefeef5fcc990306ecf40, b79e5e4622a95417deec313cd543617b19611bea, 4cf2034fa55a20e60a24ca6924f66aaafb30b877"
099043827df60225cf33c820052716cce64d49e9a,A Review on Graph Database and its representation,https://www.semanticscholar.org/paper/099043827df60225cf33c820052716cce64d49e9,Conference,"Extensively, facts are represented characteristically as a table for the purpose of making it indexed with increased readability. Currently, the tendencies are altering as Graph databases are rapidly attaining popularity. Actually, it is appropiately termed as ""the outlook of DBMS"". The demonstration of facts within the procedure of a graph advances within the circumstances sound for the prearranged facts through a dynamic schema. This paper discusses the backbone of graph database as to why they are gaining much popularity in present situations illustrating the dissimilar types available and their distinction. Owing to the extensive usage of graph algorithms with models, neither a standardised system nor query language has been dispossessed with graph databases. Research and industry acceptance will regulate the upcoming course of graph databases. The authors have tried in representing the graph database with a real life scenario.",2018,7,"152125305, 46501582, 2744320",152125305,Rome,2,"143666627, 143834867",Y,"What are the class you are interested in?;For example, does this observation that the local curvature of the loss around minima is different for ResNets and VGG allows to interpret the difference in their performances ?",2019 International Conference on Recent Advances in Energy-efficient Computing and Communication (ICRAECC),18,"graph,facts,popularity,graph database,graph database","c84aa52bee5116f80c7740503edff4b08f733c3b, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 76b93abde3ec4e5bd84a38d488a8db209dc4ac98, b85f3a66245d483f3eb3447eaf9950bd55f2b21e, 38f5b53b49be555430f33b8363910191a3df1d14, 6a1b25f7a67395ad1e676027322913acbb0a0635, 6e74403ab75d0080c056fd66702ab1f54f04d9b1, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 4419c5720e30d5ca5158795d4c848125650b8db1, 4087e84fc695bb6433d0104ee94f9d7e9f4b7da5, ccd94602e3acecf999d0c9ba62b1a8bc02e9f696, 16fa12ebc578df676f3dda5453ad56c15a0d6702, b69a35662a2cac38eab22f4481285116bdf8c30e, f295157f37cfb43cd8d8d2690ea124edc5ea59c2, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c, 4a7477881b66d12e79c704805781d4683a6a6be1, c5c4142a01981787a71bf6ebcb791520c458ab5d, 353c88c231ce156d604e074af276422422fc73f7, cb03b665069dad5e895a2c244929ea427f1fb9d1, d9b34c6b616f75485856794478bfbeab1ea93b81, 2e7f532796eed2847d4c19e3cff03756049e81b4, 417326e51d78ba8bd2621f23e539b41bbdd336d6, 9e195234688778b2beb3528632e78dbabf816332, 54ddb00fa691728944fd8becea90a373d21597cf, 82e1e8b222aeaca19d45375b31fdc825d1a821b8, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, 733fc094e785724621c46e20db1be69f132ad9df, 71ba8a8c75e0826f94eb53ee7a4566d4fc5b5256, c292e473b3825eeb9db03c70b2e1c033aea190d5, c5d3fbc024ad9a0d28669b6030de23fc5c3f7888, bd6c027a3604d6c8fa23435bf382455b2bee436b, 3d82552eb483e5ea84b577a0e8d5f157a6085824, a9cbbef8f4426329d0687025b34287c35bdd8b38, 07cca761749bfe21c2d096ff60f32b574d5c84c4, 68a3d32416977e88cf1bfa4ad548d403f5f089d6, 2c3eef2f17369912e330281d54b535675077e4ca, c6879e43828b293567f5e2da039d23845189d6a7, 29409efa04ac99ccf01d2a011d21d5d14e870000, f476d44a4892e0c8256e50e9075f8dd3d412bfcf"
45674df7143e43bc589cfabd26dd194c2a7f090da,Computational Modelling for Bankruptcy Prediction: Semantic Data Analysis Integrating Graph Database and Financial Ontology,https://www.semanticscholar.org/paper/45674df7143e43bc589cfabd26dd194c2a7f090d,Conference,"In this paper, we propose a novel intelligent methodology to construct a Bankruptcy Prediction Computation Model, which is aimed to execute a company's financial status analysis accurately. Based on the semantic data analysis and management, our methodology considers Semantic Database System as the core of the system. It comprises three layers: an Ontology of Bankruptcy Prediction, Semantic Search Engine, and a Semantic Analysis Graph Database system. The Ontological layer defines the basic concepts of the financial risk management as well as the objects that serve as sources of knowledge for predicting a company's bankruptcy. The Graph Database layer utilises a powerful semantic data technology, which serves as a semantic data repository for our model. The article provides a detailed description of the construction of the Ontology and its informal conceptual representation. We also present a working prototype of the Graph Database system, constructed using the Neo4j application, and show the connection between well-known financial ratios. We argue that this methodology which utilises state of the art semantic data management mechanisms enables data processing and relevant computations in a more efficient way than approaches using the traditional relational database. These give us solid grounds to build a system that is capable of tackling the data of any complexity level.",2018,8,"1394550182, 144031464",1394550182,Berlin,2,"2119407396, 1380082069",Y,1. The authors tested out this new activation function on RNNs.;ATARI 2600 games: I am not sure what state restoration is.,Conference on Business Informatics,20,"data,database,system,graph database,graph database","49fce234ad7f6d2af757f078b29c0118068075a3, eef23d76e175c0cff8e81ffcb2721c10539c8cbd, ae38dc77a962161107361f213db9216ee1274037, 27e58c9e5e6d07809a45a17675a2c7135b577881, e5d720767b7a539bb2edaa98eaf572a4506a79c6, 5f51d468ce730eeade7e9f419a1fe7152582be25, aee3d7f98b966240178ef420724c840f9b61deb3, 1ed33896e82cc3810b349cdffc2039f0b8edd82e, 784141489258258b12979061d92c1a616da26525, 91bda0785eaf642515eefc9ff2ecd7ddbacaccae, 48265726215736f7dd7ceccacac488422032397c, 18d87bff073687c025f9bd23ab2dfb20d5f72a66, f497c1ece7b6f3560bb39958e2673f476d608f98, b2ee139c1a3c2c53ac2b603070bdfbcd26b50ddc, 68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a, b05306f0b142e5afb3974b1b79996e5b82653662, 5cdb31c5e3e0aa4cc2d10229793b4911f69fd78f"
a0367346bc355c36badec2d2c47ce55a320cd75ea,The study on data migration from relational database to graph database,https://www.semanticscholar.org/paper/a0367346bc355c36badec2d2c47ce55a320cd75e,Conference,"Under the background of big data, using relational databases to manage massive data may have some problems just like storage capacity and query efficiency. So, there is a new kind of databases called NoSQL to store data. However, the data models of NoSQL databases are different from relation databases. In order to finish migrating historical data from relational databases to NoSQL databases, in terms of graph database in the NoSQL databases, this paper takes ER diagram as the original model, graph model as target, and makes some transformational rules by using the relationships of entities. And this paper proposes an algorithm which can finish data migration by traversing ER diagram and using the transformational rules. This method can reduce the impact of model differences between relational databases and graph databases, ensure the integrity constraint of data, and automatically complete data migration. The experimental results show the validity and correctness of the data migration.",2018,7,"71778404, 2114140713",71778404,Rome,2,"48727916, 2109512262",Y,"For instance, since two runs of node2vec will give you highly varying embeddings (depending on the initialization), you will have to run node2vec several times to reduce the variance of your resulting discretized density maps.;Moreover, the discrimator D  (which is trained to discriminate between real or fake examples) seems to be directly used to tell if an example is throw from the targeted distribution.",Journal of Physics: Conference Series,18,"data,databases,nosql,graph database,graph database","45674df7143e43bc589cfabd26dd194c2a7f090d, 11342d45911ee8a7c9e3a94117ce774ad7036172, 64c4c6cb66457c79f89ba4f2529b57bfce19ced4, 784141489258258b12979061d92c1a616da26525, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, 709af143f78bc62413c50ea1a7ee75b0702c4f59, afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84, 7c24234042988e2f820a4350f43422ed2ad6fc52, be082d70534db088315f2cc5b42c2fdcd58c1b8c, d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea, 4a7477881b66d12e79c704805781d4683a6a6be1, 9eea59c34f139f3d2153226c8cf026e975622074, 2b7f9117eb6608a58be4c078ca3d69c0e5ccb875, 31f9eb39d840821979e5df9f34a6e92dd9c879f2, 2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c, cb03b665069dad5e895a2c244929ea427f1fb9d1, a357f1ff27e184d9a5ef69e665e8ca891032bf71"
223187cf10a24b62b9b0cf5b146cc83526df2ea5a,From DIKW pyramid to graph database: a tool for machine processing of nutritional epidemiologic research data,https://www.semanticscholar.org/paper/223187cf10a24b62b9b0cf5b146cc83526df2ea5,Conference,"There is an increased interest in the application of information technology to advance nutritional research. In nutrition science, a graph database enables the creation of multilateral logic relationships throughout the database, which can be used to electronically store, visualize, and scale the outputs of nutritional research. It provides a knowledge structure to standardize nutritional research outputs, which is both human- and machine-readable in a Resource Description Framework format. However, the development of various specific graph databases may cause difficulties for data integration and decrease human-readability. In this article, we propose an approach to develop a graph database according to the Data, Information, Knowledge, and Wisdom or “DIKW” pyramid for nutritional epidemiologic data. Then, authoritative ontologies are suggested to construct the nodes and edges of the graph database to facilitate data integration. Finally, the findability and re-usability of the knowledge in the graph database are showcased using the SPARQL and SQWRL query languages.",2018,7,"2604647, 1737629, 2016236",2604647,Podgorica,3,"3144356, 48741177, 2093582149",Y,"This is demonstrated in comparison to weight normalization in Figure 4.;Interestingly, DQN + heuristic reward approaches expert performance while behavioral cloning never achieves expert performance level even though it has actions.;The last part contains a discussion concerning the extent to which it is actually a desired or a undesired result in classical deep learning use-cases, and the authors provide intuitive conditions under which the convergence would not hold.",2019 IEEE International Conference on Big Data (Big Data),18,"graph,database,data,graph database,graph database","340f48901f72278f6bf78a04ee5b01df208cc508, a60a4e5f7f872b9825ddff5d379857c2091ca52b, 10a69070c8585dc5564b11b0e53dbe4d565f9ce1, d1ae4ab5047489c2b010c7ce72262982ad66ad60, da3f33d858586d24cb265e79eb54f3746e998f57, ec2f9076448ba25a225618603adde60caa76c4df, f69d06037134ab6fb65d90f5ac192cf9f55e498d, 150f95f9c73820e0a0fa1546140e9f2bdfd25954, 017010b941d902a467f6d329ae5e74fd67e67912, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 147c868b721c8d29df7c61db7f2360114c760614, 218062f45c15f39bc8f4fb2c930ddf20b5809b11, 5d24ed8942235324512d6cedfd8dbf54c57658b4, 89a30b5dab02c9c390a632acad481fa602859272, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, 529ff7d6441d244212cf2becafd12a7e67ac56d9, d88083e37c44461ce3e404bd57257cd3edb07d4e, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, 0a71dd8bec060195e14eb9d0a7abbc08d960d4d5, 7af07490da518c8ef3cf2ae106071df2c2d0101e, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, 8fd8cb4951ae9634a378383a880fbf16ac5d6926, c4cdf2e0d89aadb13bf9c61654ff9e17be2b01b9, b9b639522465cc606df878eee62e7f9c4bf19e62, 2c03df8b48bf3fa39054345bafabfeff15bfd11d, b781fb7f3725a9d899d3d250b378d729a8a00442, 37b0a7a6c8fb26e32b9206847e78d521a2cd5900, 8388f1be26329fa45e5807e968a641ce170ea078, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 2478f43de2f1fcd9301a81e9b42d86b1d46db02d, 742747c7a453b293352b772d0d99541c96a351c3, cde39ce861e4c7514ee07fd91b6b8aac50cbf01b, 8b417c2be7a7707f372049fb1193f0d42f799562, f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa, aa6c2afadd660fe4efbac699f7854e8f6f240c38, 29279e52008848ee494f5af1b836313ab99c25ed, f51bc74814a3452009ea5ca262d9768d08149ee6, 58ed1fbaabe027345f7bb3a6312d41c5aac63e22, 74bec51a66499fcfbced16ff3fce696acf98c9e1, 53b0cfa2eaaa9703df02b2ca9c43260c4de5df0a"
10b4b926904ad153f791ec680218e1610747a0c8a,SQL Database with physical database tuning technique and NoSQL graph database comparisons,https://www.semanticscholar.org/paper/10b4b926904ad153f791ec680218e1610747a0c8,Conference,"Relational databases are used in many organizations of various natures from last three decades such as Education, health, businesses and in many other applications. SQL databases are designed to manage structured data and show tremendous performance. Atomicity, Consistency Isolation, Durability (ACID) property of Relational databases is used to manage data integrity and consistency. Physical database techniques are used to increase the performance of relational databases. Tablespaces also called subfolder is one of the physical database technique used by Oracle SQL database. Tablespaces are used to store the data logically in separate data files. Now-a-days huge amount and varied nature (unstructured and semi structured) of data is generated by the various organizations i.e., videos, images, blogs etc. This large amount of data is not handled by the SQL databases efficiently. NoSQL databases are used to process and analyze the large amount of data efficiently. Four different types of NoSQL databases are used in the industry according to the organization requirement. In this article, first, we do the physical database tuning of the Oracle Relational database and then compared with NoSQL Graph database. Relational database performance is increased up to 50% due to physical database tuning technique (Tablespaces). Besides, physical database tuning approach of relational database NoSQL graph database performed better in all our proposed scenarios.",2018,21,"51488437, 2056158839, 2151264132, 2061173271",51488437,Luxembourg,3,"2079275650, 101370046, 150270469",Y,Why choose F = 10 and K = 3?;Other comments: - your notation is quite sloppy and may have lead to errors.;Reproducibility in continuous control is particularly problematic.,"2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)",2,"database,data,databases,graph database,graph database","8674494bd7a076286b905912d26d47f7501c4046, 96b51d940653710f9d099d89ade86b44fa9bdd8a, beb890d47bbc21a96967f9993c9d6e15686b2eac, 7904b3446775ed8c79f4f94001a16b706989c462, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, fd3259ecd1c94731f5d9bd9c0bf7417f2e147c71, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, 92912dd895c360f01a6be9c9f6d207642139525e, ade9a900acc3c138021070537840488526796d35, 9db0247728950788a2b42097d81dc0e24eed6bb2, 63316bb5b88d362051c048e864c3ae5d97a26d30, 784141489258258b12979061d92c1a616da26525, 3d82552eb483e5ea84b577a0e8d5f157a6085824, 6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5, 709af143f78bc62413c50ea1a7ee75b0702c4f59, b61b260de1599e6e89491cad9160898fcd3b34c2, 43eea2a73997294193228d50f9ff25fc5345664b, f9367342405a73ab8d6de704a149babfc0edb5fe, a1ef4052acb63356928bb440874c470ad48cb40c, 11cf88dce827bd67cbfa60400306318022e736d5, 01bf0e83159712fbbbd12171a7e268547a4cfbc5, ab606e9d148458f6d54e5d44abefd73b7990f6e0, 627d7d631fd4e0e2179f82199f014deb7ff0ea0b, 7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2, 9712624bb61abb0da989514cae558cfab61bb9d2"
32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5ba,Graphflow: An Active Graph Database,https://www.semanticscholar.org/paper/32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b,Conference,"Many applications detect the emergence or deletion of certain subgraphs in their input graphs continuously. In order to evaluate such continuous subgraph queries, these applications resort to inefficient or highly specialized solutions because existing graph databases are passive systems that only support one-time subgraph queries. We demonstrate Graphflow, a prototype active graph data-base that evaluates general one-time and continuous subgraph queries. Graphflow supports the property graph data model and the Cypher++ query language, which extends Neo4j's declarative Cypher language with subgraph-condition-action triggers. At the core of Graphflow's query processor are two worst-case optimal join algorithms called Generic Join and our new Delta Generic Join algorithm for one-time and continuous subgraph queries, respectively.",2016,108,"98182097, 32455748, 10754597, 2115896116, 1783781",98182097,Riga,3,"3011964, 113398129, 1719124",Y,"I recommend producing another new figure of doing such comparison.;At test time, (if I understand correctly, please correct me if I haven't), the model is evaluated by having multiple copies of the same test point within an episode.;I was not able to imagine a reasonable setting where we would have access to a reward function of this form without input/output examples.",SIGMOD Conference,16,"subgraph,queries,graph,graph database,graph database","213a83e61e1347ffa58da9383a4bf92f4a77a6c8, a80e26e6365b215715c182d19a9aa8bb876ac768, ff7bcaa4556cb13fc7bf03e477172493546172cd, 82d2b9d09cc339fdeac05abfb8a31f9c6eace948, a0367346bc355c36badec2d2c47ce55a320cd75e, ef25b02f3be31c699255ee05aa90a4a17461d95d, a81ba6a07bf7a2ecff871e3362a77404501d0927, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 3faeb21fe256b99391d69570053a8c2d91e9f348, 84725855d10b531eb8cbe54935dda0440c2fc750, c0aec04ee86c0724d61c976f19590fbe9c615723, e968ae8e98fff9e28468383a1826fca4a2ae5245, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, 7637ed79d30d0139901175ae4abedd822c217ab4, af9280741ef627f0d6c8437605d002d3bfc2d1b1, db6084fdb3baceddacdc726474722debe1ef7e65, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, 82973c5f56681190a0dbb4c4449ed60d5f805135, 9eacf62f1e546748428c7e4843731b1595294200, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 16fa12ebc578df676f3dda5453ad56c15a0d6702, ada0b87cd5c30d31186c38fb12e631d29426a3bf, 7bc9607c5cf3fc817675d46844f529097d579514, 8d67b76222d84dcd337b8a2c78f13837070a79ce, f381c53aeb7742e4047d06d84f9e0c4f523231a3, da8b317b99c4b8933b2c59285639eca6c3fcb869, e4788ee4f5e90c6f42cedc5116acd2d6475c3180, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, b6a7226e5f6d618370995eccad68af195ef32da2, 19cf7458db4e17c7504eee24ccf961e1dc91435c, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, c9a9517c8b867187b4f4c0c37cbc65263ea41d25, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, 59c2968fb9672a7152c52127255d8f0784bc2368, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, eacf9284a39adcd56172665f31fd5a72560bba7a, af13a92977d4f4dc5b28b13746d86111d42939e8, 1cff064f815111a71a98afda7aee1867ad617901, 2a15e9ac5593cd1f8bd3a724e78a173037c02fea, d0238f2fa88b0316ab862f843a5521eff0fd9bf5, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, 9a60d3c9d3a5b7f165325fca45bd418d651682d3, a6bba5ce9867c978210e3d056691b5c1e769b760, 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d, 2dafea864f74a477414c3b71b742f7997e216102, 6d2d6707a0674ee41d539d106e6cb359a0a6ca06, 2ee03e28208a9310a9be4032c2b04ebdddb83cc7"
31d7d7b9c7b776c639316027e6ae5f2ff2673da2a,Fast Dual Simulation Processing of Graph Database Queries,https://www.semanticscholar.org/paper/31d7d7b9c7b776c639316027e6ae5f2ff2673da2,Conference,"Graph database query languages feature expressive yet computationally expensive pattern matching capabilities. Answering optional query clauses in SPARQL for instance renders the query evaluation problem immediately PSPACE-complete. Light-weight graph pattern matching relations, such as simulation, have recently been investigated as promising alternatives to more expensive query mechanisms like, e.g., computing subgraph isomorphism. Still, pattern matching alone lacks expressive query capabilities: graph patterns may be combined by usual inner joins. However, including more sophisticated operators is inevitable to make solutions more useful for emerging applications. In this paper we bridge this gap by introducing a new dual simulation process operating on SPARQL queries. In addition to supporting the full syntactic structure of SPARQL queries, it features polynomial-time pattern matching to compute an overapproximation of the query results. Moreover, to achieve running times competing with state-of-the-art database systems, we develop a novel algorithmic solution to dual simulation graph pattern matching, based on a system of inequalities that allows for several optimization heuristics. Finally, we achieve soundness of our process for SPARQL queries including UNION, AND and OPTIONAL operators not restricted to well-designed patterns. Our experiments on synthetic and real-world graph data promise a clear gain for graph database systems when incorporating the new dual simulation techniques.",2017,8,"3304707, 3245041, 79691050, 77790220, 1720266",3304707,Rome,2,"2145734797, 2086632521",Y,- I believe that different Monte Carlo (or Quasi-Monte Carlo) strategies can be applied in order to estimate the integral (expected value) in Eq.;Pros: - A new GAIL formulation for saving on interaction data.,IEEE International Conference on Data Engineering,34,"graph,query,pattern,graph database,graph database","f69d06037134ab6fb65d90f5ac192cf9f55e498d, 709f7a6b870cb07a4eab553adf6345b244913913, 82973c5f56681190a0dbb4c4449ed60d5f805135, fc32074b37a6d9dda535a70f9689022e70508520, d2efa91bd7d076a66bc1d1c570f6aa2da944dd88, ae026f29c2d571871f426ff4873d43b4ff90b9ad, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, 0e779fd59353a7f1f5b559b9d65fa4bfe367890c, 8fd8cb4951ae9634a378383a880fbf16ac5d6926, 0f74e7b650f346676b12c44d16d774fda9a45c9a, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, 91b9d3ab7532ea24ae70cd726355f25235b1fe8b, 6745a82c9236f0eec576904eb50ea700ca5a7d7c, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 597bd2e45427563cdf025e53a3239006aa364cfc, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c, cbad0923db89f23febcbd6192ff4149289ff2ad9, 63d8426ba1f51a8525dd19fd8ec92934ec71aea5, 75ea299834d6949e89e91d006677343ddab44e49, d78e61d0fe29b823f9630ccfa647c3029ec21f2e, 3adb779bb37d22e3aa299364c2a337603801ca5c, 5c45a5d05ac564adb67811eeb9d41d6460c70135, 740b5dec469e6b54e5e191a577a9b6a6ad8562e3, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, 03532123ccffae8d411264320e8a5ae2b6eddea0, ce9ca56036307217ea565644d3d3bd74b879e045, 915ab7b6ca3633230403d47dbb31cf74888cc5c9, adc180e1fe404b650fca3bb7970e43bdce34a611, 8dd0c1e955c66092ff951941a151336211e6e171, 624b2f14be4287d6a400cdf88a6f911b434b182e, bd6c027a3604d6c8fa23435bf382455b2bee436b, 31a61d009442436d04b9d4e1c5beee37172289ae, a1ef4052acb63356928bb440874c470ad48cb40c, 933baeec555352784848a93284c9dd0e79477759, 41c93960a066876d5e4f1dacaef75cd8daa2791f, e968ae8e98fff9e28468383a1826fca4a2ae5245, ce7499d6862df8269c655220049c3ed20b9b6f5e"
746a81aa26d3ebfb81acfd6af958d6a21603cd21a,Design and Implementation of Movie Recommender System Based on Graph Database,https://www.semanticscholar.org/paper/746a81aa26d3ebfb81acfd6af958d6a21603cd21,Conference,"with the continuous development of Internet technology, information overload is becoming more and more serious. It's getting harder to get useful information from the network. Although the search engine can help users find information they need from the vast amounts of information in a certain extent, but cannot completely solve the problem of information overload, when users cannot accurately describe the information they need, you need to recommend system to help users find valuable information for users. So recommender systems are becoming more and more important. The movie recommender system implemented in this paper is based on the traditional user-based collaborative filtering algorithm, and the user project scoring matrix is pre filled. At the same time, database technology of this system uses graph database which is good at dealing with complex relations. In data visualization, the degree of recommendation of a movie is expressed by the size of the node and the thickness of the edge, so as to improve the user experience.",2016,17,"46255467, 49673164, 2112638989, 34701398",46255467,San Marino,3,"30671790, 2162042348, 1729109",Y,"The paper makes some bold claims.;Without this baseline, it is hard to tell whether GAN training is even useful.;3. There is a lack comparison to other methods such as Shaham et al. (2017).",Web Information System and Application Conference,13,"information,users,system,graph database,graph database","3087b58cbfc6eb4a3076a180e21d6b872293f9a8, c5c4142a01981787a71bf6ebcb791520c458ab5d, eed62d36d1b976ac3873c83645f1c25f5096f89c, 0574a3abc98f0e6bd035a4ff4ea107cfba45d3d4, ca011427853d34ce4ec9ccafde8a70c9eacc3e21, 597bd2e45427563cdf025e53a3239006aa364cfc, bd6c027a3604d6c8fa23435bf382455b2bee436b, e359e8960b0b09e8685a32927b7818f4b06ef881, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, 811df72e210e20de99719539505da54762a11c6d, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, df138c7425e787cac2f9d3ab7775c0fb5294a83e, 98e6c6d860383fea5bbad145deed51514d23b86c, da5d78b3e3a1544fde98fba86088e1215e97cbe8, f18be38578ee52aa7071c404d42e3d53ae003122, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, d7fddafbbc372da4fa884f67bdc32db71b888806, 8ba8a0d18a06752f5a39996ccf1e914da0941443, 185e7d2a761594451b02ace240356dadad2aef78, d1e701665e73faa648cb15473952576f40e8e122, 287a7da1801a07cf7fd85ffcc23c79504876ecc0, 4e13a8e8ba8d33e15ed037bfca7c651047533990, ce2d5b5856bb6c9ab5c2390eb8b180c75a162055, b889b1d6944213bc2ca29e3ad07ee65ede20892d, f3b8a3ffa0bdfe5578e0f0f44ea3b66cad38c032, 10b4b926904ad153f791ec680218e1610747a0c8, 5dfde01d761d97c3a6c609007531973eb1229d09, 697f2f3598057cd17cff7749d768cae0993c6727, 25761ba4bdc054bfe902fe7c5d6338be6d00d491, b7034546bee38ba13d3b312fce893a22e33ce4dd, f4c4e148546089123f8da5db4fb246ab4062bd40, 0ae18b28d8dc00cd4641488084ead5df2a449c89"
c6879e43828b293567f5e2da039d23845189d6a7a,"Managing Cyber Threat Intelligence in a Graph Database: Methods of Analyzing Intrusion Sets, Threat Actors, and Campaigns",https://www.semanticscholar.org/paper/c6879e43828b293567f5e2da039d23845189d6a7,Conference,"Efforts to cope jointly with the ever-increasing number of breach incidents have resulted in the establishment of the standard format and protocol and given birth to many consultative groups. In addition, various channels that distribute Cyber Threat Intelligence information free of charge have emerged, and studies on utilizing such channels have spread. As the market for sharing information professionally is expanding, the need to manage the shared information in various ways in order to achieve better result has arisen. This paper proposes a standardized management structure and method based on the standardized format and a meaning and standard of Cyber Threat Intelligence that can be shared outside when loading OSINT information collected from various channels into the graph database. This paper also proposes a method of supporting the detection provided by existing security equipment with the information saved in the graph database and an effective method of analysis. Lastly, the paper discusses the advantages that can be expected from saving cyber threat information in the graph database developed using information collected from the outside.",2017,10,"2108129412, 9460711, 153378387, 2151900144, 2109139810",2108129412,Valletta,3,"2059129841, 1713648, 152125305",Y,"The experimental results seem promising, although not earthshattering.;Perhaps one could use a different (less striped) animal, e.g. raccoon.;Without experiments over other datasets and wall clock time results, it is hard to appreciate the significance of this improvement.",International Conference on Platform Technology and Service,17,"information,channels,cyber,graph database,graph database","be2b0396de9431bae931642516a1d3e4906329f5, 598231eb906b183f7a2a408ef4536127e11e3de9, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, a2e667e4382aaa8e02a17d0522c1a910790ab65b, 2af8907d4a974ae41044581f5e5d67317cb08568, 14fe35149aed6a47b6ebfd207deb7681b9446bb6, 7637ed79d30d0139901175ae4abedd822c217ab4, 7171a0e9b07ebc98a32eb912262613efc20f283a, c24d47ff95cd4bda073c75ec24ececaa3b10c995, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, ed935c6b359a7a486c28240d796e84897d095125, e2e7d964c09e27d334fcb8761d69918630629387, 152877c51df17cdd4a87d19e452c6daecfadf6c3, 1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7, 52e510271b172d098ec9b107a4159216ec08527e, 5031790972d496547b6613d46a4a0134c824db6e, b69a35662a2cac38eab22f4481285116bdf8c30e, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 6628f9ee35e36cdfdcac8a46cef4dba8d529a83b, 22c141b489e6e189f5996537b0a908fc10f90de7, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, 7904b3446775ed8c79f4f94001a16b706989c462, b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1, 518a7c79968a56d63a691d42f8378be6c776167e, 9eea59c34f139f3d2153226c8cf026e975622074, 3faeb21fe256b99391d69570053a8c2d91e9f348, 02a1e8e77f501675945890df45fbdc11726cb0ba, 9727206903eb40d4fa42606711bad3402f2ba9aa, 00d1f3423a33f73ca6aee884a58834547475d2f0, 22cba8f244258e0bba7ff4bb70c4e5b5ac3e2382, 925af6dcb0f8e6f3a5b2613400277be4b5434d10, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, 43e6e8d6663d83f1b74cf5a2be7b040b0928f867, 9b3e8d202488dc29e601fc471a25a2af9002659e"
9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2aa,Graph Database for Recipe Recommendations,https://www.semanticscholar.org/paper/9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a,Conference,"Graph databases represent a paradigm shift from relational databases with a strong support for “ relationships”. As compared to relational databases which compute relationships at runtime, graph databases persist relationships for fast querying and data retrieval. This work presents a recipe recommender as a graph database, Neo4j application. Given any set of ingredients, this application recommends a variety of recipes with the help of a data set containing thousands of ingredients. Further based on availability of ingredients with a user, this application helps discover the list of possible dishes with these ingredients. In order to implement this application, ingredients and recipes have been crawled from cookery based websites using Python scripts. The crawled data has been inserted into the Neo4j database and subsequently inter-relationships between ingredients and recipes nodes have been analyzed. Execution of self designed queries has verified the time-efficiency of the proposed approach.",2017,4,"82008243, 150281558, 3357166, 2081215",82008243,Podgorica,3,"1753210, 40071013, 49627183",Y,"A temporal clustering model and a DCNN decoder are applied on the encoded representations and jointly trained.;This paper shows an observation of “super-convergence” when training resnet with cyclical learning rates but does not provide conclusive analysis or experiment results.;PSNR is evaluated on predicted frames -- PSNR does not appear to be explicitly defined but I am taking it to be the metric defined in the 2nd paragraph from the bottom on page 7.  The new model ""EEN"" is compared to a deterministic model and conditional GAN.  The GAN never seems to perform well -- the authors claim mode collapse, but I wonder if the GAN was simply hard to train in the first place and this is the key reason?","2018 7th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",6,"ingredients,application,graph,graph database,graph database","b27c98d8378848f2c23a067f2c5196f3b5a07572, 57a3fc6d0aaad3c10c793b4e59390ca04c935282, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, b415e836d447ea9efb7629a1de67cd2a6f9e7ba8, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, 332e0eab5fba8e6940f3e481f542a99ac17b9717, 3a58efcc4558727cc5c131c44923635da4524f33, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, 28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8, 024006d4c2a89f7acacc6e4438d156525b60a98f, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, 10aa2be24951e6de76b630482a645d79354c4cde, 430f3c265935abb45bc84f3ae81c570ef778aac0, be2b0396de9431bae931642516a1d3e4906329f5, d916776e0c6a04b0def4c22257c188776c2edab2, af9280741ef627f0d6c8437605d002d3bfc2d1b1, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, 033a50c4515b153b6e706018075c333c64981fd7, 2e5d2f2dc01b150dffc163a9f457848e9b5b5c38"
fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53a,GeaBase: A High-Performance Distributed Graph Database for Industry-Scale Applications,https://www.semanticscholar.org/paper/fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53,Conference,"Graph analytics have been gaining tractions rapidly in the past few years. It has a wide array of application areas in the industry, ranging from e-commerce, social network and recommendation systems to fraud detection and virtually any problem that requires insights into data connections, not just data itself. In this paper, we present GeaBase, a new distributed graph database that provides the capability to store and analyze graph-structured data in real-time at massive scale. We describe the details of the system and the implementation, including a novel update architecture, called Update Center (UC), and a new language that is suitable for both graph traversal and analytics. We also compare the performance of GeaBase to a widely used open-source graph database Titan. Experiments show that GeaBase is up to 182x faster than Titan in our testing scenarios. We also achieves 22x higher throughput on social network workloads in the comparison.",2016,12,"2677700, 7806657, 3358986, 1943322867, 2144151933, 2108966388, 2114147314, 24812041, 2218569240",2677700,San Marino,2,"46175739, 2065041692",Y,4. The localization performance of the proposed attention mechanism is evaluated by weakly-supervised semantic segmentation tasks.;The authors do provide a novel formulation and demonstrate the gains on a variety of concrete problems taken form the literature.,International Conference on Advanced Cloud and Big Data,16,"graph,data,geabase,graph database,graph database","cbc1e8bbfe98f94c0d13d111b824cf603b62712c, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632, d1bb57da8593a2071b3ea8026865352ab3f7206a, 1bc34cb22131554ba18f6ba9e6ede5beb42939f1, ef25b02f3be31c699255ee05aa90a4a17461d95d, 71ba8a8c75e0826f94eb53ee7a4566d4fc5b5256, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 9e3816be8cf4821d74e258de10ee471382936a30, ce7499d6862df8269c655220049c3ed20b9b6f5e, 96618554bd3c3e27be8071a7a636b7e29475fa73, 4a7477881b66d12e79c704805781d4683a6a6be1, b69a35662a2cac38eab22f4481285116bdf8c30e, 3cfe075af77bf0364e6ddecb3d223960d06e8927, 598231eb906b183f7a2a408ef4536127e11e3de9, 627d7d631fd4e0e2179f82199f014deb7ff0ea0b, c07802ed8a25998e9bd44ee1ddbcc63b7eb34060, 3be6a57d6db95bba2962a1f3476414a0a9b230b5, c84aa52bee5116f80c7740503edff4b08f733c3b, 364128bcce9836d60e685bb717b80f30e25092e0, 3adb779bb37d22e3aa299364c2a337603801ca5c, be383c607d4d357c763d2329ab71799c6e1393b4, db4cf9f6a653d5c15973e836c800ea47743251ae, e02f91d625cd32290d4ede0f31284da115844316, dd4d82299b4209db539d639f836fcee663cf72b3, 3b87dafd5a412e25e06761f181ec199ca88a7398, 82870bc488b57cdf5ea62877109a7278af2926b3, 57a3fc6d0aaad3c10c793b4e59390ca04c935282, 4e746359afd6f81705b875d71cc499b904a320df, baafed5f8968118af04dbbb1cf172f1c10bede25, b266510f5f9b40d42b51884ad13a1867fb3284fd, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, 3c68025d95970a9b9aa1b742a678704cd09d2bf4, 156d8e2aa90b5ccc9be10477ca70deaad0151387, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, 41d6b6bd4bf44bf13b9157b603e33360e5e77a01"
7676c02ea839ff1ceb6e5e1427c42bc45e169bdea,The Spatio-Temporal Data Modeling and Application Based on Graph Database,https://www.semanticscholar.org/paper/7676c02ea839ff1ceb6e5e1427c42bc45e169bde,Conference,"Traditional spatio-temporal data model (STDM) is based on relational database, it is hard to convert problem domain model to relational model, which result in complicated query and low expansibility. In this regard, we propose the spatio-temporal data model based on graph database. The data model integrates TGIS's three key elements: space, time and attributes, and expressed spatio-temporal characteristics of TGIS explicitly. Finally, this paper gives a particular description of logistics distribution route optimization. Experimental results show that the model is proved to be appropriate for expressing the spatio-temporal process of world.",2016,9,"8507683, 2145295170, 2145734797, 2053831005, 48152160",8507683,Sarajevo,2,"1410127739, 2120664",Y,"Originality and Significance:  The area of relation extraction seems to me to be very important and probably a bit less intensively worked on that it should be.;Firstly, the paper has a fatal mathematical flaw.",International Conference on Information Science and Control Engineering,3,"model,data,database,graph database,graph database","b2ee139c1a3c2c53ac2b603070bdfbcd26b50ddc, 2c3eef2f17369912e330281d54b535675077e4ca, 239bf45c13b3f6d38c74026b535f785febf9cd08, 27e58c9e5e6d07809a45a17675a2c7135b577881, d1206ccabd1980848f14472d6548251c2fab7963, 73d4accea441aae2373828a8dc2175aa2759c38f, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, 8d0f755dea90f35f4b126a01fa3cce96b3bdd344, bc00ff34ec7772080c7039b17f7069a2f7df0889, 82e1e8b222aeaca19d45375b31fdc825d1a821b8, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, 794b3ffd28d28606230efc975eeec9f0522fb139, ea160adc0d78e54669281b8b145bcd832e648fee, 9e3816be8cf4821d74e258de10ee471382936a30, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c"
704011527f183b561ea6a75b21e4cefe5aa77fcaa,Book recommendation using Neo4j graph database in BibTeX book metadata,https://www.semanticscholar.org/paper/704011527f183b561ea6a75b21e4cefe5aa77fca,Conference,"In digital era, book has an important role in life. There are not only a lot of books for different purpose. But also, there are many book metadata which can use for another reason, such as book recommendation. By processing the book metadata, an information can be given to user that needs book recommendation. By combining BibTeX book metadata and Graph Database from Neo4j, data from metadata can be processed. Then, with cypher query by inputting author's parameter or book type's parameter, user can get book recommendation based on their input's criteria. The result is exactly the same with process the metadata manually in relational database. Neo4j, from this paper, takes 180 milliseconds to execute cypher query with author's criteria and takes 184 milliseconds to execute cypher query with book type's criteria.",2016,11,"121066428, 2803317",121066428,Nicosia,3,"2138053020, 1628391446, 150341221",Y,"In section 4, authors claim that their results are competitive with the best published results for a similar number of parameters.;Their abstract also claims to utilize a convex programming formulation.;The details of their proposed method are covered in Algorithm 1 on Page 12, where an additional GAN (generative adversarial network) I_{\gamma}, which can be regarded as the inverse function of the original GAN G_{\theta}, is trained to learn a map from the original input data space to the latent z-space.",International Conference on Science in Information Technology,2,"book,metadata,recommendation,graph database,graph database","89a30b5dab02c9c390a632acad481fa602859272, 94cb5503b191815ce77b19147d96c6fbd68f06bf, 7d873a9c49d3864709aa762f8740edcdbd7369c5, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 7bb477077968d68aa7a6059d8d6d801fb28274da, 8b417c2be7a7707f372049fb1193f0d42f799562, cbc1e8bbfe98f94c0d13d111b824cf603b62712c, 6068d39e92aef1bb0e1291e9931894c35692a85e, 024006d4c2a89f7acacc6e4438d156525b60a98f, 087dd95e13efd47aef2a6582e6801b39fc0f83d8, 97ac11e5a6440eccb70ae7146392ac138c36fa6c, 6c739540e66e895311b7347971f10ef556e06e52, 92912dd895c360f01a6be9c9f6d207642139525e, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 3cd65f70c18c703dd9bbae4d9a40a0c7e14e2dc0, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, 7aa38b85fa8cba64d6a4010543f6695dbf5f1386, 8dce62b18bdea587c07cb4769a05ca0a816d09ba, 68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a, 4267178106cef2e77284bde309dfaaf9fd46a91b, a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854, 84725855d10b531eb8cbe54935dda0440c2fc750, 639bfab64e2f35917d450013e136cb24c7755fad, 665b0c776ff7507c32793f10ce9edf90bc2f674a, a80e26e6365b215715c182d19a9aa8bb876ac768, 87048df918c34b662bc0d28894efa430d70a9206"
b889b1d6944213bc2ca29e3ad07ee65ede20892da,An X10-Based Distributed Streaming Graph Database Engine,https://www.semanticscholar.org/paper/b889b1d6944213bc2ca29e3ad07ee65ede20892d,Conference,"Streaming graph data mining has become a significant issue in high performance graph mining due to the increasing appearance of graph data sets as streams. In this paper we propose Acacia-Stream which is a scalable distributed streaming graph database engine developed with X10 programming language. Graph streams are partitioned using a streaming graph partitioner algorithm in Acacia-Stream and streaming graph processing queries are run on the graph streams. The partitioned data sets are persisted on secondary storage across X10 places. We investigate on the use of three different streaming graph partitioner algorithms called hash, Linear Deterministic Greedy, and Fennel algorithms and report their performance. Furthermore, to demonstrate Acacia-Stream's streaming graph processing capabilities we implement streaming triangle counting with Acacia-Stream. We present performance results gathered from Acacia-Stream with different large scale streaming data sets in both horizontal and vertical scalability experiments. Furthermore, we compare streaming graph loading performance of Acacia-Stream with Neo4j and Oracle's PGX graph database servers. From these experiments we observed that Acacia-Stream's Fennel partitioner based graph uploader can upload a 948MB rmat22 graph in 1283.42 seconds which is 38% faster than PGX graph database server and 12.8 times faster than Neo4j database server. Acacia-Stream's Streaming Partitioner's batch size adjustments based optimizations reduced the time used by the network communications almost by half.",2016,4,"2741023, 35709316, 35433878, 49627183, 35367497, 1971912, 2231831",2741023,Stockholm,3,"2125957, 153693432, 1500655986",Y,"Since any CoffeeScript programs can be compiled into the corresponding Javascript programs, we should assume that CoffeeScript is the only subset of Javascript (without physical difference of syntax), and this translation task may never capture the whole tendency of Javascript.;The fact that the proposed technique is simple yet yields such speedups is encouraging.;1. The authors tested out this new activation function on RNNs.",International Conference on High Performance Computing,23,"graph,acaciastream,data,graph database,graph database","2b7f9117eb6608a58be4c078ca3d69c0e5ccb875, 916455d97cd792c2eb5b00663689592e25cbc8d8, a620ea8654f86b282e0d2c1fbc6616b90ca45bd1, 5ea7bf772fecf95cbf53b2c7f719c9440322a115, 799d5a8271887adede035644d878c7bd555576df, 71a85e735a3686bef8cce3725ae5ba82e2cabb1b, 6a1b25f7a67395ad1e676027322913acbb0a0635, c48e0bd0f36c25ab83befbc7b7da369b75fd25f5, d0238f2fa88b0316ab862f843a5521eff0fd9bf5, 6f75e8b61f13562237851d8119cb2f9d49e073fb, 22c141b489e6e189f5996537b0a908fc10f90de7, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, 529ff7d6441d244212cf2becafd12a7e67ac56d9, b781fb7f3725a9d899d3d250b378d729a8a00442, ca011427853d34ce4ec9ccafde8a70c9eacc3e21, 0258bab20bc8574ee602012081a17db89339f12d, 9a60d3c9d3a5b7f165325fca45bd418d651682d3, 7536bce1007a765fd097a7cc8ea62208a8c89b85, 8dd0c1e955c66092ff951941a151336211e6e171, 1b0aa15937fdf59103a5213bccf09cff83d0ee3e, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 0311ace1d499cadd1cc0c515a625d1d045f60d25, 661ccdb41fe977d47273e586389cacc1489f3286, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, b61b260de1599e6e89491cad9160898fcd3b34c2, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, 13c4e5a6122f3fa2663f63e49537091da6532f35, d88083e37c44461ce3e404bd57257cd3edb07d4e, 537f5e8e4139392cd2d108f32495e5b2b80151ac, 43e6e8d6663d83f1b74cf5a2be7b040b0928f867, db528269ef800727245c0fcb35b692d29c1ccdc9, 20a3ed03888037e2802fa9abad02ffa0a8dcc228, b977e8de38dc0d13817bca1ed20036badfe2a58c, 6ff909c6fe089fc8ebfc64eca0f0c3cc34ba277f, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 23466d271676ae467cbe85bb1993682f3502e840, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 1eaab9b33f1261744567455a14830e8a92796cf5"
28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8a,Milvus: A Purpose-Built Vector Data Management System,https://www.semanticscholar.org/paper/28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8,Conference,"Recently, there has been a pressing need to manage high-dimensional vector data in data science and AI applications. This trend is fueled by the proliferation of unstructured data and machine learning (ML), where ML models usually transform unstructured data into feature vectors for data analytics, e.g., product recommendation. Existing systems and algorithms for managing vector data have two limitations: (1) They incur serious performance issue when handling large-scale and dynamic vector data; and (2) They provide limited functionalities that cannot meet the requirements of versatile applications. This paper presents Milvus, a purpose-built data management system to efficiently manage large-scale vector data. Milvus supports easy-to-use application interfaces (including SDKs and RESTful APIs); optimizes for the heterogeneous computing platform with modern CPUs and GPUs; enables advanced query processing beyond simple vector similarity search; handles dynamic data for fast updates while ensuring efficient query processing; and distributes data across multiple nodes to achieve scalability and availability. We first describe the design and implementation of Milvus. Then we demonstrate the real-world use cases supported by Milvus. In particular, we build a series of 10 applications (e.g., image/video search, chemical structure analysis, COVID-19 dataset search, personalized recommendation, biological multi-factor authentication, intelligent question answering) on top of Milvus. Finally, we experimentally evaluate Milvus with a wide range of systems including two open source systems (Vearch and Microsoft SPTAG) and three commercial systems. Experiments show that Milvus is up to two orders of magnitude faster than the competitors while providing more functionalities. Now Milvus is deployed by hundreds of organizations worldwide and it is also recognized as an incubation-stage project of the LF AI & Data Foundation. Milvus is open-sourced at https://github.com/milvus-io/milvus.",2020,90,"2141735101, 1666260553, 1764236, 145914256, 2153916637, 2153701193, 2144797977, 46909714, 2136332740, 2115215055, 2114076472, 2116525062, 2113959133, 2117315688, 2111258930, 2116566794, 2129460589, 2113623161, 2144097347, 2113618679, 2156252582, 2113619066",2141735101,Sofia,2,"2206684539, 49997612",Y,"Also, do the cluster centroids appear to be roughly stable over many runs of the algorithm?;Intriguing two phase RL approach for learning neural controllers for discrete programs This paper presents a reinforcement learning based approach to learn context-free parsers from pairs of input programs and their corresponding parse trees.",SIGMOD Conference,20,"data,milvus,vector,data management,data management","a747e8f2659df479c0092301b9658fc582423df1, ff75865cde62592d068b2afd055c57c81d77158b, 7c8bd41e9cebdb01f445a51ba43839c8d9b3ab91, c2528e88d5554e9df9f9d482ad46cb5331c4d794, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, d88083e37c44461ce3e404bd57257cd3edb07d4e, 17fc5c9b37ba63fb3280ddf9909a183523e7bcc3, 4bd3c9e1bb1ca2df62b66201616b8740300efd0a, 16753e0317730e8c1b297338300a8c6163dd06f2, bdb68c5e2369633b20e733774ac66eb4600c34d1, 079b57837221413bf99ab40999c77c29e280e0c2, ba687027ed6012f613e1f9a9cefe7683bb192934, 84a36e19f9394f22b34f79756fa9628a795e02ea, 1a60a9d1eef24e123c27a9eee5a399ac2b620fee, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, f208b3fb28c556ab62f9d202b7beae89700a338a, 58ed1fbaabe027345f7bb3a6312d41c5aac63e22, ff74bfbd9ebf4c54809873aecb04be27e9402cb8, 3a7bbc46795929f0eace82b64c44c92a48682fb5, d0238f2fa88b0316ab862f843a5521eff0fd9bf5, 8f99f5cb3950fcd1628c6a563be4b4fc4966fa66, 94deb62af3054c49e7d80bd7eb3ed5efe990fc0b, 8760bc7631c0cb04e7138254e9fd6451b7def8ca"
28cc044d5ba938472bc53d87240583982ad21663a,Data Management for Data Science - Towards Embedded Analytics,https://www.semanticscholar.org/paper/28cc044d5ba938472bc53d87240583982ad21663,Conference,"textabstractThe rise of Data Science has caused an influx of new usersin need of data management solutions. However, insteadof utilizing existing RDBMS solutions they are opting touse a stack of independent solutions for data storage andprocessing glued together by scripting languages. This is notbecause they do not need the functionality that an integratedRDBMS provides, but rather because existing RDBMS im-plementations do not cater to their use case. To solve theseissues, we propose a new class of data management systems:embedded analytical systems. These systems are tightlyintegrated with analytical tools, and provide fast and effi-cient access to the data stored within them. In this work,we describe the unique challenges and opportunities w.r.tworkloads, resilience and cooperation that are faced by thisnew class of systems and the steps we have taken towardsaddressing them in the DuckDB system.",2019,27,"3428490, 3011964",3428490,Tirana,3,"2193954145, 1720381, 1939292",Y,"The papers is well written and clear, and proposes an interesting idea of representing graphs as multi-channel image-like structures.;Shouldn’t they be adaptive values with respect to the number of candidate traces found so far?;Also, I think it’s a bit of an exaggeration to call a gap of 2.71 nats “much tighter” than a gap of 3.01 nats.",Conference on Innovative Data Systems Research,19,"data,solutions,systems,data management,data management","d7b820af40a9e2660ef700d39f7b2e27b43435c5, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, 9b529fe170823f95509585d5aa39fa01a43558fd, b0c34618ffd1154f35863e2ce7250ac6b6f2c424, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, e89c37b7c2ff465db43c4b9f674867ec4b98aa8b, 9f5b82d9915d0752957602224c5056be7e749c83, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, 7a1e71cb1310c4a873e7a4e54d1a6dab0553adce, 72afe82af4c2ca100c36eb35292e85d806527f0a, f72d3f58ff73353978e224af348448b34d27cf7b, 78aab73ed574393ab421f25b3a0e3f7343e64748, fb0aeed456bff8607fab2bf443e9d86d51c3dff6, f64670a5f54fcce339a916497a001cbf02a9a04f, bb1118fb9fd86da6a2f65770353d8fb4362d9883, e57aeb158a38ccf33d2f0f5a8f63f1209497e329, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, eda6756ab2844c390584686dc5e6385f4a8369cd, d93bcf0685c15c45d078eafea565969c04daccd3, f4cfc7cbad257f1688772d59f694c16189dba811, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 41d04aa3c25dcfbf1b44ce666c48759e03c216c7, b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1, 60caa5b3d066e13feac496fd0736e976970eb09f, bf324b5d23344984883d89a1dca22a39ca473031, c0e6cd2ec3bc9eb46c7d45bb708854da3327339e, 794b3ffd28d28606230efc975eeec9f0522fb139, 34e9852f588f75eba81c66a3e5f867a794a5a690, c30c0092bf4eb8a44faec3fc60cdd5006276bcdc, b7f5973dbf2ef76fcc3aea616a7f72ca79f09020, 48c73c389c3f36d70407eb8309a0b41578c15fc8, 9a60d3c9d3a5b7f165325fca45bd418d651682d3, 024006d4c2a89f7acacc6e4438d156525b60a98f"
2737a61f6557fe7bf53a608c668de2eff1f582f0a,GPU-accelerated data management under the test of time,https://www.semanticscholar.org/paper/2737a61f6557fe7bf53a608c668de2eff1f582f0,Conference,"GPUs are becoming increasingly popular in large scale data center installations due to their strong, embarrassingly parallel, processing capabilities. Data management systems are riding the wave by using GPUs to accelerate query execution, mainly for analytical workloads. However, this acceleration comes at the price of a slow interconnect which imposes strong restrictions in bandwidth and latency when bringing data from the main memory to the GPU for processing. The related research in data management systems mostly relies on late materialization and data sharing to mitigate the overheads introduced by slow interconnects even in the standard CPU processing case. Finally, workload trends move beyond analytical to fresh data processing, typically referred to as Hybrid Transactional and Analytical Processing (HTAP). Therefore, we experience an evolution in three different axes: interconnect technology, GPU architecture, and workload characteristics. In this paper, we break the evolution of the technological landscape into steps and we study the applicability and performance of late materialization and data sharing in each one of them. We demonstrate that the standard PCIe interconnect substantially limits the performance of state-of-the-art GPUs and we propose a hybrid materialization approach which combines eager with lazy data transfers. Further, we show that the wide gap between GPU and PCIe throughput can be bridged through efﬁcient data sharing techniques. Finally, we provide an H 2 TAP system design which removes software-level interference and we show that the interference in the memory bus is minimal, allowing data transfer optimizations as in OLAP workloads",2019,22,"10195630, 66243223, 3416158, 88694945, 2649908, 1728318",10195630,Reykjavik,3,"4478199, 1693689, 2152798056",Y,"3. The paper wants to find a good trade-off on speed and accuracy.;Cons/Questions/Suggestions The distinction between the convolutional and fully-connected layers (called “classifiers”) in the approach description (sec;However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator.",Conference on Innovative Data Systems Research,19,"data,processing,gpus,data management,data management","ff7bcaa4556cb13fc7bf03e477172493546172cd, 91fc647899f801c9d351349ce73779918f90a713, b22ed1ea1d174af48c655d48e284afc239ebfa6a, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, fee8f63972906214b77f16cfeca0b93ee8f36ba2, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, cc1e82125f7f8636b25ccdcdb63e8f812add7f87, 22cba8f244258e0bba7ff4bb70c4e5b5ac3e2382, ec7f5dc077480df149bcd4358a3aa8441878ca59, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, 43e624ddeed82df944a6cae0dedec3372438e243, f90f526b101cb8a0260f5165a3875928c58ae48a, cb03b665069dad5e895a2c244929ea427f1fb9d1, fa7b8acd47631bada5b66049824bfd335ac6bf8f, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, 647c4a9331e01e31a4350361d3460f0397fe694f, 77a59de2e2b832321875cadcf9619dc313f02384, f4c4e148546089123f8da5db4fb246ab4062bd40, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, b55e490637babd50dab3cdaaa3a60a2be6eb1cbb, 2eda2921a8da4b325f9d05f556594a5884c398a7, 8d67b76222d84dcd337b8a2c78f13837070a79ce, 28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8, f3d594544126e202dbd81c186ca3ce448af5255c, ce54e3b89a2570035b70885e6901ad4c92ae41c9, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa, 709af143f78bc62413c50ea1a7ee75b0702c4f59, fb30e18bbfc8de7bf7df55af7d40c0d757d1942e, e89c37b7c2ff465db43c4b9f674867ec4b98aa8b, 63d8426ba1f51a8525dd19fd8ec92934ec71aea5, da3f33d858586d24cb265e79eb54f3746e998f57, 472644c5f4155635cf9e9e37540bfa53c20e7610, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 742747c7a453b293352b772d0d99541c96a351c3, a8b995f0da78a79447dfb18c2337972b044f4239, d21703674ae562bae4a849a75847cdd9ead417df, e24b8a9531573d284647239affc6c855505b0de4, 0a92bc2dc8a216e6aced83edc0358241066833df, eef23d76e175c0cff8e81ffcb2721c10539c8cbd, 5371896313ac227eb819038dd55f213cb42b99e2"
b05306f0b142e5afb3974b1b79996e5b82653662a,Rethinking Data Management Systems for Disaggregated Data Centers,https://www.semanticscholar.org/paper/b05306f0b142e5afb3974b1b79996e5b82653662,Conference,"One recent trend of cloud data center design is resource disaggregation . Instead of having server units with “converged” compute, memory, and storage resources, a disaggregated data center (DDC) has pools of resources of each type connected via a network. While the systems community has been investigating the research challenges of DDC by designing new OS and network stacks, the implications of DDC for next-generation database systems remain unclear. In this paper, we take a ﬁrst step towards understanding how DDCs might affect the design of relational databases, discuss the potential advantages and drawbacks in the context of data processing, and outline research challenges in addressing them.",2019,24,"2112197162, 2111027355, 143857631, 144237796, 30894196, 35206168",2112197162,Ljubljana,3,"2093481779, 2144151933, 123445664",Y,"The idea of enforcing information isolation is brilliant.;A thorough exploration of techniques for unsupervised translation, a very strong start for this problem This paper describes an approach to train a neural machine translation system without parallel data.;While LINE or SDNE, which the authors cite, may not run on some of the larger datasets they can be tested on the smaller datasets.",Conference on Innovative Data Systems Research,19,"data,ddc,center,data management,data management","ac67d5f9c89d8d72fbd074f94079608220348f3f, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, e23b2e47b0ac6f50000078828f27571804dcd6a2, eeac4411ae119c6c7ac33a11f762f2495b4dd960, 87f17f939b79bb3e1d2746993a2c1cda48cb1b32, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, e576a2d97950b1f6831f88575dd3f370053f6af7, f63e917638553414526a0cc8550de4ad2d83fe7a, 43e624ddeed82df944a6cae0dedec3372438e243, fa77a44f3f1857361a50c3137d623c35ef8a5739, 634fe61f3ab6692ea4733989a1c76e793bb8b69e, 447884e7da189102189a156966623335c72199b0, 6be56f559a74c0124526242e70cbdfd16cbc60a7, ec7f5dc077480df149bcd4358a3aa8441878ca59, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, 5a32ebacd5c32d52734f9d2a2cfb5d0cdbe469e2, dad8965c5a4c0a0ea1eb3837c6a9c3b42597c2ce, 5c45a5d05ac564adb67811eeb9d41d6460c70135, 8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801, 14014c024674991149f3ecf9314c93f7e029ef1a, 456c011594ecacdd24298a161787389ccbe4b88b, 2478f43de2f1fcd9301a81e9b42d86b1d46db02d, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 6c739540e66e895311b7347971f10ef556e06e52, c48e0bd0f36c25ab83befbc7b7da369b75fd25f5, 5030702fea15d66a73fc997325431f1d7945ad9a, f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6, fa63c3f53413ced7946623889c416e34a28676ea, 3a74bed911ccf213d9595b2b02a5b1c4ac4dcaf8, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 90e4330fed2da147dd41490e8ad638b618112b3d, 332e0eab5fba8e6940f3e481f542a99ac17b9717, f9c990b1b5724e50e5632b94fdb7484ece8a6ce7, 6b3756d32ab5b0a5715a5cfc3672290d2d643017, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344"
eebf1a2705bf4ac256d141b0067f1b0ea1dc7632a,Data Management Challenges for Deep Learning,https://www.semanticscholar.org/paper/eebf1a2705bf4ac256d141b0067f1b0ea1dc7632,Conference,"Deep learning is one of the most exciting and fast-growing techniques in Artificial Intelligence. The unique capacity of deep learning models to automatically learn patterns from the data differentiates it from other machine learning techniques. Deep learning is responsible for a significant number of recent breakthroughs in AI. However, deep learning models are highly dependent on the underlying data. So, consistency, accuracy, and completeness of data is essential for a deep learning model. Thus, data management principles and practices need to be adopted throughout the development process of deep learning models. The objective of this study is to identify and categorise data management challenges faced by practitioners in different stages of end-to-end development. In this paper, a case study approach is employed to explore the data management issues faced by practitioners across various domains when they use real-world data for training and deploying deep learning models. Our case study is intended to provide valuable insights to the deep learning community as well as for data scientists to guide discussion and future research in applied deep learning with real-world data.",2018,48,"1423751237, 144913779, 40225855, 1826416, 1939487",1423751237,Monaco,2,"1402912902, 22654490",Y,"Interesting experiments but lack of model description The authors propose to use a byte level RNN to classify reviews.;If the contribution of the paper is the ""output stochastic"" noise model, I think it is worth experimenting with the design options one has with such a model.",EUROMICRO Conference on Software Engineering and Advanced Applications,44,"data,learning,models,data management,data management","db0cc2f21b20cbc0ab8946090967399c25709614, 69a72ff5b30642d11c96635e99aadad3140d33a7, 82870bc488b57cdf5ea62877109a7278af2926b3, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, 0a829289a16ae48837cc2905635435db98bacc76, 993df7df129f8d18816877d69923d7df7b347d85, 98e6c6d860383fea5bbad145deed51514d23b86c, d88083e37c44461ce3e404bd57257cd3edb07d4e, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, cc5726fc0ebb84f741f3496a3c52ced162c596ba, 5d433da6d0f143f20936379910104d2bb139d4ae, a2a514ed839dafdd0fb76d6c2615f25f35bf8087, 94cb5503b191815ce77b19147d96c6fbd68f06bf, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, ade9a900acc3c138021070537840488526796d35, 7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2, 410fba9f03212257d0881811802e6620e59bc827, 2bc3644ce4de7fce5812c1455e056649a47c1bbf, 799d5a8271887adede035644d878c7bd555576df, c6879e43828b293567f5e2da039d23845189d6a7, 9fcdbfdf28245010c875ce85502351fe05c04b49, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, 10a69070c8585dc5564b11b0e53dbe4d565f9ce1, c50a909e20bd07f4aea09dc6dae539b45b406a96, 21042565ec941f4fa31ac5a0af85a1a84ff21f1b, b6593b5ad6b0e00232ee88cf2bc578645c1299a0, 87f17f939b79bb3e1d2746993a2c1cda48cb1b32"
cf41991d89301c3c12420d150792cb1163999962a,Data Management in Supply Chain Using Blockchain: Challenges and a Case Study,https://www.semanticscholar.org/paper/cf41991d89301c3c12420d150792cb1163999962,Conference,"Supply chain management (SCM) is fundamental for gaining financial, environmental and social benefits in the supply chain industry. However, traditional SCM mechanisms usually suffer from a wide scope of issues such as lack of information sharing, long delays for data retrieval, and unreliability in product tracing. Recent advances in blockchain technology show great potential to tackle these issues due to its salient features including immutability, transparency, and decentralization. Although there are some proof-of-concept studies and surveys on blockchain-based SCM from the perspective of logistics, the underlying technical challenges are not clearly identified. In this paper, we provide a comprehensive analysis of potential opportunities, new requirements, and principles of designing blockchain-based SCM systems. We summarize and discuss four crucial technical challenges in terms of scalability, throughput, access control, data retrieval and review the promising solutions. Finally, a case study of designing blockchain-based food traceability system is reported to provide more insights on how to tackle these technical challenges in practice.",2018,71,"46476972, 144115026, 2108824565, 1380530147, 1388034901, 2065096893, 2152801260, 1524736374, 48362791",46476972,Zagreb,3,"1688882, 145720154, 1737629",Y,"These five objectives together are sufficient to achieve impressive English <-> German and Engish <-> French results in Multi30k, a bilingual image caption scenario with short simple sentences, and to achieve a strong start for a standard WMT scenario.;The authors do provide a novel formulation and demonstrate the gains on a variety of concrete problems taken form the literature.;[ds]SSM is integrated with I2A, which generates rollouts of future states, based on the inferred hidden states from the d/sSSM-VAE model.",International Conference on Computer Communications and Networks,27,"scm,challenges,supply,data management,data management","41d4e093d5f7ed5aae1aaa9eb6c037742e4cf9b1, f4c4e148546089123f8da5db4fb246ab4062bd40, 643da4c4de1954daeac571a82367241db012a8bf, 25761ba4bdc054bfe902fe7c5d6338be6d00d491, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, 83cebf919635504786fc220d569284842b0f0a09, eadb1e7da375939e25083ae3936c4f4ef1f2a719, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, ae38dc77a962161107361f213db9216ee1274037, f208b3fb28c556ab62f9d202b7beae89700a338a, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, 8281b3e87965cbf2fa9f8bc066f08fd9108ef850, 3db1841fd5f2561a11dfbd8173616b3e695c84a1, 1a4c6856292b8c64d19a812a77f0aa6fd47cb96c, 11be2469ab1d1c508e7b6e14148990741ba87884, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, d3855b7351c11145e51301e6b686f748ca35c802, 9a0965beef113cc37491004b1848149e00300561, 3a083d843f891b3574494c385699c21766ce8b7a, bd6c027a3604d6c8fa23435bf382455b2bee436b, 63adc1e5086481e36b19b62707a96b799da51e59, 5371896313ac227eb819038dd55f213cb42b99e2, 957f5b1e7ca48891c2e279aefbfa0f04d989c21e, bbed457fd04ba4972018382d1a01a0bdde399d3c, 704011527f183b561ea6a75b21e4cefe5aa77fca, 64306bbddb4da7a4e06f990a0167d55fbbbbec82, 5d8e450c8c7d9faa8222f1d0eaf34b01755c9eef, 69a72ff5b30642d11c96635e99aadad3140d33a7, e5d720767b7a539bb2edaa98eaf572a4506a79c6, e32a2519b59d62cff6cb8136ee242dc3754ed57b, 8b417c2be7a7707f372049fb1193f0d42f799562, 70ce56e9a2181489d59f7170dc01fe8ba310a8e5, 0fa554d981809c5eb78956c779f75092c4f6c16b, 9619cde5c79d91ca5c432186668618312175f8dd, 01bf0e83159712fbbbd12171a7e268547a4cfbc5"
9727206903eb40d4fa42606711bad3402f2ba9aaa,Decentralized IoT Data Management Using BlockChain and Trusted Execution Environment,https://www.semanticscholar.org/paper/9727206903eb40d4fa42606711bad3402f2ba9aa,Conference,"Due to the centralization of authority in the management of data generated by IoT devices, there is a lack of transparency in how user data is being shared among third party entities. With the popularity of adoption of blockchain technology, which provide decentralized management of assets such as currency as seen in Bitcoin, we propose a decentralized system of data management for IoT devices where all data access permission is en-forced using smart contracts and the audit trail of data access is stored in the blockchain. With smart contracts applications, multiple parties can specify rules to govern their interactions which is independently enforced in the blockchain without the need for a centralized system. We provide a framework that store the hash of the data in the blockchain and store the raw data in a secure storage platform using trusted execution environment (TEE). In particular, we consider Intel SGX as a part of TEE that ensure data security and privacy for sensitive part of the application (code and data).",2017,108,"39712836, 38804742, 145155297, 3071249",39712836,Vienna,3,"1630331317, 152355659, 2855934",Y,"2) compressing the embedding space using pca;The basic idea is to use a multi-step dynamics model as a ""baseline"" (more properly a control variate, as the terminology in the paper uses, but I think baselines are more familiar to the RL community) to reduce the variance of a policy gradient estimator, while remaining unbiased.;Creative and interesting The paper introduces an application of Graph Neural Networks (Li's Gated Graph Neural Nets, GGNNs, specifically) for reasoning about programs and programming.",IEEE International Conference on Information Reuse and Integration,17,"data,management,iot,data management,data management","1a37223175138bc1aa53b425ea2fdd0b382405a5, 8dce62b18bdea587c07cb4769a05ca0a816d09ba, d1206ccabd1980848f14472d6548251c2fab7963, 7637ed79d30d0139901175ae4abedd822c217ab4, 27245e65a27bde90b5b0bb25d157bb75a0ad8b5a, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, 643da4c4de1954daeac571a82367241db012a8bf, 247dec05283a1a521f99253a6cca6a5858cac0d2, 3a083d843f891b3574494c385699c21766ce8b7a, 0a92bc2dc8a216e6aced83edc0358241066833df, 239bf45c13b3f6d38c74026b535f785febf9cd08, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 4d8f0ae904779a50b2e18fec49e51a5661a98d8a, b0b770fb8c7760749c88e3c83ae173cdb07f7bd5, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, febe776e285dc5e72c7e3ee697a87a794e1c00ff, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 00e18c603e60d861c4e99c541e4d65ef442d5945, db4cf9f6a653d5c15973e836c800ea47743251ae, 1cf2e9e198feef3893da2800a7949f6880ddc084, 194073c405e9c362c955e9ac31979ddbc037ff8d, 9eacf62f1e546748428c7e4843731b1595294200, 4087e84fc695bb6433d0104ee94f9d7e9f4b7da5, 785a1c77fabd1ad870a9dbc98d235d2fcbdbaa6f, 2bc3644ce4de7fce5812c1455e056649a47c1bbf, ce9ca56036307217ea565644d3d3bd74b879e045, 23466d271676ae467cbe85bb1993682f3502e840, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 2af8907d4a974ae41044581f5e5d67317cb08568, 5406e153957dd7a165264da6e6e5d81251997404, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, 0e9a44ce661c3535d5ce747912540080324489f5"
aea731e7cf33aa3d482b13f42cedbc1adb3271c6a,"The “Problem” of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation",https://www.semanticscholar.org/paper/aea731e7cf33aa3d482b13f42cedbc1adb3271c6,Conference,"Human variation in labeling is often considered noise. Annotation projects for machine learning (ML) aim at minimizing human label variation, with the assumption to maximize data quality and in turn optimize and maximize machine learning metrics. However, thisconventional practice assumes that there exists a *ground truth*, and neglects that there exists genuine human variation in labeling due to disagreement, subjectivity in annotation or multiple plausible answers.In this position paper, we argue that this big open problem of human label variation persists and critically needs more attention to move our field forward. This is because human label variation impacts all stages of the ML pipeline: *data, modeling and evaluation*. However, few works consider all of these dimensions jointly; and existing research is fragmented. We reconcile different previously proposed notions of human label variation, provide a repository of publicly-available datasets with un-aggregated labels, depict approaches proposed so far, identify gaps and suggest ways forward. As datasets are becoming increasingly available, we hope that this synthesized view on the “problem” will lead to an open discussion on possible strategies to devise fundamentally new directions.",2021,58,2022124,2022124,Dublin,3,"151500725, 144259957, 2223551699",Y,"Specifically, for KDE and OC-SVM, a naive PCA is used to reduce the data dimension.;The proposed method achieves perfect accuracy in every condition.;Review This paper describes a method for computing representations for out-of-vocabulary words, e.g. based on their spelling or dictionary definitions.",Conference on Empirical Methods in Natural Language Processing,21,"variation,label,labeling,data modeling,data modeling","ed935c6b359a7a486c28240d796e84897d095125, ec7f5dc077480df149bcd4358a3aa8441878ca59, b473e91cbe80c8b46451b49153cd5f93030480ab, 6d2d6707a0674ee41d539d106e6cb359a0a6ca06, 1452b25a7680bbb2c66dd7dfca6704292405da92, b0b770fb8c7760749c88e3c83ae173cdb07f7bd5, b2114228411d367cfa6ca091008291f250a2c490, 3705919b880f4f8dc37483a704e14dd078cb9ac4, bc00ff34ec7772080c7039b17f7069a2f7df0889, 1051abf1e3dae90241ad15b3f98f2e41197ee611, 8281b3e87965cbf2fa9f8bc066f08fd9108ef850, 8bb9db78b4413b92cdeeae9e24e955aab9c87ae1, 7ed665355ac78bf0c394602dd9d26075195ce2f2, 492c389d560d9db39c758d07e635408d2e0eaf7d, 1a37223175138bc1aa53b425ea2fdd0b382405a5, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, 08aaaf7ae3d61875e7bcf5c1bb0df4f17066e300, 99f06e88e76f1af51d08d7adfb26d758ebc6acab"
a85c45ce7c893388e8eafa8a653b042e1497db48a,Cross-Node Federated Graph Neural Network for Spatio-Temporal Data Modeling,https://www.semanticscholar.org/paper/a85c45ce7c893388e8eafa8a653b042e1497db48,Conference,"Vast amount of data generated from networks of sensors, wearables, and the Internet of Things (IoT) devices underscores the need for advanced modeling techniques that leverage the spatio-temporal structure of decentralized data due to the need for edge computation and licensing (data access) issues. While federated learning (FL) has emerged as a framework for model training without requiring direct data sharing and exchange, effectively modeling the complex spatio-temporal dependencies to improve forecasting capabilities still remains an open problem. On the other hand, state-of-the-art spatio-temporal forecasting models assume unfettered access to the data, neglecting constraints on data sharing. To bridge this gap, we propose a federated spatio-temporal model -- Cross-Node Federated Graph Neural Network (CNFGNN) -- which explicitly encodes the underlying graph structure using graph neural network (GNN)-based architecture under the constraint of cross-node federated learning, which requires that data in a network of nodes is generated locally on each node and remains decentralized. CNFGNN operates by disentangling the temporal dynamics modeling on devices and spatial dynamics on the server, utilizing alternating optimization to reduce the communication cost, facilitating computations on the edge devices. Experiments on the traffic flow forecasting task show that CNFGNN achieves the best forecasting performance in both transductive and inductive learning settings with no extra computation cost on edge devices, while incurring modest communication cost.",2020,70,"27737939, 2267664, 47909531",27737939,Riga,3,"2837279, 1792647, 1720266",Y,"2. Indeed, AlexNet is a good seedbed to test binary methods.;The proposed method is new and technically sound.;- I suggest to divide Section 3.1 in two subsections.",Knowledge Discovery and Data Mining,26,"data,edge,forecasting,data modeling,data modeling","e3b37c1c955b2b10809040ce277edae5333b99c3, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7, b34fc78de28be598e21118d7cb9d84d63374addc, bd6c027a3604d6c8fa23435bf382455b2bee436b, a830083704284c8c5ddaf04f676c6ce23d583942, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, 099043827df60225cf33c820052716cce64d49e9, ff75865cde62592d068b2afd055c57c81d77158b, aee3d7f98b966240178ef420724c840f9b61deb3, dad8965c5a4c0a0ea1eb3837c6a9c3b42597c2ce, 4afa7d8e2de43b0b67366b1bce8768f5a246d153, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, 549e933821fdf7cd0309dacaae99c8284cbfcc24, 11be2469ab1d1c508e7b6e14148990741ba87884, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 740b5dec469e6b54e5e191a577a9b6a6ad8562e3, 0427110f0e79f41e69a8eb00a3ec8868bac26a4f, 90aca7b4cdd728b28b2fb5b4dc3ae3e37daa5b94, 14014c024674991149f3ecf9314c93f7e029ef1a, 971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67, 91d24a94a276f5b226b07fe294561482b1d4104d, 3fadb72fb150197aa5eb88a65b244763fb45ab9b, 5031790972d496547b6613d46a4a0134c824db6e, 19cf7458db4e17c7504eee24ccf961e1dc91435c, 0894585294c67193ff3190240554677b56fd79a0, addae423490bbe82da4fb2fc265237178686b4e8, 4e58100b319d74f97ed550a4e5fa32dea8c06fe1, 32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b, aea731e7cf33aa3d482b13f42cedbc1adb3271c6, e57aeb158a38ccf33d2f0f5a8f63f1209497e329, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, be383c607d4d357c763d2329ab71799c6e1393b4, bdb68c5e2369633b20e733774ac66eb4600c34d1, 2346d121f38fc19c77e0b062415519843f478163, 033a50c4515b153b6e706018075c333c64981fd7, d6e1e4f0ad898ca6ac37e6e139a77fa3982170d4, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, e02f91d625cd32290d4ede0f31284da115844316, ce51eff5b529ee572dab1c1f38f20adc8e89bab2, b61b260de1599e6e89491cad9160898fcd3b34c2, 492c389d560d9db39c758d07e635408d2e0eaf7d, 04fa3ebc4c0c0b4a2d2d1a3fc612134a05057696"
d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5a,An Effective and Scalable Data Modeling for Enterprise Big Data Platform,https://www.semanticscholar.org/paper/d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5,Conference,"The enormous growth of the internet, enterprise applications, social media, and IoT devices in the current time caused a huge spike in enterprise data growth. Big data platform provided scalable storage to manage enterprise data growth and served easier data access to decision-makers, stakeholders and business users. It is a well-known challenge to classify, organize and store all this data and process it to provide business insights. Due to nature, variety, velocity, volume and value of data make it difficult to effectively process big data. Enterprises face challenges to apply complex business rules, to generate insights and to support data-driven decisions in a timely fashion. As big data lake integrates streams of data from a bunch of business units, stakeholders usually analyze enterprise-wide data from various data models. Data models are a vital component of Big data platform. Users may do complex processing, run queries and perform big table joins to generate required metrics depending on the available data models. It is usually a time consuming and resource-intensive process to find the value from data. It is a no-brainer that big data platform in the enterprise needs high-quality data modeling methods to reach an optimal mix of cost, performance, and quality. This paper addresses these challenges by proposing an effective and scalable way to organize and store data in Big Data Lake. It presents some of the basic principles and methodology to build scalable data models in a distributed environment. It also describes how it overcomes common challenges and presents findings.",2018,21,31933741,31933741,Monaco,2,"2113909888, 93841942",Y,Follow up experiments extend the basic setup significantly.;In its current state and format the major issue with this work is the lack of more in-depth performance analysis which would help the reader draw more solid conclusions about the generalization of the approach.,2019 IEEE International Conference on Big Data (Big Data),18,"data,enterprise,business,data modeling,data modeling","c665003881c3c35589d1e48da1ee7234b48f2ac8, 78aab73ed574393ab421f25b3a0e3f7343e64748, a2243db6cb2ffdf48ce54397fed47de992d2c8e6, 9727206903eb40d4fa42606711bad3402f2ba9aa, 8c33ca066e2ab615e24c65198c794114436053dd, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, 0b24f27d920bd1d23c8f6a1ab2b603c5c14f0a36, a5710a7a54b50c26f3c40c37a19b2ec63330e90b, 03532123ccffae8d411264320e8a5ae2b6eddea0, 9583ac53a19cdf0db81fef6eb0b63e66adbe2324, 60119658af638693f6de23d8466968e60c428ac7, 07cca761749bfe21c2d096ff60f32b574d5c84c4, a22f3398ea865426c89ee66f4824ec626e56a864, ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba, 9e66ae24a541255c2d931184498ee116ce81478a, 1051abf1e3dae90241ad15b3f98f2e41197ee611, 54ddb00fa691728944fd8becea90a373d21597cf, ec2f9076448ba25a225618603adde60caa76c4df, 740b5dec469e6b54e5e191a577a9b6a6ad8562e3, e02f91d625cd32290d4ede0f31284da115844316, 9efd70d2c06733704220313fb67720aa45c6362a, 155f27879f185f1ab04107c91c2ae7cf6a910a03, b6a7226e5f6d618370995eccad68af195ef32da2, 933baeec555352784848a93284c9dd0e79477759, b0ee814c7a3eed260c9913861329c9f73e880d00, b52db9e41e15f76bdcfbe674abe0314af545c430, 182acc7a1c9a63cfdef9f86ee9df43fd75e05060, 3924aa213ff891812c66a6909ab902684d3eb107, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, d6f002d88638de71114dab083f0ea8ceea6b6a5a, 4895c430c7810b45840b58cc9182f12143013a43, 0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f, 8c7aee0bbc062d5b4bcd34951b1e002274288206, 82870bc488b57cdf5ea62877109a7278af2926b3, 15370f51d666ab8ef17185679553c6a8647b2a15, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 150f95f9c73820e0a0fa1546140e9f2bdfd25954, 92930ed3560ea6c86d53cf52158bc793b089054d, d8a5474f450330ad25c1e22f27e88f3630cb840d, f3d594544126e202dbd81c186ca3ce448af5255c, f0c27af6c330d5c3b0a8eb376a69ce92c85badd7, f75b70c9d7078724b592ec3e21de705e7b6ff73f, 915ab7b6ca3633230403d47dbb31cf74888cc5c9, 94de6bab96ad533169dbc3bf9e6557581e59cb6f, 92912dd895c360f01a6be9c9f6d207642139525e, febe776e285dc5e72c7e3ee697a87a794e1c00ff, d2a5dcecd2ffdf03473df1688091f08fadb114a3, 73d4accea441aae2373828a8dc2175aa2759c38f"
818de553ecd306735971aba04bbfc29d17457084a,Robust Spectral Clustering for Noisy Data: Modeling Sparse Corruptions Improves Latent Embeddings,https://www.semanticscholar.org/paper/818de553ecd306735971aba04bbfc29d17457084,Conference,"Spectral clustering is one of the most prominent clustering approaches. However, it is highly sensitive to noisy input data. In this work, we propose a robust spectral clustering technique able to handle such scenarios. To achieve this goal, we propose a sparse and latent decomposition of the similarity graph used in spectral clustering. In our model, we jointly learn the spectral embedding as well as the corrupted data - thus, enhancing the clustering performance overall. We propose algorithmic solutions to all three established variants of spectral clustering, each showing linear complexity in the number of edges. Our experimental analysis confirms the significant potential of our approach for robust spectral clustering. Supplementary material is available at www.kdd.in.tum.de/RSC.",2016,63,"11754930, 22654490, 3075189",11754930,Brussels,2,"1996394, 2081346",Y,The experiments are very clearly presented and solidly designed.;1. This manuscript provides the sufficient and necessary characterization of critical points for deep networks.,Knowledge Discovery and Data Mining,22,"clustering,data,approaches,data modeling,data modeling","8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, a620ea8654f86b282e0d2c1fbc6616b90ca45bd1, 634fe61f3ab6692ea4733989a1c76e793bb8b69e, d7b820af40a9e2660ef700d39f7b2e27b43435c5, 182180bd69ea6d2f59225ded5ddc900b8558ab9f, b11468ed3a6215f1ee109fe5b2e0bd0e0e2f0eb1, d916776e0c6a04b0def4c22257c188776c2edab2, 215fc60307f741b9db059204e41db8bfb879e606, f18be38578ee52aa7071c404d42e3d53ae003122, 42d3ae61ca296558cba61446bd95b8a6e5b1082d, 74b4f16c5ac91e3e7c88ae81cc8c91416b71d151, 6dd481d5eb8d76d4b61a1829c7687d008e0937ab, 08764019e9762da527253b37b0ff39c46a4206b7, 69d49a06f09cf934310ccbf3bb2a360fa719272d, 1c2efb418f79b5d29913e014a1dfd78865221c39, f72d3f58ff73353978e224af348448b34d27cf7b, 1c748f86182a62d44d5b44316db510f8d833e19f, c84389369720dcd2f004c48e58fbac2c45c8f092, 456c011594ecacdd24298a161787389ccbe4b88b, 76b93abde3ec4e5bd84a38d488a8db209dc4ac98, 2141334fad7248fc707607bc9453d44686ae07a7, 047286f5b9315a8e8bf56c4fc936e62f21495892, 139a0c7a60667979dcb57eae677f75ff3f0b0196, 0e141942fa265142f41a2a26eb17b6005d3af29e, f72053903270d9a7f41108461ad04d5aa075218d, 371a343457a4fbff00000bf4faa29b2b2f85744c, ba9b6f805feb62c978d384211f910790643a023e, 43e624ddeed82df944a6cae0dedec3372438e243, f295157f37cfb43cd8d8d2690ea124edc5ea59c2, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, fdc57c18f3b636c3273542327ae540217972558f, b7034546bee38ba13d3b312fce893a22e33ce4dd, 0574a3abc98f0e6bd035a4ff4ea107cfba45d3d4, 332e0eab5fba8e6940f3e481f542a99ac17b9717, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, 8b417c2be7a7707f372049fb1193f0d42f799562, 03532123ccffae8d411264320e8a5ae2b6eddea0, bb0cec8f2d34cfb9dbf8bffd1a5de499311ae098, 537f5e8e4139392cd2d108f32495e5b2b80151ac, 4f480bae3196dbbc27ab383bce33478ea963f9b3, beb890d47bbc21a96967f9993c9d6e15686b2eac, 9583ac53a19cdf0db81fef6eb0b63e66adbe2324, 182acc7a1c9a63cfdef9f86ee9df43fd75e05060, e5194ae88d63c7549678b1b73cfdaf7112164272, 2ee03e28208a9310a9be4032c2b04ebdddb83cc7, f28e387d4229c5f690ce4570a391c0f47e7155c7, 119a9e5b563cf1134897553ee49325b5a5bd9fb9, 6dc4883228c95e8a332320fcc587a0ff33c84d59, b9f190339ce0b4cdad3464c45ec3266a7369fb7c, 6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91"
595101f13b961d69c553ce1ed24f60f3f1085e02a,RecSSD: near data processing for solid state drive based recommendation inference,https://www.semanticscholar.org/paper/595101f13b961d69c553ce1ed24f60f3f1085e02,Conference,"Neural personalized recommendation models are used across a wide variety of datacenter applications including search, social media, and entertainment. State-of-the-art models comprise large embedding tables that have billions of parameters requiring large memory capacities. Unfortunately, large and fast DRAM-based memories levy high infrastructure costs. Conventional SSD-based storage solutions offer an order of magnitude larger capacity, but have worse read latency and bandwidth, degrading inference performance. RecSSD is a near data processing based SSD memory system customized for neural recommendation inference that reduces end-to-end model inference latency by 2× compared to using COTS SSDs across eight industry-representative models.",2020,80,"49212289, 2633839, 1481699378, 2061231, 2797270, 1896817, 2255803",49212289,Stockholm,3,"144588806, 2059271276, 32528506",Y,"On the negative side, the paper is only qualitative.;So, the main contribution of the paper is to do all the sensitivity calculations with respect to SVM problem and then use the importance sampling theory to obtain bounds on the coreset size.;If the text is from the user, a named entity recognizer is used.",International Conference on Architectural Support for Programming Languages and Operating Systems,25,"models,inference,recommendation,data processing,data processing","b58e98029d53d69ccc7089dca7b01bf050b5ad2b, b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0, 4be9368abc2474d6fd38639e523cf03af1873fd9, 0af24df95f8a0b7e6fc257a98a5820314d1243f3, f406aceba4f29cc7cfbe7edb2f52f01374486589, 538fbdb8013ab43a9b5d725461b294ad29fcced7, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, 0b24f27d920bd1d23c8f6a1ab2b603c5c14f0a36, 7c24234042988e2f820a4350f43422ed2ad6fc52, 54ddb00fa691728944fd8becea90a373d21597cf, be082d70534db088315f2cc5b42c2fdcd58c1b8c, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283, 60119658af638693f6de23d8466968e60c428ac7, 0456ed94f6c455f99cb67abe8a28aeb3ef9f489b, 91fc647899f801c9d351349ce73779918f90a713, 0fe0f08e71e188cb2a663f36acb296d9eb6fe694, 287ba5bf00d96af1596aaf80c178392a9c4fcc28, 2f44ab0e52eb98fdfc0086638887f175b6f2fe4b, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, 6422f4b9e3bedf585170bffc7105ffe2061e87ae, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, d93bcf0685c15c45d078eafea565969c04daccd3, 736ef8a32d6c5f76a21d61299300cf796480d507, dd2deed2ce6e110236a1280db765fa02c7488eb1, 7a4fe2f003241ad97bf1778e527cb0306fa90da2, 084a93c8ac0230ea9fe64d445ad1d6af5ea0b3b3, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, db4cf9f6a653d5c15973e836c800ea47743251ae, f04d8e558c11ac0fc9318c4f3d61b651c62ddcfa, 72243bfa618fd6c3c01bb7b4885a4a456b2a6071, 661ccdb41fe977d47273e586389cacc1489f3286"
ada0b87cd5c30d31186c38fb12e631d29426a3bfa,Spark SQL: Relational Data Processing in Spark,https://www.semanticscholar.org/paper/ada0b87cd5c30d31186c38fb12e631d29426a3bf,Conference,"Spark SQL is a new module in Apache Spark that integrates relational processing with Spark's functional programming API. Built on our experience with Shark, Spark SQL lets Spark programmers leverage the benefits of relational processing (e.g. declarative queries and optimized storage), and lets SQL users call complex analytics libraries in Spark (e.g. machine learning). Compared to previous systems, Spark SQL makes two main additions. First, it offers much tighter integration between relational and procedural processing, through a declarative DataFrame API that integrates with procedural Spark code. Second, it includes a highly extensible optimizer, Catalyst, built using features of the Scala programming language, that makes it easy to add composable rules, control code generation, and define extension points. Using Catalyst, we have built a variety of features (e.g. schema inference for JSON, machine learning types, and query federation to external databases) tailored for the complex needs of modern data analysis. We see Spark SQL as an evolution of both SQL-on-Spark and of Spark itself, offering richer APIs and optimizations while keeping the benefits of the Spark programming model.",2014,1288,"144482217, 2066641, 1387529184, 40199213, 2536434, 2086593199, 39309572, 2403754, 143666627, 38565890, 143834867",144482217,Vaduz,3,"2082427140, 36347083, 2116579935",Y,"Detailed comments: The problem of unsupervised time series clustering is important and challenging.;This paper is clearly written, proposes a simple model and seems to outperform current methods.;Does each component is related to a certain topic?",SIGMOD Conference,14,"spark,sql,processing,data processing,data processing","545f108575314031f35c617c4ac35a10133c50e3, 0f38b3d717e0fcc6eacc9c6e78b252227440e04e, 34e9852f588f75eba81c66a3e5f867a794a5a690, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, 7fa30c8af436b7df513ef41e2cccd8a2404f37f8, 2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, 42543dc42e65609bbbf2be470d54dd923532c36a, 916455d97cd792c2eb5b00663689592e25cbc8d8, 9712624bb61abb0da989514cae558cfab61bb9d2, 0f4d00d01d43d3967ee92b58481b5ad530a944d1, 156609022dd6258c60238859622da0a1683bd062, cf5fddf6717e88e2bbed6b0bfe54dfeb311e6789, d2a609ffb814442d0728aef9f6616f9cd775face, b6593b5ad6b0e00232ee88cf2bc578645c1299a0, 39602922b04885047254444fd1a1586d797617ce, 0c4ba06a12584cb63a85294f796108d359fe9835, 3faeb21fe256b99391d69570053a8c2d91e9f348, 2c5ab7d87e3342d2dba7d1d113ca1b16c545e344, 1c748f86182a62d44d5b44316db510f8d833e19f, d983427574a41bf485e31ab983acc1c742e9e24c, f72d3f58ff73353978e224af348448b34d27cf7b, 2346d121f38fc19c77e0b062415519843f478163, c997ed5ac5772bb0fce8bb11b8c86bc33310f39a, 1109d62ebd2b29a7dc148bc30dd6cfc803a63dec, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b, bb1118fb9fd86da6a2f65770353d8fb4362d9883, a781cea542e99c6bb9422858e7c04eaef18c7673, 63de6db7245f634ecbef4c505099874c1ba65145, 80dd97954ddf3edd22d4cb21f0ac31b7ffed6bbf, c9645aa4ea31903e02e201b877fd3e1466adff4f, 1fcddebb7b51175ac412009ec1c26cc29fb925cf, 10aa2be24951e6de76b630482a645d79354c4cde, 4419c5720e30d5ca5158795d4c848125650b8db1, c24d47ff95cd4bda073c75ec24ececaa3b10c995, 4cd033a56b19f87f6adfefeef5fcc990306ecf40, 0894585294c67193ff3190240554677b56fd79a0, 8674494bd7a076286b905912d26d47f7501c4046, a747e8f2659df479c0092301b9658fc582423df1, cb12fe714dd4fa1d14b6a8023d494c14c89e90ea, 2ddcd47b28eb4b43317a34cf56e83309f5347699, 28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, 695bdc6e24608364491b9418a220c65a7cd17413, c0aec04ee86c0724d61c976f19590fbe9c615723"
ac91892a8a6b6c3e97aa92b6fa8d54b42cade0eea,Learning scheduling algorithms for data processing clusters,https://www.semanticscholar.org/paper/ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee,Conference,"Efficiently scheduling data processing jobs on distributed compute clusters requires complex algorithms. Current systems use simple, generalized heuristics and ignore workload characteristics, since developing and tuning a scheduling policy for each workload is infeasible. In this paper, we show that modern machine learning techniques can generate highly-efficient policies automatically. Decima uses reinforcement learning (RL) and neural networks to learn workload-specific scheduling algorithms without any human instruction beyond a high-level objective, such as minimizing average job completion time. However, off-the-shelf RL techniques cannot handle the complexity and scale of the scheduling problem. To build Decima, we had to develop new representations for jobs' dependency graphs, design scalable RL models, and invent RL training methods for dealing with continuous stochastic job arrivals. Our prototype integration with Spark on a 25-node cluster shows that Decima improves average job completion time by at least 21% over hand-tuned scheduling heuristics, achieving up to 2x improvement during periods of high cluster load.",2017,509,"2512621, 1962485, 2043402, 40071013, 79404966",2512621,Podgorica,3,"3024698, 1473151134, 151213231",Y,"The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss.;This paper studies the problem of multi-label learning for text copora.;So, I wish to see a section on testing with Resnet and GoogleNet.","Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",17,"scheduling,rl,decima,data processing,data processing","9312e5efa0dcef1445d45a41771f12e2a8dc6715, 03aeb4520e760a906393aaf9c1bf4e526483d081, ce51eff5b529ee572dab1c1f38f20adc8e89bab2, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, f28e387d4229c5f690ce4570a391c0f47e7155c7, b9c974380649749320f4a02e33b2e5014e7f1756, 2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb, a5710a7a54b50c26f3c40c37a19b2ec63330e90b, 02a1e8e77f501675945890df45fbdc11726cb0ba, c6e242fce9e5e4ba2c4dc869cfb0afabf2020852, 518a7c79968a56d63a691d42f8378be6c776167e, e7b0e2710821a9fe1e3d2b9a09dd94514b2d5ea5, 752604994a7ca548ff2954114fc61a501d857b1c, b61b260de1599e6e89491cad9160898fcd3b34c2, 69a72ff5b30642d11c96635e99aadad3140d33a7, b067177b1e17287185eb3b82ccc3d7c646b3ec40, d21703674ae562bae4a849a75847cdd9ead417df, 5d433da6d0f143f20936379910104d2bb139d4ae, 1eaab9b33f1261744567455a14830e8a92796cf5, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, cc017a62c605a0749e35a1264a46d62e78fb68b7, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, 8adb47deeef943c2c1bae41f9498a382fb818a16, 0ecf3f089e6dc7944b440227069b9d0143e18d78, fb00016c1e048b9373803add001c1ec7e877cb23, 42375e9a309fe5bf5c671c918cd8c9d5d85568cc"
f90f526b101cb8a0260f5165a3875928c58ae48aa,NATSA: A Near-Data Processing Accelerator for Time Series Analysis,https://www.semanticscholar.org/paper/f90f526b101cb8a0260f5165a3875928c58ae48a,Conference,"Time series analysis is a key technique for extracting and predicting events in domains as diverse as epidemiology, genomics, neuroscience, environmental sciences, economics, and more. Matrix profile, the state-of-the-art algorithm to perform time series analysis, computes the most similar subsequence for a given query subsequence within a sliced time series. Matrix profile has low arithmetic intensity, but it typically operates on large amounts of time series data. In current computing systems, this data needs to be moved between the off-chip memory units and the on-chip computation units for performing matrix profile. This causes a major performance bottleneck as data movement is extremely costly in terms of both execution time and energy. In this work, we present NATSA, the first Near-Data Processing accelerator for time series analysis. The key idea is to exploit modern 3D-stacked High Bandwidth Memory (HBM) to enable efficient and fast specialized matrix profile computation near memory, where time series data resides. NATSA provides three key benefits: 1) quickly computing the matrix profile for a wide range of applications by building specialized energy-efficient floating-point arithmetic processing units close to HBM, 2) improving the energy efficiency and execution time by reducing the need for data movement over slow and energy-hungry buses between the computation units and the memory units, and 3) analyzing time series data at scale by exploiting low-latency, high-bandwidth, and energy-efficient memory access provided by HBM. Our experimental evaluation shows that NATSA improves performance by up to 14.2× (9.9× on average) and reduces energy by up to 27.2 × (19.4 × on average), over the state-of-the-art multi-core implementation. NATSA also improves performance by 6.3 × and reduces energy by 10.2 × over a general-purpose NDP platform with 64 in-order cores.",2019,62,"2061067079, 1939292, 46175739, 3387282, 1474355913, 2059547812, 3190187, 145929920",2061067079,Sofia,3,"1738190, 37722032, 47781311",Y,"The encoder network from NEM can be seen as a recurrent network which takes one latent variable theta_k at time t and some input x to produce the next latent variable theta_k at time t+1.;How does this compare to the near-identity constraints in resnets in Shaham et al.;I assume that it involves \hat{M}, but it would be good to formally define this notation.",ICCD,37,"time,series,data,data processing,data processing","bc44c0c64a473e035b11ae60a1993ad3db1acd2e, f3b8a3ffa0bdfe5578e0f0f44ea3b66cad38c032, 3a083d843f891b3574494c385699c21766ce8b7a, 31f9eb39d840821979e5df9f34a6e92dd9c879f2, 75ea299834d6949e89e91d006677343ddab44e49, 340f48901f72278f6bf78a04ee5b01df208cc508, b889b1d6944213bc2ca29e3ad07ee65ede20892d, 57a3fc6d0aaad3c10c793b4e59390ca04c935282, d1ae4ab5047489c2b010c7ce72262982ad66ad60, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, 239bf45c13b3f6d38c74026b535f785febf9cd08, 5030702fea15d66a73fc997325431f1d7945ad9a, 4b06c7e29280b1c6bc05c9df39023b48fef02c93, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, 43bafb4997515d2904abfca8214f2fc806680fc3, b067177b1e17287185eb3b82ccc3d7c646b3ec40, 4e13a8e8ba8d33e15ed037bfca7c651047533990"
d9a7fa7616a327367696e19b1846519745cd43ffa,The Devil Is in the Details: Delving Into Unbiased Data Processing for Human Pose Estimation,https://www.semanticscholar.org/paper/d9a7fa7616a327367696e19b1846519745cd43ff,Conference,"Recently, the leading performance of human pose estimation is dominated by top-down methods. Being a fundamental component in training and inference, data processing has not been systematically considered in pose estimation community, to the best of our knowledge. In this paper, we focus on this problem and find that the devil of top-down pose estimator is in the biased data processing. Specifically, by investigating the standard data processing in state-of-the-art approaches mainly including data transformation and encoding-decoding, we find that the results obtained by common flipping strategy are unaligned with the original ones in inference. Moreover, there is statistical error in standard encoding-decoding during both training and inference. Two problems couple together and significantly degrade the pose estimation performance. Based on quantitative analyses, we then formulate a principled way to tackle this dilemma. Data is processed in continuous space based on unit length (the intervals between pixels) instead of in discrete space with pixel, and a combined classification and regression approach is adopted to perform encoding-decoding. The Unbiased Data Processing (UDP) for human pose estimation can be achieved by combining the two together. UDP not only boosts the performance of existing methods by a large margin but also plays a important role in result reproducing and future exploration. As a model-agnostic approach, UDP promotes SimpleBaseline-ResNet50-256x192 by 1.5 AP (70.2 to 71.7) and HRNet-W32-256x192 by 1.7 AP (73.5 to 75.2) on COCO test-dev set. The HRNet-W48-384x288 equipped with UDP achieves 76.5 AP and sets a new state-of-the-art for human pose estimation. The source code is publicly available for further research.",2018,134,"47513708, 2118932732, 2065456437, 143986385",47513708,Oslo,3,"2070951368, 2112197162, 6667699",Y,"This paper also would probably be more suitable for a chemoinformatics journal, where the rationale learning would be highly appreciated.;But beyond this, and despite fairly reasonable familiarity with the subject, I simply don't understand other elements that the paper is talking about.;Some empirical observations are made, but it is not discussed whether what is observed is surprising in any way, or just as expected?",Computer Vision and Pattern Recognition,18,"data,estimation,udp,data processing,data processing","7998468d99ab07bb982294d1c9b53a3bf3934fa6, 371a343457a4fbff00000bf4faa29b2b2f85744c, eff6546819d25df0bccdc89f02554a43a4f1c464, da9b8b4073e6ad44b3da66e1e117cb1ddbf8836d, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, 7a4fe2f003241ad97bf1778e527cb0306fa90da2, f50440b10052864586e105e39e7eec390842d5e4, febe776e285dc5e72c7e3ee697a87a794e1c00ff, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, a18f02e5c24e1f924aea268dd343bbdea234f2bb, a0a79dad89857a96f8f71b14238e5237cbfc4787, 6544259ff6b335b1dcec75e031b6d57e5b9509f4, 4895c430c7810b45840b58cc9182f12143013a43, b22ed1ea1d174af48c655d48e284afc239ebfa6a, fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53, e7a7735104448371dde788542ebfc6af6485ea43, ae026f29c2d571871f426ff4873d43b4ff90b9ad, 846883b7761cb5fe4468d42bf9d328b5d1030175, 6db0f8d396371078590faa7b34ae2e0e1b154a60, e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0, 6a6ad9eb495739f4c80e7c09598720c3d5c5dff7, 417326e51d78ba8bd2621f23e539b41bbdd336d6, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 02b1607af35b48f0bd716367caf6a7428b969369, 6eb8caebbffc7e5b301b66dc36acad46b4dca5c9, 2737a61f6557fe7bf53a608c668de2eff1f582f0, 024006d4c2a89f7acacc6e4438d156525b60a98f, ee6f23590783adec7cf6b2030c6a46f3117a708e, 4fcfe83c05402b5c5fb6e853082e74af6379d7f9, 8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6, ada0b87cd5c30d31186c38fb12e631d29426a3bf, 43e6e8d6663d83f1b74cf5a2be7b040b0928f867"
a8ee35c445c627bce7cb1b192a1ad7db2a98ea5ba,Panthera: holistic memory management for big data processing over hybrid memories,https://www.semanticscholar.org/paper/a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b,Conference,"Modern data-parallel systems such as Spark rely increasingly on in-memory computing that can significantly improve the efficiency of iterative algorithms. To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy-inefficient. Emerging non-volatile memory (NVM) technologies offers high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages (e.g., Scala and Java) and executed on top of a managed runtime (e.g., the Java Virtual Machine) that already performs various dimensions of memory management. Supporting hybrid physical memories adds in a new dimension, creating unique challenges in data replacement and migration. This paper proposes Panthera, a semantics-aware, fully automated memory management technique for Big Data processing over hybrid memories. Panthera analyzes user programs on a Big Data system to infer their coarse-grained access patterns, which are then passed down to the Panthera runtime for efficient data placement and migration. For Big Data applications, the coarse-grained data division is accurate enough to guide GC for data layout, which hardly incurs data monitoring and moving overhead. We have implemented Panthera in OpenJDK and Apache Spark. An extensive evaluation with various datasets and applications demonstrates that Panthera reduces energy by 32 – 52% at only a 1 – 9% execution time overhead.",2018,49,"2121328398, 1734175, 145153097, 3260130, 144041053, 145929920, 46496975, 32215073, 38394648",2121328398,Brussels,3,"2086632521, 2274111800, 1682773",Y,"Overall it feels as if this is an interesting project but that it is not yet ready for publication.;* A thorough discussion on mixing in feature space, as well as a baseline which mizes in feature space.;I would strongly encourage the authors to try harder datasets such as COCO, VOC etc.",ACM-SIGPLAN Symposium on Programming Language Design and Implementation,39,"data,panthera,memory,data processing,data processing","714f47bbedcadd7ebc44d2d5010f13323fc6a256, 89a30b5dab02c9c390a632acad481fa602859272, 7c8bd41e9cebdb01f445a51ba43839c8d9b3ab91, ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8, ccd94602e3acecf999d0c9ba62b1a8bc02e9f696, 0894585294c67193ff3190240554677b56fd79a0, ff7bcaa4556cb13fc7bf03e477172493546172cd, 9181b0d801dfcd7723a3ede201f0543078e2c149, 8ee45aeb7c97e3346cc62f216f673b91277ac718, b080d072cfde697180db3234da08903c092e72c3, 5bbe106eeba21bcb4ac7d3ffc128f2ab581ffdfc, 00e18c603e60d861c4e99c541e4d65ef442d5945, 28a5a53dafacebad8a7c47773079caeffb9a5baa, 88932b1563316e4ed7a61c4c2c0c90d7aa9bb308, e576a2d97950b1f6831f88575dd3f370053f6af7, c2413fa296543159b32d16350d9e29f7db528790, e359e8960b0b09e8685a32927b7818f4b06ef881, 322d91190acd8ac8c64598f5126947b0485ba249, 8760bc7631c0cb04e7138254e9fd6451b7def8ca, b9b639522465cc606df878eee62e7f9c4bf19e62, f9c990b1b5724e50e5632b94fdb7484ece8a6ce7, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 3ec02e36cc0ba16292b255e9f90c3725521e2ec9, 213a83e61e1347ffa58da9383a4bf92f4a77a6c8, d57f11ed40c3cdcbb36cb758191db4f2c9372965, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 447884e7da189102189a156966623335c72199b0, ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7, 77a096d80eb4dd4ccd103d1660c5a5498f7d026b, f86f1748d1b6d22870f4347fd5d65314ba800583, 9a51df7eaf2457b034c49ca8ca4ca3b81e6c08c0, db0cc2f21b20cbc0ab8946090967399c25709614, 74b63c4cdc719a646ce014d084a0ce0ac66d9139, 55fa5be5288f6097ae5bd2dfe58fc07b3b39bfb6, 82a09f72d7b9587e3b519b1cd9640a5a611f3480, cf5dfc4a9f7a82b32640128ca10832eace55880e, e7354193f0e7fdd1b72725935ff2741cc7b8eeb7, c9b56cb026a38e39bb0228faac57accd6f65e6f7, 4dcbe71d261ba5caa0a06d5c265b5509edbbe1c4, 33fc8fe30dd1a59e6abbe6919ca4fe70d31fdb12, 5f51d468ce730eeade7e9f419a1fe7152582be25, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, c6e242fce9e5e4ba2c4dc869cfb0afabf2020852, be383c607d4d357c763d2329ab71799c6e1393b4, 2880ac931c11176aee6d42a7e7bb0703aacde3f9, 7909428ca7d813331bbe4d33d07ab09e984b41b4, d47a682723f710395454687319bb55635e653105"
59c2968fb9672a7152c52127255d8f0784bc2368a,Using the ESP32 Microcontroller for Data Processing,https://www.semanticscholar.org/paper/59c2968fb9672a7152c52127255d8f0784bc2368,Conference,This article deals with experiences with the development of applications of the ESP32 microcontrollers and provides a comprehensive review of the possibilities of applications development on this platform in the area of data measurement and processing. Microcontrollers usually connect with IoT modules and other smart sensors and provide data to the superior system. This paper also describes implementation of application with the version of connected OLED display and with ESP32 Wrover development board with integrated display.,2018,107,"2354124, 151167821, 31277482",2354124,San Marino,3,"29629766, 2119044211, 5430731",Y,"Also, the detailed specification of the VAE should be detailed.;) The dataset classifier (sec 4.3.4) could be learnt end-to-end by using a softmax output of the dataset classifier as the alpha weighting.;This second part was confusing - although the maze can be viewed as a graph, many other works apply ConvNets to maze environments [1, 2, 3], and their work has little relation to other work on graph CNNs.",International Conference on Innovative Computing and Cloud Computing,19,"development,applications,esp32,data processing,data processing","d2a505586c0da20752b98f63c7760b6a5c41e28d, 96273b87cd0eb9d1c9a12afae621ce2abdbbab36, b05306f0b142e5afb3974b1b79996e5b82653662, 0cec0c296efedb814342b4b841d4583efbfc6777, bcc82ce554942880814243fc8c08a88b9d2aad09, 43eea2a73997294193228d50f9ff25fc5345664b, df7d26339adf4eb0c07160947b9d2973c24911ba, 54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f, bad4c08f03587e38ee960e2aa76e16d722826e7c, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, b613887337a5d2e8fc8773037116be81e6346835, 39444c55f07839ac6a0d1839472a982f8fb447bb, 3705919b880f4f8dc37483a704e14dd078cb9ac4, 02fa2389b1b64b661192e224bed8af6df0ce80f6, 3b230f14c46e7e177e9bebb2ebc9f46b346b646d, a830083704284c8c5ddaf04f676c6ce23d583942, 9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 08be4e23951a0def1c5d235befbb39c8d8d373a3, 696b388ee6221c6dbcfd647a06883b2bfee773d9, b889b1d6944213bc2ca29e3ad07ee65ede20892d, 8b28792f8405b737229afb92c99c579b86d8aa98, 8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801, cd8a9914d50b0ac63315872530274d158d6aff09, f397b593de771752e7002a954eb531f3ef6a975e, 64c4c6cb66457c79f89ba4f2529b57bfce19ced4, 4b1280229ced73f6c86550f24ef01490fde52285, 980858461df7c4349f17b427686c5bcbcffbdc04, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, b3dbe1b460a3b66df1653508c9eed7dd51dee4d2, 9eacf62f1e546748428c7e4843731b1595294200, da8b317b99c4b8933b2c59285639eca6c3fcb869, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 1d7531db9272f7838e33616075e1e64532fd013a, 834fdec542153aae5fe725df801aac87ba5e8f56, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, 8f99f5cb3950fcd1628c6a563be4b4fc4966fa66, 929305892d4ddae575a0fc23227a8139f7681632, ae38dc77a962161107361f213db9216ee1274037"
dcc8d6a87c69ca44cb6636d343347ddd6c8c3860a,DPI: The Data Processing Interface for Modern Networks,https://www.semanticscholar.org/paper/dcc8d6a87c69ca44cb6636d343347ddd6c8c3860,Conference,"As data processing evolves towards large scale, distributed platforms, the network will necessarily play a substantial role in achieving efficiency and performance. Increasingly, switches, network cards, and protocols are becoming more flexible while programmability at all levels (aka, software defined networks) opens up many possibilities to tailor the network to data processing applications and to push processing down to the network elements. 
 
In this paper, we propose DPI, an interface providing a set of simple yet powerful abstractions flexible enough to exploit features of modern networks (e.g., RDMA or in-network processing) suitable for data processing. Mirroring the concept behind the Message Passing Interface (MPI) used extensively in high-performance computing, DPI is an interface definition rather than an implementation so as to be able to bridge different networking technologies and to evolve with them. In the paper we motivate and discuss key primitives of the interface and present a number of use cases that show the potential of DPI for data-intensive applications, such as analytic engines and distributed database systems.",2018,31,"144641551, 2691974, 2754078, 145527641, 66030040, 3087426, 1413903838, 48469973, 2108724347, 46533048",144641551,Skopje,3,"8847603, 2260830380, 1453724884",Y,"The partitioning of each task must currently be designed by hand.;The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse.;In the experiment, the proposed method shows no or slight degradation of accuracy with big savings in communication cost.",Conference on Innovative Data Systems Research,18,"processing,network,data,data processing,data processing","b85f3a66245d483f3eb3447eaf9950bd55f2b21e, b6593b5ad6b0e00232ee88cf2bc578645c1299a0, 53103ae318a19569ac82cee5062de2cf73bf386c, 4a7eea3ec3080ecb277bfe466afce4822a1071d7, a85c45ce7c893388e8eafa8a653b042e1497db48, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, 811df72e210e20de99719539505da54762a11c6d, cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5, 1d7531db9272f7838e33616075e1e64532fd013a, 0427110f0e79f41e69a8eb00a3ec8868bac26a4f, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, ca05ca5f70616456d0f2e05c4a51cbf8b8d273f7, 89858723bec341178f2b00d34ea3016baaaf71a6, 31a61d009442436d04b9d4e1c5beee37172289ae, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, d78e61d0fe29b823f9630ccfa647c3029ec21f2e, 11342d45911ee8a7c9e3a94117ce774ad7036172, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, 2a56bd89fd1a9457f0705142540ffc4396fad4f7, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, 0a829289a16ae48837cc2905635435db98bacc76, 2bc9d6830610e18f110aa7984f7c0271d6b17eeb, 57a3fc6d0aaad3c10c793b4e59390ca04c935282, 139a0c7a60667979dcb57eae677f75ff3f0b0196, a9640bac0b45a804d07fc5914feb08af8f2a73f2, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, f0eb17c6def9aaf95ef3c8ce12b9c3ceb9030274, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, 78df3ba26593620ab689fe5a97b7e739434a053b, 3337e4b2e63e5e99171f9da9d161771f5810c27a, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, 74b4f16c5ac91e3e7c88ae81cc8c91416b71d151, 819167ace2f0caae7745d2f25a803979be5fbfae"
2b061f7f108fdd4e90452aaaead574c7b4b5b780a,"IoT Data Processing in the Fog: Functions, Streams, or Batch Processing?",https://www.semanticscholar.org/paper/2b061f7f108fdd4e90452aaaead574c7b4b5b780,Conference,"When processing IoT data on a large scale, the cloud is no longer sufficient and it has been proposed to move parts of the computation closer to the IoT devices – the so-called fog computing. There are also three basic processing paradigms today that lend themselves to IoT data processing: stream and batch processing as well as serverless functions. Where to place which part of the data processing and which processing paradigm to choose, however, is often unclear. In this paper, we give an overview of all three paradigms as well as different data processing use-cases. We use these to derive a decision framework which provides general guidelines for placement of processing and the respectively suitable paradigm when designing a large-scale IoT data processing architecture.",2018,33,"52198091, 3077067",52198091,Madrid,2,"7557913, 1666260553",Y,"Interestingly, DQN + heuristic reward approaches expert performance while behavioral cloning never achieves expert performance level even though it has actions.;Moreover, the authors proposed updating the parameter \theta of the generator g_\theta.",International Conference on Fog Computing,18,"processing,data,iot,data processing,data processing","05b8b67451fb105576c58af960e6e6d98f9103e7, 6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91, 89858723bec341178f2b00d34ea3016baaaf71a6, 1109d62ebd2b29a7dc148bc30dd6cfc803a63dec, ed935c6b359a7a486c28240d796e84897d095125, 17fca92ffd527c78c5dc6c7953e96671743807fa, e32a2519b59d62cff6cb8136ee242dc3754ed57b, 7d873a9c49d3864709aa762f8740edcdbd7369c5, 5687c9e8da574453fd873662b95caec70dac9d1e, cd5b4b113d5fa90b5dc5aa372111b89d18df88fb, 752604994a7ca548ff2954114fc61a501d857b1c, a8b995f0da78a79447dfb18c2337972b044f4239, 795550a5294eb05ea4f3b14f0b1c21a405493d85, b6becea767675ea6ee43c78ce747077a5050019c, 3087b58cbfc6eb4a3076a180e21d6b872293f9a8, 5031790972d496547b6613d46a4a0134c824db6e, bc00ff34ec7772080c7039b17f7069a2f7df0889, 56266342b01a4f2ddc28a1e8401dbbad105736a5, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, f9253a3e2627f1c2a0a7e2cea320a4ec4e4d2ff9, 5bbe106eeba21bcb4ac7d3ffc128f2ab581ffdfc, 22c141b489e6e189f5996537b0a908fc10f90de7, c5d3fbc024ad9a0d28669b6030de23fc5c3f7888, 557ee73ff4e87ca0cb1b6c78af0730a2d94b710f, 9db0247728950788a2b42097d81dc0e24eed6bb2, 48fc9c42522184c652742255fdf31f7b9ed7ebae, a6bba5ce9867c978210e3d056691b5c1e769b760, bad4c08f03587e38ee960e2aa76e16d722826e7c, ef35da3a3c471a6a15c7a3b09586483eb50cbef0, 31a61d009442436d04b9d4e1c5beee37172289ae, 9a60d3c9d3a5b7f165325fca45bd418d651682d3, ce54e3b89a2570035b70885e6901ad4c92ae41c9, e5d720767b7a539bb2edaa98eaf572a4506a79c6, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, f7b69b8babfa607f2e8b0371f24cb720a7a827d6, 92afbbe41174a545f9da9992e33c9a9592e529aa, 28cc044d5ba938472bc53d87240583982ad21663, 14014c024674991149f3ecf9314c93f7e029ef1a, 736ef8a32d6c5f76a21d61299300cf796480d507, 74bc39003e65119eaa6ba339a61b45b417a638b7, ee6f23590783adec7cf6b2030c6a46f3117a708e, b00d4452cd9869526169b7a2fe893c0a7c31eb4c, 529ff7d6441d244212cf2becafd12a7e67ac56d9"
2f44ab0e52eb98fdfc0086638887f175b6f2fe4ba,Benchmarking Distributed Stream Data Processing Systems,https://www.semanticscholar.org/paper/2f44ab0e52eb98fdfc0086638887f175b6f2fe4b,Conference,"The need for scalable and efficient stream analysis has led to the development of many open-source streaming data processing systems (SDPSs) with highly diverging capabilities and performance characteristics. While first initiatives try to compare the systems for simple workloads, there is a clear gap of detailed analyses of the systems' performance characteristics. In this paper, we propose a framework for benchmarking distributed stream processing engines. We use our suite to evaluate the performance of three widely used SDPSs in detail, namely Apache Storm, Apache Spark, and Apache Flink. Our evaluation focuses in particular on measuring the throughput and latency of windowed operations, which are the basic type of operations in stream analytics. For this benchmark, we design workloads based on real-life, industrial use-cases inspired by the online gaming industry. The contribution of our work is threefold. First, we give a definition of latency and throughput for stateful operators. Second, we carefully separate the system under test and driver, in order to correctly represent the open world model of typical stream processing deployments and can, therefore, measure system performance under realistic conditions. Third, we build the first benchmarking framework to define and test the sustainable performance of streaming systems. Our detailed evaluation highlights the individual characteristics and use-cases of each system.",2017,145,"3317889, 1731210, 1680579, 7482477, 2868254, 1733290",3317889,Sarajevo,3,"2082426870, 1967677, 2772470",Y,The authors indicate that they do not need to compare to variational methods because Gal and Ghahramani 2015 compare already to those methods.;This section could be improved by demonstrating the approach on more datasets.;There are many multi-agent techniques that can be applied to the problem that would have served as a better baseline.,IEEE International Conference on Data Engineering,33,"performance,stream,systems,data processing,data processing","dbabab9bf5955558f73a37644f4bb626106a6d73, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, a81ba6a07bf7a2ecff871e3362a77404501d0927, 3789eb72c32ecf5e33442570358dd786dd67c8a2, d3855b7351c11145e51301e6b686f748ca35c802, 03532123ccffae8d411264320e8a5ae2b6eddea0, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, facd5f5deb152229ceb1803434d8690a09ab4129, ac67d5f9c89d8d72fbd074f94079608220348f3f, 11cf88dce827bd67cbfa60400306318022e736d5, 91e611c3e8705002438fb4439733e47ddec85b5d, 5030702fea15d66a73fc997325431f1d7945ad9a, 8dd0c1e955c66092ff951941a151336211e6e171, 9f71d4bd511a4797c4f0c0122350d3381adc8a2e, 9c1265fd47df8d825f0ddd6d9ab834002592e38e, c15f30a3e84910a28cc560e7db097fd99339e8c1, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, 72243bfa618fd6c3c01bb7b4885a4a456b2a6071, 2478f43de2f1fcd9301a81e9b42d86b1d46db02d, 3994334c81478a4b17341eb1f494dbccbb73d999, 9e195234688778b2beb3528632e78dbabf816332, c0aec04ee86c0724d61c976f19590fbe9c615723, ed9e7821b3e51c7e59183300d6c8cf90c8de0f26, 8894d431a768a35dc7ca4d762ebdba4f407b978c, d280cf82a6144b3e168840802b1a8a14d4eaccb9, 11be2469ab1d1c508e7b6e14148990741ba87884, b05306f0b142e5afb3974b1b79996e5b82653662, 74bc39003e65119eaa6ba339a61b45b417a638b7, 0fa554d981809c5eb78956c779f75092c4f6c16b, 818de553ecd306735971aba04bbfc29d17457084"
1b3675fc0f2b16743b1e1f0c2f84829cfdb3d34fa,DeepSense: A Unified Deep Learning Framework for Time-Series Mobile Sensing Data Processing,https://www.semanticscholar.org/paper/1b3675fc0f2b16743b1e1f0c2f84829cfdb3d34f,Conference,"Mobile sensing and computing applications usually require time-series inputs from sensors, such as accelerometers, gyroscopes, and magnetometers. Some applications, such as tracking, can use sensed acceleration and rate of rotation to calculate displacement based on physical system models. Other applications, such as activity recognition, extract manually designed features from sensor inputs for classification. Such applications face two challenges. On one hand, on-device sensor measurements are noisy. For many mobile applications, it is hard to find a distribution that exactly describes the noise in practice. Unfortunately, calculating target quantities based on physical system and noise models is only as accurate as the noise assumptions. Similarly, in classification applications, although manually designed features have proven to be effective, it is not always straightforward to find the most robust features to accommodate diverse sensor noise patterns and heterogeneous user behaviors. To this end, we propose DeepSense, a deep learning framework that directly addresses the aforementioned noise and feature customization challenges in a unified manner. DeepSense integrates convolutional and recurrent neural networks to exploit local interactions among similar mobile sensors, merge local interactions of different sensory modalities into global interactions, and extract temporal relationships to model signal dynamics. DeepSense thus provides a general signal estimation and classification framework that accommodates a wide range of applications. We demonstrate the effectiveness of DeepSense using three representative and challenging tasks: car tracking with motion sensors, heterogeneous human activity recognition, and user identification with biometric motion analysis. DeepSense significantly outperforms the state-of-the-art methods for all three tasks. In addition, we show that DeepSense is feasible to implement on smartphones and embedded devices thanks to its moderate energy consumption and low latency.",2015,529,"2325031, 3225210, 2580112, 2085709, 1730531",2325031,Dublin,2,"143695559, 1402912902",Y,"I found it would have also been interesting and helpful to define and show a new metric that incorporates both accuracy and compression rate into a single metric, e.g., how much accuracy is lost (or gained) per compression rate relatively to the baseline of no compression.;A full implementation of binary CNN with code This paper builds on Binary-NET [Hubara et al. 2016] and expands it to CNN architectures.",The Web Conference,25,"applications,deepsense,noise,data processing,data processing","10d89b13a6309a531c35701d37d3bd76a27a3942, 34ca47eed139a7f0694611528f75debc43385518, a357f1ff27e184d9a5ef69e665e8ca891032bf71, 256db9dba1978f004a67c86ffc321563b1aee79a, 7c217cc7524251f42887438834912e06129c3299, 695bdc6e24608364491b9418a220c65a7cd17413, 155eeede0c070f1f017ba5c9f6cacd7ae0b098aa, e2a58fd18961c3941102989e3a3d0d27c615e015, 5bbe106eeba21bcb4ac7d3ffc128f2ab581ffdfc, 49fce234ad7f6d2af757f078b29c0118068075a3, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, 925af6dcb0f8e6f3a5b2613400277be4b5434d10, eda6756ab2844c390584686dc5e6385f4a8369cd, f4cfc7cbad257f1688772d59f694c16189dba811, bad4c08f03587e38ee960e2aa76e16d722826e7c, aca6d5f3866372a4506cf15773ae298f18c3f453, e1e43d6bdb1419e08af833cf4899a460f70da26c, af13a92977d4f4dc5b28b13746d86111d42939e8, 16753e0317730e8c1b297338300a8c6163dd06f2, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, fd0496ab020acf366375615ab40235e6dd3c5897, c84aa52bee5116f80c7740503edff4b08f733c3b, 590b617c08d34bc6caed7e4490c0b22a9c516e86, 98ce7af921e7c52d81df64d632d34eb09522cd75, 84725855d10b531eb8cbe54935dda0440c2fc750, 643da4c4de1954daeac571a82367241db012a8bf, 1d5adacc5d4d226e76c35bf19018f9e76759f127, 46c266b3d1274dacd7fce27ee8cb4d587f087a58, 23466d271676ae467cbe85bb1993682f3502e840, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, da3f33d858586d24cb265e79eb54f3746e998f57, 82870bc488b57cdf5ea62877109a7278af2926b3, 676664ee7471738577f641e6159e7596625b7fdb, b9b639522465cc606df878eee62e7f9c4bf19e62, b473e91cbe80c8b46451b49153cd5f93030480ab, 58ed1fbaabe027345f7bb3a6312d41c5aac63e22, d983427574a41bf485e31ab983acc1c742e9e24c, d1a6b3a5efde3783b53f822dc8dd00aaac934b95, 1bb022b27ddb987352bfc002c8381a6f646d0ebb, 2743e66939b30c43affb3c9e31f20cfac2109045, dec26f0640e3a4fdb116735526302ccb9f49867e, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, be2b0396de9431bae931642516a1d3e4906329f5, c6e242fce9e5e4ba2c4dc869cfb0afabf2020852, b889b1d6944213bc2ca29e3ad07ee65ede20892d, ce51eff5b529ee572dab1c1f38f20adc8e89bab2, 1ca6d6682204f0214338f7797bea056444e908bd, 87048df918c34b662bc0d28894efa430d70a9206, cf41991d89301c3c12420d150792cb1163999962, 6be56f559a74c0124526242e70cbdfd16cbc60a7"
639bfab64e2f35917d450013e136cb24c7755fada,Portable and Error-Free DNA-Based Data Storage,https://www.semanticscholar.org/paper/639bfab64e2f35917d450013e136cb24c7755fad,Conference,"The paper involves critical evaluation of all the significant encryption, decryption, and cryptography techniques that are being used for DNA (deoxyribonucleic acid) data Storage. This paper covers the basics of Data storage in DNA and how it can be used to stockpile data and how it is highly promising in changing the data storage methods of the world in the foreseeable future. All the vital methods which are being used for DNA data storage have been discussed. These methods are also be applicable for data storage density and graphs are plotted. This paper also examines how DNA is being used as a tool for cryptography along with the fundamental limitations of DNA storage. Towards the end, the future scope of DNA data storage is questioned, and with the help of a density graph the research predicts whether ""DNA data storage has a future scope or not""ƒ",2020,16,"2144633223, 2146970349, 2118511088",2144633223,Ljubljana,2,"150127950, 1736651",Y,"A number of different fine-tuning regimes are explored.;[1] Wang, Weiran, Honglak Lee, and Karen Livescu.","2021 4th International Conference on Recent Developments in Control, Automation & Power Engineering (RDCAPE)",3,"data,storage,dna,data storage,data storage","256e95ca331cbd35b3a23cc306b6627e6771a963, bc44c0c64a473e035b11ae60a1993ad3db1acd2e, a18f02e5c24e1f924aea268dd343bbdea234f2bb, e5194ae88d63c7549678b1b73cfdaf7112164272, 0ecf3f089e6dc7944b440227069b9d0143e18d78, 22cba8f244258e0bba7ff4bb70c4e5b5ac3e2382, 8ab93bee04cd5bdbd96002b2e325d02f61ba695a, 8760bc7631c0cb04e7138254e9fd6451b7def8ca, 25761ba4bdc054bfe902fe7c5d6338be6d00d491, 2ed691a353fa48403d493ab658f5f267a42f0bf1, 54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f, b9c974380649749320f4a02e33b2e5014e7f1756, b52db9e41e15f76bdcfbe674abe0314af545c430, bb826d9ccd116076a267dfcb048cdd747c11b255, 709f7a6b870cb07a4eab553adf6345b244913913, 390bcf15a1b13cb0d5966859c35c69a31238e838, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 285d13bf3cbe6a8a0f164f584d84f8b74067271f, 590b617c08d34bc6caed7e4490c0b22a9c516e86, c9cc6a3c50b32e801f20b4f0ed5daf3e9550f9c1, 82870bc488b57cdf5ea62877109a7278af2926b3"
c84389369720dcd2f004c48e58fbac2c45c8f092a,XIndex: a scalable learned index for multicore data storage,https://www.semanticscholar.org/paper/c84389369720dcd2f004c48e58fbac2c45c8f092,Conference,"We present XIndex, a concurrent ordered index designed for fast queries. Similar to a recent proposal of the learned index, XIndex uses learned models to optimize index efficiency. Comparing with the learned index, XIndex is able to effectively handle concurrent writes without affecting the query performance by leveraging fine-grained synchronization and a new compaction scheme, Two-Phase Compaction. Furthermore, XIndex adapts its structure according to run-time workload characteristics to support dynamic workload. We demonstrate the advantages of XIndex with both YCSB and TPC-C (KV), a TPC-C variant for key-value stores. XIndex achieves up to 3.2X and 4.4X performance improvement comparing with Masstree and Wormhole, respectively, on a 24-core machine, and it is open-sourced1.",2019,62,"41211459, 2115725948, 1508399092, 1508456598, 8491577, 2136369359, 2118438836",41211459,Copenhagen,2,"2551387, 2108451006",Y,"- The claim of Theorem 2 in appendix B does not follow from its proof: what is proven is that the value of S(w) lies in an interval [1-e..1+e] with a certain probability for all w.;Only simple baselines (eg autoencoder, kmeans) implemented by this paper are included.",ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming,24,"xindex,index,performance,data storage,data storage","d617f51833860dc50d202af7f80be71304b2e994, b0b770fb8c7760749c88e3c83ae173cdb07f7bd5, 4b06c7e29280b1c6bc05c9df39023b48fef02c93, 9817bf0f78047452761e950c02a1a56f59a1e593, 087dd95e13efd47aef2a6582e6801b39fc0f83d8, 11be2469ab1d1c508e7b6e14148990741ba87884, 2afa490dde7a8c582d889530c7f8b042fef6a8b7, 1051abf1e3dae90241ad15b3f98f2e41197ee611, db4cf9f6a653d5c15973e836c800ea47743251ae, 9619cde5c79d91ca5c432186668618312175f8dd, b0c34618ffd1154f35863e2ce7250ac6b6f2c424, 32a849fe3020144e5ba82ba0442ac571f554ca31, 218062f45c15f39bc8f4fb2c930ddf20b5809b11, e70ddfb04969b663ef8cd711a70a0acf563d6e5f, ecef432e7f6c9f431d5b34706a8de1fdebec46f9, 10a0541be17d10d922ffc68a3dae55a13d9c1ab9, 2141334fad7248fc707607bc9453d44686ae07a7, fa7b8acd47631bada5b66049824bfd335ac6bf8f, 9312e5efa0dcef1445d45a41771f12e2a8dc6715, 0095acc4f2c3255cf38fdf844003c97858adb418, aa6c2afadd660fe4efbac699f7854e8f6f240c38, d21703674ae562bae4a849a75847cdd9ead417df, 1562390dd212516cd857009cbd4f857a902d1f3d, 4e13a8e8ba8d33e15ed037bfca7c651047533990, 35adeef964fd344288febc7def2780007587724f, b135e330cc1473c8c24fa63bb9a5b64f51993f9e, b889b1d6944213bc2ca29e3ad07ee65ede20892d, 7904b3446775ed8c79f4f94001a16b706989c462, e8b7a9be9f2d0578a95319ed5841978e10429967, bd6c027a3604d6c8fa23435bf382455b2bee436b, 10aa2be24951e6de76b630482a645d79354c4cde, f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6, 4a7eea3ec3080ecb277bfe466afce4822a1071d7, 1bb022b27ddb987352bfc002c8381a6f646d0ebb"
29279e52008848ee494f5af1b836313ab99c25eda,Efficient Decentralized Data Storage Based on Public Blockchain and IPFS,https://www.semanticscholar.org/paper/29279e52008848ee494f5af1b836313ab99c25ed,Conference,"Blockchain technology has enabled the keeping of a decentralized, tamper-proof, immutable, and ordered ledger of transactional events. Efforts to leverage such a ledger may be challenging when data storage requirements exceed most blockchain protocols’ current capacities. Storing large amounts of decentralized data while maintaining system efficiency is the challenge that we target. This paper proposes using the IPFS distributed hash table (DHT) technology to store information immutably and in a decentralized manner to mitigate the high cost of storage. A storage system involving blockchain and other storage systems in concert should be based on immutable data and allow removal of data from malicious users in the DHT. Efficiency is improved by decreasing the overall processing time in the blockchain with the help of DHT technology and introducing an agreement service that communicate with the blockchain via a RESTful API. We demonstrate the applicability of the proposed method and conclude that the combination of IPFS and blockchain provides efficient cryptographic storage, immutable history and overall better efficiency in a decentralized manner.",2019,14,"36877027, 145667775, 2509535",36877027,Vaduz,3,"3410500, 36347083, 1752784087",Y,"The paper frequently refers to ""embedding"" ""imaginary trajectories"" into the dynamics model, and I still have no idea what this is actually referring to (the definition at the start of section 4 is completely opaque to me).;One could also argue that the manuscript would profit from a better theoretical analysis of the IRL problem, say: C. A. Rothkopf, C. Dimitrakakis.;The encoder network from NEM can be seen as a recurrent network which takes one latent variable theta_k at time t and some input x to produce the next latent variable theta_k at time t+1.",2020 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE),19,"storage,data,blockchain,data storage,data storage","39602922b04885047254444fd1a1586d797617ce, 8ab93bee04cd5bdbd96002b2e325d02f61ba695a, a781cea542e99c6bb9422858e7c04eaef18c7673, f156ecbbb9243522275490d698c6825f4d2e01af, db6ad6ded1cfa26fdc7437f27fb823ec533e96fe, f381c53aeb7742e4047d06d84f9e0c4f523231a3, 3813b88a4ec3c63919df47e9694b577f4691f7e5, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, 8799ec4bdf98bc313aa8c41a706a385e026d1f88, 48b71e096febdf1e3e07bea80c78dfcb421fff2c, 34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61, d235a9085e0543fcbe502fbc269f9a8ee01dcbab, b795c74a0150ec091003ffbaa5bd7d74487c137b, 5406e153957dd7a165264da6e6e5d81251997404, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 5687c9e8da574453fd873662b95caec70dac9d1e, 4fcfe83c05402b5c5fb6e853082e74af6379d7f9, 5f51d468ce730eeade7e9f419a1fe7152582be25, bb826d9ccd116076a267dfcb048cdd747c11b255, b977e8de38dc0d13817bca1ed20036badfe2a58c, 1661d0d8d47cac41e01c59c60aac3675b4396698, 4e13a8e8ba8d33e15ed037bfca7c651047533990, 3bc9bb1f2218dcbd15c3b7cdfcb43077a3f30779, 9181b0d801dfcd7723a3ede201f0543078e2c149, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 9e195234688778b2beb3528632e78dbabf816332, d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, 6dd481d5eb8d76d4b61a1829c7687d008e0937ab, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, 6c755fc901d0b41a5d73c265f64a5aacf62e83b8, 256ccb81d8c3bb63f4299195584b8e9f9d60187a, 2bc3644ce4de7fce5812c1455e056649a47c1bbf, 96b51d940653710f9d099d89ade86b44fa9bdd8a, a2a514ed839dafdd0fb76d6c2615f25f35bf8087, 86dbd884043eb5807c61d2c65b813e673b4a04fa, 811df72e210e20de99719539505da54762a11c6d, 88932b1563316e4ed7a61c4c2c0c90d7aa9bb308, dcbaf58b16ac7ef947879ea37c021466357b291a, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 4875aa46599dc2d7e8292fc563347cf78fa12c8d, c9f320789e98d2c7a798a9705e26dbe317677966, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, f12930cd5f58990badc1a7c5d2749cad004cfb0e, 8674494bd7a076286b905912d26d47f7501c4046, 343500e0052eb1b683f32b00efbbd1331c94184a, d8a5474f450330ad25c1e22f27e88f3630cb840d, 2ddcd47b28eb4b43317a34cf56e83309f5347699"
492c389d560d9db39c758d07e635408d2e0eaf7da,COMPARATIVE STUDY OF NOSQL DATABASES FOR BIG DATA STORAGE,https://www.semanticscholar.org/paper/492c389d560d9db39c758d07e635408d2e0eaf7d,Conference,"Nowadays the amount of data that is required to be stored in databases relates to big data and that data is required to be stored in real time, but knowing that DBMSs cannot support such a thing, we use the NoSQL databases. NoSQL databases support dynamic scheme design, offering the potential for increased flexibility, scalability and adaptability over relational software. This makes them suitable for Web Applications, content management systems, and other uses that involve large amounts of non-uniform data that require more frequent updates and formats in different domains. There are already a large number of NoSQL databases, with a number of over 255. Their performance depends on various factors, so it is important to compare them based on the requirements of an application. So, during this research there will be made a comparison of some of the most popular NoSQL databases for big data storage.",2019,14,"14833620, 2285582737, 20802526, 40992700",14833620,Brussels,2,"41037252, 2061706386",Y,"“We can quantitatively determine how close the posterior is to a FFG distribution by comparing the Optimal FFG bound and the Optimal Flow bound.”: Why not just compare the optimal with the AIS evaluation?;simple, effective method, some discussion/understanding missing This paper proposes a new method of detecting in vs. out of distribution samples.","14th International Conference on Computer Graphics, Visualization, Computer Vision and Image Processing, 5th International Conference on Big Data Analytics, Data Mining and Computational Intelligence and 9th International Conference on Theory and Practice in Modern Computing",13,"data,nosql,number,data storage,data storage","2afa490dde7a8c582d889530c7f8b042fef6a8b7, 1051abf1e3dae90241ad15b3f98f2e41197ee611, a5710a7a54b50c26f3c40c37a19b2ec63330e90b, a926093ef103c02fd5ef7310e0d41b83d1958ed5, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, c292e473b3825eeb9db03c70b2e1c033aea190d5, b2f8df88de24fe0ef74bda0eec1a0c3e7329b9f9, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, 5c39e37022661f81f79e481240ed9b175dec6513, 818de553ecd306735971aba04bbfc29d17457084, 8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, c50a909e20bd07f4aea09dc6dae539b45b406a96, c84aa52bee5116f80c7740503edff4b08f733c3b, 780c725848aac1118d00c8bb306719ec803369cd, 8ec5896b4490c6e127d1718ffc36a3439d84cb81, 14dd50979af27bd2574c8068db11d27028b56afd, f04d8e558c11ac0fc9318c4f3d61b651c62ddcfa, f9c602cc436a9ea2f9e7db48c77d924e09ce3c32, cf41991d89301c3c12420d150792cb1163999962, b61b260de1599e6e89491cad9160898fcd3b34c2, fa75a55760e6ea49b39b83cb85c99a22e1088254, 1986318d8a565fbff8fde545b8d0c2012c6462d8, b58e98029d53d69ccc7089dca7b01bf050b5ad2b, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, 557ee73ff4e87ca0cb1b6c78af0730a2d94b710f, d1a6b3a5efde3783b53f822dc8dd00aaac934b95, 224c11bc51b4959bc787d6681c2b152468294b11, b795c74a0150ec091003ffbaa5bd7d74487c137b, a1d36749b89e46a8eaadf8ba40788741c192fb1e, e8b7a9be9f2d0578a95319ed5841978e10429967, be7cb8f79bc018e57467168fc0c7f8ad59bba04f, 1597449a7f64b6bd24639b4deab96c8a8c184177, 8713452753fd01de5616121af93e173d4f74eaf6, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 57e6cca1479a4642f867e69b4dee93d14259dc3d, ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba, 8d942a3b52e2ad16ff8e5970be59591970d89fae, 74b4f16c5ac91e3e7c88ae81cc8c91416b71d151, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 7fa273f450251523e6b7fcc2eb3fdbdfd4a30493, 634fe61f3ab6692ea4733989a1c76e793bb8b69e"
cb12fe714dd4fa1d14b6a8023d494c14c89e90eaa,ElfStore: A Resilient Data Storage Service for Federated Edge and Fog Resources,https://www.semanticscholar.org/paper/cb12fe714dd4fa1d14b6a8023d494c14c89e90ea,Conference,"Edge and fog computing have grown popular as IoT deployments become wide-spread. While application composition and scheduling on such resources are being explored, there exists a gap in a distributed data storage service on the edge and fog layer, instead depending solely on the cloud for data persistence. Such a service should reliably store and manage data on fog and edge devices, even in the presence of failures, and offer transparent discovery and access to data for use by edge computing applications. Here, we present ElfStore, a first-of-its-kind edge-local federated store for streams of data blocks. It uses reliable fog devices as a super-peer overlay to monitor the edge resources, offers federated metadata indexing using Bloom filters, locates data within 2-hops, and maintains approximate global statistics about the reliability and storage capacity of edges. Edges host the actual data blocks, and we use a unique differential replication scheme to select edges on which to replicate blocks, to guarantee a minimum reliability and to balance storage utilization. Our experiments on two IoT virtual deployments with 20 and 272 devices show that ElfStore has low overheads, is bound only by the network bandwidth, has scalable performance, and offers tunable resilience.",2018,23,"48439542, 2229703159, 1761220",48439542,Belgrade,2,"2112455515, 48576745",Y,My concern is that one-bit system is already complicated to implement.;* Figure 2 seems like a test made to work for this method and does not add much to the paper.,2019 IEEE International Conference on Web Services (ICWS),18,"data,edge,fog,data storage,data storage","aea731e7cf33aa3d482b13f42cedbc1adb3271c6, 33ec7eb2168e37e3007d1059aa96b9a63254b4da, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, 8babcaf89f8537dc628a029ebf932100f57289fd, 0fdfcd2308a1fdece7e26b6ee10ee1ca2fa51ee6, 01bf0e83159712fbbbd12171a7e268547a4cfbc5, 695bdc6e24608364491b9418a220c65a7cd17413, bc44c0c64a473e035b11ae60a1993ad3db1acd2e, 780c7ead33428d282044519fee5e773ad56d5a2c, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, 0c4070272fb7d6b98971a107b022ff8abf0aa55e, 83cebf919635504786fc220d569284842b0f0a09, 22cba8f244258e0bba7ff4bb70c4e5b5ac3e2382, 8fd8cb4951ae9634a378383a880fbf16ac5d6926, 9675f7484a4d3d278a5a637f75a1b81d7d008c4e, 71854ff4306cf65c3c2161f7be2d0346275f72d5, 0d065e8688c38bb0148203a1738f47184a5b58d3, 42375e9a309fe5bf5c671c918cd8c9d5d85568cc, 07cca761749bfe21c2d096ff60f32b574d5c84c4, 44e1dd74f0446ec91221189ad3a65edb1a0208fe, 7e9905710a5991a017ffc0473dd612cfce4b7b2e, 73a6e1234a3a38bef93f9097d59f01d5032cbc92, 0090023afc66cd2741568599057f4e82b566137c, 5b6ec746d309b165f9f9def873a2375b6fb40f3d, 712e32e2da67428ba6c6add1605410e1c3792883, 42543dc42e65609bbbf2be470d54dd923532c36a, 1597449a7f64b6bd24639b4deab96c8a8c184177, e30d9b8ce108d982169621b88a5e3fb69fec70e1, eb9e0da8b7170e3ca4364f2f9010599c2d2556f1, 8894d431a768a35dc7ca4d762ebdba4f407b978c, 7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2, 0c00a328fa7cd56ee60338c54e89bd48310db80b, 41c93960a066876d5e4f1dacaef75cd8daa2791f, 32a849fe3020144e5ba82ba0442ac571f554ca31, b55e490637babd50dab3cdaaa3a60a2be6eb1cbb, 16fa12ebc578df676f3dda5453ad56c15a0d6702, 155f27879f185f1ab04107c91c2ae7cf6a910a03, b52db9e41e15f76bdcfbe674abe0314af545c430, e968ae8e98fff9e28468383a1826fca4a2ae5245, dcc8d6a87c69ca44cb6636d343347ddd6c8c3860, 40416ac3bf78583eea37661b1b446e9939245b3e, f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6, 31a61d009442436d04b9d4e1c5beee37172289ae, 74bc39003e65119eaa6ba339a61b45b417a638b7, 32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b, b9c974380649749320f4a02e33b2e5014e7f1756, 2b061f7f108fdd4e90452aaaead574c7b4b5b780, 20d92dc48ab6a52c087ae37eb394acb8f65f28eb, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, c9cc6a3c50b32e801f20b4f0ed5daf3e9550f9c1"
90e4330fed2da147dd41490e8ad638b618112b3da,Fast Erasure Coding for Data Storage,https://www.semanticscholar.org/paper/90e4330fed2da147dd41490e8ad638b618112b3d,Conference,"Various techniques have been proposed in the literature to improve erasure code computation efficiency, including optimizing bitmatrix design and computation schedule, common XOR (exclusive-OR) operation reduction, caching management techniques, and vectorization techniques. These techniques were largely proposed individually, and, in this work, we seek to use them jointly. To accomplish this task, these techniques need to be thoroughly evaluated individually and their relation better understood. Building on extensive testing, we develop methods to systematically optimize the computation chain together with the underlying bitmatrix. This led to a simple design approach of optimizing the bitmatrix by minimizing a weighted computation cost function, and also a straightforward coding procedure—follow a computation schedule produced from the optimized bitmatrix to apply XOR-level vectorization. This procedure provides better performances than most existing techniques (e.g., those used in ISA-L and Jerasure libraries), and sometimes can even compete against well-known but less general codes such as EVENODD, RDP, and STAR codes. One particularly important observation is that vectorizing the XOR operations is a better choice than directly vectorizing finite field operations, not only because of the flexibility in choosing finite field size and the better encoding throughput, but also its minimal migration efforts onto newer CPUs.",2019,28,"1500391633, 143804003",1500391633,Podgorica,2,"2039003, 2158141874",Y,I feel that the authors should give a more prominent disclaimer to potential users of the test.;Flexible muscle-based locomotion for bipedal creatures.,USENIX Conference on File and Storage Technologies,19,"techniques,computation,bitmatrix,data storage,data storage","598231eb906b183f7a2a408ef4536127e11e3de9, 0e141942fa265142f41a2a26eb17b6005d3af29e, 0574a3abc98f0e6bd035a4ff4ea107cfba45d3d4, ae026f29c2d571871f426ff4873d43b4ff90b9ad, 4be9368abc2474d6fd38639e523cf03af1873fd9, f0eb17c6def9aaf95ef3c8ce12b9c3ceb9030274, 88e0bee9e7165c1d156dccb965060e52ffc8e1c0, 10a69070c8585dc5564b11b0e53dbe4d565f9ce1, 1452b25a7680bbb2c66dd7dfca6704292405da92, 2ee03e28208a9310a9be4032c2b04ebdddb83cc7, d6f002d88638de71114dab083f0ea8ceea6b6a5a, 49fce234ad7f6d2af757f078b29c0118068075a3, 17fc5c9b37ba63fb3280ddf9909a183523e7bcc3, 5b6ec746d309b165f9f9def873a2375b6fb40f3d, 09f54c64b39f5f7e7570f9f4ce3e3af544401e14, 1cd497e82bdc46a9d3d28b2316ea7cbe9aee5467, d88083e37c44461ce3e404bd57257cd3edb07d4e, e02f91d625cd32290d4ede0f31284da115844316, ecef432e7f6c9f431d5b34706a8de1fdebec46f9, 96023195e889fc258e6ff30aa99d250982dfae01, 8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba, 1cff064f815111a71a98afda7aee1867ad617901, e3b37c1c955b2b10809040ce277edae5333b99c3, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, 01f0f5205d03870f172ae8f04e33356d5a0af221, ae38dc77a962161107361f213db9216ee1274037, b415e836d447ea9efb7629a1de67cd2a6f9e7ba8, a85c45ce7c893388e8eafa8a653b042e1497db48, d93bcf0685c15c45d078eafea565969c04daccd3, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 00d1f3423a33f73ca6aee884a58834547475d2f0, 9181b0d801dfcd7723a3ede201f0543078e2c149, aa6c2afadd660fe4efbac699f7854e8f6f240c38, 71a85e735a3686bef8cce3725ae5ba82e2cabb1b, 3a74bed911ccf213d9595b2b02a5b1c4ac4dcaf8, f406aceba4f29cc7cfbe7edb2f52f01374486589, 21042565ec941f4fa31ac5a0af85a1a84ff21f1b, 0090023afc66cd2741568599057f4e82b566137c, 6dd3e404aac097f63d42dc19fd08c7ec281f90b4, 4e58100b319d74f97ed550a4e5fa32dea8c06fe1, 64c4c6cb66457c79f89ba4f2529b57bfce19ced4, 819167ace2f0caae7745d2f25a803979be5fbfae, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, 218062f45c15f39bc8f4fb2c930ddf20b5809b11, 33ce8103b129149eb78ca2fa48538e25c9242c08, 223187cf10a24b62b9b0cf5b146cc83526df2ea5, cd20ed8e70f6459f5617350f6b84c5c6d5929cb7, 7872f34e2a164c5cf3c34a7a7433dc3342b6c7ea, 771a858c35f6d6e6d1017dde95368de3794738a6, 2af8907d4a974ae41044581f5e5d67317cb08568"
695bdc6e24608364491b9418a220c65a7cd17413a,Semantic Data Querying over NoSQL Databases with Apache Spark,https://www.semanticscholar.org/paper/695bdc6e24608364491b9418a220c65a7cd17413,Conference,"The rapid growth of semantic data in the form of RDF triples demands a scalable distributed storage and efficient query processing engine for its management and reuse. To overcome the limitation of native RDF stores and traditional relational database management systems and scale adequately with the exponential increase in the size of RDF datasets, Big Data processing infrastructure like Hadoop with MapReduce have been used. NoSQL databases such as HBase and Cassandra for storing large-scale RDF data and in-memory data processing to execute SPARQL query as SQL query using Apache Spark is proposed in this paper. This paper presents techniques for distributed RDF data storage and querying schemes for HBase and Cassandra clusters. We also present a compiler that translates SPARQL queries into their Spark SQL equivalent for execution. An empirical comparison of HBase and Cassandra systems using datasets and queries from Berlin SPARQL Benchmark (BSBM) and SPARQL Performance Benchmark (SP2Bench) on Microsoft Azure cloud is presented.",2017,8,"2042697689, 1720381",2042697689,Stockholm,2,"49573525, 2220547623",Y,"It seems like the proposed method is quite generic, so why not apply it to a stronger baseline?;The experiments are very clearly presented and solidly designed.",IEEE International Conference on Information Reuse and Integration,17,"data,rdf,sparql,data querying,data querying","8d06f13ec042f0d8d845af5deadeb3dd9ad11f70, c468bbde6a22d961829e1970e6ad5795e05418d1, 53b047e503f4c24602f376a774d653f7ed56c024, 156d8e2aa90b5ccc9be10477ca70deaad0151387, f14fc9e399d44463a17cc47a9b339b58f6ef7502, c2a448bb511ebae41a87e69891da8bbf17ddba3d, df7336844a31165db0ae08f1cd0f560c9e3faeea, e23b2e47b0ac6f50000078828f27571804dcd6a2, fc77048474ccd34c6507701591c2e6ab3ca647ef, ea160adc0d78e54669281b8b145bcd832e648fee, 5dfde01d761d97c3a6c609007531973eb1229d09, 8babcaf89f8537dc628a029ebf932100f57289fd, 643da4c4de1954daeac571a82367241db012a8bf, 54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f, 8b7b2e88207fc2dd849f5d83c9b6e890f0174abc, dd2deed2ce6e110236a1280db765fa02c7488eb1, 9e5d389fcdce4c0a6827ebc804b4e863aa28c14c, 102ebe229df18c8733ea1b8def56cd79996e2178, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, 0ae18b28d8dc00cd4641488084ead5df2a449c89, 0574a3abc98f0e6bd035a4ff4ea107cfba45d3d4, 7909428ca7d813331bbe4d33d07ab09e984b41b4"
92912dd895c360f01a6be9c9f6d207642139525ea,An ontology-based approach to improve data querying and organization of Alzheimer’s Disease data,https://www.semanticscholar.org/paper/92912dd895c360f01a6be9c9f6d207642139525e,Conference,"The recent advances in biotechnology and IT have led to an ever-increasing availability of public biomedical data distributed in large databases. Analyzing this huge volume of data is a challenging task because of its complexity, high heterogeneity and its multiple and numerous correlated factors. In the framework of neurodegenerative diseases, the last years have witnessed the creation of specialized databases such as the international projects ADNI (Alzheimer’s Disease Neuroimaging Initiative). The main problems to fully exploit this database are related to the querying, integration, and analysis of data themselves. Here, we aim to develop a detailed ontology for clinical multidimensional datasets from ADNI repository in order to simplify the data access and to obtain new diagnostic knowledge about Alzheimer’s Disease.",2017,5,"2756615, 3341829, 50064714, 6922855, 6339256, 2866596, 2484334, 2794706",2756615,Brussels,3,"2104195713, 2162779709, 39765564",Y,"Just because such a baseline wasn't previously proposed in literature (in the narrow scope of this problem) doesn't mean it's not an obvious baseline to try.;Deterministic latent models seem to work better than stochastic ones.;Moreover, a basic system called delta t block implements one level of full approximation and is stoked several times.",IEEE International Conference on Bioinformatics and Biomedicine,17,"data,adni,alzheimer,data querying,data querying","1c2efb418f79b5d29913e014a1dfd78865221c39, 9b54941de1e21826ecc28b32730ac3f69991ede4, 2a15e9ac5593cd1f8bd3a724e78a173037c02fea, 5ea7bf772fecf95cbf53b2c7f719c9440322a115, 1ed33896e82cc3810b349cdffc2039f0b8edd82e, 5d433da6d0f143f20936379910104d2bb139d4ae, ae026f29c2d571871f426ff4873d43b4ff90b9ad, a357f1ff27e184d9a5ef69e665e8ca891032bf71, b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8, 94deb62af3054c49e7d80bd7eb3ed5efe990fc0b, b2114228411d367cfa6ca091008291f250a2c490, 102ebe229df18c8733ea1b8def56cd79996e2178, d47a6cd76bbd2defc3622e9f4baca6dbe399cfe1, 2c3eef2f17369912e330281d54b535675077e4ca, 215fc60307f741b9db059204e41db8bfb879e606, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, 834fdec542153aae5fe725df801aac87ba5e8f56, b52db9e41e15f76bdcfbe674abe0314af545c430, 182acc7a1c9a63cfdef9f86ee9df43fd75e05060, 4a7477881b66d12e79c704805781d4683a6a6be1, 82663577cf1d08235bb56ad648c9dad36343ccfb, f0aefc9a20ab597ce109ad3a52f1b60eb7f44cd6, 3a7bbc46795929f0eace82b64c44c92a48682fb5, e5d720767b7a539bb2edaa98eaf572a4506a79c6, 3fadb72fb150197aa5eb88a65b244763fb45ab9b, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, 681253389d2cc27103753749f4c7556699d55471, d7b820af40a9e2660ef700d39f7b2e27b43435c5, d86084808994ac54ef4840ae65295f3c0ec4decd, 59c414c9efb77562f5d1aad8af14eaac968c69c0, 38179848e2d6a3ad373b1793848816111428ac36, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, ea160adc0d78e54669281b8b145bcd832e648fee, 590b617c08d34bc6caed7e4490c0b22a9c516e86, e70ddfb04969b663ef8cd711a70a0acf563d6e5f, a60a4e5f7f872b9825ddff5d379857c2091ca52b, 15d1450a8797e2feaa4c0ca4ebcd43c8cbd8b61d, 7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca, 5f51d468ce730eeade7e9f419a1fe7152582be25"
0ecf3f089e6dc7944b440227069b9d0143e18d78a,SpeakQL: Towards Speech-driven Multimodal Querying of Structured Data,https://www.semanticscholar.org/paper/0ecf3f089e6dc7944b440227069b9d0143e18d78,Conference,"Speech-driven querying is becoming popular in new device environments such as smartphones, tablets, and even conversational assistants. However, such querying is largely restricted to natural language. Typed SQL remains the gold standard for sophisticated structured querying although it is painful in many environments, which restricts when and how users consume their data. In this work, we propose to bridge this gap by designing a speech-driven querying system and interface for structured data we call SpeakQL. We support a practically useful subset of regular SQL and allow users to query in any domain with novel touch/speech based human-in-the-loop correction mechanisms. Automatic speech recognition (ASR) introduces myriad forms of errors in transcriptions, presenting us with a technical challenge. We exploit our observations of SQL's properties, its grammar, and the queried database to build a modular architecture. We present the first dataset of spoken SQL queries and a generic approach to generate them for any arbitrary schema. Our experiments show that SpeakQL can automatically correct a large fraction of errors in ASR transcriptions. User studies show that SpeakQL can help users specify SQL queries significantly faster with a speedup of average 2.7x and up to 6.7x compared to typing on a tablet device. SpeakQL also reduces the user effort in specifying queries by a factor of average 10x and up to 60x compared to raw typing effort.",2019,12,"144759563, 47319783, 2119303314, 1796044",144759563,Moscow,2,"2691974, 2273645",Y,The proposed method achieves perfect accuracy in every condition.;What is the per-pixel likelihood obtained on the CIFAR dataset and what is the likelihood on a model where T=1 (for omniglot/cifar)?,SIGMOD Conference,19,"speakql,sql,users,data querying,data querying","4afa7d8e2de43b0b67366b1bce8768f5a246d153, fe4c5074f021cd1c810892c1b6bc267b23aa6e5c, 456c011594ecacdd24298a161787389ccbe4b88b, 665b0c776ff7507c32793f10ce9edf90bc2f674a, 82973c5f56681190a0dbb4c4449ed60d5f805135, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, a5710a7a54b50c26f3c40c37a19b2ec63330e90b, 1562390dd212516cd857009cbd4f857a902d1f3d, e02a757617c2c42eb62889cc4d4aee3765928303, 2e965b5d97c2d6fb4af284307735be39283792ba, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 9e540662619327a3056d9e40bb58058868f6f805, 3bc9bb1f2218dcbd15c3b7cdfcb43077a3f30779, ca011427853d34ce4ec9ccafde8a70c9eacc3e21, b2e791a569f5f1e64354ee6e7a3b9e291b8d9651, 15370f51d666ab8ef17185679553c6a8647b2a15, 6a1b25f7a67395ad1e676027322913acbb0a0635, 740b5dec469e6b54e5e191a577a9b6a6ad8562e3"
8799ec4bdf98bc313aa8c41a706a385e026d1f88a,Enabling Automatic Discovery and Querying of Web APIs at Web Scale using Linked Data Standards,https://www.semanticscholar.org/paper/8799ec4bdf98bc313aa8c41a706a385e026d1f88,Conference,"To help in making sense of the ever-increasing number of data sources available on the Web, in this article we tackle the problem of enabling automatic discovery and querying of data sources at Web scale. To pursue this goal, we suggest to (1) provision rich descriptions of data sources and query services thereof, (2) leverage the power of Web search engines to discover data sources, and (3) rely on simple, well-adopted standards that come with extensive tooling. We apply these principles to the concrete case of SPARQL micro-services that aim at querying Web APIs using SPARQL. The proposed solution leverages SPARQL Service Description, SHACL, DCAT, VoID, Schema.org and Hydra to express a rich functional description that allows a software agent to decide whether a micro-service can help in carrying out a certain task. This description can be dynamically transformed into a Web page embedding rich markup data. This Web page is both a human-friendly documentation and a machine-readable description that makes it possible for humans and machines alike to discover and invoke SPARQL micro-services at Web scale, as if they were just another data source. We report on a prototype implementation that is available on-line for test purposes, and that can be effectively discovered using Google’s Dataset Search engine.",2018,24,"35202970, 1390195074, 1773317, 1753013",35202970,Zagreb,2,"49997612, 2060154",Y,"(This is hinted at by the mention of fully convolutional networks.)  The method could just as easily be applied to learn a task-specific rotation of the fully-connected layer weights.;Stronger ankles are more generally correlated with a heavier body rather than heavy feet, given that a key role of the ankle is to be able to provide a ""push"" to the body at the end of a stride, and perhaps less for ""lifting the foot"".",The Web Conference,18,"web,data,sources,data querying,data querying","2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c, 02fa2389b1b64b661192e224bed8af6df0ce80f6, 96273b87cd0eb9d1c9a12afae621ce2abdbbab36, 579476d19566efc842929ea6bdd18ab760c8cfa2, 2bc9d6830610e18f110aa7984f7c0271d6b17eeb, c665003881c3c35589d1e48da1ee7234b48f2ac8, 71ba8a8c75e0826f94eb53ee7a4566d4fc5b5256, 14014c024674991149f3ecf9314c93f7e029ef1a, 16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277, 152877c51df17cdd4a87d19e452c6daecfadf6c3, a9640bac0b45a804d07fc5914feb08af8f2a73f2, c84aa52bee5116f80c7740503edff4b08f733c3b, 215fc60307f741b9db059204e41db8bfb879e606, 9f5b82d9915d0752957602224c5056be7e749c83, 12c9bcf710d30ba991fb765ace07f177f53ecfd9"
01bf0e83159712fbbbd12171a7e268547a4cfbc5a,Querying Data Lakes using Spark and Presto,https://www.semanticscholar.org/paper/01bf0e83159712fbbbd12171a7e268547a4cfbc5,Conference,"Squerall is a tool that allows the querying of heterogeneous, large-scale data sources by leveraging state-of-the-art Big Data processing engines: Spark and Presto. Queries are posed on-demand against a Data Lake, i.e., directly on the original data sources without requiring prior data transformation. We showcase Squerall's ability to query five different data sources, including inter alia the popular Cassandra and MongoDB. In particular, we demonstrate how it can jointly query heterogeneous data sources, and how interested developers can easily extend it to support additional data sources. Graphical user interfaces (GUIs) are offered to support users in (1) building intra-source queries, and (2) creating required input files.",2018,14,"3059455, 2235966, 1705658, 2362078, 145044578",3059455,Copenhagen,2,"3353468, 2254255",Y,"For evaluating whether the data point x is anomalous or not, we search for a latent representation z such that x \approx g_\theta(z).;2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing.",The Web Conference,18,"data,sources,support,data querying,data querying","4afa7d8e2de43b0b67366b1bce8768f5a246d153, f18be38578ee52aa7071c404d42e3d53ae003122, e7b0e2710821a9fe1e3d2b9a09dd94514b2d5ea5, 72afe82af4c2ca100c36eb35292e85d806527f0a, 046eb47d56beb8069b0098e3d01608f81ebb6849, 74b4f16c5ac91e3e7c88ae81cc8c91416b71d151, 391a5f286f814d852dddcab1b2b68e5c1af6c79e, c50a909e20bd07f4aea09dc6dae539b45b406a96, 430f3c265935abb45bc84f3ae81c570ef778aac0, 390bcf15a1b13cb0d5966859c35c69a31238e838, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 78aab73ed574393ab421f25b3a0e3f7343e64748, fc32074b37a6d9dda535a70f9689022e70508520, 742747c7a453b293352b772d0d99541c96a351c3, 2f44ab0e52eb98fdfc0086638887f175b6f2fe4b, 1562390dd212516cd857009cbd4f857a902d1f3d, a18f02e5c24e1f924aea268dd343bbdea234f2bb, f7b69b8babfa607f2e8b0371f24cb720a7a827d6, c0aec04ee86c0724d61c976f19590fbe9c615723, 19b93280f17696a4ddfa2c75490a50ab107addf2, 3adb779bb37d22e3aa299364c2a337603801ca5c, 1fcddebb7b51175ac412009ec1c26cc29fb925cf, ccd94602e3acecf999d0c9ba62b1a8bc02e9f696, 2ddcd47b28eb4b43317a34cf56e83309f5347699, 18fff2d1ef494f2726b93d2f8a119b874f2e3d1d, bdb68c5e2369633b20e733774ac66eb4600c34d1, 5d24ed8942235324512d6cedfd8dbf54c57658b4, ef35da3a3c471a6a15c7a3b09586483eb50cbef0, 10b4b926904ad153f791ec680218e1610747a0c8, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, 2478f43de2f1fcd9301a81e9b42d86b1d46db02d, d1e701665e73faa648cb15473952576f40e8e122, f51bc74814a3452009ea5ca262d9768d08149ee6, e5d720767b7a539bb2edaa98eaf572a4506a79c6, 287ba5bf00d96af1596aaf80c178392a9c4fcc28, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, 9f5b82d9915d0752957602224c5056be7e749c83, dcbaf58b16ac7ef947879ea37c021466357b291a, 07b01d665646009439ca206378cc35e095ec6cd2, b0c34618ffd1154f35863e2ce7250ac6b6f2c424, 45de91a919780d5540872cf047986a370625e61c, 3813b88a4ec3c63919df47e9694b577f4691f7e5, d3855b7351c11145e51301e6b686f748ca35c802, ce212cb873a54e5716da53a66b10298ac013008a, f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa, d7fddafbbc372da4fa884f67bdc32db71b888806, 39444c55f07839ac6a0d1839472a982f8fb447bb, 13c4e5a6122f3fa2663f63e49537091da6532f35, eebf1a2705bf4ac256d141b0067f1b0ea1dc7632"
538fbdb8013ab43a9b5d725461b294ad29fcced7a,Sub-millisecond Stateful Stream Querying over Fast-evolving Linked Data,https://www.semanticscholar.org/paper/538fbdb8013ab43a9b5d725461b294ad29fcced7,Conference,"Applications like social networking, urban monitoring and market feed processing require stateful stream query: a query consults not only streaming data but also stored data to extract timely information; useful information from streaming data also needs to be continuously and consistently integrated into stored data to serve inflight and future queries. However, prior streaming systems either focus on stream computation, or are not stateful, or cannot provide low latency and high throughput to handle the fast-evolving linked data and increasing concurrency of queries. This paper presents Wukong+S, a distributed stream querying engine that provides sub-millisecond stateful query at millions of queries per-second over fast-evolving linked data. Wukong+S uses an integrated design that combines the stream processor and the persistent store with efficient state sharing, which avoids the cross-system cost and sub-optimal query plan in conventional composite designs (e.g., Storm/Heron+Wukong). Wukong+S uses a hybrid store to differentially manage timeless data and timing data accordingly and provides an efficient stream index with locality-aware partitioning to facilitate fast access to streaming data. Wukong+S further provides decentralized vector timestamps with bounded snapshot scalarization to scale with nodes and massive queries at efficient memory usage. We have designed Wukong+S conforming to the RDF data model and Continuous SPARQL (C-SPARQL) query interface and have implemented Wukong+S by extending a state-of-the-art static RDF store (namely Wukong). Evaluation on an 8-node RDMA-capable cluster using LSBench and CityBench shows that Wukong+S significantly outperforms existing system designs (e.g., CSPARQL-engine, Storm/Heron+Wukong, and Spark Streaming/Structured Streaming) for both latency and throughput, usually at the scale of orders of magnitude.",2016,48,"2108123758, 144593763, 2118438836",2108123758,Monaco,3,"49889487, 35202970, 47947548",Y,"If this is not the case for some reason, more detailed explanation is needed.;3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling.;As is often the case with such comparisons, it is hard to disentangle from where exactly come the speedups.",Symposium on Operating Systems Principles,25,"data,wukongs,stream,data querying,data querying","df138c7425e787cac2f9d3ab7775c0fb5294a83e, f62ab3fcc45eb787f4eb3213a3ffcae97799e9e5, ce2d5b5856bb6c9ab5c2390eb8b180c75a162055, 6548106035c7208ad498730627874a482734b9ac, b5904cd5dbf73b8d5ff13517de490c292d877ee0, 63316bb5b88d362051c048e864c3ae5d97a26d30, 3705919b880f4f8dc37483a704e14dd078cb9ac4, 78e40584f0d149bf6f98beb5561b7b83cb68e1b1, ada0b87cd5c30d31186c38fb12e631d29426a3bf, 079b57837221413bf99ab40999c77c29e280e0c2, d0238f2fa88b0316ab862f843a5521eff0fd9bf5, e2a85a6766b982ff7c8980e57ca6342d22493827, beb890d47bbc21a96967f9993c9d6e15686b2eac, e3b37c1c955b2b10809040ce277edae5333b99c3, 22c141b489e6e189f5996537b0a908fc10f90de7, 417326e51d78ba8bd2621f23e539b41bbdd336d6, a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f, e9a986c8ff6c2f381d026fe014f6aaa865f34da7, 11342d45911ee8a7c9e3a94117ce774ad7036172, bf324b5d23344984883d89a1dca22a39ca473031, 263a58f4fd32caca1dad2351af4d711aec451fe6, 31f9eb39d840821979e5df9f34a6e92dd9c879f2, 7ae2783a9196fb4bc2a610ae812d19722daddce5, 4895c430c7810b45840b58cc9182f12143013a43, b6becea767675ea6ee43c78ce747077a5050019c, a13149a80855412d970d0de2b41c611f4cf7e1da, b415e836d447ea9efb7629a1de67cd2a6f9e7ba8, e75cb933d387ecb184010ff07d0ee43fc1750e2a, facd5f5deb152229ceb1803434d8690a09ab4129, e7a7735104448371dde788542ebfc6af6485ea43, 75c364909914f17791837ec88090262aa6656d3e, ed9e7821b3e51c7e59183300d6c8cf90c8de0f26, 63adc1e5086481e36b19b62707a96b799da51e59, c997ed5ac5772bb0fce8bb11b8c86bc33310f39a, 8fedd23c1604dfeed02b75f8d38c1d7e33beee3a, 56ae2837b6cd4d143bbfa4a4b06811d70126106f, cd74acb268404cde24f5131a22d04d48776b283e, eeac4411ae119c6c7ac33a11f762f2495b4dd960, 846883b7761cb5fe4468d42bf9d328b5d1030175, e359e8960b0b09e8685a32927b7818f4b06ef881, ec7f5dc077480df149bcd4358a3aa8441878ca59, 0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4, 047286f5b9315a8e8bf56c4fc936e62f21495892"
19b93280f17696a4ddfa2c75490a50ab107addf2a,SHAHED: A MapReduce-based system for querying and visualizing spatio-temporal satellite data,https://www.semanticscholar.org/paper/19b93280f17696a4ddfa2c75490a50ab107addf2,Conference,"Remote sensing data collected by satellites are now made publicly available by several space agencies. This data is very useful for scientists pursuing research in several applications including climate change, desertification, and land use change. The benefit of this data comes from its richness as it provides an archived history for over 15 years of satellite observations for natural phenomena such as temperature and vegetation. Unfortunately, the use of such data is very limited due to the huge size of archives (> 500TB) and the limited capabilities of traditional applications. This paper introduces SHAHED; a MapReduce-based system for querying, visualizing, and mining large scale satellite data. SHAHED considers both the spatial and temporal aspects of the data to provide efficient query processing at large scale. The core of SHAHED is composed of four main components. The uncertainty component recovers missing data in the input which comes from cloud coverage and satellite mis-alignment. The indexing component provides a novel multi-resolution quad-tree-based spatio-temporal index structure, which indexes satellite data efficiently with minimal space overhead. The querying component answers selection and aggregate queries in real-time using the constructed index. Finally, the visualization component uses MapReduce programs to generate heat map images and videos for user queries. A set of experiments running on a live system deployed on a cluster of machines show the efficiency of the proposed design. All the features supported by SHAHED are made accessible through an easy to use Web interface that hides the complexity of the system and provides a nice user experience.",2014,76,"1876123, 1756679, 1388622435, 1404602362, 32653213, 2635216",1876123,Amsterdam,2,"80842917, 2035210",Y,"For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers.;To me, the comparison makes sense but it also shows that the ideas presented here are less novel than they might initially seem.",IEEE International Conference on Data Engineering,30,"data,satellite,component,data querying,data querying","9db0247728950788a2b42097d81dc0e24eed6bb2, d8348b802c9133d9e396d4ad809b020d5be42863, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, 97f456643712e9618edd7465676c62af3c8ae690, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, c9cc6a3c50b32e801f20b4f0ed5daf3e9550f9c1, 94de6bab96ad533169dbc3bf9e6557581e59cb6f, 733fc094e785724621c46e20db1be69f132ad9df, f1300d9be8254b028337d9757755ba906fe6955b, 8fcbd1cd1ee2211bd183b986900042470ee7f440, 4a7477881b66d12e79c704805781d4683a6a6be1, 819167ace2f0caae7745d2f25a803979be5fbfae, a8b995f0da78a79447dfb18c2337972b044f4239, d9d325ca670a1aa215e3e39023f8abf17dae7584, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 0e33833f5e2e2719edfba1d142eb4d27f96e799f, cb3968152f7d93f53d24b00279a90d5071ddc85a, e936f248b2c0489316ed1521656af2564c3502c3, 62ccd99a65bfc7c735ae1f33b75b107665de95df, 024006d4c2a89f7acacc6e4438d156525b60a98f, 00d1f3423a33f73ca6aee884a58834547475d2f0, 4f8d648c52edf74e41b0996128aa536e13cc7e82, f5ac3ec506a5a01807dd7196c0c52eef008b78cc, d7fddafbbc372da4fa884f67bdc32db71b888806, bf07f2927dca481653b8c60b2dc982fe4a7dfd4e, 48265726215736f7dd7ceccacac488422032397c, 185e7d2a761594451b02ace240356dadad2aef78, cc78babfacce48e715dac56886d7dd9746cfcab0"
fd0496ab020acf366375615ab40235e6dd3c5897a,Querying Geo-Textual Data: Spatial Keyword Queries and Beyond,https://www.semanticscholar.org/paper/fd0496ab020acf366375615ab40235e6dd3c5897,Conference,"Over the past decade, we have moved from a predominantly desktop based web to a predominantly mobile web, where users most often access the web from mobile devices such as smartphones. In addition, we are witnessing a proliferation of geo-located, textual web content. Motivated in part by these developments, the research community has been hard at work enabling the efficient computation of a variety of query functionality on geo-textual data, yielding a sizable body of literature on the querying of geo-textual data. With a focus on different types of keyword-based queries on geo-textual data, the tutorial also explores topics such as continuous queries on streaming geo-textual data, queries that retrieve attractive regions of geo-textual objects, and queries that extract properties, e.g., topics and top-$k$ frequent words, of the objects in regions. The tutorial is designed to offer an overview of the problems addressed in this body of literature and offers an overview of pertinent concepts and techniques. In addition, the tutorial suggests open problems and new research direction.",2015,43,"144145211, 145917501",144145211,Monaco,3,"46174961, 3268360, 2273645419",Y,"- What is the feedback in the CIFAR-10 experiments?;This is a solid paper about model evaluation in the chemical domain.;If the loss for the task is proper, then it's well known how to construct a divergence which coincides with the optimal risk.",SIGMOD Conference,15,"web,data,queries,data querying,data querying","e2e8ea9bcdb83deb2787ce89db1b51f7ccb1bd1e, 9ea0757c750ab1222a7442d3485a74d1c526b04c, a453ce8a3de86a170c79a1082ef358c3adf4e612, 3cc2f69951cd24fe61be4cf32d62afbac297bc2b, 4419c5720e30d5ca5158795d4c848125650b8db1, 6ba00c2386f2edc0b43eec442cd1923b5d964633, aca6d5f3866372a4506cf15773ae298f18c3f453, 0090023afc66cd2741568599057f4e82b566137c, b2ee139c1a3c2c53ac2b603070bdfbcd26b50ddc, 8f942d4c17ef1809be64388517002774fe2548f0, eb76aabdb294814d36b11f1d961f3a0fa2d04e57, 0af24df95f8a0b7e6fc257a98a5820314d1243f3, e576a2d97950b1f6831f88575dd3f370053f6af7, 119a9e5b563cf1134897553ee49325b5a5bd9fb9, ada0b87cd5c30d31186c38fb12e631d29426a3bf, 3ec02e36cc0ba16292b255e9f90c3725521e2ec9, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, 1e1cf81a1113482be3f0c280db994a832cb9426a, 537f5e8e4139392cd2d108f32495e5b2b80151ac, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 7676c02ea839ff1ceb6e5e1427c42bc45e169bde, 10a69070c8585dc5564b11b0e53dbe4d565f9ce1, e7b0e2710821a9fe1e3d2b9a09dd94514b2d5ea5, b613887337a5d2e8fc8773037116be81e6346835, bcc82ce554942880814243fc8c08a88b9d2aad09, cb23a59fdf3ade707600f076df4ff27a03941fba, 82a09f72d7b9587e3b519b1cd9640a5a611f3480, 1ddb24103c5089fd2bdfc05af41c69f39cfacc88, f4207bcad24fe4eaf3849d6c1fe38c36545b5cb3, d9d325ca670a1aa215e3e39023f8abf17dae7584, 247dec05283a1a521f99253a6cca6a5858cac0d2, 7bc9607c5cf3fc817675d46844f529097d579514, 31f9eb39d840821979e5df9f34a6e92dd9c879f2, eeac4411ae119c6c7ac33a11f762f2495b4dd960, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, c9f320789e98d2c7a798a9705e26dbe317677966, 695bdc6e24608364491b9418a220c65a7cd17413, 2f44ab0e52eb98fdfc0086638887f175b6f2fe4b, e75cb933d387ecb184010ff07d0ee43fc1750e2a, 9e66ae24a541255c2d931184498ee116ce81478a, 53b0cfa2eaaa9703df02b2ca9c43260c4de5df0a, f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f, e02a757617c2c42eb62889cc4d4aee3765928303, bd6c027a3604d6c8fa23435bf382455b2bee436b, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, 73d4accea441aae2373828a8dc2175aa2759c38f, 7fa30c8af436b7df513ef41e2cccd8a2404f37f8, 09797949fc70fbad8f3fed4f6cf4a91a9c709652, 2eda2921a8da4b325f9d05f556594a5884c398a7, 8388f1be26329fa45e5807e968a641ce170ea078"
52e510271b172d098ec9b107a4159216ec08527ea,Querying Big Data by Accessing Small Data,https://www.semanticscholar.org/paper/52e510271b172d098ec9b107a4159216ec08527e,Conference,"This paper investigates the feasibility of querying big data by accessing a bounded amount of the data. We study boundedly evaluable queries under a form of access constraints, when their evaluation cost is determined by the queries and constraints only. While it is undecidable to determine whether FO queries are boundedly evaluable, we show that for several classes of FO queries, the bounded evaluability problem is decidable. We also provide characterization and effective syntax for their boundedly evaluable queries. When a query Q is not boundedly evaluable, we study two approaches to approximately answering Q under access constraints. (1) We search for upper and lower envelopes of Q that are boundedly evaluable and warrant a constant accuracy bound. (2) We instantiate a minimum set of variables (parameters) in Q such that the specialized query is boundedly evaluable. We study problems for deciding the existence of envelopes and bounded specialized queries, and establish their complexity for various classes of FO queries.",2014,42,"144502903, 1729031, 2125469004, 2063047670, 2069299218",144502903,Vilnius,3,"1791253, 35202970, 2145907057",Y,"3. This paper lack original technical contribution from themselves.;The authors show through several experiments that the divide and conquer (DnC) technique can solve more complex tasks than can be solved with conventional policy gradient methods (TRPO is used as the baseline).;Wasserstein distances for eliminating batch effect, not enough novelty and no thorough comparisons to other methods.",ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,33,"queries,study,constraints,data querying,data querying","8f99f5cb3950fcd1628c6a563be4b4fc4966fa66, d997beefc0922d97202789d2ac307c55c2c52fba, b977e8de38dc0d13817bca1ed20036badfe2a58c, 3705919b880f4f8dc37483a704e14dd078cb9ac4, 1a4c6856292b8c64d19a812a77f0aa6fd47cb96c, 4e746359afd6f81705b875d71cc499b904a320df, 6a6ad9eb495739f4c80e7c09598720c3d5c5dff7, 0a829289a16ae48837cc2905635435db98bacc76, dd5fc2d34af35c6c7428205fff04c8f527b9e8d4, 21042565ec941f4fa31ac5a0af85a1a84ff21f1b, 579476d19566efc842929ea6bdd18ab760c8cfa2, 08be4e23951a0def1c5d235befbb39c8d8d373a3, b7f5973dbf2ef76fcc3aea616a7f72ca79f09020, b795c74a0150ec091003ffbaa5bd7d74487c137b, 3994334c81478a4b17341eb1f494dbccbb73d999, e7b0e2710821a9fe1e3d2b9a09dd94514b2d5ea5, 68a3d32416977e88cf1bfa4ad548d403f5f089d6, 965359b3008ab50dd04e171551220ec0e7f83aba, 3fea7ba9490c306484a8fdfe94323ff4e009e1c7, 90e4330fed2da147dd41490e8ad638b618112b3d, c6e242fce9e5e4ba2c4dc869cfb0afabf2020852, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, 3774fb4b6aa11d88c562ee5702c66cae5881af0d, 0863a5ce955e5193e535e1442086dc460dd295f0, 7a4fe2f003241ad97bf1778e527cb0306fa90da2, 5bc511aa30f72720260d792e57537379fb04c395, 96b51d940653710f9d099d89ade86b44fa9bdd8a, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, d3f9a39e49abfdf084da558e305be5473c8740e5, ecef432e7f6c9f431d5b34706a8de1fdebec46f9, 453fdfeefd6498a65be339d7e8722f6f3288907e, 780c7ead33428d282044519fee5e773ad56d5a2c, b09139c153bac8893e8faea2b3a59159234caadc, 1109d62ebd2b29a7dc148bc30dd6cfc803a63dec"
d57f11ed40c3cdcbb36cb758191db4f2c9372965a,An extension of SPARQL with fuzzy navigational capabilities for querying fuzzy RDF data,https://www.semanticscholar.org/paper/d57f11ed40c3cdcbb36cb758191db4f2c9372965,Conference,"The Resource Description Framework (RDF) is the graph-based standard data model for representing semantic web information, and SPARQL is the standard query language for querying RDF data. Because of the huge volume of linked open data published on the web, these standards have aroused a large interest in the last years. This paper proposes a fuzzy extension of the SPARQL language that improves its expressiveness and usability. This extension allows (1) to query a fuzzy RDF data model, and (2) to express fuzzy preferences on data and on the structure of the data graph, which has not been proposed in any previous fuzzy extensions of SPARQL.",2015,24,"1741623, 50458739, 2673479",1741623,Athens,3,"49889487, 40228633, 8347899",Y,"An additional GAN loss is used on the generator output to encourage the output to look like summaries -- this procedure only requires unpaired summaries.;In the paper, the authors proposed using GAN for anomaly detection.;The authors propose a centralized solution to the problem by adapting the Deep Q-learning Network model.",IEEE International Conference on Fuzzy Systems,15,"data,rdf,sparql,data querying,data querying","f5ac3ec506a5a01807dd7196c0c52eef008b78cc, 0e9a44ce661c3535d5ce747912540080324489f5, 5d433da6d0f143f20936379910104d2bb139d4ae, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 7eaac9847257c32afd450017d1348ecda4dcaade, 8388f1be26329fa45e5807e968a641ce170ea078, 07b01d665646009439ca206378cc35e095ec6cd2, 0084f3cb0a1754272151c5268a783f24bf5676a0, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, d6e1e4f0ad898ca6ac37e6e139a77fa3982170d4, 0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4, ea160adc0d78e54669281b8b145bcd832e648fee, 0bca61986b8edeaf33018d0203b44110f2480110, 6f31f59f7b4bd751b5d8657d8a878bef571f9a4c, ef35da3a3c471a6a15c7a3b09586483eb50cbef0, 3b9732bb07dc99bde5e1f9f75251c6ea5039373e, 799d5a8271887adede035644d878c7bd555576df, 0a71dd8bec060195e14eb9d0a7abbc08d960d4d5, 819f2778eba0d4c9eea86307bedaaeed94dc751d, 82e1e8b222aeaca19d45375b31fdc825d1a821b8, 75c364909914f17791837ec88090262aa6656d3e, 11c3154d709c74dbbe702e7f7c46a37224f9cc36, 241d6ca92c4bc65ac3ee903e4732f70bff5c5e9f, bbed457fd04ba4972018382d1a01a0bdde399d3c, f9c990b1b5724e50e5632b94fdb7484ece8a6ce7, fe4c5074f021cd1c810892c1b6bc267b23aa6e5c, a22f3398ea865426c89ee66f4824ec626e56a864, 10ea29fda06bdbe56f591909d89f3194b452ac91, 9712624bb61abb0da989514cae558cfab61bb9d2, 9b54941de1e21826ecc28b32730ac3f69991ede4, a81ba6a07bf7a2ecff871e3362a77404501d0927, dca4d9abbc82e57dfa52f932e893d467a63e0682, 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c, 4cf2034fa55a20e60a24ca6924f66aaafb30b877, 31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a, 69e76e16740ed69f4dc55361a3d319ac2f1293dd, 4419c5720e30d5ca5158795d4c848125650b8db1, 322d91190acd8ac8c64598f5126947b0485ba249, 88a724083b2cfcc096448c28e6973c8f761ee463, 013eb12ce5468f79d58bf859653f4929c5a2bd14, 9e195234688778b2beb3528632e78dbabf816332, 5c107f5aa33e3e5d3b3ea9dc17e04a51765b0983, 0c8c500cec9b74ebc7be44c52b79d2bd78234605, 2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, 54ddb00fa691728944fd8becea90a373d21597cf, d6f002d88638de71114dab083f0ea8ceea6b6a5a"
cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5a,Querying RDF Data Using A Multigraph-based Approach,https://www.semanticscholar.org/paper/cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5,Conference,"RDF is a standard for the conceptual description of knowledge , and SPARQL is the query language conceived to query RDF data. The RDF data is cherished and exploited by various domains such as life sciences, Semantic Web, social network, etc. Further, its integration at Web-scale compels RDF management engines to deal with complex queries in terms of both size and structure. In this paper, we propose AMbER (Attributed Multigraph Based Engine for RDF querying), a novel RDF query engine specifically designed to optimize the computation of complex queries. AMbER leverages subgraph matching techniques and extends them to tackle the SPARQL query problem. First of all RDF data is represented as a multigraph, and then novel indexing structures are established to efficiently access the information from the multigraph. Finally a SPARQL query is represented as a multigraph, and the SPARQL querying problem is reduced to the subgraph homomorphism problem. AMbER exploits structural properties of the query multigraph as well as the proposed indexes, in order to tackle the problem of subgraph homomorphism. The performance of AMbER, in comparison with state-of-the-art systems, has been extensively evaluated over several RDF benchmarks. The advantages of employing AMbER for complex SPARQL queries have been experimentally validated.",2015,14,"2810491, 1789397, 1744598, 1725656",2810491,Kiev,2,"2845020, 2065048323",Y,My main concern with this work is that I don't see any mechanism in the framework that prevents an expert  (or few of them) to win all examples except its own learning capacities.;The proposed method is new and technically sound.,International Conference on Extending Database Technology,15,"rdf,query,amber,data querying,data querying","1d174f0e3c391368d0f3384a144a6c7487f2a143, fa26a6d434450b185e669170e79fd3e1d29716bf, 85328b4a8132bf4299f8cd7f8e79e850d561c8fc, 22c141b489e6e189f5996537b0a908fc10f90de7, 0574a3abc98f0e6bd035a4ff4ea107cfba45d3d4, ef25b02f3be31c699255ee05aa90a4a17461d95d, 3337e4b2e63e5e99171f9da9d161771f5810c27a, 6db0f8d396371078590faa7b34ae2e0e1b154a60, 10ea29fda06bdbe56f591909d89f3194b452ac91, e449b9b3fe04fe260731a3c74d2123bf6eaadf5b, 3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b, 079b57837221413bf99ab40999c77c29e280e0c2, 182acc7a1c9a63cfdef9f86ee9df43fd75e05060, 8f942d4c17ef1809be64388517002774fe2548f0, b415e836d447ea9efb7629a1de67cd2a6f9e7ba8, d9a7fa7616a327367696e19b1846519745cd43ff, 152877c51df17cdd4a87d19e452c6daecfadf6c3, 7c8bd41e9cebdb01f445a51ba43839c8d9b3ab91, 915ab7b6ca3633230403d47dbb31cf74888cc5c9, ae38dc77a962161107361f213db9216ee1274037, be2b0396de9431bae931642516a1d3e4906329f5, b61b260de1599e6e89491cad9160898fcd3b34c2, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, e2a85a6766b982ff7c8980e57ca6342d22493827, 3dfa820702b6181c9964931f0a4d47fd298bf429, 01f0f5205d03870f172ae8f04e33356d5a0af221, 965359b3008ab50dd04e171551220ec0e7f83aba"
b69a35662a2cac38eab22f4481285116bdf8c30ea,A fuzzy extension of SPARQL for querying gradual RDF data,https://www.semanticscholar.org/paper/b69a35662a2cac38eab22f4481285116bdf8c30e,Conference,"In this work, the first stones of a flexible approach to linked data querying based on fuzzy set theory are laid. Flexibility refers to the capability of expressing flexible queries over a RDF model containing gradual information.",2015,4,"1741623, 50458739, 145433223, 2673479",1741623,Warsaw,3,"1693182792, 5973699, 2108025636",Y,"A step in the right direction, with interesting results, but not a huge level of novelty.;How D will handle an example far from fake or real ones ?;Both datasets should be compared to LASSO as well.",Research Challenges in Information Science,15,"stones,flexible,approach,data querying,data querying","eda6756ab2844c390584686dc5e6385f4a8369cd, 752604994a7ca548ff2954114fc61a501d857b1c, a85c45ce7c893388e8eafa8a653b042e1497db48, 371a343457a4fbff00000bf4faa29b2b2f85744c, 2880ac931c11176aee6d42a7e7bb0703aacde3f9, 6e74403ab75d0080c056fd66702ab1f54f04d9b1, 771a858c35f6d6e6d1017dde95368de3794738a6, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, e576a2d97950b1f6831f88575dd3f370053f6af7, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, 30cc95639cffca4ffa8c0eafbc502636c0c88fa5, f7b69b8babfa607f2e8b0371f24cb720a7a827d6, 8dd0c1e955c66092ff951941a151336211e6e171, 09797949fc70fbad8f3fed4f6cf4a91a9c709652, 340f48901f72278f6bf78a04ee5b01df208cc508, a453ce8a3de86a170c79a1082ef358c3adf4e612, 2dafea864f74a477414c3b71b742f7997e216102, 62df84d6a4d26f95e4714796c2337c9848cc13b5, da34bdb0d7a6b4a94c22d2f00d89ec877be1ae3f, e5194ae88d63c7549678b1b73cfdaf7112164272, eccad1576e6c67b2c93a5cb8d384d038fbb161d6, 746a81aa26d3ebfb81acfd6af958d6a21603cd21, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, 37b9478b792fb1e7ad5f2c322ed4445b93df6c9e, 7d873a9c49d3864709aa762f8740edcdbd7369c5, 1cd497e82bdc46a9d3d28b2316ea7cbe9aee5467, 08764019e9762da527253b37b0ff39c46a4206b7, 3e53b6d0d30be2a802c6b7b34b75f6e67ab8204f, bcc9245ebf57072c07c5a9d4eff3aaaac36cb7c6, 7536bce1007a765fd097a7cc8ea62208a8c89b85, a9cbbef8f4426329d0687025b34287c35bdd8b38, ac4350d3a2673a4c34fa949714ede8b45fd04f1d, be2b0396de9431bae931642516a1d3e4906329f5, 185e7d2a761594451b02ace240356dadad2aef78, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 69a72ff5b30642d11c96635e99aadad3140d33a7, ead6121fbc787d508dc6a6d7106f72bf0d647d03, 40416ac3bf78583eea37661b1b446e9939245b3e, 34e9852f588f75eba81c66a3e5f867a794a5a690, 17fca92ffd527c78c5dc6c7953e96671743807fa, 91d6e8ba5dd90b02fe3bd870b19da13a6167af53, 3ea34401909978d3d3d0c25c8746e02c7d2a7c77, b473e91cbe80c8b46451b49153cd5f93030480ab"
1661d0d8d47cac41e01c59c60aac3675b4396698a,A demonstration of Shahed: A MapReduce-based system for querying and visualizing satellite data,https://www.semanticscholar.org/paper/1661d0d8d47cac41e01c59c60aac3675b4396698,Conference,"Several space agencies such as NASA are continuously collecting datasets of earth dynamics-e.g., temperature, vegetation, and cloud coverage-through satellites. This data is stored in a publicly available archive for scientists and researchers and is very useful for studying climate, desertification, and land use change. The benefit of this data comes from its richness as it provides an archived history for over 15 years of satellite observations. Unfortunately, the use of such data is very limited due to the huge size of archives (> 500TB) and the limited capabilities of traditional applications. In this demo, we present Shahed, an interactive system which provides an efficient way to index, query, and visualize satellite datasets available in NASA archive. Shahed is composed of four main modules. The uncertainty module resolves data uncertainty imposed by the satellites. The indexing module organizes the data in a novel multi-resolution spatio-temporal index designed for satellite data. The querying module uses the indexes to answer both spatiotemporal selection and aggregate queries provided by the user. The visualization module generates images, videos, and multi-level images which gives an insight of data distribution and dynamics over time. This demo gives users a hands-on experience with Shahed through a map-based web interface in which users can browse the available datasets using the map, issue spatiotemporal queries, and visualize the results as images or videos.",2014,9,"1876123, 1388622435, 1404602362, 2279240, 2635216, 3087877, 1756679",1876123,Andorra,3,"1394550182, 40443723, 2083106",Y,"Is this on bAbi as well?;The “obverter” technique is quite interesting since it incorporates the concept from the theory of mind which is similar to human alignment or AGI approach.;For example, when showing that the head direction cells generalize in the new mazes how can we be sure that it is not using a common lighting scheme common to both train and test mazes to orient itself?",IEEE International Conference on Data Engineering,30,"data,module,datasets,data querying,data querying","98e6c6d860383fea5bbad145deed51514d23b86c, b01ad71bd376f36546f02204784908da9577bb0b, 7bb477077968d68aa7a6059d8d6d801fb28274da, 4f8d648c52edf74e41b0996128aa536e13cc7e82, 597bd2e45427563cdf025e53a3239006aa364cfc, eacf9284a39adcd56172665f31fd5a72560bba7a, a13149a80855412d970d0de2b41c611f4cf7e1da, 244e08e109f6f4fa0f846977e1aef1e7b6a9e816, e67a2817089312746d69b38ce9abfdc4b1bc69c3, db6ad6ded1cfa26fdc7437f27fb823ec533e96fe, 56ae2837b6cd4d143bbfa4a4b06811d70126106f, 6b3756d32ab5b0a5715a5cfc3672290d2d643017, a80e26e6365b215715c182d19a9aa8bb876ac768, bb826d9ccd116076a267dfcb048cdd747c11b255, 88a724083b2cfcc096448c28e6973c8f761ee463, cd8a9914d50b0ac63315872530274d158d6aff09, 6dc4883228c95e8a332320fcc587a0ff33c84d59, 74b63c4cdc719a646ce014d084a0ce0ac66d9139, 02a1e8e77f501675945890df45fbdc11726cb0ba, 5e31fa4e69d1a3587230f5d134c0b7e2ed84a742"
fb0aeed456bff8607fab2bf443e9d86d51c3dff6a,SMART-KG: Hybrid Shipping for SPARQL Querying on the Web,https://www.semanticscholar.org/paper/fb0aeed456bff8607fab2bf443e9d86d51c3dff6,Conference,"While Linked Data (LD) provides standards for publishing (RDF) and (SPARQL) querying Knowledge Graphs (KGs) on the Web, serving, accessing and processing such open, decentralized KGs is often practically impossible, as query timeouts on publicly available SPARQL endpoints show. Alternative solutions such as Triple Pattern Fragments (TPF) attempt to tackle the problem of availability by pushing query processing workload to the client side, but suffer from unnecessary transfer of irrelevant data on complex queries with large intermediate results. In this paper we present smart-KG, a novel approach to share the load between servers and clients, while significantly reducing data transfer volume, by combining TPF with shipping compressed KG partitions. Our evaluations show that smart-KG outperforms state-of-the-art client-side solutions and increases server-side availability towards more cost-effective and balanced hosting of open and decentralized KGs.",2019,28,"20829758, 144341429, 144841747, 2065791442, 1708607",20829758,Belgrade,3,"2273679997, 49298465, 1614034792",Y,"However, that is moved completely to the Appendix.;- Top of page 5, when you describe the checkerboard BFS: please include a visualization somewhere, it could be in the Appendix.;This seems to prove that an NER is useful for tasks where DB queries are needed, rather than that the dynamic NE-Table construction is useful.",The Web Conference,19,"data,kgs,sparql,data querying,data querying","dadfb3ff45e19dc22456a645f441bbeb17c93c9c, 2743e66939b30c43affb3c9e31f20cfac2109045, 709f7a6b870cb07a4eab553adf6345b244913913, d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b, 40416ac3bf78583eea37661b1b446e9939245b3e, 7ed665355ac78bf0c394602dd9d26075195ce2f2, b7a717233ec3ff37385ab1b06816d0ca375f5bb3, 8760bc7631c0cb04e7138254e9fd6451b7def8ca, b5270b32a17fcc2e3dc209add6f4e4ba709b7358, 0ac1a1ccb16c8441f42ff8de743fc93f0e08c15c, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, b01ad71bd376f36546f02204784908da9577bb0b, eb9e0da8b7170e3ca4364f2f9010599c2d2556f1, 5687c9e8da574453fd873662b95caec70dac9d1e, 89a30b5dab02c9c390a632acad481fa602859272, b34fc78de28be598e21118d7cb9d84d63374addc"
b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8a,Expressive Time Series Querying with Hand-Drawn Scale-Free Sketches,https://www.semanticscholar.org/paper/b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8,Conference,"We present Qetch, a tool where users freely sketch patterns on a scale-less canvas to query time series data without specifying query length or amplitude. We study how humans sketch time series patterns --- humans preserve visually salient perceptual features but often non-uniformly scale and locally distort a pattern --- and we develop a novel matching algorithm that accounts for human sketching errors. Qetch enables the easy construction of complex and expressive queries with two key features: regular expressions over sketches and relative positioning of sketches to query multiple time-aligned series. Through user studies, we demonstrate the effectiveness of Qetch's different interaction features. We also demonstrate the effectiveness of Qetch's matching algorithm compared to popular algorithms on targeted, and exploratory query-by-sketch search tasks on a variety of data sets.",2017,43,"40964207, 1698925",40964207,Vilnius,3,"144132699, 47319783, 2317297",Y,"There's clear value in having good inductive biases (e.g. expressed in the form of the discriminator architecture) when defining divergences for practical applications.;The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy?;A good approach with some open questions on related work, scalability, and robustness The authors propose an approach for zero-shot visual learning.",International Conference on Human Factors in Computing Systems,17,"query,series,features,data querying,data querying","980858461df7c4349f17b427686c5bcbcffbdc04, a56bf7ee9a56d8f84079684339a953c2df9ce76b, 73d4accea441aae2373828a8dc2175aa2759c38f, d9a7fa7616a327367696e19b1846519745cd43ff, 9712624bb61abb0da989514cae558cfab61bb9d2, a80e26e6365b215715c182d19a9aa8bb876ac768, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, 929305892d4ddae575a0fc23227a8139f7681632, b9f190339ce0b4cdad3464c45ec3266a7369fb7c, f14fc9e399d44463a17cc47a9b339b58f6ef7502, 94de6bab96ad533169dbc3bf9e6557581e59cb6f, 02fa2389b1b64b661192e224bed8af6df0ce80f6, 06d7cb8c8816360feb33c3367073e0ef66d7d0b0, 7909428ca7d813331bbe4d33d07ab09e984b41b4, b613887337a5d2e8fc8773037116be81e6346835, 545f108575314031f35c617c4ac35a10133c50e3, fa63c3f53413ced7946623889c416e34a28676ea, 537f5e8e4139392cd2d108f32495e5b2b80151ac, c24d47ff95cd4bda073c75ec24ececaa3b10c995, d7b820af40a9e2660ef700d39f7b2e27b43435c5, 3b87dafd5a412e25e06761f181ec199ca88a7398, 29409efa04ac99ccf01d2a011d21d5d14e870000, 704011527f183b561ea6a75b21e4cefe5aa77fca, b473e91cbe80c8b46451b49153cd5f93030480ab, 390bcf15a1b13cb0d5966859c35c69a31238e838, 155eeede0c070f1f017ba5c9f6cacd7ae0b098aa, 2eda2921a8da4b325f9d05f556594a5884c398a7, 709f7a6b870cb07a4eab553adf6345b244913913, 8ca3be8e47289aef296d3e5804dc22b37dc1f635, 518a7c79968a56d63a691d42f8378be6c776167e, 94214d6d922ce095719d488642cbcc75dc52f273, 915ab7b6ca3633230403d47dbb31cf74888cc5c9, 5d8e450c8c7d9faa8222f1d0eaf34b01755c9eef, ab606e9d148458f6d54e5d44abefd73b7990f6e0, cc017a62c605a0749e35a1264a46d62e78fb68b7, c997ed5ac5772bb0fce8bb11b8c86bc33310f39a, cd29c25c489562b409a60f83365f93f33ee1a0a1, c468bbde6a22d961829e1970e6ad5795e05418d1, a453ce8a3de86a170c79a1082ef358c3adf4e612, 0df5a4f9cc8a244715fe9968732497d2ac2a7cd1, b00d4452cd9869526169b7a2fe893c0a7c31eb4c"
10cf0045bc0f58aa3699e4451f65b12a08019c5ca,"Demonstration of Taghreed: A system for querying, analyzing, and visualizing geotagged microblogs",https://www.semanticscholar.org/paper/10cf0045bc0f58aa3699e4451f65b12a08019c5c,Conference,"This paper demonstrates Taghreed; a full-fledged system for efficient and scalable querying, analyzing, and visualizing geotagged microblogs, such as tweets. Taghreed supports a wide variety of queries on all microblogs attributes. In addition, it is able to manage a large number (billions) of microblogs for relatively long periods, e.g., months. Taghreed consists of four main components: (1) indexer, (2) query engine, (3) recovery manager, and (4) visualizer. Taghreed indexer efficiently digests incoming microblogs with high arrival rates in light main-memory indexes. When the memory becomes full, the memory contents are flushed to disk indexes which are managing billions of microblogs efficiently. On memory failure, the recovery manager restores the memory contents from backup copies. Taghreed query engine consists of two modules: a query optimizer and a query processor. The query optimizer generates an optimized query plan to be executed by the query processor to provide low query responses. Taghreed visualizer features to its users a wide variety of spatiotemporal queries and presents the answers on a map-based user interface that allows an interactive exploration. Taghreed is the first system that addresses all these challenges collectively for geotagged microblogs data. The system is demonstrated based on real system implementation through different scenarios that show system functionality and internals.",2014,71,"143811079, 2877140, 1388622435, 2186764, 35205093, 2635216, 3087877, 1756679",143811079,Tirana,2,"2166312768, 2285813122",Y,"So, the main contribution of the paper is to do all the sensitivity calculations with respect to SVM problem and then use the importance sampling theory to obtain bounds on the coreset size.;The proposed algorithm in 3 out of 5 datasets, two of theme with statistical significant.",IEEE International Conference on Data Engineering,30,"query,microblogs,system,data querying,data querying","4ab38afa44f1da43746cd4a01344c8ec212b9de3, c3df199cbca74763c4ae9889409bbd4aa29b6255, bcc82ce554942880814243fc8c08a88b9d2aad09, 7536bce1007a765fd097a7cc8ea62208a8c89b85, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, 37b0a7a6c8fb26e32b9206847e78d521a2cd5900, 736ef8a32d6c5f76a21d61299300cf796480d507, 3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8, 0b24f27d920bd1d23c8f6a1ab2b603c5c14f0a36, b34fc78de28be598e21118d7cb9d84d63374addc, 0599f45e03ac2016321df0dd653ba4c0034c79d5, a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854, b6a7226e5f6d618370995eccad68af195ef32da2, 82870bc488b57cdf5ea62877109a7278af2926b3, 709f7a6b870cb07a4eab553adf6345b244913913, eda6756ab2844c390584686dc5e6385f4a8369cd, cf5fddf6717e88e2bbed6b0bfe54dfeb311e6789, 9e27190f2d9b2167d4a66b88696def4585072fd5, b80e2af7ffa85fe2f02ea8390e4915d55c19d199, 8babcaf89f8537dc628a029ebf932100f57289fd, 8bba999de25bfb288b3f7f88e1d907aab02638b6, 54814b0669f20f07ec8d8c7e4fabddfadfe66b1a, a357f1ff27e184d9a5ef69e665e8ca891032bf71, fd3259ecd1c94731f5d9bd9c0bf7417f2e147c71, 69e76e16740ed69f4dc55361a3d319ac2f1293dd, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, 13c4e5a6122f3fa2663f63e49537091da6532f35, b05306f0b142e5afb3974b1b79996e5b82653662, cd29c25c489562b409a60f83365f93f33ee1a0a1, 381c7853690a0fee6f00d2608a7779737f1365f9, 38179848e2d6a3ad373b1793848816111428ac36, cf5dfc4a9f7a82b32640128ca10832eace55880e, c9f320789e98d2c7a798a9705e26dbe317677966, d2a5dcecd2ffdf03473df1688091f08fadb114a3, b904dcdbd7c7b33938583f2f57d05ca70e121ea9, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, 5d8e450c8c7d9faa8222f1d0eaf34b01755c9eef, bb826d9ccd116076a267dfcb048cdd747c11b255, fb5413afba689d16543215c5a2ddbc5b78a52007"
b5270b32a17fcc2e3dc209add6f4e4ba709b7358a,Recent Advances in Querying Probabilistic Knowledge Bases,https://www.semanticscholar.org/paper/b5270b32a17fcc2e3dc209add6f4e4ba709b7358,Conference,"We give a survey on recent advances at the forefront of research on probabilistic knowledge bases for representing and querying large-scale automatically extracted data. We concentrate especially on increasing the semantic expressivity of formalisms for representing and querying probabilistic knowledge (i) by giving up the closed-world assumption, (ii) by allowing for commonsense knowledge (and in parallel giving up the tuple-independence assumption), and (iii) by giving up the closed-domain assumption, while preserving some computational properties of query answering in such formalisms.",2017,19,"1784772, 49633004, 1690572",1784772,Budapest,3,"2129460589, 10708829, 3382568",Y,"Moreover, I like the qualitative experiment (Figure 2) in which the tree is used to vary a latent dimension to change the digit’s class.;Standard idea, great results This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives.;All the experiments use large tasks - it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task.",International Joint Conference on Artificial Intelligence,17,"knowledge,assumption,formalisms,data querying,data querying","9727206903eb40d4fa42606711bad3402f2ba9aa, f1664bbaddedea8c250873e7610ab07e53fa7132, d88083e37c44461ce3e404bd57257cd3edb07d4e, 665b0c776ff7507c32793f10ce9edf90bc2f674a, dec26f0640e3a4fdb116735526302ccb9f49867e, 834cdfca7cc041a6fa0db3da5493c6754bea845b, 718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95, 4f8d648c52edf74e41b0996128aa536e13cc7e82, b000a4b30f6206f6cfb033a79aad1ba810c972a4, 545f108575314031f35c617c4ac35a10133c50e3, d8348b802c9133d9e396d4ad809b020d5be42863, b0b770fb8c7760749c88e3c83ae173cdb07f7bd5, 633e2fbfc0b21e959a244100937c5853afca4853, 7ae2783a9196fb4bc2a610ae812d19722daddce5, 84a36e19f9394f22b34f79756fa9628a795e02ea, a2ec47b9bcc95d2456a8a42199233e5d9129ef18, df7d26339adf4eb0c07160947b9d2973c24911ba, 780c725848aac1118d00c8bb306719ec803369cd, 1d7531db9272f7838e33616075e1e64532fd013a, 73d4accea441aae2373828a8dc2175aa2759c38f, d617f51833860dc50d202af7f80be71304b2e994, ce157cea880c9ab64de64f11a531202f5348fa05, 5406e153957dd7a165264da6e6e5d81251997404, 7571ed4cf1bbdcf891b576a0da12c910b1f0c72f, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 156d8e2aa90b5ccc9be10477ca70deaad0151387, 0894585294c67193ff3190240554677b56fd79a0, 0a92bc2dc8a216e6aced83edc0358241066833df, 5d764e3cb11d09a8f2ec8bdce3b390d6e42d3f8a, 6548106035c7208ad498730627874a482734b9ac, cc78babfacce48e715dac56886d7dd9746cfcab0, 33e332837e91c1048c3ed165cd16bf7607c3bf06"
2e7f532796eed2847d4c19e3cff03756049e81b4a,Diversified Top-k Subgraph Querying in a Large Graph,https://www.semanticscholar.org/paper/2e7f532796eed2847d4c19e3cff03756049e81b4,Conference,"Subgraph querying in a large data graph is interesting for different applications. A recent study shows that top-k diversified results are useful since the number of matching subgraphs can be very large. In this work, we study the problem of top-k diversified subgraph querying that asks for a set of up to k subgraphs isomorphic to a given query graph, and that covers the largest number of vertices. We propose a novel level-based algorithm for this problem which supports early termination and has a theoretical approximation guarantee. From experiments, most of our results on real datasets used in previous works are near optimal with a query time within 10ms on a commodity machine.",2015,44,"2149234208, 1699363, 2186806073",2149234208,Ljubljana,2,"1394550182, 31131132",Y,"In general, this is an interesting direction to explore, the idea is interesting, however, I would like to see more experiments;But the aspect I like most about this paper is the experimental analysis.",SIGMOD Conference,15,"subgraph,graph,study,data querying,data querying","44e1dd74f0446ec91221189ad3a65edb1a0208fe, 597bd2e45427563cdf025e53a3239006aa364cfc, 3b552b5adae3ab82f874fd2ef1477862f5a9d056, 2dafea864f74a477414c3b71b742f7997e216102, 430f3c265935abb45bc84f3ae81c570ef778aac0, b889b1d6944213bc2ca29e3ad07ee65ede20892d, a0a79dad89857a96f8f71b14238e5237cbfc4787, bbed457fd04ba4972018382d1a01a0bdde399d3c, a60a4e5f7f872b9825ddff5d379857c2091ca52b, 598231eb906b183f7a2a408ef4536127e11e3de9, b266510f5f9b40d42b51884ad13a1867fb3284fd, c9b56cb026a38e39bb0228faac57accd6f65e6f7, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, c889d6f98e6d79b89c3a6adf8a921f88fa6ba518, 83cebf919635504786fc220d569284842b0f0a09, 0863a5ce955e5193e535e1442086dc460dd295f0, 371a343457a4fbff00000bf4faa29b2b2f85744c, 09797949fc70fbad8f3fed4f6cf4a91a9c709652, b27a78366868ca47098e00dda74dd1b167b3a80d, bdb68c5e2369633b20e733774ac66eb4600c34d1, 7904b3446775ed8c79f4f94001a16b706989c462, 595101f13b961d69c553ce1ed24f60f3f1085e02, d57f11ed40c3cdcbb36cb758191db4f2c9372965, 676664ee7471738577f641e6159e7596625b7fdb, 784141489258258b12979061d92c1a616da26525, f3eb8b6b836c0ef419e1fa565476e6892c8717ff, 8c7aee0bbc062d5b4bcd34951b1e002274288206, b9f190339ce0b4cdad3464c45ec3266a7369fb7c, 1452b25a7680bbb2c66dd7dfca6704292405da92, e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0, 1d7531db9272f7838e33616075e1e64532fd013a, 8ec5896b4490c6e127d1718ffc36a3439d84cb81, 1051abf1e3dae90241ad15b3f98f2e41197ee611, e4b52a1a00e9db941326fc857b95245cbfb60bce, b52db9e41e15f76bdcfbe674abe0314af545c430, a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b, 0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1, 4ab38afa44f1da43746cd4a01344c8ec212b9de3, 1d174f0e3c391368d0f3384a144a6c7487f2a143, 0e141942fa265142f41a2a26eb17b6005d3af29e, 147c868b721c8d29df7c61db7f2360114c760614, 6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786, 22ebfc211d184ed615729378a43fde175bf14478, b79e5e4622a95417deec313cd543617b19611bea, 46c266b3d1274dacd7fce27ee8cb4d587f087a58"
834fdec542153aae5fe725df801aac87ba5e8f56a,Graph Querying Meets HCI: State of the Art and Future Directions,https://www.semanticscholar.org/paper/834fdec542153aae5fe725df801aac87ba5e8f56,Conference,"Querying graph databases has emerged as an important research problem for real-world applications that center on large graph data. Given the syntactic complexity of graph query languages (e.g., SPARQL, Cypher), visual graph query interfaces make it easy for non-expert users to query such graph data repositories. In this tutorial, we survey recent developments in the emerging area of visual graph querying paradigm that bridges traditional graph querying with human computer interaction (HCI). We discuss manual and data-driven visual graph query interfaces, various strategies and guidance for constructing graph queries visually, interleaving processing of graph queries and visual actions, and visual exploration of graph query results. In addition, the tutorial suggests open problems and new research directions. In summary, in this tutorial we review and summarize the research thus far into HCI and graph querying in the database community, giving researchers a snapshot of the current state of the art in this topic, and future research directions.",2016,20,"1730344, 1891554, 2128664093",1730344,Skopje,3,"1764236, 1723636206, 3471557",Y,"2) compressing the embedding space using pca;It is investigated how several RL strategies perform on a large, standardized data set.;2. The proposed attention mechanism seems to be fairly domain (or task ) specific, and may not be beneficial for strong generalization (generalization over unseen category).",SIGMOD Conference,16,"graph,research,query,data querying,data querying","ce2d5b5856bb6c9ab5c2390eb8b180c75a162055, d997beefc0922d97202789d2ac307c55c2c52fba, 087dd95e13efd47aef2a6582e6801b39fc0f83d8, 39602922b04885047254444fd1a1586d797617ce, 89af855962927fb89a673a221f6f394a6f3dfc6a, 48c73c389c3f36d70407eb8309a0b41578c15fc8, f6dab84c2c00ab92d8ee9d9359d7e530512114f9, d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b, b7a717233ec3ff37385ab1b06816d0ca375f5bb3, adc61e21eafecfbf6ebecc570f9f913659a2bfb2, 0d6ef817813d04a3b3ec6c3ce008e104fb3e587a, e2e8ea9bcdb83deb2787ce89db1b51f7ccb1bd1e, ce7499d6862df8269c655220049c3ed20b9b6f5e, c2413fa296543159b32d16350d9e29f7db528790, 76f87dfb737ac0e44df1fc331b422de2b9f0a632, 57f5bf937f7393b691428747a9078d3124e6bcce, facd5f5deb152229ceb1803434d8690a09ab4129, 4aec7bcb292cb763341a22fc67a1cfaceb3a2c67, 705e49afd92130f2bc1e0d4d0b1f6cb14e88803f, 2737a61f6557fe7bf53a608c668de2eff1f582f0, f31c2ddb7bb3f3f6ef4219143901cc0cddca5968, bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b, fe2492b7b8cf6d1d10b7ea38e0f7f853bd679d52, f0eb17c6def9aaf95ef3c8ce12b9c3ceb9030274, 4e746359afd6f81705b875d71cc499b904a320df, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, 0fe0f08e71e188cb2a663f36acb296d9eb6fe694, 322d91190acd8ac8c64598f5126947b0485ba249, e3b94a5f28522e6825aff16ff07d56bd70d26c96, e2e7d964c09e27d334fcb8761d69918630629387, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, 92afbbe41174a545f9da9992e33c9a9592e529aa, 9181b0d801dfcd7723a3ede201f0543078e2c149, 41c93960a066876d5e4f1dacaef75cd8daa2791f, 78e40584f0d149bf6f98beb5561b7b83cb68e1b1, 60caa5b3d066e13feac496fd0736e976970eb09f, aa9bf8b4c4ce8c28b9f47d7ce4f416aa9726bb0d, 15370f51d666ab8ef17185679553c6a8647b2a15, ad10ef93675513a68b93d54f3a461160b53318a3, 2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0, eb9e0da8b7170e3ca4364f2f9010599c2d2556f1, 0601e9e434b30320c316c76228b97c093fa98ad6, 7909428ca7d813331bbe4d33d07ab09e984b41b4, 71ba8a8c75e0826f94eb53ee7a4566d4fc5b5256, 90aca7b4cdd728b28b2fb5b4dc3ae3e37daa5b94, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4, 02fa2389b1b64b661192e224bed8af6df0ce80f6"
87048df918c34b662bc0d28894efa430d70a9206a,Crowdsourced Clustering: Querying Edges vs Triangles,https://www.semanticscholar.org/paper/87048df918c34b662bc0d28894efa430d70a9206,Conference,"We consider the task of clustering items using answers from non-expert crowd workers. In such cases, the workers are often not able to label the items directly, however, it is reasonable to assume that they can compare items and judge whether they are similar or not. An important question is what queries to make, and we compare two types: random edge queries, where a pair of items is revealed, and random triangles, where a triple is. Since it is far too expensive to query all possible edges and/or triangles, we need to work with partial observations subject to a fixed query budget constraint. When a generative model for the data is available (and we consider a few of these) we determine the cost of a query by its entropy; when such models do not exist we use the average response time per query of the workers as a surrogate for the cost. In addition to theoretical justification, through several simulations and experiments on two real data sets on Amazon Mechanical Turk, we empirically demonstrate that, for a fixed budget, triangle queries uniformly outperform edge queries. Even though, in contrast to edge queries, triangle queries reveal dependent edges, they provide more reliable edges and, for a fixed budget, many more of them. We also provide a sufficient condition on the number of observations, edge densities inside and outside the clusters and the minimum cluster size required for the exact recovery of the true adjacency matrix via triangle queries using a convex optimization-based clustering algorithm.",2015,45,"40275640, 1736279",40275640,Berlin,2,"2115338656, 2086632521",Y,It is also simple and easy to understand.;This section could be improved by demonstrating the approach on more datasets.,Neural Information Processing Systems,15,"items,queries,query,data querying,data querying","e32a2519b59d62cff6cb8136ee242dc3754ed57b, 98b9086750f08a21c8778ab986339321e9caf790, 079b57837221413bf99ab40999c77c29e280e0c2, b3dbe1b460a3b66df1653508c9eed7dd51dee4d2, 9e27190f2d9b2167d4a66b88696def4585072fd5, ba687027ed6012f613e1f9a9cefe7683bb192934, 75c364909914f17791837ec88090262aa6656d3e, 5d24ed8942235324512d6cedfd8dbf54c57658b4, 4267178106cef2e77284bde309dfaaf9fd46a91b, 0a71dd8bec060195e14eb9d0a7abbc08d960d4d5, 746a81aa26d3ebfb81acfd6af958d6a21603cd21, fc32074b37a6d9dda535a70f9689022e70508520, a22f3398ea865426c89ee66f4824ec626e56a864, 155f27879f185f1ab04107c91c2ae7cf6a910a03, da8b317b99c4b8933b2c59285639eca6c3fcb869, 4087e84fc695bb6433d0104ee94f9d7e9f4b7da5, 4bad6ed9a4cdec5912dcd21d12c0cf9291fe04c8, 3a083d843f891b3574494c385699c21766ce8b7a, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c, 8dd0c1e955c66092ff951941a151336211e6e171, b04550f0722e9614163855ab36fc2430b931a3fe, 0574de8b7ea2048a78ae9d2b5d26f315e6fa1ee7, 86dbd884043eb5807c61d2c65b813e673b4a04fa, d6bc29a897fd85e7187dc33c3c974b8879462237, ba9b6f805feb62c978d384211f910790643a023e, fee8f63972906214b77f16cfeca0b93ee8f36ba2, 6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91, 9c11d9e75c6136943d2fa7ff7cb1f1090d406658, a8d76d84408c1fe6b1543084e6cec3dfc4ede429, f156ecbbb9243522275490d698c6825f4d2e01af, 9bbcb01e7a6bc28ef6e2cda8ffe1ac8fb86def47, 40416ac3bf78583eea37661b1b446e9939245b3e, 73d4accea441aae2373828a8dc2175aa2759c38f, 38f5b53b49be555430f33b8363910191a3df1d14, 10cf0045bc0f58aa3699e4451f65b12a08019c5c, 3ca3ff98405b43fab32dcd7cbd6bd34261386e35, 0235c8697bf5ab8aef776d822746ce26fac0f7ba, 68897d00c4307d21ceaf1a5eb2a21340f2e94c66, 695bdc6e24608364491b9418a220c65a7cd17413, 48c73c389c3f36d70407eb8309a0b41578c15fc8, 6c739540e66e895311b7347971f10ef556e06e52, 590b617c08d34bc6caed7e4490c0b22a9c516e86, d9d325ca670a1aa215e3e39023f8abf17dae7584, f04d8e558c11ac0fc9318c4f3d61b651c62ddcfa, f9253a3e2627f1c2a0a7e2cea320a4ec4e4d2ff9, 659bdc9813de1667ebe9bd66dbbef78bc1ed0348, 1ab91d6ac7afc1a0121487a9089fa70edc1634d4"
7eaac9847257c32afd450017d1348ecda4dcaadea,NOUS: Construction and Querying of Dynamic Knowledge Graphs,https://www.semanticscholar.org/paper/7eaac9847257c32afd450017d1348ecda4dcaade,Conference,"The ability to construct domain specific knowledge graphs (KG) and perform question-answering or hypothesis generation is a transformative capability. Despite their value, automated construction of knowledge graphs remains an expensive technical challenge that is beyond the reach for most enterprises and academic institutions. We propose an end-toend framework for developing custom knowledge graph driven analytics for arbitrary application domains. The uniqueness of our system lies A) in its combination of curated KGs along with knowledge extracted from unstructured text, B) support for advanced trending and explanatory questions on a dynamic KG, and C) the ability to answer queries where the answer is embedded across multiple data sources.",2015,50,"7617146, 39073194, 1950842, 1786639, 1882948, 121917790, 143840196",7617146,Valletta,2,"2064711347, 46255467",Y,"Concerning the lack of mathematical rigour in the statistical physics literature on which the authors comment, they might want to relate to a very recent work https://arxiv.org/pdf/1708.03395.pdf work that sets all the past statistical physics results on optimal generalization in single layer neural networks on fully rigorous basis by proving that the corresponding formulas stemming from the replica method are indeed correct.;Also, the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies, i.e., a priori information.",IEEE International Conference on Data Engineering,32,"knowledge,ability,graphs,data querying,data querying","538fbdb8013ab43a9b5d725461b294ad29fcced7, da5d78b3e3a1544fde98fba86088e1215e97cbe8, 4e13a8e8ba8d33e15ed037bfca7c651047533990, 633e2fbfc0b21e959a244100937c5853afca4853, e30d9b8ce108d982169621b88a5e3fb69fec70e1, 73d4accea441aae2373828a8dc2175aa2759c38f, 8dce62b18bdea587c07cb4769a05ca0a816d09ba, 3faeb21fe256b99391d69570053a8c2d91e9f348, c5d3fbc024ad9a0d28669b6030de23fc5c3f7888, e8b7a9be9f2d0578a95319ed5841978e10429967, 3caf34532597683c980134579b156cd0d7db2f40, 68ade27b68c05d1b60b8a9d8f8aadbe76900c60c, c3df199cbca74763c4ae9889409bbd4aa29b6255, e02f91d625cd32290d4ede0f31284da115844316, ff75865cde62592d068b2afd055c57c81d77158b, 6ba00c2386f2edc0b43eec442cd1923b5d964633, 409896dee6e4e3de5fe0d62c3d8b78498d36229b, 53c9f3c34d8481adaf24df3b25581ccf1bc53f5c, 6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb, 8f9e864fab09bbae4a46a2a62bb954db1a88eb3e, 634fe61f3ab6692ea4733989a1c76e793bb8b69e, 343500e0052eb1b683f32b00efbbd1331c94184a, cf5dfc4a9f7a82b32640128ca10832eace55880e, 7b9b756ab509cb9f52dbac95e3e901d571f0784f, 4f480bae3196dbbc27ab383bce33478ea963f9b3, 182180bd69ea6d2f59225ded5ddc900b8558ab9f, 82870bc488b57cdf5ea62877109a7278af2926b3, 28a5a53dafacebad8a7c47773079caeffb9a5baa, ade9a900acc3c138021070537840488526796d35, 00bbc94806ca0821a9c82d8aedf16f0e6263b89f, 32524aa3ae8522542753ed7e6f4cca3970e4acab, 96273b87cd0eb9d1c9a12afae621ce2abdbbab36, 6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91, 88932b1563316e4ed7a61c4c2c0c90d7aa9bb308, e7f3478fd8aac6940a4bf4f5eb60ac38f6b0b85b, d2efa91bd7d076a66bc1d1c570f6aa2da944dd88, b05306f0b142e5afb3974b1b79996e5b82653662, b80e2af7ffa85fe2f02ea8390e4915d55c19d199, 19cf7458db4e17c7504eee24ccf961e1dc91435c, 7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e, e1e43d6bdb1419e08af833cf4899a460f70da26c, fe44200fed05f9a7c656f2245deded8fd5f5e1e6, 147c868b721c8d29df7c61db7f2360114c760614"
