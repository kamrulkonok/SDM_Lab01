paperId,title,url,publicationTypes,abstract,year,citationCount,journal_volume,authorIds,corresponding_author,venue,journal_name,keywords
58ed1fbaabe027345f7bb3a6312d41c5aac63e22,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,https://www.semanticscholar.org/paper/58ed1fbaabe027345f7bb3a6312d41c5aac63e22,JournalArticle,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",2020,1686,abs/2005.11401,"[145222654.0, 3439053.0, 1716179427.0, 40052301.0, 2067091563.0, 39589154.0, 103131985.0, 35084211.0, 144105277.0, 2620211.0, 48662861.0, 1743722.0]",145222654.0,Moscow,Neural Information Processing Systems,"['models', 'tasks', 'language', 'nlp', 'NLP']"
f4c4e148546089123f8da5db4fb246ab4062bd40,Evaluation of ChatGPT for NLP-based Mental Health Applications,https://www.semanticscholar.org/paper/f4c4e148546089123f8da5db4fb246ab4062bd40,JournalArticle,"Large language models (LLM) have been successful in several natural language understanding tasks and could be relevant for natural language processing (NLP)-based mental health application research. In this work, we report the performance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three text-based mental health classification tasks: stress detection (2-class classification), depression detection (2-class classification), and suicidality detection (5-class classification). We obtained annotated social media posts for the three classification tasks from public datasets. Then ChatGPT API classified the social media posts with an input prompt for classification. We obtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression detection, and suicidality detection, respectively. A baseline model that always predicted the dominant class resulted in F1 scores of 0.35, 0.60, and 0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a potential use of language models for mental health classification tasks.",2023,35,abs/2303.15727,[2120664.0],2120664.0,Reykjavik,arXiv.org,"['classification', 'detection', 'language', 'nlp', 'NLP']"
03532123ccffae8d411264320e8a5ae2b6eddea0,Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP,https://www.semanticscholar.org/paper/03532123ccffae8d411264320e8a5ae2b6eddea0,JournalArticle,"Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple""retrieve-then-read""pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp",2022,124,abs/2212.14024,"[144112155.0, 50818255.0, 32551341.0, 145385471.0, 145419642.0, 144922861.0, 143834867.0]",144112155.0,Vilnius,arXiv.org,"['dsp', 'rm', 'incontext', 'nlp', 'NLP']"
ce9ca56036307217ea565644d3d3bd74b879e045,Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP,https://www.semanticscholar.org/paper/ce9ca56036307217ea565644d3d3bd74b879e045,JournalArticle,"Abstract ⚠ This paper contains prompts and model outputs that are offensive in nature. When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model’s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1",2021,244,9,"[32246932.0, 40941940.0, 144418438.0]",32246932.0,San Marino,Transactions of the Association for Computational Linguistics,"['language', 'models', 'model', 'nlp', 'NLP']"
a0f788f6de0fb83d623c875a98120e3f347f70d1,Biomedical and clinical English model packages for the Stanza Python NLP library,https://www.semanticscholar.org/paper/a0f788f6de0fb83d623c875a98120e3f347f70d1,JournalArticle,"Abstract Objective The study sought to develop and evaluate neural natural language processing (NLP) packages for the syntactic analysis and named entity recognition of biomedical and clinical English text. Materials and Methods We implement and train biomedical and clinical English NLP pipelines by extending the widely used Stanza library originally designed for general NLP tasks. Our models are trained with a mix of public datasets such as the CRAFT treebank as well as with a private corpus of radiology reports annotated with 5 radiology-domain entities. The resulting pipelines are fully based on neural networks, and are able to perform tokenization, part-of-speech tagging, lemmatization, dependency parsing, and named entity recognition for both biomedical and clinical text. We compare our systems against popular open-source NLP libraries such as CoreNLP and scispaCy, state-of-the-art models such as the BioBERT models, and winning systems from the BioNLP CRAFT shared task. Results For syntactic analysis, our systems achieve much better performance compared with the released scispaCy models and CoreNLP models retrained on the same treebanks, and are on par with the winning system from the CRAFT shared task. For NER, our systems substantially outperform scispaCy, and are better or on par with the state-of-the-art performance from BioBERT, while being much more computationally efficient. Conclusions We introduce biomedical and clinical NLP packages built for the Stanza library. These packages offer performance that is similar to the state of the art, and are also optimized for ease of use. To facilitate research, we make all our models publicly available. We also provide an online demonstration (http://stanza.run/bio).",2021,101,28,"[49889487.0, 49889860.0, 50531624.0, 144783904.0, 2356307.0]",49889487.0,Prague,J. Am. Medical Informatics Assoc.,"['nlp', 'models', 'systems', 'packages', 'NLP']"
9a0424bdd12cdcdf45b556b0b9dcc6fc5a55520b,FORCES NLP: an efficient implementation of interior-point methods for multistage nonlinear nonconvex programs,https://www.semanticscholar.org/paper/9a0424bdd12cdcdf45b556b0b9dcc6fc5a55520b,JournalArticle,"ABSTRACT Real-time implementation of optimisation-based control and trajectory planning can be very challenging for nonlinear systems. As a result, if an implementation based on a fixed linearisation is not suitable, the nonlinear problems are typically locally approximated online, in order to leverage the speed and robustness of embedded solvers for convex quadratic programs (QP) developed during the last decade. The purpose of this paper is to demonstrate that, using simple standard building blocks from nonlinear programming, combined with a structure-exploiting linear system solver, it is possible to achieve computation times in the range typical of solvers for QPs, while retaining nonlinearities and solving the nonlinear programs (NLP) to local optimality. The implemented algorithm is an interior-point method with approximate Hessians and adaptive barrier rules, and is provided as an extension to the C code generator FORCES. Three detailed examples are provided that illustrate a significant improvement in control performance when solving NLPs, with computation times that are comparable with those achieved by fast approximate schemes and up to an order of magnitude faster than the state-of-the-art interior-point solver IPOPT.",2020,164,93,"[35437368.0, 2057413.0, 30521589.0, 144746954.0]",35437368.0,Chisinau,International Journal of Control,"['implementation', 'control', 'order', 'nlp', 'NLP']"
10aa2be24951e6de76b630482a645d79354c4cde,Quantifying Social Biases in NLP: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics,https://www.semanticscholar.org/paper/10aa2be24951e6de76b630482a645d79354c4cde,JournalArticle,"Abstract Measuring bias is key for better understanding and addressing unfairness in NLP/ML models. This is often done via fairness metrics, which quantify the differences in a model’s behaviour across a range of demographic groups. In this work, we shed more light on the differences and similarities between the fairness metrics used in NLP. First, we unify a broad range of existing metrics under three generalized fairness metrics, revealing the connections between them. Next, we carry out an extensive empirical comparison of existing metrics and demonstrate that the observed differences in bias measurement can be systematically explained via differences in parameter choices for our generalized metrics.",2021,68,9,"[147783277.0, 2065005.0, 39888194.0]",147783277.0,Ljubljana,Transactions of the Association for Computational Linguistics,"['metrics', 'differences', 'fairness', 'nlp', 'NLP']"
c30c0092bf4eb8a44faec3fc60cdd5006276bcdc,Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking,https://www.semanticscholar.org/paper/c30c0092bf4eb8a44faec3fc60cdd5006276bcdc,JournalArticle,"Graph neural networks (GNNs) have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs (e.g. syntactic trees or co-reference structures) contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected $L_0$ norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions.",2020,154,abs/2010.00577,"[8804828.0, 41019080.0, 144889265.0]",8804828.0,Andorra,International Conference on Learning Representations,"['model', 'gnns', 'nlp', 'models', 'NLP']"
6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786,BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models,https://www.semanticscholar.org/paper/6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786,JournalArticle,"Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose \Name, the first task-agnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.",2021,62,abs/2110.02467,"[5781518.0, 65844131.0, 2109329406.0, 16042895.0, 2146331573.0, 49298465.0, 2093474992.0]",5781518.0,Prague,International Conference on Learning Representations,"['nlp', 'models', 'tasks', 'backdoor', 'NLP']"
d88c1255876b62fb5f5a8b292098ca430710a540,The NLP Cookbook: Modern Recipes for Transformer Based Deep Learning Architectures,https://www.semanticscholar.org/paper/d88c1255876b62fb5f5a8b292098ca430710a540,JournalArticle,"In recent years, Natural Language Processing (NLP) models have achieved phenomenal success in linguistic and semantic tasks like text classification, machine translation, cognitive dialogue systems, information retrieval via Natural Language Understanding (NLU), and Natural Language Generation (NLG). This feat is primarily attributed due to the seminal Transformer architecture, leading to designs such as BERT, GPT (I, II, III), etc. Although these large-size models have achieved unprecedented performances, they come at high computational costs. Consequently, some of the recent NLP architectures have utilized concepts of transfer learning, pruning, quantization, and knowledge distillation to achieve moderate model sizes while keeping nearly similar performances as achieved by their predecessors. Additionally, to mitigate the data size challenge raised by language models from a knowledge extraction perspective, Knowledge Retrievers have been built to extricate explicit data documents from a large corpus of databases with greater efficiency and accuracy. Recent research has also focused on superior inference by providing efficient attention to longer input sequences. In this paper, we summarize and examine the current state-of-the-art (SOTA) NLP models that have been employed for numerous NLP tasks for optimal performance and efficiency. We provide a detailed understanding and functioning of the different architectures, a taxonomy of NLP designs, comparative evaluations, and future directions in NLP.",2021,54,9,"[2141026590.0, 143794144.0]",2141026590.0,Warsaw,IEEE Access,"['nlp', 'language', 'models', 'knowledge', 'NLP']"
da5d78b3e3a1544fde98fba86088e1215e97cbe8,All NLP Tasks Are Generation Tasks: A General Pretraining Framework,https://www.semanticscholar.org/paper/da5d78b3e3a1544fde98fba86088e1215e97cbe8,JournalArticle,"There have been various types of pretraining architectures including autoregressive models (e.g., GPT), autoencoding models (e.g., BERT), and encoder-decoder models (e.g., T5). On the other hand, NLP tasks are different in nature, with three main categories being classification, unconditional generation, and conditional generation. However, none of the pretraining frameworks performs the best for all tasks, which introduces inconvenience for model development and selection. We propose a novel pretraining framework GLM (General Language Model) to address this challenge. Compared to previous work, our architecture has three major benefits: (1) it performs well on classification, unconditional generation, and conditional generation tasks with one single pretrained model; (2) it outperforms BERT-like models on classification due to improved pretrain-finetune consistency; (3) it naturally handles variable-length blank filling which is crucial for many downstream tasks. Empirically, GLM substantially outperforms BERT on the SuperGLUE natural language understanding benchmark with the same amount of pre-training data. Moreover, GLM with 1.25× parameters of BERTLarge achieves the best performance in NLU, conditional and unconditional generation at the same time, which demonstrates its generalizability to different downstream tasks.1 Equal contribution Department of Computer Science and Technology, Tsinghua Univerisity, Beijing, China Beijing Academy of Artificial Intelligence, Beijing, China Massachusetts Institute of Technology, Cambridge, U.S.A. Recurrent AI, Ltd.. Correspondence to: Zhilin Yang <kimi_yang@rcrai.com>, Jie Tang <jietang@tsinghua.edu.cn>. The codes and pre-trained models are available at https: //github.com/THUDM/GLM All [START] NLP tasks are generation tasks All NLP tasks [END] are generation tasks",2021,49,abs/2103.10360,"[66395694.0, 5606742.0, 2111312892.0, 2055623340.0, 40125294.0, 2109512754.0, 2109541439.0]",66395694.0,Budapest,arXiv.org,"['generation', 'tasks', 'models', 'nlp', 'NLP']"
e02a757617c2c42eb62889cc4d4aee3765928303,The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus,https://www.semanticscholar.org/paper/e02a757617c2c42eb62889cc4d4aee3765928303,JournalArticle,"In order to address increasing demands of real-world applications, the research for knowledge-intensive NLP (KI-NLP) should advance by capturing the challenges of a truly open-domain environment: web-scale knowledge, lack of structure, inconsistent quality and noise. To this end, we propose a new setup for evaluating existing knowledge intensive tasks in which we generalize the background corpus to a universal web snapshot. We investigate a slate of NLP tasks which rely on knowledge - either factual or common sense, and ask systems to use a subset of CCNet - the Sphere corpus - as a knowledge source. In contrast to Wikipedia, otherwise a common background corpus in KI-NLP, Sphere is orders of magnitude larger and better reflects the full diversity of knowledge on the web. Despite potential gaps in coverage, challenges of scale, lack of structure and lower quality, we find that retrieval from Sphere enables a state of the art system to match and even outperform Wikipedia-based models on several tasks. We also observe that while a dense index can outperform a sparse BM25 baseline on Wikipedia, on Sphere this is not yet possible. To facilitate further research and minimise the community's reliance on proprietary, black-box search engines, we share our indices, evaluation metrics and infrastructure.",2021,48,abs/2112.09924,"[120174856.0, 40052301.0, 2067091563.0, 113568063.0, 2966239.0, 1410231361.0, 145222654.0, 1628391446.0, 3024698.0, 2072801764.0, 48662861.0]",120174856.0,Belgrade,arXiv.org,"['knowledge', 'sphere', 'nlp', 'tasks', 'NLP']"
df7336844a31165db0ae08f1cd0f560c9e3faeea,BadNL: Backdoor Attacks Against NLP Models,https://www.semanticscholar.org/paper/df7336844a31165db0ae08f1cd0f560c9e3faeea,JournalArticle,"Machine learning (ML) has progressed rapidly during the past decade and ML models have been deployed in various real-world applications. Meanwhile, machine learning models have been shown to be vulnerable to various security and privacy attacks. One attack that has attracted a great deal of attention recently is the backdoor attack. Specifically, the adversary poisons the target model training set, to mislead any input with an added secret trigger to a target class, while keeping the accuracy for original inputs unchanged. 
Previous backdoor attacks mainly focus on computer vision tasks. In this paper, we present the first systematic investigation of the backdoor attack against models designed for natural language processing (NLP) tasks. Specifically, we propose three methods to construct triggers in the NLP setting, including Char-level, Word-level, and Sentence-level triggers. Our Attacks achieve an almost perfect success rate without jeopardizing the original model utility. For instance, using the word-level triggers, our backdoor attack achieves 100% backdoor accuracy with only a drop of 0.18%, 1.26%, and 0.19% in the models utility, for the IMDB, Amazon, and Stanford Sentiment Treebank datasets, respectively.",2020,130,abs/2006.01043,"[151257231.0, 66697271.0, 144588806.0, 2026855.0, 2145954003.0]",151257231.0,Stockholm,arXiv.org,"['models', 'attack', 'backdoor', 'nlp', 'NLP']"
f9c602cc436a9ea2f9e7db48c77d924e09ce3c32,Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms,https://www.semanticscholar.org/paper/f9c602cc436a9ea2f9e7db48c77d924e09ce3c32,JournalArticle,"We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL",2017,6944,abs/1708.07747,"[145642373.0, 4565995.0, 2742129.0]",145642373.0,Sarajevo,arXiv.org,"['images', 'training', 'dataset', 'machine learning', 'Machine Learning']"
9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,JournalArticle,"TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.",2016,10416,abs/1603.04467,"[2057642721.0, 2078528337.0, 144758007.0, 2445241.0, 2545358.0, 48738717.0, 32131713.0, 36347083.0, 49959210.0, 145139947.0, 1780892.0, 153440022.0, 2064102917.0, 2060655766.0, 2090818.0, 39978391.0, 1944541.0, 40527594.0, 1942300.0, 3369421.0, 30415265.0, 3089272.0, 144375552.0, 20154699.0, 37232298.0, 144927151.0, 1789737.0, 32163737.0, 1701686.0, 35210462.0, 2080690.0, 2657155.0, 2053781980.0, 1765169.0, 1689108.0, 47941411.0, 145233583.0, 35078078.0, 2117163698.0, 2152198093.0]",2057642721.0,Belgrade,arXiv.org,"['interface', 'algorithms', 'implementation', 'machine learning', 'Machine Learning']"
597bd2e45427563cdf025e53a3239006aa364cfc,Open Graph Benchmark: Datasets for Machine Learning on Graphs,https://www.semanticscholar.org/paper/597bd2e45427563cdf025e53a3239006aa364cfc,JournalArticle,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .",2020,1841,abs/2005.00687,"[48594758.0, 3410500.0, 2095762.0, 2047998.0, 40046694.0, 2156641189.0, 1754926.0, 1702139.0]",48594758.0,Madrid,Neural Information Processing Systems,"['ogb', 'graph', 'datasets', 'machine learning', 'Machine Learning']"
69a72ff5b30642d11c96635e99aadad3140d33a7,CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation,https://www.semanticscholar.org/paper/69a72ff5b30642d11c96635e99aadad3140d33a7,JournalArticle,"Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.",2021,686,abs/2102.04664,"[2115338656.0, 2278834796.0, 50052368.0, 145505727.0, 47090739.0, 37488446.0, 2064509404.0, 1943097969.0, 71790825.0, 39483833.0, 1410115257.0, 2143359114.0, 24962156.0, 2135918679.0, 40626221.0, 50175330.0, 92660691.0, 46429989.0, 145507437.0, 1702996983.0, 2072784644.0, 1803054.0]",2115338656.0,Tallinn,NeurIPS Datasets and Benchmarks,"['codexglue', 'datasets', 'research', 'machine learning', 'Machine Learning']"
e2a85a6766b982ff7c8980e57ca6342d22493827,Adversarial Machine Learning at Scale,https://www.semanticscholar.org/paper/e2a85a6766b982ff7c8980e57ca6342d22493827,JournalArticle,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.",2016,2703,abs/1611.01236,"[145714153.0, 153440022.0, 1751569.0]",145714153.0,Rome,International Conference on Learning Representations,"['training', 'examples', 'models', 'machine learning', 'Machine Learning']"
7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e,CrypTen: Secure Multi-Party Computation Meets Machine Learning,https://www.semanticscholar.org/paper/7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e,JournalArticle,"Secure multi-party computation (MPC) allows parties to perform computations on data while keeping that data private. This capability has great potential for machine-learning applications: it facilitates training of machine-learning models on private data sets owned by different parties, evaluation of one party's private model using another party's private data, etc. Although a range of studies implement machine-learning models via secure MPC, such implementations are not yet mainstream. Adoption of secure MPC is hampered by the absence of flexible software frameworks that""speak the language""of machine-learning researchers and engineers. To foster adoption of secure MPC in machine learning, we present CrypTen: a software framework that exposes popular secure MPC primitives via abstractions that are common in modern machine-learning frameworks, such as tensor computations, automatic differentiation, and modular neural networks. This paper describes the design of CrypTen and measure its performance on state-of-the-art models for text classification, speech recognition, and image classification. Our benchmarks show that CrypTen's GPU support and high-performance communication between (an arbitrary number of) parties allows it to perform efficient private evaluation of modern machine-learning models under a semi-honest threat model. For example, two parties using CrypTen can securely predict phonemes in speech recordings using Wav2Letter faster than real-time. We hope that CrypTen will spur adoption of secure MPC in the machine-learning community.",2021,218,abs/2109.00984,"[2713842.0, 2262405.0, 144479015.0, 2264597.0, 3407874.0, 1803520.0]",2713842.0,Vaduz,Neural Information Processing Systems,"['machinelearning', 'mpc', 'parties', 'machine learning', 'Machine Learning']"
9e27190f2d9b2167d4a66b88696def4585072fd5,SoilGrids250m: Global gridded soil information based on machine learning,https://www.semanticscholar.org/paper/9e27190f2d9b2167d4a66b88696def4585072fd5,JournalArticle,"This paper describes the technical development and accuracy assessment of the most recent and improved version of the SoilGrids system at 250m resolution (June 2016 update). SoilGrids provides global predictions for standard numeric soil properties (organic carbon, bulk density, Cation Exchange Capacity (CEC), pH, soil texture fractions and coarse fragments) at seven standard depths (0, 5, 15, 30, 60, 100 and 200 cm), in addition to predictions of depth to bedrock and distribution of soil classes based on the World Reference Base (WRB) and USDA classification systems (ca. 280 raster layers in total). Predictions were based on ca. 150,000 soil profiles used for training and a stack of 158 remote sensing-based soil covariates (primarily derived from MODIS land products, SRTM DEM derivatives, climatic images and global landform and lithology maps), which were used to fit an ensemble of machine learning methods—random forest and gradient boosting and/or multinomial logistic regression—as implemented in the R packages ranger, xgboost, nnet and caret. The results of 10–fold cross-validation show that the ensemble models explain between 56% (coarse fragments) and 83% (pH) of variation with an overall average of 61%. Improvements in the relative accuracy considering the amount of variation explained, in comparison to the previous version of SoilGrids at 1 km spatial resolution, range from 60 to 230%. Improvements can be attributed to: (1) the use of machine learning instead of linear regression, (2) to considerable investments in preparing finer resolution covariate layers and (3) to insertion of additional soil profiles. Further development of SoilGrids could include refinement of methods to incorporate input uncertainties and derivation of posterior probability distributions (per pixel), and further automation of spatial modeling so that soil maps can be generated for potentially hundreds of soil variables. Another area of future research is the development of methods for multiscale merging of SoilGrids predictions with local and/or national gridded soil products (e.g. up to 50 m spatial resolution) so that increasingly more accurate, complete and consistent global soil information can be produced. SoilGrids are available under the Open Data Base License.",2017,2378,12,"[2856207.0, 7549589.0, 145253407.0, 9583743.0, 30504500.0, 9030720.0, 2228185935.0, 3376186.0, 3018223.0, 1402912902.0, 145669099.0, 145632581.0, 145028966.0, 49399380.0, 100653750.0, 32830771.0, 2924968.0, 2146245.0, 4953836.0]",2856207.0,Oslo,PLoS ONE,"['soil', 'soilgrids', 'resolution', 'machine learning', 'Machine Learning']"
2afa490dde7a8c582d889530c7f8b042fef6a8b7,Machine learning–accelerated computational fluid dynamics,https://www.semanticscholar.org/paper/2afa490dde7a8c582d889530c7f8b042fef6a8b7,JournalArticle,"Significance Accurate simulation of fluids is important for many science and engineering problems but is very computationally demanding. In contrast, machine-learning models can approximate physics very quickly but at the cost of accuracy. Here we show that using machine learning inside traditional fluid simulations can improve both accuracy and speed, even on examples very different from the training data. Our approach opens the door to applying machine learning to large-scale physical modeling tasks like airplane design and climate prediction. Numerical simulation of fluids plays an essential role in modeling many physical phenomena, such as weather, climate, aerodynamics, and plasma physics. Fluids are well described by the Navier–Stokes equations, but solving these equations at scale remains daunting, limited by the computational cost of resolving the smallest spatiotemporal features. This leads to unfavorable trade-offs between accuracy and tractability. Here we use end-to-end deep learning to improve approximations inside computational fluid dynamics for modeling two-dimensional turbulent flows. For both direct numerical simulation of turbulence and large-eddy simulation, our results are as accurate as baseline solvers with 8 to 10× finer resolution in each spatial dimension, resulting in 40- to 80-fold computational speedups. Our method remains stable during long simulations and generalizes to forcing functions and Reynolds numbers outside of the flows where it is trained, in contrast to black-box machine-learning approaches. Our approach exemplifies how scientific computing can leverage machine learning and hardware accelerators to improve simulations without sacrificing accuracy or generalization.",2021,536,118,"[40833997.0, 2119124568.0, 2078196952.0, 2286764736.0, 36397553.0, 2257229905.0]",40833997.0,Reykjavik,Proceedings of the National Academy of Sciences of the United States of America,"['simulation', 'accuracy', 'fluids', 'machine learning', 'Machine Learning']"
62df84d6a4d26f95e4714796c2337c9848cc13b5,MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems,https://www.semanticscholar.org/paper/62df84d6a4d26f95e4714796c2337c9848cc13b5,JournalArticle,"MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. 
This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.",2015,2146,abs/1512.01274,"[1913774.0, 2124778071.0, 2110420880.0, 1491081747.0, 48246959.0, 1508337194.0, 39102205.0, 2113742783.0, 151505981.0, 38448016.0]",1913774.0,Rome,arXiv.org,"['mxnet', 'expression', 'tensor', 'machine learning', 'Machine Learning']"
c5c4142a01981787a71bf6ebcb791520c458ab5d,FedML: A Research Library and Benchmark for Federated Machine Learning,https://www.semanticscholar.org/paper/c5c4142a01981787a71bf6ebcb791520c458ab5d,JournalArticle,"Federated learning is a rapidly growing research field in the machine learning domain. Although considerable research efforts have been made, existing libraries cannot adequately support diverse algorithmic development (e.g., diverse topology and flexible message exchange), and inconsistent dataset and model usage in experiments make fair comparisons difficult. In this work, we introduce FedML, an open research library and benchmark that facilitates the development of new federated learning algorithms and fair performance comparisons. FedML supports three computing paradigms (distributed training, mobile on-device training, and standalone simulation) for users to conduct experiments in different system environments. FedML also promotes diverse algorithmic research with flexible and generic API design and reference baseline implementations. A curated and comprehensive benchmark dataset for the non-I.I.D setting aims at making a fair comparison. We believe FedML can provide an efficient and reproducible means of developing and evaluating algorithms for the federated learning research community. We maintain the source code, documents, and user community at this https URL.",2020,420,abs/2007.13518,"[31927890.0, 2239461.0, 144491689.0, 2116031156.0, 2109798334.0, 2118775509.0, 2927870.0, 2034349211.0, 49660254.0, 2144035454.0, 144259957.0, 1505828520.0, 1614034792.0, 145711633.0, 153096457.0, 145599558.0, 121011351.0]",31927890.0,Andorra,arXiv.org,"['research', 'fedml', 'development', 'machine learning', 'Machine Learning']"
696b388ee6221c6dbcfd647a06883b2bfee773d9,Universal Differential Equations for Scientific Machine Learning,https://www.semanticscholar.org/paper/696b388ee6221c6dbcfd647a06883b2bfee773d9,JournalArticle,"
 In the context of science, the well-known adage “a picture is worth a thousand words” might well be “a model is worth a thousand datasets.” Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring ""big data"". In this work demonstrate how a mathematical object, which we denote universal differential equations (UDEs), can be utilized as a theoretical underpinning to a diverse array of problems in scientific machine learning to yield efficient algorithms and generalized approaches. The UDE model augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating the training of physics-informed neural networks and large-eddy simulations, can all be transformed into UDE training problems that are efficiently solved by a single software methodology.",2020,448,abs/2001.04385,"[5365695.0, 20859037.0, 118225834.0, 1482544386.0, 123251938.0, 93421340.0, 2056678977.0, 37288593.0]",5365695.0,Skopje,arXiv.org,"['data', 'model', 'models', 'machine learning', 'Machine Learning']"
31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a,Prediction of Heart Disease Using a Combination of Machine Learning and Deep Learning,https://www.semanticscholar.org/paper/31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a,JournalArticle,"The correct prediction of heart disease can prevent life threats, and incorrect prediction can prove to be fatal at the same time. In this paper different machine learning algorithms and deep learning are applied to compare the results and analysis of the UCI Machine Learning Heart Disease dataset. The dataset consists of 14 main attributes used for performing the analysis. Various promising results are achieved and are validated using accuracy and confusion matrix. The dataset consists of some irrelevant features which are handled using Isolation Forest, and data are also normalized for getting better results. And how this study can be combined with some multimedia technology like mobile devices is also discussed. Using deep learning approach, 94.2% accuracy was obtained.",2021,249,2021,"[2066165404.0, 6806161.0, 2274111800.0, 2758504.0, 1577665701.0, 2218973562.0]",2066165404.0,Vaduz,Computational Intelligence and Neuroscience,"['results', 'dataset', 'prediction', 'machine learning', 'Machine Learning']"
4d8f0ae904779a50b2e18fec49e51a5661a98d8a,MRI-Based Brain Tumor Classification Using Ensemble of Deep Features and Machine Learning Classifiers,https://www.semanticscholar.org/paper/4d8f0ae904779a50b2e18fec49e51a5661a98d8a,JournalArticle,"Brain tumor classification plays an important role in clinical diagnosis and effective treatment. In this work, we propose a method for brain tumor classification using an ensemble of deep features and machine learning classifiers. In our proposed framework, we adopt the concept of transfer learning and uses several pre-trained deep convolutional neural networks to extract deep features from brain magnetic resonance (MR) images. The extracted deep features are then evaluated by several machine learning classifiers. The top three deep features which perform well on several machine learning classifiers are selected and concatenated as an ensemble of deep features which is then fed into several machine learning classifiers to predict the final output. To evaluate the different kinds of pre-trained models as a deep feature extractor, machine learning classifiers, and the effectiveness of an ensemble of deep feature for brain tumor classification, we use three different brain magnetic resonance imaging (MRI) datasets that are openly accessible from the web. Experimental results demonstrate that an ensemble of deep features can help improving performance significantly, and in most cases, support vector machine (SVM) with radial basis function (RBF) kernel outperforms other machine learning classifiers, especially for large datasets.",2021,223,21,"[2837279.0, 48740398.0, 1969352.0]",2837279.0,San Marino,Italian National Conference on Sensors,"['machine', 'features', 'classifiers', 'machine learning', 'Machine Learning']"
74b4f16c5ac91e3e7c88ae81cc8c91416b71d151,Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning,https://www.semanticscholar.org/paper/74b4f16c5ac91e3e7c88ae81cc8c91416b71d151,JournalArticle,"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.",2020,311,abs/2002.05651,"[40068904.0, 2143911040.0, 8365320.0, 2563117.0, 1746807.0, 145134886.0]",40068904.0,Riga,arXiv.org,"['energy', 'carbon', 'machine', 'machine learning', 'Machine Learning']"
573fd2ce97c70bb29097e8efb28a27af791225ca,BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain,https://www.semanticscholar.org/paper/573fd2ce97c70bb29097e8efb28a27af791225ca,JournalArticle,"Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a \emph{BadNet}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of {25}\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.",2017,1296,abs/1708.06733,"[2367353.0, 1398683279.0, 1696125.0]",2367353.0,Helsinki,arXiv.org,"['networks', 'network', 'sign', 'machine learning', 'Machine Learning']"
f86f1748d1b6d22870f4347fd5d65314ba800583,Reconciling modern machine-learning practice and the classical bias–variance trade-off,https://www.semanticscholar.org/paper/f86f1748d1b6d22870f4347fd5d65314ba800583,JournalArticle,"Significance While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.",2018,1268,116,"[145520115.0, 143724861.0, 143791100.0, 151213231.0]",145520115.0,Andorra,Proceedings of the National Academy of Sciences of the United States of America,"['performance', 'models', 'data', 'machine learning', 'Machine Learning']"
d0ab11de3077490c80a08abd0fb8827bac84c454,MoleculeNet: a benchmark for molecular machine learning,https://www.semanticscholar.org/paper/d0ab11de3077490c80a08abd0fb8827bac84c454,JournalArticle,"A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms.",2017,1272,9,"[9957625.0, 2378027.0, 5932099.0, 145986494.0, 9959840.0, 5929246.0, 40867019.0, 1806271.0]",9957625.0,Nicosia,Chemical Science,"['scale', 'benchmark', 'machine', 'machine learning', 'Machine Learning']"
71a85e735a3686bef8cce3725ae5ba82e2cabb1b,Underspecification Presents Challenges for Credibility in Modern Machine Learning,https://www.semanticscholar.org/paper/71a85e735a3686bef8cce3725ae5ba82e2cabb1b,JournalArticle,"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.",2020,546,23,"[1396841807.0, 145993598.0, 40497400.0, 1874006.0, 48401711.0, 2638246.0, 2110195795.0, 1695378.0, 144154709.0, 28552618.0, 2420527.0, 2815290.0, 3108448.0, 3451901.0, 6413143.0, 34302129.0, 2146275249.0, 6322777.0, 2007712128.0, 4836115.0, 145071265.0, 81408931.0, 144223091.0, 2065412355.0, 25897803.0, 2035210.0, 88478180.0, 144042306.0, 3212089.0, 6454443.0, 2007741250.0, 46537606.0, 2974320.0, 3316311.0, 1524732527.0, 20825661.0, 2856607.0, 2520251.0, 2743563.0, 1733143.0]",1396841807.0,Dublin,Journal of machine learning research,"['pipelines', 'domains', 'underspecification', 'machine learning', 'Machine Learning']"
2bc3644ce4de7fce5812c1455e056649a47c1bbf,Effective Heart Disease Prediction Using Hybrid Machine Learning Techniques,https://www.semanticscholar.org/paper/2bc3644ce4de7fce5812c1455e056649a47c1bbf,JournalArticle,"Heart disease is one of the most significant causes of mortality in the world today. Prediction of cardiovascular disease is a critical challenge in the area of clinical data analysis. Machine learning (ML) has been shown to be effective in assisting in making decisions and predictions from the large quantity of data produced by the healthcare industry. We have also seen ML techniques being used in recent developments in different areas of the Internet of Things (IoT). Various studies give only a glimpse into predicting heart disease with ML techniques. In this paper, we propose a novel method that aims at finding significant features by applying machine learning techniques resulting in improving the accuracy in the prediction of cardiovascular disease. The prediction model is introduced with different combinations of features and several known classification techniques. We produce an enhanced performance level with an accuracy level of 88.7% through the prediction model for heart disease with the hybrid random forest with a linear model (HRFLM).",2019,791,7,"[150302778.0, 9727014.0, 144369609.0]",150302778.0,Rome,IEEE Access,"['disease', 'prediction', 'techniques', 'machine learning', 'Machine Learning']"
a0a79dad89857a96f8f71b14238e5237cbfc4787,Judging LLM-as-a-judge with MT-Bench and Chatbot Arena,https://www.semanticscholar.org/paper/a0a79dad89857a96f8f71b14238e5237cbfc4787,JournalArticle,"Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",2023,913,abs/2306.05685,"[2149970173.0, 2537924.0, 2209360681.0, 92721493.0, 1390573666.0, 2152482391.0, 143872641.0, 2141335450.0, 2117961435.0, 143977260.0, 145140331.0, 144307989.0, 2055174324.0]",2149970173.0,Tallinn,Neural Information Processing Systems,"['preferences', 'benchmarks', 'agreement', 'llm', 'LLM']"
e2a58fd18961c3941102989e3a3d0d27c615e015,Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic,https://www.semanticscholar.org/paper/e2a58fd18961c3941102989e3a3d0d27c615e015,JournalArticle,"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.",2023,196,abs/2306.15195,"[32811782.0, 2156120640.0, 13886055.0, 2109975984.0, 2075369514.0, 1395873384.0]",32811782.0,Copenhagen,arXiv.org,"['regions', 'language', 'tasks', 'llm', 'LLM']"
7a1e71cb1310c4a873e7a4e54d1a6dab0553adce,"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",https://www.semanticscholar.org/paper/7a1e71cb1310c4a873e7a4e54d1a6dab0553adce,JournalArticle,"Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.",2023,377,abs/2306.01116,"[2179371701.0, 1739899643.0, 80424302.0, 101370046.0, 2069293685.0, 2219490144.0, 91723794.0, 1967677.0, 143945447.0]",2179371701.0,Prague,arXiv.org,"['models', 'data', 'web', 'llm', 'LLM']"
929305892d4ddae575a0fc23227a8139f7681632,Jailbroken: How Does LLM Safety Training Fail?,https://www.semanticscholar.org/paper/929305892d4ddae575a0fc23227a8139f7681632,JournalArticle,"Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of""jailbreak""attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes.",2023,219,abs/2307.02483,"[143797846.0, 3033269.0, 5164568.0]",143797846.0,Bern,Neural Information Processing Systems,"['safety', 'models', 'failure', 'llm', 'LLM']"
2c5ab7d87e3342d2dba7d1d113ca1b16c545e344,AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration,https://www.semanticscholar.org/paper/2c5ab7d87e3342d2dba7d1d113ca1b16c545e344,JournalArticle,"Large language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth). In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. Our method is based on the observation that weights are not equally important: protecting only 1% of salient weights can greatly reduce quantization error. We then propose to search for the optimal per-channel scaling that protects the salient weights by observing the activation, not weights. AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs' generalization ability on different domains and modalities, without overfitting to the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks. Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement an efficient and flexible inference framework tailored for LLMs on the edge, offering more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPU (NVIDIA Jetson Orin 64GB).",2023,147,abs/2306.00978,"[46698300.0, 2214687479.0, 150127950.0, 2202210853.0, 2219266839.0, 2115659426.0]",46698300.0,Prague,arXiv.org,"['quantization', 'awq', 'weights', 'llm', 'LLM']"
003ef1cd670d01af05afa0d3c72d72228f494432,LLM+P: Empowering Large Language Models with Optimal Planning Proficiency,https://www.semanticscholar.org/paper/003ef1cd670d01af05afa0d3c72d72228f494432,JournalArticle,"Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems.\footnote{The code and results are publicly available at https://github.com/Cranial-XIX/llm-pddl.git.",2023,157,abs/2304.11477,"[145306564.0, 48751979.0, 2682457.0, 2155193246.0, 40295359.0, 3045593.0, 2113909888.0]",145306564.0,Bucharest,arXiv.org,"['language', 'llms', 'planning', 'llm', 'LLM']"
1a4c6856292b8c64d19a812a77f0aa6fd47cb96c,AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework,https://www.semanticscholar.org/paper/1a4c6856292b8c64d19a812a77f0aa6fd47cb96c,JournalArticle,"This technical report presents AutoGen , 1 a new framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable , and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. AutoGen ’s design offers multiple advantages: a) it gracefully navigates the strong but imperfect generation and reasoning abilities of these LLMs; b) it leverages human understanding and intelligence, while providing valuable automation through conversations between agents; c) it simplifies and unifies the implementation of complex LLM workflows as automated agent chats. We provide many diverse examples of how developers can easily use AutoGen to effectively solve tasks or build applications, ranging from coding, mathematics, operations research, entertainment, online decision-making, question answering, etc.",2023,131,abs/2308.08155,"[1777934740.0, 33340656.0, 47540245.0, 2115853457.0, 2116579935.0, 3055912.0, 3369602.0, 2231868268.0, 2232024704.0, 2116627318.0]",1777934740.0,Sofia,arXiv.org,"['autogen', 'llm', 'agents', 'applications', 'LLM']"
fa75a55760e6ea49b39b83cb85c99a22e1088254,NExT-GPT: Any-to-Any Multimodal LLM,https://www.semanticscholar.org/paper/fa75a55760e6ea49b39b83cb85c99a22e1088254,JournalArticle,"While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/",2023,110,abs/2309.05519,"[1957924118.0, 2142672912.0, 1990265392.0, 144540018.0, 144078686.0]",1957924118.0,Vaduz,arXiv.org,"['modalities', 'content', 'nextgpt', 'llm', 'LLM']"
4be7d1524edb0137599a5cc95f72844b85a52fe1,LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,https://www.semanticscholar.org/paper/4be7d1524edb0137599a5cc95f72844b85a52fe1,JournalArticle,"Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.",2022,337,abs/2208.07339,"[3239480.0, 35084211.0, 2037496520.0, 1982950.0]",3239480.0,Warsaw,arXiv.org,"['performance', 'models', 'inference', 'llm', 'LLM']"
db4cf9f6a653d5c15973e836c800ea47743251ae,Prompt Injection attack against LLM-integrated Applications,https://www.semanticscholar.org/paper/db4cf9f6a653d5c15973e836c800ea47743251ae,JournalArticle,"Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.",2023,84,abs/2306.05499,"[2153627626.0, 73776889.0, 22799258.0, 3088630.0, 2146331573.0, 39584070.0, 51225422.0, 2124949853.0, 2152798056.0]",2153627626.0,Rome,arXiv.org,"['injection', 'applications', 'attack', 'llm', 'LLM']"
ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7,ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,https://www.semanticscholar.org/paper/ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7,JournalArticle,"Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.",2023,102,abs/2308.07201,"[2151547817.0, 2109136284.0, 48576745.0, 2231240312.0, 2195745651.0, 2222862479.0, 2215497308.0]",2151547817.0,Zagreb,arXiv.org,"['evaluation', 'llms', 'language', 'llm', 'LLM']"
017010b941d902a467f6d329ae5e74fd67e67912,LLM-Pruner: On the Structural Pruning of Large Language Models,https://www.semanticscholar.org/paper/017010b941d902a467f6d329ae5e74fd67e67912,JournalArticle,"Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner",2023,77,abs/2305.11627,"[15532066.0, 150110431.0, 48631088.0]",15532066.0,Bratislava,Neural Information Processing Systems,"['llms', 'language', 'models', 'llm', 'LLM']"
0983883619a0ca597d055d0e58da2f514052913d,"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration",https://www.semanticscholar.org/paper/0983883619a0ca597d055d0e58da2f514052913d,JournalArticle,"Although instruction-tuned large language models (LLMs) have exhibited remarkable capabilities across various NLP tasks, their effectiveness on other data modalities beyond text has not been fully studied. In this work, we propose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual, audio, and textual information. Macaw-LLM consists of three main components: a modality module for encoding multi-modal data, a cognitive module for harnessing pretrained LLMs, and an alignment module for harmonizing diverse representations. Our novel alignment module seamlessly bridges multi-modal features to textual features, simplifying the adaptation process from the modality modules to the cognitive module. In addition, we construct a large-scale multi-modal instruction dataset in terms of multi-turn dialogue, including 69K image instances and 50K video instances. We have made our data, code and model publicly available, which we hope can pave the way for future research in multi-modal LLMs and expand the capabilities of LLMs to handle diverse data modalities and address complex real-world scenarios.",2023,62,abs/2306.09093,"[2082426870.0, 2145209409.0, 1800190.0, 14799547.0, 47655790.0, 2112455515.0, 2072684668.0, 2909321.0]",2082426870.0,Paris,arXiv.org,"['module', 'llms', 'data', 'llm', 'LLM']"
51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df,SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression,https://www.semanticscholar.org/paper/51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df,JournalArticle,"Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B parameter LLM on a single 24 GB consumer GPU without any performance degradation at 15% speedup thus making powerful LLMs available to consumer without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime. Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.",2023,60,abs/2306.03078,"[3239480.0, 2219555303.0, 52257721.0, 2006108901.0, 1502248377.0, 9543395.0, 2113838061.0, 1713648.0, 3311387.0]",3239480.0,Luxembourg,arXiv.org,"['llms', 'quantization', 'parameter', 'llm', 'LLM']"
f51bc74814a3452009ea5ca262d9768d08149ee6,Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model,https://www.semanticscholar.org/paper/f51bc74814a3452009ea5ca262d9768d08149ee6,JournalArticle,"The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks. Inspired by such successes, we adopt such an instruction-tuned LLM Flan-T5 as the text encoder for text-to-audio (TTA) generation -- a task where the goal is to generate an audio from its textual description. The prior works on TTA either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5. Consequently, our latent diffusion model (LDM)-based approach TANGO outperforms the state-of-the-art AudioLDM on most metrics and stays comparable on the rest on AudioCaps test set, despite training the LDM on a 63 times smaller dataset and keeping the text encoder frozen. This improvement might also be attributed to the adoption of audio pressure level-based sound mixing for training set augmentation, whereas the prior methods take a random mix.",2023,53,abs/2304.13731,"[32528506.0, 35122767.0, 3391951.0, 1746416.0]",32528506.0,London,arXiv.org,"['encoder', 'language', 'llm', 'text', 'LLM']"
43e6e8d6663d83f1b74cf5a2be7b040b0928f867,X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages,https://www.semanticscholar.org/paper/43e6e8d6663d83f1b74cf5a2be7b040b0928f867,JournalArticle,"Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces independently. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. Our experiments show that X-LLM demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 84.5\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. And we also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.",2023,52,abs/2305.04160,"[49102717.0, 49835220.0, 2112675689.0, 2120251897.0, 145458655.0, 2111044238.0, 145764891.0]",49102717.0,Prague,arXiv.org,"['x2l', 'language', 'llms', 'llm', 'LLM']"
6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2,LLM-QAT: Data-Free Quantization Aware Training for Large Language Models,https://www.semanticscholar.org/paper/6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2,JournalArticle,"Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings.",2023,50,abs/2305.17888,"[2109370860.0, 9185192.0, 2112729504.0, 48025720.0, 37502184.0, 2121361882.0, 152345059.0, 2065915235.0, 144137037.0]",2109370860.0,Chisinau,arXiv.org,"['quantization', 'methods', 'model', 'llm', 'LLM']"
ccd94602e3acecf999d0c9ba62b1a8bc02e9f696,PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization,https://www.semanticscholar.org/paper/ccd94602e3acecf999d0c9ba62b1a8bc02e9f696,JournalArticle,"Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our results indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation ability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage. All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM.",2023,67,abs/2306.05087,"[2108024273.0, 2164113313.0, 1557363420.0, 2145500840.0, 35504092.0, 2051536212.0, 1657285750.0, 2143721734.0, 1519290245.0, 1576441343.0, 145235149.0, 2145403564.0, 2211964951.0]",2108024273.0,Bucharest,arXiv.org,"['pandalm', 'evaluation', 'models', 'llm', 'LLM']"
e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0,LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models,https://www.semanticscholar.org/paper/e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0,JournalArticle,"Recent advancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse images. However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning. This work proposes to enhance prompt understanding capabilities in diffusion models. Our method leverages a pretrained large language model (LLM) for grounded generation in a novel two-stage process. In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off-the-shelf diffusion model for layout-grounded image generation. Both stages utilize existing pretrained models without additional model parameter optimization. Our method significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts that require various capabilities, doubling the generation accuracy across four tasks on average. Furthermore, our method enables instruction-based multi-round scene specification and can handle prompts in languages not supported by the underlying diffusion model. We anticipate that our method will unleash users' creativity by accurately following more complex prompts. Our code, demo, and benchmark are available at: https://llm-grounded-diffusion.github.io",2023,45,abs/2305.13655,"[2054733874.0, 2132473300.0, 3383328.0, 1753210.0]",2054733874.0,Stockholm,arXiv.org,"['diffusion', 'model', 'models', 'llm', 'LLM']"
16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277,Time-LLM: Time Series Forecasting by Reprogramming Large Language Models,https://www.semanticscholar.org/paper/16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277,JournalArticle,"Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.",2023,44,abs/2310.01728,"[2254096428.0, 2255363760.0, 2253908414.0, 2237992280.0, 2253786576.0, 2119204984.0, 2254173316.0, 2253824408.0, 2256011160.0, 2254047333.0, 2253561592.0]",2254096428.0,Tirana,arXiv.org,"['time', 'series', 'models', 'llm', 'LLM']"
92930ed3560ea6c86d53cf52158bc793b089054d,BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset,https://www.semanticscholar.org/paper/92930ed3560ea6c86d53cf52158bc793b089054d,JournalArticle,"In this paper, we introduce the \textsc{BeaverTails} dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 30,207 question-answer (QA) pairs and 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails. Warning: this paper contains example data that may be offensive or harmful.",2023,63,abs/2307.04657,"[2154630502.0, 2210950163.0, 14548852.0, 2190800297.0, 2221446410.0, 2221566410.0, 2217316509.0, 47796324.0]",2154630502.0,Amsterdam,Neural Information Processing Systems,"['safety', 'llms', 'harmlessness', 'llm', 'LLM']"
d1a6b3a5efde3783b53f822dc8dd00aaac934b95,SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification,https://www.semanticscholar.org/paper/d1a6b3a5efde3783b53f822dc8dd00aaac934b95,JournalArticle,"The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree veriﬁcation. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM’s outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is veriﬁed by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree veriﬁer instead of an incremental decoder, which signiﬁcantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.",2023,42,abs/2305.09781,"[1720763480.0, 2638632.0, 2185953295.0, 2217941981.0, 2188983263.0, 2217487123.0, 2111498904.0, 1389546686.0, 2217683047.0, 2072782550.0]",1720763480.0,Skopje,arXiv.org,"['tree', 'llm', 'language', 'models', 'LLM']"
54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f,ImageBind-LLM: Multi-modality Instruction Tuning,https://www.semanticscholar.org/paper/54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f,JournalArticle,"We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by a proposed visual cache model for further cross-modal embedding enhancement. The training-free cache model retrieves from three million image features extracted by ImageBind, which effectively mitigates the training-inference modality discrepancy. Notably, with our approach, ImageBind-LLM can respond to instructions of diverse modalities and demonstrate significant language generation quality. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.",2023,41,abs/2309.03905,"[150147382.0, 2115713503.0, 1485702259.0, 144740494.0, 2153917002.0, 2238398546.0, 3393556.0, 2238219059.0, 2238210433.0, 2237599228.0, 2238395310.0, 2204576896.0, 2125957.0, 2238219937.0, 27577617.0, 49404547.0, 2059129841.0]",150147382.0,Monaco,arXiv.org,"['multimodality', 'image', 'imagebindllm', 'llm', 'LLM']"
34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61,"LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked",https://www.semanticscholar.org/paper/34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61,JournalArticle,"Large language models (LLMs) are popular for high-quality text generation but can produce harmful content, even when aligned with human values through reinforcement learning. Adversarial prompts can bypass their safety measures. We propose LLM Self Defense, a simple approach to defend against these attacks by having an LLM screen the induced responses. Our method does not require any fine-tuning, input preprocessing, or iterative output generation. Instead, we incorporate the generated content into a pre-defined prompt and employ another instance of an LLM to analyze the text and predict whether it is harmful. We test LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent LLMs against various types of attacks, such as forcefully inducing affirmative responses to prompts and prompt engineering attacks. Notably, LLM Self Defense succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5 and Llama 2.",2023,41,abs/2308.07308,"[153223021.0, 2133413904.0, 2070368863.0, 1793506.0]",153223021.0,Monaco,arXiv.org,"['llm', 'self', 'defense', 'attacks', 'LLM']"
1ab91d6ac7afc1a0121487a9089fa70edc1634d4,Certifying LLM Safety against Adversarial Prompting,https://www.semanticscholar.org/paper/1ab91d6ac7afc1a0121487a9089fa70edc1634d4,JournalArticle,"Large language models (LLMs) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an LLM and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees. Given a prompt, our procedure erases tokens individually and inspects the resulting subsequences using a safety filter. Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size. We implement the safety filter in two ways, using Llama 2 and DistilBERT, and compare the performance of erase-and-check for the two cases. We defend against three attack modes: i) adversarial suffix, where an adversarial sequence is appended at the end of a harmful prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Our experimental results demonstrate that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts. Additionally, we propose three efficient empirical defenses: i) RandEC, a randomized subsampling version of erase-and-check; ii) GreedyEC, which greedily erases tokens that maximize the softmax score of the harmful class; and iii) GradEC, which uses gradient information to optimize tokens to erase. We demonstrate their effectiveness against adversarial prompts generated by the Greedy Coordinate Gradient (GCG) attack algorithm. The code for our experiments is available at https://github.com/aounon/certified-llm-safety.",2023,41,abs/2309.02705,"[31910622.0, 40228633.0, 2822290.0, 34389431.0, 1892673.0]",31910622.0,Oslo,arXiv.org,"['safety', 'prompts', 'tokens', 'llm', 'LLM']"
22ebfc211d184ed615729378a43fde175bf14478,"Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following",https://www.semanticscholar.org/paper/22ebfc211d184ed615729378a43fde175bf14478,JournalArticle,"We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM.",2023,37,abs/2309.00615,"[2237599228.0, 2115713503.0, 2237590429.0, 2193267071.0, 1387903470.0, 150147382.0, 32811782.0, 144740494.0, 2237583550.0, 49404547.0, 1714602.0]",2237599228.0,Minsk,arXiv.org,"['3d', 'pointbind', 'multimodality', 'llm', 'LLM']"
7637ed79d30d0139901175ae4abedd822c217ab4,3D-LLM: Injecting the 3D World into Large Language Models,https://www.semanticscholar.org/paper/7637ed79d30d0139901175ae4abedd822c217ab4,JournalArticle,"Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Speciﬁcally, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efﬁciently train 3D-LLMs, we ﬁrst utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin ( e.g. , the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https:",2023,60,abs/2307.12981,"[151261268.0, 2226286961.0, 2158502526.0, 9696154.0, 15394275.0, 2111329651.0, 2056157586.0]",151261268.0,Riga,Neural Information Processing Systems,"['3d', '3dllms', 'models', 'llm', 'LLM']"
ce212cb873a54e5716da53a66b10298ac013008a,BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents,https://www.semanticscholar.org/paper/ce212cb873a54e5716da53a66b10298ac013008a,JournalArticle,"The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, \textit{i.e.} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at \url{https://github.com/salesforce/BOLAA}.",2023,34,abs/2308.05960,"[2223887365.0, 2087699735.0, 2108313930.0, 2147380988.0, 71926704.0, 2223748790.0, 22758695.0, 5478513.0, 9200530.0, 2309967.0, 2115800155.0, 2122258484.0, 46507194.0, 2054594326.0, 1702137.0]",2223887365.0,Luxembourg,arXiv.org,"['laa', 'laas', 'llm', 'llms', 'LLM']"
e3052ebca5eeae6a8a73e44517903d39746f5f3a,From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning,https://www.semanticscholar.org/paper/e3052ebca5eeae6a8a73e44517903d39746f5f3a,JournalArticle,"In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model's expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both efficiency and resource-conscious advancements. Codes, data, and models are available: https://github.com/tianyi-lab/Cherry_LLM",2023,35,abs/2308.12032,"[2150655891.0, 2144289768.0, 2111336489.0, 1391200710.0, 2108451006.0, 145292435.0, 66063851.0, 2213956781.0, 91353860.0]",2150655891.0,Bern,arXiv.org,"['models', 'data', 'instruction', 'llm', 'LLM']"
fb00016c1e048b9373803add001c1ec7e877cb23,Head-to-Tail: How Knowledgeable are Large Language Models (LLM)? A.K.A. Will LLMs Replace Knowledge Graphs?,https://www.semanticscholar.org/paper/fb00016c1e048b9373803add001c1ec7e877cb23,JournalArticle,"Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs? To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.",2023,50,abs/2308.10168,"[49871029.0, 15574937.0, 47291370.0, 2229372676.0, 2152050780.0]",49871029.0,Tallinn,arXiv.org,"['llms', 'knowledge', 'llm', 'terms', 'LLM']"
ead6121fbc787d508dc6a6d7106f72bf0d647d03,Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents,https://www.semanticscholar.org/paper/ead6121fbc787d508dc6a6d7106f72bf0d647d03,JournalArticle,"In this paper, we present a novel framework for enhancing the capabilities of large language models (LLMs) by leveraging the power of multi-agent systems. Our framework introduces a collaborative environment where multiple intelligent agent components, each with distinctive attributes and roles, work together to handle complex tasks more efficiently and effectively. We demonstrate the practicality and versatility of our framework through case studies in artificial general intelligence (AGI), specifically focusing on the Auto-GPT and BabyAGI models. We also examine the""Gorilla""model, which integrates external APIs into the LLM. Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations. By modeling various domains such as courtroom simulations and software development scenarios, we showcase the potential applications and benefits of our proposed multi-agent system. Our framework provides an avenue for advancing the capabilities and performance of LLMs through collaboration and knowledge exchange among intelligent agents.",2023,50,abs/2306.03314,"[2237440339.0, 2163313042.0]",2237440339.0,Bern,arXiv.org,"['framework', 'capabilities', 'models', 'llm', 'LLM']"
8b28792f8405b737229afb92c99c579b86d8aa98,Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations,https://www.semanticscholar.org/paper/8b28792f8405b737229afb92c99c579b86d8aa98,JournalArticle,"We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.",2023,29,abs/2312.06674,"[2065277797.0, 17097160.0, 2273682220.0, 150282885.0, 2273645788.0, 2272672481.0, 2273645419.0, 2273679997.0, 2223748737.0, 2273657478.0, 2072010.0]",2065277797.0,Bucharest,arXiv.org,"['llama', 'guard', 'model', 'llm', 'LLM']"
43e624ddeed82df944a6cae0dedec3372438e243,Accelerating LLM Inference with Staged Speculative Decoding,https://www.semanticscholar.org/paper/43e624ddeed82df944a6cae0dedec3372438e243,JournalArticle,"Recent advances with large language models (LLM) illustrate their diverse capabilities. We propose a novel algorithm, staged speculative decoding, to accelerate LLM inference in small-batch, on-device scenarios. We address the low arithmetic intensity of small-batch inference by improving upon previous work in speculative decoding. First, we restructure the speculative batch as a tree, which reduces generation costs and increases the expected tokens per batch. Second, we add a second stage of speculative decoding. Taken together, we reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L model while perfectly preserving output quality.",2023,28,abs/2308.04623,"[2683016.0, 82614785.0]",2683016.0,Brussels,arXiv.org,"['llm', 'inference', 'speculative', 'batch', 'LLM']"
10a0541be17d10d922ffc68a3dae55a13d9c1ab9,LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code Generation,https://www.semanticscholar.org/paper/10a0541be17d10d922ffc68a3dae55a13d9c1ab9,JournalArticle,"There has been a recent explosion of research on Large Language Models (LLMs) for software engineering tasks, in particular code generation. However, results from LLMs can be highly unstable; nondeterministically returning very different codes for the same prompt. Non-determinism is a potential menace to scientific conclusion validity. When non-determinism is high, scientific conclusions simply cannot be relied upon unless researchers change their behaviour to control for it in their empirical analyses. This paper conducts an empirical study to demonstrate that non-determinism is, indeed, high, thereby underlining the need for this behavioural change. We choose to study ChatGPT because it is already highly prevalent in the code generation research literature. We report results from a study of 829 code generation problems from three code generation benchmarks (i.e., CodeContests, APPS, and HumanEval). Our results reveal high degrees of non-determinism: the ratio of coding tasks with zero equal test output across different requests is 72.73%, 60.40%, and 65.85% for CodeContests, APPS, and HumanEval, respectively. In addition, we find that setting the temperature to 0 does not guarantee determinism in code generation, although it indeed brings less non-determinism than the default configuration (temperature=1). These results confirm that there is, currently, a significant threat to scientific conclusion validity. In order to put LLM-based research on firmer scientific foundations, researchers need to take into account non-determinism in drawing their conclusions.",2023,38,abs/2308.02828,"[1492047220.0, 51250527.0, 145836176.0, 2146058962.0]",1492047220.0,Vaduz,arXiv.org,"['nondeterminism', 'code', 'generation', 'llm', 'LLM']"
cd29c25c489562b409a60f83365f93f33ee1a0a1,Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM,https://www.semanticscholar.org/paper/cd29c25c489562b409a60f83365f93f33ee1a0a1,JournalArticle,"Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments on open-source large language models, we demonstrate that RA-LLM can successfully defend against both state-of-the-art adversarial prompts and popular handcrafted jailbreaking prompts by reducing their attack success rates from nearly 100% to around 10% or less.",2023,37,abs/2309.14348,"[143695559.0, 2110924166.0, 7557913.0]",143695559.0,Belgrade,arXiv.org,"['llms', 'alignmentbreaking', 'attacks', 'llm', 'LLM']"
be383c607d4d357c763d2329ab71799c6e1393b4,Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases,https://www.semanticscholar.org/paper/be383c607d4d357c763d2329ab71799c6e1393b4,JournalArticle,"Due to the recent improvements and wide availability of Large Language Models (LLMs), they have posed a serious threat to academic integrity in education. Modern LLM-generated text detectors attempt to combat the problem by offering educators with services to assess whether some text is LLM-generated. In this work, we have collected 124 submissions from computer science students before the creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this data to evaluate eight publicly-available LLM-generated text detectors through the measures of accuracy, false positives, and resilience. The purpose of this work is to inform the community of what LLM-generated text detectors work and which do not, but also to provide insights for educators to better maintain academic integrity in their courses. Our results find that CopyLeaks is the most accurate LLM-generated text detector, GPTKit is the best LLM-generated text detector to reduce false positives, and GLTR is the most resilient LLM-generated text detector. We also express concerns over 52 false positives (of 114 human written submissions) generated by GPTZero. Finally, we note that all LLM-generated text detectors are less accurate with code, other languages (aside from English), and after the use of paraphrasing tools (like QuillBot). Modern detectors are still in need of improvements so that they can offer a full-proof solution to help maintain academic integrity. Further, their usability can be improved by facilitating a smooth API integration, providing clear documentation of their features and the understandability of their model(s), and supporting more commonly used languages.",2023,17,abs/2307.07411,"[2223551699.0, 8722618.0, 153842213.0, 1397294204.0]",2223551699.0,London,arXiv.org,"['text', 'detectors', 'integrity', 'llm', 'LLM']"
04fa3ebc4c0c0b4a2d2d1a3fc612134a05057696,Prompt Injection Attacks and Defenses in LLM-Integrated Applications,https://www.semanticscholar.org/paper/04fa3ebc4c0c0b4a2d2d1a3fc612134a05057696,JournalArticle,"Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we conduct a systematic evaluation on prompt injection attacks and their defenses with 10 LLMs and 7 tasks. We hope our frameworks can inspire future research in this field. Our code is available at https://github.com/liu00222/Open-Prompt-Injection.",2023,15,abs/2310.12815,"[1604963563.0, 2260844526.0, 2260340372.0, 2257508100.0, 144516687.0]",1604963563.0,Paris,arXiv.org,"['attacks', 'injection', 'applications', 'llm', 'LLM']"
d84cf745c534c010b8e55e5a4a04878906848dc3,TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series,https://www.semanticscholar.org/paper/d84cf745c534c010b8e55e5a4a04878906848dc3,JournalArticle,"This work summarizes two ways to accomplish Time-Series (TS) tasks in today's Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data; TS-for-LLM (data-centric) converts TS into a model-friendly representation to enable the pre-trained LLM to handle TS data. Given the lack of data, limited resources, semantic context requirements, and so on, this work focuses on TS-for-LLM, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, where the TS embedding space is aligned to LLM embedding layer space, then creates soft prompts to make LLM more open to that embeddings, and finally implements TS tasks using the frozen LLM. We also demonstrate the feasibility of TS-for-LLM through theory and experiments. Experiments are carried out on TS classification, forecasting, and representation tasks using eight frozen LLMs with various structures and sizes. The results show that the pre-trained LLM with TEST strategy can achieve better or comparable performance than today's SOTA TS models and offer benefits for few-shot and generalization. By treating LLM as the pattern machine, TEST can endow LLM's ability to process TS data without compromising language ability. We hope that this study will serve as a foundation for future work to support TS+LLM progress.",2023,22,abs/2308.08241,"[144102247.0, 2110479359.0, 2115263944.0, 2317297.0]",144102247.0,Reykjavik,arXiv.org,"['llm', 'data', 'work', 'tasks', 'LLM']"
6f75e8b61f13562237851d8119cb2f9d49e073fb,Can LLM-Generated Misinformation Be Detected?,https://www.semanticscholar.org/paper/6f75e8b61f13562237851d8119cb2f9d49e073fb,JournalArticle,"The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.",2023,34,abs/2309.13788,"[2163546329.0, 145800151.0]",2163546329.0,Reykjavik,arXiv.org,"['misinformation', 'llms', 'question', 'llm', 'LLM']"
c12b80b44d9acfe6cd92fdf965264c4b706c367c,ToolQA: A Dataset for LLM Question Answering with External Tools,https://www.semanticscholar.org/paper/c12b80b44d9acfe6cd92fdf965264c4b706c367c,JournalArticle,"Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning. To overcome these challenges, external tools can be used to enhance LLMs' question-answering abilities. However, current evaluation methods do not distinguish between questions that can be answered using LLMs' internal knowledge and those that require external information through tool use. To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs' ability to use external tools for question answering. Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions. Importantly, we strive to minimize the overlap between our benchmark data and LLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use reasoning abilities. We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements. Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements. Our data and code are freely available to the broader scientific community on GitHub.",2023,48,abs/2306.13304,"[8103389.0, 1633124736.0, 2119044211.0, 2118180896.0, 145657504.0]",8103389.0,Belgrade,Neural Information Processing Systems,"['llms', 'tools', 'data', 'llm', 'LLM']"
4f480bae3196dbbc27ab383bce33478ea963f9b3,LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models,https://www.semanticscholar.org/paper/4f480bae3196dbbc27ab383bce33478ea963f9b3,JournalArticle,"We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.",2023,33,abs/2305.13711,"[1557269413.0, 1725643.0]",1557269413.0,Skopje,NLP4CONVAI,"['evaluation', 'llmeval', 'method', 'llm', 'LLM']"
00e18c603e60d861c4e99c541e4d65ef442d5945,LLM in a flash: Efficient Large Language Model Inference with Limited Memory,https://www.semanticscholar.org/paper/00e18c603e60d861c4e99c541e4d65ef442d5945,JournalArticle,"Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First,""windowing""strategically reduces data transfer by reusing previously activated neurons, and second,""row-column bundling"", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.",2023,24,abs/2312.11514,"[1398372702.0, 2256998308.0, 2275238488.0, 1397475144.0, 2237803694.0, 2237799101.0, 32371083.0, 1682124.0]",1398372702.0,Reykjavik,arXiv.org,"['memory', 'data', 'dram', 'llm', 'LLM']"
cc78babfacce48e715dac56886d7dd9746cfcab0,RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit,https://www.semanticscholar.org/paper/cc78babfacce48e715dac56886d7dd9746cfcab0,JournalArticle,"Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrieval-augmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a {RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including {request rewriting, document retrieval, passage extraction, answer generation, and fact checking} modules. Our toolkit is publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.",2023,24,abs/2306.05212,"[1830383266.0, 4376097.0, 2243360876.0, 2219726925.0, 1897235.0, 153693432.0]",1830383266.0,Lisbon,arXiv.org,"['llms', 'systems', 'llm', 'user', 'LLM']"
84725855d10b531eb8cbe54935dda0440c2fc750,Don't Make Your LLM an Evaluation Benchmark Cheater,https://www.semanticscholar.org/paper/84725855d10b531eb8cbe54935dda0440c2fc750,JournalArticle,"Large language models~(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, \ie \emph{benchmark leakage}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct extensive experiments to study the effect of benchmark leverage, and find that it can dramatically boost the evaluation results, which would finally lead to an unreliable assessment of model performance. To improve the use of existing evaluation benchmarks, we finally present several guidelines for both LLM developers and benchmark maintainers. We hope this work can draw attention to appropriate training and evaluation of LLMs.",2023,37,abs/2311.01964,"[2265383494.0, 1900406.0, 2256558402.0, 2265461972.0, 2257376413.0, 2265519430.0, 2257310922.0, 2186578511.0, 2161986932.0]",2265383494.0,Ljubljana,arXiv.org,"['evaluation', 'model', 'benchmarks', 'llm', 'LLM']"
9fcdbfdf28245010c875ce85502351fe05c04b49,Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View,https://www.semanticscholar.org/paper/9fcdbfdf28245010c875ce85502351fe05c04b49,JournalArticle,"As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets\footnote{\url{https://github.com/zjunlp/MachineSoM}.}, hoping to catalyze further research in this promising avenue.",2023,23,abs/2310.02124,"[2253784578.0, 2152775219.0, 2253472695.0]",2253784578.0,Madrid,arXiv.org,"['llm', 'nlp', 'systems', 'collaboration', 'LLM']"
98ce7af921e7c52d81df64d632d34eb09522cd75,Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization,https://www.semanticscholar.org/paper/98ce7af921e7c52d81df64d632d34eb09522cd75,JournalArticle,"Large language model (LLM) agents have been shown effective on a wide range of tasks, and by ensembling multiple LLM agents, their performances could be further improved. Existing approaches employ a fixed set of agents to interact with each other in a static architecture, which limits their generalizability to various tasks and requires strong human prior in designing these agents. In this work, we propose to construct a strategic team of agents communicating in a dynamic interaction architecture based on the task query. Specifically, we build a framework named Dynamic LLM-Agent Network ($\textbf{DyLAN}$) for LLM-agent collaboration on complicated tasks like reasoning and code generation. DyLAN enables agents to interact for multiple rounds in a dynamic architecture with inference-time agent selection and an early-stopping mechanism to improve performance and efficiency. We further design an automatic agent team optimization algorithm based on an unsupervised metric termed $\textit{Agent Importance Score}$, enabling the selection of best agents based on the contribution each agent makes. Empirically, we demonstrate that DyLAN performs well in both reasoning and code generation tasks with reasonable computational cost. DyLAN achieves 13.0% and 13.3% improvement on MATH and HumanEval, respectively, compared to a single execution on GPT-35-turbo. On specific subjects of MMLU, agent team optimization in DyLAN increases accuracy by up to 25.0%.",2023,23,abs/2310.02170,"[2117942065.0, 2121290295.0, 2255345252.0, 2254850259.0, 2254124342.0]",2117942065.0,Minsk,arXiv.org,"['agents', 'dylan', 'agent', 'llm', 'LLM']"
a22f3398ea865426c89ee66f4824ec626e56a864,RET-LLM: Towards a General Read-Write Memory for Large Language Models,https://www.semanticscholar.org/paper/a22f3398ea865426c89ee66f4824ec626e56a864,JournalArticle,"Large language models (LLMs) have significantly advanced the field of natural language processing (NLP) through their extensive parameters and comprehensive data utilization. However, existing LLMs lack a dedicated memory unit, limiting their ability to explicitly store and retrieve knowledge for various tasks. In this paper, we propose RET-LLM a novel framework that equips LLMs with a general write-read memory unit, allowing them to extract, store, and recall knowledge from the text as needed for task performance. Inspired by Davidsonian semantics theory, we extract and save knowledge in the form of triplets. The memory unit is designed to be scalable, aggregatable, updatable, and interpretable. Through qualitative evaluations, we demonstrate the superiority of our proposed framework over baseline approaches in question answering tasks. Moreover, our framework exhibits robust performance in handling temporal-based question answering tasks, showcasing its ability to effectively manage time-dependent information.",2023,14,abs/2305.14322,"[2054744.0, 51894641.0, 2133037029.0, 144418438.0]",2054744.0,Budapest,arXiv.org,"['llms', 'memory', 'unit', 'llm', 'LLM']"
7bc9607c5cf3fc817675d46844f529097d579514,Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data,https://www.semanticscholar.org/paper/7bc9607c5cf3fc817675d46844f529097d579514,JournalArticle,"Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.",2023,20,8,"[9551276.0, 1490485182.0, 2115461706.0, 4590999.0, 145135778.0, 144021446.0, 2156149847.0]",9551276.0,Minsk,Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies,"['llms', 'health', 'tasks', 'llm', 'LLM']"
0f4d00d01d43d3967ee92b58481b5ad530a944d1,A First Look at LLM-Powered Generative News Recommendation,https://www.semanticscholar.org/paper/0f4d00d01d43d3967ee92b58481b5ad530a944d1,JournalArticle,"Personalized news recommendation systems have become essential tools for users to navigate the vast amount of online news content, yet existing news recommenders face significant challenges such as the cold-start problem, user profile modeling, and news content understanding. Previous works have typically followed an inflexible routine to address a particular challenge through model design, but are limited in their ability to understand news content and capture user interests. In this paper, we introduce GENRE, an LLM-powered generative news recommendation framework, which leverages pretrained semantic knowledge from large language models to enrich news data. Our aim is to provide a flexible and unified solution for news recommendation by moving from model design to prompt design. We showcase the use of GENRE for personalized news generation, user profiling, and news summarization. Extensive experiments with various popular recommendation models demonstrate the effectiveness of GENRE. We will publish our code and data 1 for other researchers to reproduce our work.",2023,20,abs/2305.06566,"[150270469.0, 2257286538.0, 2257233277.0, 2187512110.0]",150270469.0,Stockholm,arXiv.org,"['news', 'recommendation', 'content', 'llm', 'LLM']"
11cf88dce827bd67cbfa60400306318022e736d5,D4: Improving LLM Pretraining via Document De-Duplication and Diversification,https://www.semanticscholar.org/paper/11cf88dce827bd67cbfa60400306318022e736d5,JournalArticle,"Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever data selection can significantly improve LLM pre-training, calls into question the common practice of training for a single epoch on as much data as possible, and demonstrates a path to keep improving our models past the limits of randomly sampling web data.",2023,22,abs/2308.12284,"[2551387.0, 2257241733.0, 2201435.0, 4690624.0]",2551387.0,London,Neural Information Processing Systems,"['data', 'training', 'selection', 'llm', 'LLM']"
b85f3a66245d483f3eb3447eaf9950bd55f2b21e,PentestGPT: An LLM-empowered Automatic Penetration Testing Tool,https://www.semanticscholar.org/paper/b85f3a66245d483f3eb3447eaf9950bd55f2b21e,JournalArticle,"Penetration testing, a crucial industrial practice for ensuring system security, has traditionally resisted automation due to the extensive expertise required by human professionals. Large Language Models (LLMs) have shown significant advancements in various domains, and their emergent abilities suggest their potential to revolutionize industries. In this research, we evaluate the performance of LLMs on real-world penetration testing tasks using a robust benchmark created from test machines with platforms. Our findings reveal that while LLMs demonstrate proficiency in specific sub-tasks within the penetration testing process, such as using testing tools, interpreting outputs, and proposing subsequent actions, they also encounter difficulties maintaining an integrated understanding of the overall testing scenario. In response to these insights, we introduce PentestGPT, an LLM-empowered automatic penetration testing tool that leverages the abundant domain knowledge inherent in LLMs. PentestGPT is meticulously designed with three self-interacting modules, each addressing individual sub-tasks of penetration testing, to mitigate the challenges related to context loss. Our evaluation shows that PentestGPT not only outperforms LLMs with a task-completion increase of 228.6\% compared to the \gptthree model among the benchmark targets but also proves effective in tackling real-world penetration testing challenges. Having been open-sourced on GitHub, PentestGPT has garnered over 4,700 stars and fostered active community engagement, attesting to its value and impact in both the academic and industrial spheres.",2023,16,abs/2308.06782,"[73776889.0, 2153627626.0, 1947239202.0, 2217847684.0, 22799258.0, 2110355317.0, 2146331573.0, 2152798056.0, 1775709.0, 1717963.0]",73776889.0,Amsterdam,arXiv.org,"['testing', 'penetration', 'llms', 'llm', 'LLM']"
da9b8b4073e6ad44b3da66e1e117cb1ddbf8836d,Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels,https://www.semanticscholar.org/paper/da9b8b4073e6ad44b3da66e1e117cb1ddbf8836d,JournalArticle,"Zero-shot text rankers powered by recent LLMs achieve remarkable ranking performance by simply prompting. Existing prompts for pointwise LLM rankers mostly ask the model to choose from binary relevance labels like""Yes""and""No"". However, the lack of intermediate relevance label options may cause the LLM to provide noisy or biased answers for documents that are partially relevant to the query. We propose to incorporate fine-grained relevance labels into the prompt for LLM rankers, enabling them to better differentiate among documents with different levels of relevance to the query and thus derive a more accurate ranking. We study two variants of the prompt template, coupled with different numbers of relevance levels. Our experiments on 8 BEIR data sets show that adding fine-grained relevance labels significantly improves the performance of LLM rankers.",2023,19,abs/2310.14122,"[39371343.0, 2099586642.0, 2261281337.0, 2261361394.0, 2110134250.0, 2261356664.0, 1815447.0]",39371343.0,Belgrade,arXiv.org,"['llm', 'relevance', 'rankers', 'labels', 'LLM']"
709af143f78bc62413c50ea1a7ee75b0702c4f59,MART: Improving LLM Safety with Multi-round Automatic Red-Teaming,https://www.semanticscholar.org/paper/709af143f78bc62413c50ea1a7ee75b0702c4f59,JournalArticle,"Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses. While effective, manual red-teaming is costly, and existing automatic red-teaming typically discovers safety risks without addressing them. In this paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which incorporates both automatic adversarial prompt writing and safe response generation, significantly increasing red-teaming scalability and the safety of the target LLM. Specifically, an adversarial LLM and a target LLM interplay with each other in an iterative manner, where the adversarial LLM aims to generate challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned with safety aligned data on these adversarial prompts. In each round, the adversarial LLM crafts better attacks on the updated target LLM, while the target LLM also improves itself through safety fine-tuning. On adversarial prompt benchmarks, the violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs with extensive adversarial prompt writing. Notably, model helpfulness on non-adversarial prompts remains stable throughout iterations, indicating the target LLM maintains strong performance on instruction following.",2023,15,abs/2311.07689,"[2243417222.0, 2266920681.0, 2266467782.0, 2072010.0, 2266748478.0, 2266712798.0, 2253929707.0, 3375249.0]",2243417222.0,San Marino,arXiv.org,"['target', 'llm', 'safety', 'llms', 'LLM']"
eda6756ab2844c390584686dc5e6385f4a8369cd,LLM-assisted Generation of Hardware Assertions,https://www.semanticscholar.org/paper/eda6756ab2844c390584686dc5e6385f4a8369cd,JournalArticle,"The security of computer systems typically relies on a hardware root of trust. As vulnerabilities in hardware can have severe implications on a system, there is a need for techniques to support security verification activities. Assertion-based verification is a popular verification technique that involves capturing design intent in a set of assertions that can be used in formal verification or testing-based checking. However, writing security-centric assertions is a challenging task. In this work, we investigate the use of emerging large language models (LLMs) for code generation in hardware assertion generation for security, where primarily natural language prompts, such as those one would see as code comments in assertion files, are used to produce SystemVerilog assertions. We focus our attention on a popular LLM and characterize its ability to write assertions out of the box, given varying levels of detail in the prompt. We design an evaluation framework that generates a variety of prompts, and we create a benchmark suite comprising real-world hardware designs and corresponding golden reference assertions that we want to generate with the LLM.",2023,21,abs/2306.14027,"[2113610946.0, 3437933.0, 143645422.0, 1398683279.0, 2057985118.0, 1707355.0, 2220547623.0, 73770687.0, 102544543.0, 51280874.0]",2113610946.0,Skopje,arXiv.org,"['assertions', 'hardware', 'security', 'llm', 'LLM']"
0894585294c67193ff3190240554677b56fd79a0,From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?,https://www.semanticscholar.org/paper/0894585294c67193ff3190240554677b56fd79a0,JournalArticle,"Large Language Models (LLMs) have found widespread applications in various domains, including web applications, where they facilitate human interaction via chatbots with natural language interfaces. Internally, aided by an LLM-integration middleware such as Langchain, user prompts are translated into SQL queries used by the LLM to provide meaningful responses to users. However, unsanitized user prompts can lead to SQL injection attacks, potentially compromising the security of the database. Despite the growing interest in prompt injection vulnerabilities targeting LLMs, the specific risks of generating SQL injection attacks through prompt injections have not been extensively studied. In this paper, we present a comprehensive examination of prompt-to-SQL (P$_2$SQL) injections targeting web applications based on the Langchain framework. Using Langchain as our case study, we characterize P$_2$SQL injections, exploring their variants and impact on application security through multiple concrete examples. Furthermore, we evaluate 7 state-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacks across language models. Our findings indicate that LLM-integrated applications based on Langchain are highly susceptible to P$_2$SQL injection attacks, warranting the adoption of robust defenses. To counter these attacks, we propose four effective defense techniques that can be integrated as extensions to the Langchain framework. We validate the defenses through an experimental evaluation with a real-world use case application.",2023,16,abs/2308.01990,"[2231175914.0, 2248088902.0, 2231158981.0, 2231663636.0]",2231175914.0,Rome,arXiv.org,"['langchain', 'attacks', 'applications', 'llm', 'LLM']"
1562390dd212516cd857009cbd4f857a902d1f3d,MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents,https://www.semanticscholar.org/paper/1562390dd212516cd857009cbd4f857a902d1f3d,JournalArticle,"Significant advancements have occurred in the application of Large Language Models (LLMs) for various tasks and social simulations. Despite this, their capacities to coordinate within task-oriented social contexts are under-explored. Such capabilities are crucial if LLMs are to effectively mimic human-like social behavior and produce meaningful results. To bridge this gap, we introduce collaborative generative agents, endowing LLM-based Agents with consistent behavior patterns and task-solving abilities. We situate these agents in a simulated job fair environment as a case study to scrutinize their coordination skills. We propose a novel framework that equips collaborative generative agents with human-like reasoning abilities and specialized skills. Our evaluation demonstrates that these agents show promising performance. However, we also uncover limitations that hinder their effectiveness in more complex coordination tasks. Our work provides valuable insights into the role and evolution of LLMs in task-oriented social simulations.",2023,20,abs/2310.06500,"[2257259410.0, 2257107248.0, 2243348413.0]",2257259410.0,Vilnius,arXiv.org,"['agents', 'tasks', 'simulations', 'llm', 'LLM']"
32524aa3ae8522542753ed7e6f4cca3970e4acab,Can an Embodied Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation,https://www.semanticscholar.org/paper/32524aa3ae8522542753ed7e6f4cca3970e4acab,JournalArticle,"We present language-guided exploration (LGX), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to an uniquely described target object in a previously unseen environment. Our approach makes use of large language models (LLMs) for this task by leveraging the LLM's commonsense-reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via real-world experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects.",2023,21,9,"[1395945114.0, 2057888241.0, 1699159.0]",1395945114.0,London,IEEE Robotics and Automation Letters,"['object', 'navigation', 'lgx', 'llm', 'LLM']"
681253389d2cc27103753749f4c7556699d55471,Temporal Data Meets LLM - Explainable Financial Time Series Forecasting,https://www.semanticscholar.org/paper/681253389d2cc27103753749f4c7556699d55471,JournalArticle,"This paper presents a novel study on harnessing Large Language Models' (LLMs) outstanding knowledge and reasoning abilities for explainable financial time series forecasting. The application of machine learning models to financial time series comes with several challenges, including the difficulty in cross-sequence reasoning and inference, the hurdle of incorporating multi-modal signals from historical news, financial knowledge graphs, etc., and the issue of interpreting and explaining the model results. In this paper, we focus on NASDAQ-100 stocks, making use of publicly accessible historical stock price data, company metadata, and historical economic/financial news. We conduct experiments to illustrate the potential of LLMs in offering a unified solution to the aforementioned challenges. Our experiments include trying zero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with a public LLM model Open LLaMA. We demonstrate our approach outperforms a few baselines, including the widely applied classic ARMA-GARCH model and a gradient-boosting tree model. Through the performance comparison results and a few examples, we find LLMs can make a well-thought decision by reasoning over information from both textual news and price time series and extracting insights, leveraging cross-sequence information, and utilizing the inherent knowledge embedded within the LLM. Additionally, we show that a publicly available LLM such as Open-LLaMA, after fine-tuning, can comprehend the instruction to generate explainable forecasts and achieve reasonable performance, albeit relatively inferior in comparison to GPT-4.",2023,18,abs/2306.11025,"[2118210915.0, 2141144864.0, 48907594.0, 2190030596.0, 47781311.0, 2220668748.0]",2118210915.0,Rome,arXiv.org,"['model', 'llm', 'llms', 'knowledge', 'LLM']"
9db0247728950788a2b42097d81dc0e24eed6bb2,LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent,https://www.semanticscholar.org/paper/9db0247728950788a2b42097d81dc0e24eed6bb2,JournalArticle,"3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs significantly improve the grounding capability, especially for complex language queries, making LLM-Grounder an effective approach for 3D vision-language tasks in robotics. Videos and interactive demos can be found on the project website https://chat-with-nerf.github.io/ .",2023,17,abs/2309.12311,"[46477844.0, 2243416833.0, 1390481263.0, 2243383240.0, 2243403473.0, 1786435.0, 2243382885.0]",46477844.0,Bern,arXiv.org,"['3d', 'language', 'llmgrounder', 'llm', 'LLM']"
0e9a44ce661c3535d5ce747912540080324489f5,Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena,https://www.semanticscholar.org/paper/0e9a44ce661c3535d5ce747912540080324489f5,JournalArticle,"Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM performance and occasional outperformance by simpler methods indicate opportunities for further advancements in LLM design and the value of our simulation environment for ongoing testing and refinement.",2023,20,abs/2310.05746,"[5040052.0, 2145968425.0, 2062940513.0, 3165738.0, 46666605.0]",5040052.0,Vilnius,arXiv.org,"['llms', 'skills', 'llm', 'advancements', 'LLM']"
263a58f4fd32caca1dad2351af4d711aec451fe6,Evil Geniuses: Delving into the Safety of LLM-based Agents,https://www.semanticscholar.org/paper/263a58f4fd32caca1dad2351af4d711aec451fe6,JournalArticle,"Rapid advancements in large language models (LLMs) have revitalized in LLM-based agents, exhibiting impressive human-like behaviors and cooperative capabilities in various scenarios. However, these agents also bring some exclusive risks, stemming from the complexity of interaction environments and the usability of tools. This paper delves into the safety of LLM-based agents from three perspectives: agent quantity, role definition, and attack level. Specifically, we initially propose to employ a template-based attack strategy on LLM-based agents to find the influence of agent quantity. In addition, to address interaction environment and role specificity issues, we introduce Evil Geniuses (EG), an effective attack method that autonomously generates prompts related to the original role to examine the impact across various role definitions and attack levels. EG leverages Red-Blue exercises, significantly improving the generated prompt aggressiveness and similarity to original roles. Our evaluations on CAMEL, Metagpt and ChatDev based on GPT-3.5 and GPT-4, demonstrate high success rates. Extensive evaluation and discussion reveal that these agents are less robust, prone to more harmful behaviors, and capable of generating stealthier content than LLMs, highlighting significant safety challenges and guiding future research. Our code is available at https://github.com/T1aNS1R/Evil-Geniuses.",2023,17,abs/2311.11855,"[2243374631.0, 2109457982.0, 2267384625.0, 3431029.0, 2267649469.0]",2243374631.0,Kiev,arXiv.org,"['agents', 'role', 'attack', 'llm', 'LLM']"
16753e0317730e8c1b297338300a8c6163dd06f2,VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning,https://www.semanticscholar.org/paper/16753e0317730e8c1b297338300a8c6163dd06f2,JournalArticle,"Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models. This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency groupings of the entities and backgrounds. Next, guided by this output from the video planner, our video generator, Layout2Vid, has explicit control over spatial layouts and can maintain temporal consistency of entities/backgrounds across scenes, while only trained with image-level annotations. Our experiments demonstrate that VideoDirectorGPT framework substantially improves layout and movement control in both single- and multi-scene video generation and can generate multi-scene videos with visual consistency across scenes, while achieving competitive performance with SOTAs in open-domain single-scene T2V generation. We also demonstrate that our framework can dynamically control the strength for layout guidance and can also generate videos with user-provided images. We hope our framework can inspire future work on better integrating the planning ability of LLMs into consistent long video generation.",2023,16,abs/2309.15091,"[39729530.0, 2008198436.0, 2706729.0, 143977268.0]",39729530.0,Zagreb,arXiv.org,"['video', 'generation', 'llms', 'llm', 'LLM']"
be2b0396de9431bae931642516a1d3e4906329f5,Low-code LLM: Visual Programming over LLMs,https://www.semanticscholar.org/paper/be2b0396de9431bae931642516a1d3e4906329f5,JournalArticle,"Effectively utilizing LLMs for complex tasks is challenging, often involving a time-consuming and uncontrollable prompt engineering process. This paper intro-duces a novel human-LLM interaction framework, Low-code LLM . It incorporates six types of simple low-code visual programming interactions, all supported by clicking, dragging, or text editing, to achieve more controllable and stable responses. Through visual interaction with a graphical user interface, users can incorporate their ideas into the workﬂow without writing trivial prompts. The proposed Low-code LLM framework consists of a Planning LLM that designs a structured planning workﬂow for complex tasks, which can be correspondingly edited and conﬁrmed by users through low-code visual programming operations, and an Executing LLM that generates responses following the user-conﬁrmed workﬂow. We highlight three advantages of the low-code LLM: controllable generation results, user-friendly human-LLM interaction, and broadly applicable scenarios. We demonstrate its beneﬁts using four typical applications. By introducing this approach, we aim to bridge the gap between humans and LLMs, enabling more effective and efﬁcient utilization of LLMs for complex tasks. Our system will be soon publicly available at LowCodeLLM .",2023,22,abs/2304.08103,"[116275985.0, 35374367.0, 51198241.0, 2108725194.0, 3887469.0, 48741177.0, 2151101534.0, 2214585980.0, 2212903837.0, 2111130689.0, 2069738994.0, 2072609829.0]",116275985.0,Budapest,arXiv.org,"['llm', 'tasks', 'interaction', 'lowcode', 'LLM']"
529ff7d6441d244212cf2becafd12a7e67ac56d9,FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning,https://www.semanticscholar.org/paper/529ff7d6441d244212cf2becafd12a7e67ac56d9,JournalArticle,"LLMs have demonstrated great capabilities in various NLP tasks. Different entities can further improve the performance of those LLMs on their specific downstream tasks by fine-tuning LLMs. When several entities have similar interested tasks, but their data cannot be shared because of privacy concerns regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. However, fine-tuning LLMs in federated learning settings still lacks adequate support from existing FL frameworks because it has to deal with optimizing the consumption of significant communication and computational resources, data preparation for different tasks, and distinct information protection demands. This paper first discusses these challenges of federated fine-tuning LLMs, and introduces our package FS-LLM as a main contribution, which consists of the following components: (1) we build an end-to-end benchmarking pipeline, automizing the processes of dataset preprocessing, federated fine-tuning execution, and performance evaluation on federated LLM fine-tuning; (2) we provide comprehensive federated parameter-efficient fine-tuning algorithm implementations and versatile programming interfaces for future extension in FL scenarios with low communication and computation costs, even without accessing the full model; (3) we adopt several accelerating and resource-efficient operators for fine-tuning LLMs with limited resources and the flexible pluggable sub-routines for interdisciplinary study. We conduct extensive experiments to validate the effectiveness of FS-LLM and benchmark advanced LLMs with state-of-the-art parameter-efficient fine-tuning algorithms in FL settings, which also yields valuable insights into federated fine-tuning LLMs for the research community. To facilitate further research and adoption, we release FS-LLM at https://github.com/alibaba/FederatedScope/tree/llm.",2023,21,abs/2309.00363,"[2162042348.0, 2237427279.0, 2007553615.0, 49025612.0, 2162036220.0, 2211993531.0, 52133762.0, 2237607166.0, 1696332.0, 2237499232.0]",2162042348.0,Tirana,arXiv.org,"['llms', 'tasks', 'entities', 'llm', 'LLM']"
cb3968152f7d93f53d24b00279a90d5071ddc85a,Understanding the Effects of RLHF on LLM Generalisation and Diversity,https://www.semanticscholar.org/paper/cb3968152f7d93f53d24b00279a90d5071ddc85a,JournalArticle,"Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the tradeoff between generalisation and diversity.",2023,15,abs/2310.06452,"[2066422293.0, 2008790203.0, 31434304.0, 2256999781.0, 2072738644.0, 1864353.0, 48647153.0]",2066422293.0,Bratislava,arXiv.org,"['models', 'rlhf', 'diversity', 'llm', 'LLM']"
70ca38ad480c0be0eca89ccc4972d6cc9a5824da,LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins,https://www.semanticscholar.org/paper/70ca38ad480c0be0eca89ccc4972d6cc9a5824da,JournalArticle,"Large language model (LLM) platforms, such as ChatGPT, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. While these plugins extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the potential for the types of issues that we outline in our attack taxonomy. We conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future LLM-based computing platforms.",2023,16,abs/2309.10254,"[2066229431.0, 1769675.0, 3268360.0]",2066229431.0,Vaduz,arXiv.org,"['llm', 'platforms', 'plugins', 'framework', 'LLM']"
139a0c7a60667979dcb57eae677f75ff3f0b0196,LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?,https://www.semanticscholar.org/paper/139a0c7a60667979dcb57eae677f75ff3f0b0196,JournalArticle,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks.",2023,17,abs/2307.10719,"[2224017005.0, 47473421.0, 2681954.0, 1967156.0, 2734935.0]",2224017005.0,Athens,arXiv.org,"['censorship', 'llms', 'problem', 'llm', 'LLM']"
dd4d82299b4209db539d639f836fcee663cf72b3,"LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples",https://www.semanticscholar.org/paper/dd4d82299b4209db539d639f836fcee663cf72b3,JournalArticle,"Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception. And the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that non-sense prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. This phenomenon forces us to revisit that hallucination may be another view of adversarial examples, and it shares similar features with conventional adversarial examples as the basic feature of LLMs. Therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. Finally, we explore basic feature of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub.",2023,31,abs/2310.01469,"[2256816230.0, 2253467098.0, 2253826413.0, 2253434320.0, 2265968596.0]",2256816230.0,Vilnius,arXiv.org,"['llms', 'hallucination', 'prompts', 'llm', 'LLM']"
7c217cc7524251f42887438834912e06129c3299,To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis,https://www.semanticscholar.org/paper/7c217cc7524251f42887438834912e06129c3299,JournalArticle,"Recent research has highlighted the importance of dataset size in scaling language models. However, large language models (LLMs) are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs. To further enhance LLMs, a straightforward approach is to repeat the pre-training data for additional epochs. In this study, we empirically investigate three key aspects under this approach. First, we explore the consequences of repeating pre-training data, revealing that the model is susceptible to overfitting, leading to multi-epoch degradation. Second, we examine the key factors contributing to multi-epoch degradation, finding that significant factors include dataset size, model parameters, and training objectives, while less influential factors consist of dataset quality and model FLOPs. Finally, we explore whether widely used regularization can alleviate multi-epoch degradation. Most regularization techniques do not yield significant improvements, except for dropout, which demonstrates remarkable effectiveness but requires careful tuning when scaling up the model size. Additionally, we discover that leveraging mixture-of-experts (MoE) enables cost-effective and efficient hyper-parameter tuning for computationally intensive dense LLMs with comparable trainable parameters, potentially impacting efficient LLM development on a broader scale.",2023,24,abs/2305.13230,"[2144332771.0, 2117786875.0, 150341221.0, 2109654065.0, 2054451943.0]",2144332771.0,Prague,Neural Information Processing Systems,"['model', 'size', 'data', 'llm', 'LLM']"
3c8a456509e6c0805354bd40a35e3f2dbf8069b1,"PyTorch: An Imperative Style, High-Performance Deep Learning Library",https://www.semanticscholar.org/paper/3c8a456509e6c0805354bd40a35e3f2dbf8069b1,JournalArticle,"Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.",2019,31617,abs/1912.01703,"[3407277.0, 39793298.0, 1403239967.0, 1977806.0, 2065251344.0, 114250963.0, 2059271276.0, 3370429.0, 3365851.0, 3029482.0, 3050846.0, 1473151134.0, 2052812305.0, 2375710.0, 10707709.0, 41203992.0, 22236100.0, 32163737.0, 152599430.0, 2113829116.0, 2127604.0]",3407277.0,Sofia,Neural Information Processing Systems,"['pytorch', 'speed', 'principles', 'deep learning', 'Deep Learning']"
7aa38b85fa8cba64d6a4010543f6695dbf5f1386,Towards Deep Learning Models Resistant to Adversarial Attacks,https://www.semanticscholar.org/paper/7aa38b85fa8cba64d6a4010543f6695dbf5f1386,JournalArticle,"Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",2017,9383,abs/1706.06083,"[143826246.0, 17775913.0, 152772922.0, 2754804.0, 2869958.0]",143826246.0,Belgrade,International Conference on Learning Representations,"['networks', 'models', 'security', 'deep learning', 'Deep Learning']"
54ddb00fa691728944fd8becea90a373d21597cf,Understanding deep learning requires rethinking generalization,https://www.semanticscholar.org/paper/54ddb00fa691728944fd8becea90a373d21597cf,JournalArticle,"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. 
Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. 
We interpret our experimental findings by comparison with traditional models.",2016,4263,abs/1611.03530,"[151505981.0, 1751569.0, 1775622.0, 9229182.0, 1689108.0]",151505981.0,Sarajevo,International Conference on Learning Representations,"['networks', 'training', 'regularization', 'deep learning', 'Deep Learning']"
2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c,Understanding deep learning (still) requires rethinking generalization,https://www.semanticscholar.org/paper/2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c,JournalArticle,"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models. We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.",2021,1364,64,"[151505981.0, 1751569.0, 1775622.0, 9229182.0, 1689108.0]",151505981.0,Sarajevo,Communications of the ACM,"['networks', 'training', 'regularization', 'deep learning', 'Deep Learning']"
b79e5e4622a95417deec313cd543617b19611bea,Deep Learning using Rectified Linear Units (ReLU),https://www.semanticscholar.org/paper/b79e5e4622a95417deec313cd543617b19611bea,JournalArticle,"We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer $h_{n - 1}$ in a neural network, then multiply it by weight parameters $\theta$ to get the raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$, i.e. $f(o) = \max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide class predictions $\hat{y}$ through argmax function, i.e. argmax $f(x)$.",2018,2349,abs/1803.08375,[26412983.0],26412983.0,Belgrade,arXiv.org,"['function', 'relu', 'classification', 'deep learning', 'Deep Learning']"
14014c024674991149f3ecf9314c93f7e029ef1a,"Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges",https://www.semanticscholar.org/paper/14014c024674991149f3ecf9314c93f7e029ef1a,JournalArticle,"The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.",2021,793,abs/2104.13478,"[1732570.0, 143627859.0, 2056266.0, 1742197495.0]",1732570.0,Tirana,arXiv.org,"['learning', 'methods', 'principles', 'deep learning', 'Deep Learning']"
024006d4c2a89f7acacc6e4438d156525b60a98f,Continuous control with deep reinforcement learning,https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f,JournalArticle,"We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",2015,10952,abs/1509.02971,"[2542999.0, 2323922.0, 1863250.0, 2801204.0, 1968210.0, 2109481.0, 145824029.0, 1688276.0]",2542999.0,Belgrade,International Conference on Learning Representations,"['algorithm', 'action', 'domain', 'deep learning', 'Deep Learning']"
8388f1be26329fa45e5807e968a641ce170ea078,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,https://www.semanticscholar.org/paper/8388f1be26329fa45e5807e968a641ce170ea078,JournalArticle,"In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",2015,12658,abs/1511.06434,"[38909097.0, 2096458.0, 2127604.0]",38909097.0,Monaco,International Conference on Learning Representations,"['cnns', 'learning', 'networks', 'deep learning', 'Deep Learning']"
0c00a328fa7cd56ee60338c54e89bd48310db80b,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,https://www.semanticscholar.org/paper/0c00a328fa7cd56ee60338c54e89bd48310db80b,JournalArticle,"The discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks, such as Gaussian denoising, single image super-resolution, and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing.",2016,5760,26,"[144110274.0, 1724520.0, 39773686.0, 1803714.0, 36685537.0]",144110274.0,London,IEEE Transactions on Image Processing,"['image', 'model', 'gaussian', 'deep learning', 'Deep Learning']"
8ec5896b4490c6e127d1718ffc36a3439d84cb81,On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,https://www.semanticscholar.org/paper/8ec5896b4490c6e127d1718ffc36a3439d84cb81,JournalArticle,"The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say $32$-$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.",2016,2508,abs/1609.04836,"[2844898.0, 2205699.0, 2784955.0, 1711231.0, 144669504.0]",2844898.0,Tirana,International Conference on Learning Representations,"['methods', 'generalization', 'largebatch', 'deep learning', 'Deep Learning']"
e30d9b8ce108d982169621b88a5e3fb69fec70e1,Using Deep Learning for Image-Based Plant Disease Detection,https://www.semanticscholar.org/paper/e30d9b8ce108d982169621b88a5e3fb69fec70e1,JournalArticle,"Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.",2016,2313,7,"[24178944.0, 2068146094.0, 3046313.0]",24178944.0,Sofia,Frontiers in Plant Science,"['crop', 'learning', 'disease', 'deep learning', 'Deep Learning']"
150f95f9c73820e0a0fa1546140e9f2bdfd25954,Temporal Graph Networks for Deep Learning on Dynamic Graphs,https://www.semanticscholar.org/paper/150f95f9c73820e0a0fa1546140e9f2bdfd25954,JournalArticle,"Graph Neural Networks (GNNs) have recently become increasingly popular due to their ability to learn complex systems of relations or interactions arising in a broad spectrum of problems ranging from biology and particle physics to social networks and recommendation systems. Despite the plethora of different models for deep learning on graphs, few approaches have been proposed thus far for dealing with graphs that present some sort of dynamic nature (e.g. evolving features or connectivity over time). In this paper, we present Temporal Graph Networks (TGNs), a generic, efficient framework for deep learning on dynamic graphs represented as sequences of timed events. Thanks to a novel combination of memory modules and graph-based operators, TGNs are able to significantly outperform previous approaches being at the same time more computationally efficient. We furthermore show that several previous models for learning on dynamic graphs can be cast as specific instances of our framework. We perform a detailed ablation study of different components of our framework and devise the best configuration that achieves state-of-the-art performance on several transductive and inductive prediction tasks for dynamic graphs.",2020,386,abs/2006.10637,"[2056294358.0, 2029302.0, 51484149.0, 1775620.0, 2500309.0, 1732570.0]",2056294358.0,Oslo,arXiv.org,"['graphs', 'networks', 'framework', 'deep learning', 'Deep Learning']"
31f10a6f602bef0306ac37322f84f6163c8a8ecb,CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning,https://www.semanticscholar.org/paper/31f10a6f602bef0306ac37322f84f6163c8a8ecb,JournalArticle,"We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.",2017,2257,abs/1711.05225,"[2706258.0, 46559852.0, 29972904.0, 145951921.0, 3776937.0, 15069782.0, 51235411.0, 30043065.0, 2356307.0, 3474704.0, 4204731.0, 34699434.0]",2706258.0,Stockholm,arXiv.org,"['chexnet', 'radiologists', 'diseases', 'deep learning', 'Deep Learning']"
f9314fd99be5f2b1b3efcfab87197d578160d553,Deep Learning Enabled Semantic Communication Systems,https://www.semanticscholar.org/paper/f9314fd99be5f2b1b3efcfab87197d578160d553,JournalArticle,"Recently, deep learned enabled end-to-end communication systems have been developed to merge all physical layer blocks in the traditional communication systems, which make joint transceiver optimization possible. Powered by deep learning, natural language processing has achieved great success in analyzing and understanding a large amount of language texts. Inspired by research results in both areas, we aim to provide a new view on communication systems from the semantic level. Particularly, we propose a deep learning based semantic communication system, named DeepSC, for text transmission. Based on the Transformer, the DeepSC aims at maximizing the system capacity and minimizing the semantic errors by recovering the meaning of sentences, rather than bit- or symbol-errors in traditional communications. Moreover, transfer learning is used to ensure the DeepSC applicable to different communication environments and to accelerate the model training process. To justify the performance of semantic communications accurately, we also initialize a new metric, named sentence similarity. Compared with the traditional communication system without considering semantic information exchange, the proposed DeepSC is more robust to channel variation and is able to achieve better performance, especially in the low signal-to-noise (SNR) regime, as demonstrated by the extensive simulation results.",2020,493,69,"[66320873.0, 67022972.0, 1410112765.0, 143604406.0]",66320873.0,Warsaw,IEEE Transactions on Signal Processing,"['communication', 'deepsc', 'systems', 'deep learning', 'Deep Learning']"
91e611c3e8705002438fb4439733e47ddec85b5d,fastai: A Layered API for Deep Learning,https://www.semanticscholar.org/paper/91e611c3e8705002438fb4439733e47ddec85b5d,JournalArticle,"fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4–5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching.",2020,695,11,"[2093348519.0, 2286145455.0]",2093348519.0,Prague,Inf.,"['library', 'learning', 'data', 'deep learning', 'Deep Learning']"
68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a,ADMM-CSNet: A Deep Learning Approach for Image Compressive Sensing,https://www.semanticscholar.org/paper/68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a,JournalArticle,"Compressive sensing (CS) is an effective technique for reconstructing image from a small amount of sampled data. It has been widely applied in medical imaging, remote sensing, image compression, etc. In this paper, we propose two versions of a novel deep learning architecture, dubbed as ADMM-CSNet, by combining the traditional model-based CS method and data-driven deep learning method for image reconstruction from sparsely sampled measurements. We first consider a generalized CS model for image reconstruction with undetermined regularizations in undetermined transform domains, and then two efficient solvers using Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing the model are proposed. We further unroll and generalize the ADMM algorithm to be two deep architectures, in which all parameters of the CS model and the ADMM algorithm are discriminatively learned by end-to-end training. For both applications of fast CS complex-valued MR imaging and CS imaging of real-valued natural images, the proposed ADMM-CSNet achieved favorable reconstruction accuracy in fast computational speed compared with the traditional and the other deep learning methods.",2020,456,42,"[2108850237.0, 2152146009.0, 1680740.0, 98220533.0]",2108850237.0,Luxembourg,IEEE Transactions on Pattern Analysis and Machine Intelligence,"['image', 'cs', 'imaging', 'deep learning', 'Deep Learning']"
af9280741ef627f0d6c8437605d002d3bfc2d1b1,Bayesian Deep Learning and a Probabilistic Perspective of Generalization,https://www.semanticscholar.org/paper/af9280741ef627f0d6c8437605d002d3bfc2d1b1,JournalArticle,"The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.",2020,501,abs/2002.08791,"[145771261.0, 7991830.0]",145771261.0,Madrid,Neural Information Processing Systems,"['marginalization', 'perspective', 'approach', 'deep learning', 'Deep Learning']"
0e779fd59353a7f1f5b559b9d65fa4bfe367890c,Geometric Deep Learning: Going beyond Euclidean data,https://www.semanticscholar.org/paper/0e779fd59353a7f1f5b559b9d65fa4bfe367890c,JournalArticle,"Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.",2016,2825,34,"[1732570.0, 143627859.0, 1688882.0, 3149531.0, 1697397.0]",1732570.0,Bern,IEEE Signal Processing Magazine,"['networks', 'data', 'structure', 'deep learning', 'Deep Learning']"
94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,Traffic Flow Prediction With Big Data: A Deep Learning Approach,https://www.semanticscholar.org/paper/94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,JournalArticle,"Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.",2015,2430,16,"[34859953.0, 2241838.0, 1903243.0, 2146248481.0, 143769760.0]",34859953.0,Minsk,IEEE transactions on intelligent transportation systems (Print),"['traffic', 'flow', 'prediction', 'deep learning', 'Deep Learning']"
94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,Traffic Flow Prediction With Big Data: A Deep Learning Approach,https://www.semanticscholar.org/paper/94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,JournalArticle,"Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.",2015,2430,16,"[34859953.0, 2241838.0, 1903243.0, 2146248481.0, 143769760.0]",34859953.0,Minsk,IEEE transactions on intelligent transportation systems (Print),"['traffic', 'flow', 'prediction', 'big data', 'Deep Learning']"
94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,Traffic Flow Prediction With Big Data: A Deep Learning Approach,https://www.semanticscholar.org/paper/94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,JournalArticle,"Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.",2015,2430,16,"[34859953.0, 2241838.0, 1903243.0, 2146248481.0, 143769760.0]",34859953.0,Prague,IEEE transactions on intelligent transportation systems (Print),"['traffic', 'flow', 'prediction', 'deep learning', 'Big Data']"
94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,Traffic Flow Prediction With Big Data: A Deep Learning Approach,https://www.semanticscholar.org/paper/94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,JournalArticle,"Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.",2015,2430,16,"[34859953.0, 2241838.0, 1903243.0, 2146248481.0, 143769760.0]",34859953.0,Prague,IEEE transactions on intelligent transportation systems (Print),"['traffic', 'flow', 'prediction', 'big data', 'Big Data']"
a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f,"Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning",https://www.semanticscholar.org/paper/a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f,JournalArticle,"Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.",2016,4153,35,"[1797022.0, 144531567.0, 39703662.0, 50706692.0, 1742135.0, 37754137.0, 1722252.0, 1691549.0, 144838131.0]",1797022.0,Monaco,IEEE Transactions on Medical Imaging,"['image', 'cnn', 'detection', 'deep learning', 'Deep Learning']"
53103ae318a19569ac82cee5062de2cf73bf386c,Splitting phonons: Building a platform for linear mechanical quantum computing,https://www.semanticscholar.org/paper/53103ae318a19569ac82cee5062de2cf73bf386c,JournalArticle,"Linear optical quantum computing provides a desirable approach to quantum computing, with only a short list of required computational elements. The similarity between photons and phonons points to the interesting potential for linear mechanical quantum computing using phonons in place of photons. Although single-phonon sources and detectors have been demonstrated, a phononic beam splitter element remains an outstanding requirement. Here we demonstrate such an element, using two superconducting qubits to fully characterize a beam splitter with single phonons. We further use the beam splitter to demonstrate two-phonon interference, a requirement for two-qubit gates in linear computing. This advances a new solid-state system for implementing linear quantum computing, further providing straightforward conversion between itinerant phonons and superconducting qubits. Description Editor’s summary Phonons are the fundamental quantum vibrations within materials, with individual phonons representing the collective motion of many trillions of atoms. Efforts are underway to determine whether these mechanical vibrations can be developed into a quantum-computing architecture just like their optical cousin, photons. Qiao et al. demonstrate a beam splitter for single phonons and controlled two-phonon interference. Adding to the ability to launch and detect single phonons, a beam splitter now provides the final piece in the toolbox to develop a mechanically based platform for quantum computing. —Ian S. Osborne A beam splitter for phonons completes the toolbox required to develop a mechanically based quantum computing system.",2023,20,380,"[2249759901.0, 15642782.0, 2249761102.0, 2282113562.0, 143717184.0, 92647195.0, 36460588.0, 1499289559.0, 2206684539.0, 102445836.0, 2175142898.0, 2249760489.0]",2249759901.0,Valletta,Science,"['phonons', 'quantum', 'computing', 'quantum computing', 'Quantum Computing']"
6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5,MQT Bench: Benchmarking Software and Design Automation Tools for Quantum Computing,https://www.semanticscholar.org/paper/6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5,JournalArticle,"Quantum software tools for a wide variety of design tasks on and across different levels of abstraction are crucial in order to eventually realize useful quantum applications. This requires practical and relevant benchmarks for new software tools to be empirically evaluated and compared to the current state of the art. Although benchmarks for specific design tasks are commonly available, the demand for an overarching cross-level benchmark suite has not yet been fully met and there is no mutual consolidation in how quantum software tools are evaluated thus far. In this work, we propose the MQT Bench benchmark suite (as part of the Munich Quantum Toolkit, MQT) based on four core traits: (1) cross-level support for different abstraction levels, (2) accessibility via an easy-to-use web interface (https://www.cda.cit.tum.de/mqtbench/) and a Python package, (3) provision of a broad selection of benchmarks to facilitate generalizability, as well as (4) extendability to future algorithms, gate-sets, and hardware architectures. By comprising more than 70,000 benchmark circuits ranging from 2 to 130 qubits on four abstraction levels, MQT Bench presents a first step towards benchmarking different abstraction levels with a single benchmark suite to increase comparability, reproducibility, and transparency.",2022,43,7,"[2103454094.0, 121839134.0, 144385184.0]",2103454094.0,Warsaw,Quantum,"['quantum', 'levels', 'benchmark', 'quantum computing', 'Quantum Computing']"
ef35da3a3c471a6a15c7a3b09586483eb50cbef0,Practical application-specific advantage through hybrid quantum computing,https://www.semanticscholar.org/paper/ef35da3a3c471a6a15c7a3b09586483eb50cbef0,JournalArticle,"Quantum computing promises to tackle technological and industrial problems insurmountable for classical computers. However, today's quantum computers still have limited demonstrable functionality, and it is expected that scaling up to millions of qubits is required for them to live up to this touted promise. The feasible route in achieving practical quantum advantage goals is to implement a hybrid operational mode that realizes the cohesion of quantum and classical computers. Here we present a hybrid quantum cloud based on a memory-centric and heterogeneous multiprocessing architecture, integrated into a high-performance computing data center grade environment. We demonstrate that utilizing the quantum cloud, our hybrid quantum algorithms including Quantum Encoding (QuEnc), Hybrid Quantum Neural Networks and Tensor Networks enable advantages in optimization, machine learning, and simulation fields. We show the advantage of hybrid algorithms compared to standard classical algorithms in both the computational speed and quality of the solution. The achieved advance in hybrid quantum hardware and software makes quantum computing useful in practice today.",2022,29,abs/2205.04858,"[31347453.0, 2087046300.0, 2164706014.0, 2557591.0, 1388013676.0, 2050228932.0, 2065938.0, 2140212060.0, 47610941.0, 143700248.0]",31347453.0,Podgorica,arXiv.org,"['quantum', 'computers', 'algorithms', 'quantum computing', 'Quantum Computing']"
b000a4b30f6206f6cfb033a79aad1ba810c972a4,Perceval: A Software Platform for Discrete Variable Photonic Quantum Computing,https://www.semanticscholar.org/paper/b000a4b30f6206f6cfb033a79aad1ba810c972a4,JournalArticle,"We introduce Perceval, an open-source software platform for simulating and interfacing with discrete-variable photonic quantum computers, and describe its main features and components. Its Python front-end allows photonic circuits to be composed from basic photonic building blocks like photon sources, beam splitters, phase-shifters and detectors. A variety of computational back-ends are available and optimised for different use-cases. These use state-of-the-art simulation techniques covering both weak simulation, or sampling, and strong simulation. We give examples of Perceval in action by reproducing a variety of photonic experiments and simulating photonic implementations of a range of quantum algorithms, from Grover's and Shor's to examples of quantum machine learning. Perceval is intended to be a useful toolkit for experimentalists wishing to easily model, design, simulate, or optimise a discrete-variable photonic experiment, for theoreticians wishing to design algorithms and applications for discrete-variable photonic quantum computing platforms, and for application designers wishing to evaluate algorithms on available state-of-the-art photonic quantum computers.",2022,27,7,"[2161241855.0, 2078675007.0, 2161242301.0, 2161242145.0, 2161242134.0, 2161241528.0, 5344810.0, 102509601.0, 88251701.0, 103146051.0, 50738985.0, 48700430.0, 2591230.0, 47627363.0, 33091515.0, 48530805.0]",2161241855.0,Prague,Quantum,"['quantum', 'simulation', 'computers', 'quantum computing', 'Quantum Computing']"
b977e8de38dc0d13817bca1ed20036badfe2a58c,Pulse based Variational Quantum Optimal Control for hybrid quantum computing,https://www.semanticscholar.org/paper/b977e8de38dc0d13817bca1ed20036badfe2a58c,JournalArticle,"This work studies pulse based variational quantum algorithms (VQAs), which are designed to determine the ground state of a quantum mechanical system by combining classical and quantum hardware. In contrast to more standard gate based methods, pulse based methods aim to directly optimize the laser pulses interacting with the qubits, instead of using some parametrized gate based circuit. Using the mathematical formalism of optimal control, these laser pulses are optimized. This method has been used in quantum computing to optimize pulses for quantum gate implementations, but has only recently been proposed for full optimization in VQAs. Pulse based methods have several advantages over gate based methods such as faster state preparation, simpler implementation and more freedom in moving through the state space. Based on these ideas, we present the development of a novel adjoint based variational method. This method can be tailored towards and applied in neutral atom quantum computers. This method of pulse based variational quantum optimal control is able to approximate molecular ground states of simple molecules up to chemical accuracy and is able to compete with the gate based variational quantum eigensolver in terms of total number of quantum evaluations. The total evolution time T and the form of the control Hamiltonian Hc are important factors in the convergence behavior to the ground state energy, both having influence on the quantum speed limit and the controllability of the system.",2022,16,7,"[32519404.0, 144910138.0, 153538835.0]",32519404.0,Rome,Quantum,"['quantum', 'pulse', 'state', 'quantum computing', 'Quantum Computing']"
6fed828456964d29517f6caf31b700d8aec82153,Enabling multi-programming mechanism for quantum computing in the NISQ era,https://www.semanticscholar.org/paper/6fed828456964d29517f6caf31b700d8aec82153,JournalArticle,"NISQ devices have several physical limitations and unavoidable noisy quantum operations, and only small circuits can be executed on a quantum machine to get reliable results. This leads to the quantum hardware under-utilization issue. Here, we address this problem and improve the quantum hardware throughput by proposing a Quantum Multi-programming Compiler (QuMC) to execute multiple quantum circuits on quantum hardware simultaneously. This approach can also reduce the total runtime of circuits. We first introduce a parallelism manager to select an appropriate number of circuits to be executed at the same time. Second, we present two different qubit partitioning algorithms to allocate reliable partitions to multiple circuits – a greedy and a heuristic. Third, we use the Simultaneous Randomized Benchmarking protocol to characterize the crosstalk properties and consider them in the qubit partition process to avoid the crosstalk effect during simultaneous executions. Finally, we enhance the mapping transition algorithm to make circuits executable on hardware using a decreased number of inserted gates. We demonstrate the performance of our QuMC approach by executing circuits of different sizes on IBM quantum hardware simultaneously. We also investigate this method on VQE algorithm to reduce its overhead.",2021,143,abs/2102.05321,"[2055318525.0, 3182535.0]",2055318525.0,Ljubljana,Quantum,"['quantum', 'circuits', 'hardware', 'quantum computing', 'Quantum Computing']"
208ac1f2ec9bf367a9981fedb6d9ea6aa9889099,Low-overhead fault-tolerant quantum computing using long-range connectivity,https://www.semanticscholar.org/paper/208ac1f2ec9bf367a9981fedb6d9ea6aa9889099,JournalArticle,"Vast numbers of qubits will be needed for large-scale quantum computing because of the overheads associated with error correction. We present a scheme for low-overhead fault-tolerant quantum computation based on quantum low-density parity-check (LDPC) codes, where long-range interactions enable many logical qubits to be encoded with a modest number of physical qubits. In our approach, logic gates operate via logical Pauli measurements that preserve both the protection of the LDPC codes and the low overheads in terms of the required number of additional qubits. Compared with surface codes with the same code distance, we estimate order-of-magnitude improvements in the overheads for processing around 100 logical qubits using this approach. Given the high thresholds demonstrated by LDPC codes, our estimates suggest that fault-tolerant quantum computation at this scale may be achievable with a few thousand physical qubits at comparable error rates to what is needed for current approaches.",2021,41,8,"[2142766693.0, 40026378.0, 3029700.0, 46610676.0]",2142766693.0,Sarajevo,Science Advances,"['qubits', 'quantum', 'overheads', 'quantum computing', 'Quantum Computing']"
3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b,Quantum PUF for Security and Trust in Quantum Computing,https://www.semanticscholar.org/paper/3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b,JournalArticle,"Quantum computing is a promising paradigm to solve computationally intractable problems. Various companies such as, IBM, Rigetti and D-Wave offer quantum computers using a cloud-based platform that possess several interesting features namely, (i) quantum hardware with various number of qubits and coupling maps exist at the cloud end that offer different computing capabilities; (ii) multiple hardware with identical coupling maps exist in the suite; (iii) coupling map of larger hardware with more number of qubits can fit the coupling map of many smaller hardware; (iv) the quality of each of the hardware is distinct; (v) user cannot validate the origination of the result obtained from a quantum hardware. In other words, the user relies on the scheduler of the cloud provider to allocate the requested hardware; (vi) the queue of quantum programs at the cloud end is typically long and maximizing the throughput, which is the key to reducing costs and helping the scientific community in their explorations. The above factors motivate a new threat model with following possibilities: (a) in future, less-trustworthy quantum computers from 3rd parties can allocate poor quality hardware to save on cost or towards satisfying their falsely-advertised qubit or quantum hardware specifications; (b) the workload scheduling algorithm could have a bug or malicious code segment which will try to maximize throughput at the cost of allocation to poor fidelity hardware. Such bugs are possible for trustworthy providers; (c) a rogue employee in trusted cloud vendor could try to sabotage the vendor’s reputation by degrading the user compute fidelity just by tampering with the scheduling algorithm or rerouting the program; (d) a rogue employee can steal information by redirecting the programs to a 3rd party quantum hardware where they have full control. If the allocated hardware is inferior in quality, the user will suffer from poor quality result or longer convergence time. We propose two flavors of a Quantum Physically Unclonable Function (QuPUF) to address this issue- one based on superposition and another based on decoherence. Our experiments on real quantum hardware reveal that temporal variations in qubit quality can degrade the quality of the proposed QuPUF. We add a parametric rotation to the QuPUF for stability. Experiments on real IBM quantum hardware show that the proposed QuPUF can achieve inter-die Hamming Distance (HD) of 55% and intra-HD as low as 4%, as compared to ideal cases of 50% and 0% respectively. The proposed QuPUFs can also be used as a standalone solution for any other application.",2021,35,11,"[2074099273.0, 153329056.0, 71667767.0, 144695135.0, 144505057.0]",2074099273.0,Madrid,IEEE Journal on Emerging and Selected Topics in Circuits and Systems,"['hardware', 'quantum', 'quality', 'quantum computing', 'Quantum Computing']"
71ba8a8c75e0826f94eb53ee7a4566d4fc5b5256,Distributed Quantum Computing and Network Control for Accelerated VQE,https://www.semanticscholar.org/paper/71ba8a8c75e0826f94eb53ee7a4566d4fc5b5256,JournalArticle,"Interconnecting small quantum computers will be essential in the future for creating large-scale, robust quantum computers. Methods for distributing monolithic quantum algorithms efficiently are, thus, needed. In this article, we consider an approach for distributing the accelerated variational quantum eigensolver algorithm over arbitrary sized—in terms of number of qubits—distributed quantum computers. We consider approaches for distributing qubit assignments of the Ansatz states required to estimate the expectation value of Hamiltonian operators in quantum chemistry in a parallelized computation and provide a systematic approach to generate distributed quantum circuits for distributed quantum computing. Moreover, we propose an architecture for a distributed quantum control system in the context of centralized and decentralized network control.",2021,31,2,"[52221692.0, 2697097.0, 32610684.0]",52221692.0,Budapest,IEEE Transactions on Quantum Engineering,"['quantum', 'computers', 'approach', 'quantum computing', 'Quantum Computing']"
bcc82ce554942880814243fc8c08a88b9d2aad09,Reading the road: challenges and opportunities on the path to responsible innovation in quantum computing,https://www.semanticscholar.org/paper/bcc82ce554942880814243fc8c08a88b9d2aad09,JournalArticle,"ABSTRACT
 Novel technologies such as quantum computing present new opportunities to support societal needs, but societal engagement is vital to secure public trust. Quantum computing technologies are at a pivotal point in their journey from foundational research to deployment, creating a moment for society to investigate, reflect, and consult on their implications. Responsible Innovation (RI) is one method for considering impacts, engaging with societal needs, reflecting on any concerns, and influencing the trajectory of the innovation in response. This paper draws on the empirical work of the RI team embedded in the Networked Quantum Information Technologies Hub. The team investigated researchers’ perceptions of RI and their understanding of societal impacts of quantum technologies, and sought to gauge the challenges of embedding RI across a multi-disciplinary, large-scale enterprise such as the UK quantum programme. The work demonstrated some of the difficulties involved in embedding RI approaches, and in creating a dialogue between innovators and societies. Finally, the authors offer recommendations to policymakers, researchers, and industrial organisations, for better practice in responsible quantum computing, and to ensure that societal considerations are discussed alongside commercial motivations. Applying RI to quantum computing at this pivotal point has implications for RI in other emerging technologies.",2021,26,35,"[100782743.0, 2092826.0, 1771117.0]",100782743.0,Luxembourg,Technology Analysis & Strategic Management,"['quantum', 'technologies', 'ri', 'quantum computing', 'Quantum Computing']"
6be56f559a74c0124526242e70cbdfd16cbc60a7,Quantum Computing - from NISQ to PISQ,https://www.semanticscholar.org/paper/6be56f559a74c0124526242e70cbdfd16cbc60a7,JournalArticle,"Given the impeding timeline of developing good quality quantum processing units, it is the moment to rethink the approach to advance quantum computing research. Rather than waiting for quantum hardware technologies to mature, we need to start assessing in tandem the impact of the occurrence of quantum computing in various scientific fields.However, to this purpose, we need to use a complementary but quite different approach than proposed by the NISQ vision, which is heavily focused on and burdened by the engineering challenges.That is why we propose and advocate the PISQ-approach: Perfect Intermediate-Scale Quantum computing based on the already known concept of perfect qubits.This will allow researchers to focus much more on the development of new applications by defining the algorithms in terms of perfect qubits and evaluate them on quantum computing simulators that are executed on supercomputers.It is not the long-term solution but will currently allow universities to research on quantum logic and algorithms and companies can already start developing their internal know-how on quantum solutions.",2021,21,41,"[1737836.0, 144330671.0, 145594823.0]",1737836.0,Brussels,IEEE Micro,"['quantum', 'computing', 'approach', 'quantum computing', 'Quantum Computing']"
390bcf15a1b13cb0d5966859c35c69a31238e838,Optimized Compiler for Distributed Quantum Computing,https://www.semanticscholar.org/paper/390bcf15a1b13cb0d5966859c35c69a31238e838,JournalArticle,"Practical distributed quantum computing requires the development of efficient compilers, able to make quantum circuits compatible with some given hardware constraints. This problem is known to be tough, even for local computing. Here, we address it on distributed architectures. As generally assumed in this scenario, telegates represent the fundamental remote (inter-processor) operations. Each telegate consists of several tasks: (i) entanglement generation and distribution, (ii) local operations, and (iii) classical communications. Entanglement generations and distribution is an expensive resource, as it is time-consuming. To mitigate its impact, we model an optimization problem that combines running-time minimization with the usage of distributed entangled states. Specifically, we formulated the distributed compilation problem as a dynamic network flow. To enhance the solution space, we extend the formulation, by introducing a predicate that manipulates the circuit given in input and parallelizes telegate tasks. To evaluate our framework, we split the problem into three sub-problems, and solve it by means of an approximation routine. Experiments demonstrate that the run-time is resistant to the problem size scaling. Moreover, we apply the proposed algorithm to compile circuits under different topologies, showing that topologies with a higher ratio between edges and nodes give rise to shallower circuits.",2021,23,4,"[1509419973.0, 34702644.0, 98912691.0, 10719003.0, 2148614280.0, 48994822.0, 3062937.0]",1509419973.0,Nicosia,ACM Transactions on Quantum Computing,"['problem', 'circuits', 'quantum', 'quantum computing', 'Quantum Computing']"
f0eb17c6def9aaf95ef3c8ce12b9c3ceb9030274,Asleep at the wheel? Responsible Innovation in quantum computing,https://www.semanticscholar.org/paper/f0eb17c6def9aaf95ef3c8ce12b9c3ceb9030274,JournalArticle,"ABSTRACT Quantum computing is an emerging set of technologies which promise to transform aspects of computing in ways that, though increasingly defined, are still largely theoretical. Responsible Innovation (RI) asserts that technologies with potentially transformative capacity on society should be approached with care and forethought; this paper is based on applying RI in one of the UK’s National Quantum Technology Hubs. Quantum computing is at a key juncture as it emerges from the laboratory to be of interest commercially. This provides an opportunity to observe and influence the trajectory of this technology. Quantum computing is widely envisioned to have major impacts on computing and society; there are, however, great uncertainties about development timescales and the scope and impact of applications. From experiences with a major quantum computing project in the UK, we discuss the challenges in applying RI to quantum computing. Existing RI practices struggle to address the societal implications of such a complex and innovative technology. We argue that uncovering the visions and sociotechnical imaginaries that inform the development this technology enables RI to make valuable insights into future societal implications of quantum computing. This provides lessons for RI in emerging technologies more widely.",2021,15,33,"[2092826.0, 100782743.0, 1771117.0, 2149472612.0]",2092826.0,Vaduz,Technology Analysis & Strategic Management,"['quantum', 'ri', 'computing', 'quantum computing', 'Quantum Computing']"
ade9a900acc3c138021070537840488526796d35,A comparative study of universal quantum computing models: towards a physical unification,https://www.semanticscholar.org/paper/ade9a900acc3c138021070537840488526796d35,JournalArticle,"Quantum computing has been a fascinating research field in quantum physics. Recent progresses motivate us to study in depth the universal quantum computing models (UQCM), which lie at the foundation of quantum computing and have tight connections with fundamental physics. Although being developed decades ago, a physically concise principle or picture to formalize and understand UQCM is still lacking. This is challenging given the diversity of still-emerging models, but important to understand the difference between classical and quantum computing. In this work, we carried out a primary attempt to unify UQCM by classifying a few of them as two categories, hence making a table of models. With such a table, some known models or schemes appear as hybridization or combination of models, and more importantly, it leads to new schemes that have not been explored yet. Our study of UQCM also leads to some insights into quantum algorithms. This work reveals the importance and feasibility of systematic study of computing models.",2021,12,3,[2124015766.0],2124015766.0,Vaduz,Quantum Engineering,"['quantum', 'models', 'study', 'quantum computing', 'Quantum Computing']"
be082d70534db088315f2cc5b42c2fdcd58c1b8c,Optimality Study of Existing Quantum Computing Layout Synthesis Tools,https://www.semanticscholar.org/paper/be082d70534db088315f2cc5b42c2fdcd58c1b8c,JournalArticle,"Layout synthesis, an important step in quantum computing, processes quantum circuits to satisfy device layout constraints. In this paper, we construct QUEKO benchmarks for this problem, which have known optimal depths and gate counts. We use QUEKO to evaluate the optimality of current layout synthesis tools, including Cirq from Google, Qiskit from IBM, <inline-formula><tex-math notation=""LaTeX"">$\mathsf {t}|\mathsf {ket}\rangle$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant=""sans-serif"">t</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant=""sans-serif"">ket</mml:mi><mml:mo>〉</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""tan-ieq1-3009140.gif""/></alternatives></inline-formula> from Cambridge Quantum Computing, and a recent academic work. To our surprise, despite over a decade of research and development by academia and industry on compilation and synthesis for quantum circuits, we are still able to demonstrate large optimality gaps: 1.5-12x on average on a smaller device and 5-45x on average on a larger device. This suggests substantial room for improvement of the efficiency of quantum computer by better layout synthesis tools. Finally, we also prove the NP-completeness of the layout synthesis problem for quantum computing. We have made the QUEKO benchmarks open-source.",2020,59,70,"[2218979994.0, 2259796.0]",2218979994.0,Dublin,IEEE transactions on computers,"['quantum', 'synthesis', 'layout', 'quantum computing', 'Quantum Computing']"
48c73c389c3f36d70407eb8309a0b41578c15fc8,QProv: A provenance system for quantum computing,https://www.semanticscholar.org/paper/48c73c389c3f36d70407eb8309a0b41578c15fc8,JournalArticle,"Quantum computing promises breakthroughs in various application areas, such as machine learning, chemistry, or simulations. However, today’s quantum computers are error ‐ prone and have limited capabilities. This leads to various challenges when developing and executing quantum algorithms, for example, the mitigation of occurring errors or the selection of a suitable quantum computer to execute a certain quantum circuit. To address these challenges, detailed information about the quantum circuit to be executed as well as past executions, and the up ‐ to ‐ date information about the available quantum computers are required. Thus, this data must be continuously collected and stored in the long ‐ term, which is currently not supported. To overcome this problem, a provenance approach is introduced for quantum computing. Therefore, relevant provenance attributes that should be gathered in the area of quantum computing are identified. Furthermore, QProv, a provenance system that automatically collects the identified provenance attributes and provides them in a uniform manner to the user is introduced. Finally, a case study with the collected provenance data and corresponding use cases that can benefit from this provenance data are presented here.",2021,10,2,"[1474569575.0, 2890261.0, 1688415.0, 51418452.0, 1565015599.0]",1474569575.0,Chisinau,IET Quantum Communication,"['quantum', 'provenance', 'computing', 'quantum computing', 'Quantum Computing']"
2c3eef2f17369912e330281d54b535675077e4ca,Representing Paths in Graph Database Pattern Matching,https://www.semanticscholar.org/paper/2c3eef2f17369912e330281d54b535675077e4ca,JournalArticle,"
 Modern graph database query languages such as GQL, SQL/PGQ, and their academic predecessor G-Core promote paths to first-class citizens in the sense that their pattern matching facility can return
 paths
 , as opposed to only nodes and edges. This is challenging for database engines, since graphs can have a large number of paths between a given node pair, which can cause huge intermediate results in query evaluation.
 
 
 We introduce the concept of
 path multiset representations (PMRs)
 , which can represent multisets of paths exponentially succinctly and therefore bring significant advantages for representing intermediate results. We give a detailed theoretical analysis that shows that they are especially well-suited for representing results of regular path queries and extensions thereof involving counting, random sampling, and unions. Our experiments show that they drastically improve scalability for regular path query evaluation, with speedups of several orders of magnitude.
",2023,7,16,"[144352362.0, 2879007.0, 2164445100.0, 2261302914.0, 1709642.0, 2434366.0]",144352362.0,Belgrade,Proceedings of the VLDB Endowment,"['paths', 'query', 'results', 'graph database', 'Graph Database']"
d1ae4ab5047489c2b010c7ce72262982ad66ad60,ByteGraph: A High-Performance Distributed Graph Database in ByteDance,https://www.semanticscholar.org/paper/d1ae4ab5047489c2b010c7ce72262982ad66ad60,JournalArticle,"Most products at ByteDance, e.g., TikTok, Douyin, and Toutiao, naturally generate massive amounts of graph data. To efficiently store, query and update massive graph data is challenging for the broad range of products at ByteDance with various performance requirements. We categorize graph workloads at ByteDance into three types: online analytical, transaction, and serving processing, where each workload has its own characteristics. Existing graph databases have different performance bottlenecks in handling these workloads and none can efficiently handle the scale of graphs at ByteDance. We developed ByteGraph to process these graph workloads with high throughput, low latency and high scalability. There are several key designs in ByteGraph that make it efficient for processing our workloads, including edge-trees to store adjacency lists for high parallelism and low memory usage, adaptive optimizations on thread pools and indexes, and geographic replications to achieve fault tolerance and availability. ByteGraph has been in production use for several years and its performance has shown to be robust for processing a wide range of graph workloads at ByteDance.",2022,6,15,"[2145413970.0, 2108844303.0, 2167037428.0, 1490915504.0, 2145763285.0, 2109512262.0, 123816348.0, 2088215217.0, 2193954145.0, 32058742.0, 2116124684.0, 2144671306.0, 2424392.0, 2182246691.0, 2112662828.0, 2184079980.0, 113398129.0, 2695617.0, 2174235132.0, 2112409349.0, 46255707.0, 2157513188.0, 2153604994.0, 2113918898.0, 50841357.0, 2116502347.0]",2145413970.0,Podgorica,Proceedings of the VLDB Endowment,"['graph', 'workloads', 'bytedance', 'graph database', 'Graph Database']"
1cff064f815111a71a98afda7aee1867ad617901,Digital Contact Tracing Based on a Graph Database Algorithm for Emergency Management During the COVID-19 Epidemic: Case Study,https://www.semanticscholar.org/paper/1cff064f815111a71a98afda7aee1867ad617901,JournalArticle,"Background The COVID-19 epidemic is still spreading globally. Contact tracing is a vital strategy in epidemic emergency management; however, traditional contact tracing faces many limitations in practice. The application of digital technology provides an opportunity for local governments to trace the contacts of individuals with COVID-19 more comprehensively, efficiently, and precisely. Objective Our research aimed to provide new solutions to overcome the limitations of traditional contact tracing by introducing the organizational process, technical process, and main achievements of digital contact tracing in Hainan Province. Methods A graph database algorithm, which can efficiently process complex relational networks, was applied in Hainan Province; this algorithm relies on a governmental big data platform to analyze multisource COVID-19 epidemic data and build networks of relationships among high-risk infected individuals, the general population, vehicles, and public places to identify and trace contacts. We summarized the organizational and technical process of digital contact tracing in Hainan Province based on interviews and data analyses. Results An integrated emergency management command system and a multi-agency coordination mechanism were formed during the emergency management of the COVID-19 epidemic in Hainan Province. The collection, storage, analysis, and application of multisource epidemic data were realized based on the government’s big data platform using a centralized model. The graph database algorithm is compatible with this platform and can analyze multisource and heterogeneous big data related to the epidemic. These practices were used to quickly and accurately identify and trace 10,871 contacts among hundreds of thousands of epidemic data records; 378 closest contacts and a number of public places with high risk of infection were identified. A confirmed patient was found after quarantine measures were implemented by all contacts. Conclusions During the emergency management of the COVID-19 epidemic, Hainan Province used a graph database algorithm to trace contacts in a centralized model, which can identify infected individuals and high-risk public places more quickly and accurately. This practice can provide support to government agencies to implement precise, agile, and evidence-based emergency management measures and improve the responsiveness of the public health emergency response system. Strengthening data security, improving tracing accuracy, enabling intelligent data collection, and improving data-sharing mechanisms and technologies are directions for optimizing digital contact tracing.",2021,15,9,"[2054289955.0, 101108644.0, 2054838160.0, 1492120545.0, 2115905112.0]",2054289955.0,Monaco,JMIR mHealth and uHealth,"['data', 'epidemic', 'contact', 'graph database', 'Graph Database']"
a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854,"MillenniumDB: A Persistent, Open-Source, Graph Database",https://www.semanticscholar.org/paper/a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854,JournalArticle,"In this systems paper, we present MillenniumDB: a novel graph database engine that is modular, persistent, and open source. MillenniumDB is based on a graph data model, which we call domain graphs, that provides a simple abstraction upon which a variety of popular graph models can be supported. The engine itself is founded on a combination of tried and tested techniques from relational data management, state-of-the-art algorithms for worst-case-optimal joins, as well as graph-specific algorithms for evaluating path queries. In this paper, we present the main design principles underlying MillenniumDB, describing the abstract graph model and query semantics supported, the concrete data model and query syntax implemented, as well as the storage, indexing, query planning and query evaluation techniques used. We evaluate MillenniumDB over real-world data and queries from the Wikidata knowledge graph, where we find that it outperforms other popular persistent graph database engines (including both enterprise and open source alternatives) that support similar query features.",2021,13,abs/2111.01540,"[2434366.0, 2059572334.0, 2772109.0, 144658846.0, 1754091.0, 34652974.0, 144007515.0, 143678972.0, 1706922.0, 134212079.0]",2434366.0,Reykjavik,arXiv.org,"['graph', 'query', 'millenniumdb', 'graph database', 'Graph Database']"
27beaa5db6c37c611f082855c6385b264874b8f5,SynLethDB 2.0: a web-based knowledge graph database on synthetic lethality for novel anticancer drug discovery,https://www.semanticscholar.org/paper/27beaa5db6c37c611f082855c6385b264874b8f5,JournalArticle,"Two genes are synthetic lethal if mutations in both genes result in impaired cell viability, while mutation of either gene does not affect the cell survival. The potential usage of synthetic lethality (SL) in anticancer therapeutics has attracted many researchers to identify synthetic lethal gene pairs. To include newly identified SLs and more related knowledge, we present a new version of the SynLethDB database to facilitate the discovery of clinically relevant SLs. We extended the first version of SynLethDB database significantly by including new SLs identified through CRISPR screening, a knowledge graph about human SLs, and new web interface, etc. Over 16,000 new SLs and 26 types of other relationships have been added, encompassing relationships among 14,100 genes, 53 cancers, and 1,898 drugs, etc. Moreover, a brand-new web interface has been developed to include modules such as SL query by disease or compound, SL partner gene set enrichment analysis and knowledge graph browsing through a dynamic graph viewer. The data can be downloaded directly from the website or through the RESTful APIs. The database is accessible online at http://synlethdb.sist.shanghaitech.edu.cn/v2.",2021,17,2022,"[2146041856.0, 150238039.0, 2155815886.0, 2152719037.0, 2108965351.0, 2146671969.0, 2115692258.0]",2146041856.0,Belgrade,bioRxiv,"['sls', 'genes', 'gene', 'graph database', 'Graph Database']"
f4cfc7cbad257f1688772d59f694c16189dba811,Columnar Storage and List-based Processing for Graph Database Management Systems,https://www.semanticscholar.org/paper/f4cfc7cbad257f1688772d59f694c16189dba811,JournalArticle,"We revisit column-oriented storage and query processing techniques in the context of contemporary graph database management systems (GDBMSs). Similar to column-oriented RDBMSs, GDBMSs support read-heavy analytical workloads that however have fundamentally different data access patterns than traditional analytical workloads. We first derive a set of desiderata for optimizing storage and query processors of GDBMS based on their access patterns. We then present the design of columnar storage, compression, and query processing techniques based on these desiderata. In addition to showing direct integration of existing techniques from columnar RDBMSs, we also propose novel ones that are optimized for GDBMSs. These include a novel list-based query processor, which avoids expensive data copies of traditional block-based processors under many-to-many joins, a new data structure we call single-indexed edge property pages and an accompanying edge ID scheme, and a new application of Jacobson's bit vector index for compressing NULL values and empty lists. We integrated our techniques into the GraphflowDB in-memory GDBMS. Through extensive experiments, we demonstrate the scalability and query performance benefits of our techniques.",2021,12,14,"[1505708061.0, 26391899.0, 1783781.0]",1505708061.0,Oslo,Proceedings of the VLDB Endowment,"['query', 'techniques', 'storage', 'graph database', 'Graph Database']"
6b3756d32ab5b0a5715a5cfc3672290d2d643017,A Graph Database Representation of Portuguese Criminal-Related Documents,https://www.semanticscholar.org/paper/6b3756d32ab5b0a5715a5cfc3672290d2d643017,JournalArticle,"Organizations have been challenged by the need to process an increasing amount of data, both structured and unstructured, retrieved from heterogeneous sources. Criminal investigation police are among these organizations, as they have to manually process a vast number of criminal reports, news articles related to crimes, occurrence and evidence reports, and other unstructured documents. Automatic extraction and representation of data and knowledge in such documents is an essential task to reduce the manual analysis burden and to automate the discovering of names and entities relationships that may exist in a case. This paper presents SEMCrime, a framework used to extract and classify named-entities and relations in Portuguese criminal reports and documents, and represent the data retrieved into a graph database. A 5WH1 (Who, What, Why, Where, When, and How) information extraction method was applied, and a graph database representation was used to store and visualize the relations extracted from the documents. Promising results were obtained with a prototype developed to evaluate the framework, namely a name-entity recognition with an F-Measure of 0.73, and a 5W1H information extraction performance with an F-Measure of 0.65.",2021,7,8,"[92089600.0, 2560362.0, 2103992921.0]",92089600.0,Valletta,Informatics,"['documents', 'data', 'extraction', 'graph database', 'Graph Database']"
db6084fdb3baceddacdc726474722debe1ef7e65,TigerGraph: A Native MPP Graph Database,https://www.semanticscholar.org/paper/db6084fdb3baceddacdc726474722debe1ef7e65,JournalArticle,"We present TigerGraph, a graph database system built from the ground up to support massively parallel computation of queries and analytics. 
TigerGraph's high-level query language, GSQL, is designed for compatibility with SQL, while simultaneously allowing NoSQL programmers to continue thinking in Bulk-Synchronous Processing (BSP) terms and reap the benefits of high-level specification. 
GSQL is sufficiently high-level to allow declarative SQL-style programming, yet sufficiently expressive to concisely specify the sophisticated iterative algorithms required by modern graph analytics and traditionally coded in general-purpose programming languages like C++ and Java. 
We report very strong scale-up and scale-out performance over a benchmark we published on GitHub for full reproducibility.",2019,53,abs/1901.08248,"[50136367.0, 47103320.0, 1738190.0, 2058056841.0]",50136367.0,Berlin,arXiv.org,"['highlevel', 'graph', 'gsql', 'graph database', 'Graph Database']"
a281d563261c738f13b9e58a525e7e265a619c93,Suitability of Graph Database Technology for the Analysis of Spatio-Temporal Data,https://www.semanticscholar.org/paper/a281d563261c738f13b9e58a525e7e265a619c93,JournalArticle,"Every day large quantities of spatio-temporal data are captured, whether by Web-based companies for social data mining or by other industries for a variety of applications ranging from disaster relief to marine data analysis. Making sense of all this data dramatically increases the need for intelligent backend systems to provide realtime query response times while scaling well (in terms of storage and performance) with increasing quantities of structured or semi-structured, multi-dimensional data. Currently, relational database solutions with spatial extensions such as PostGIS, seem to come to their limits. However, the use of graph database technology has been rising in popularity and has been found to handle graph-like spatio-temporal data much more effectively. Motivated by the need to effectively store multi-dimensional, interconnected data, this paper investigates whether or not graph database technology is better suited when compared to the extended relational approach. Three database technologies will be investigated using real world datasets namely: PostgreSQL, JanusGraph, and TigerGraph. The datasets used are the Yelp challenge dataset and an ambulance response simulation dataset, thus combining real world spatial data with realistic simulations offering more control over the dataset. Our extensive evaluation is based on how each database performs under practical data analysis scenarios similar to those found on enterprise level.",2020,5,12,"[1727622943.0, 2403851.0, 1720266.0]",1727622943.0,Copenhagen,Future Internet,"['data', 'database', 'quantities', 'graph database', 'Graph Database']"
957f5b1e7ca48891c2e279aefbfa0f04d989c21e,In-Depth Benchmarking of Graph Database Systems with the Linked Data Benchmark Council (LDBC) Social Network Benchmark (SNB),https://www.semanticscholar.org/paper/957f5b1e7ca48891c2e279aefbfa0f04d989c21e,JournalArticle,"In this study, we present the first results of a complete implementation of the LDBC SNB benchmark -- interactive short, interactive complex, and business intelligence -- in two native graph database systems---Neo4j and TigerGraph. In addition to thoroughly evaluating the performance of all of the 46 queries in the benchmark on four scale factors -- SF-1, SF-10, SF-100, and SF-1000 -- and three computing architectures -- on premise and in the cloud -- we also measure the bulk loading time and storage size. Our results show that TigerGraph is consistently outperforming Neo4j on the majority of the queries---by two or more orders of magnitude (100X factor) on certain interactive complex and business intelligence queries. The gap increases with the size of the data since only TigerGraph is able to scale to SF-1000---Neo4j finishes only 12 of the 25 business intelligence queries in reasonable time. Nonetheless, Neo4j is generally faster at bulk loading graph data up to SF-100. A key to our study is the active involvement of the vendors in the tuning of their platforms. In order to encourage reproducibility, we make all the code, scripts, and configuration parameters publicly available online.",2019,13,abs/1907.07405,[2289824.0],2289824.0,Vienna,arXiv.org,"['business', 'intelligence', 'study', 'graph database', 'Graph Database']"
537f5e8e4139392cd2d108f32495e5b2b80151ac,Implementation of a HL7-CQL Engine Using the Graph Database Neo4J,https://www.semanticscholar.org/paper/537f5e8e4139392cd2d108f32495e5b2b80151ac,JournalArticle,"The Clinical Quality Language (CQL) is a useful tool for defining search requests for data stores containing FHIR data. Unfortunately, there are only few execution engines that are able to evaluate CQL queries. As FHIR data represents a graph structure, the authors pursue the approach of storing all data contained in a FHIR server in the graph database Neo4J and to translate CQL queries into Neo4J's query language Cypher. The query results returned by the graph database are retranslated into their FHIR representation and returned to the querying user. The approach has been positively tested on publicly available FHIR servers with a handcrafted set of example CQL queries.",2019,5,267,"[1785288.0, 98034859.0, 40992258.0, 38827385.0, 145222303.0, 1755795.0, 1707592.0]",1785288.0,Ljubljana,"Jahrestagung der Deutschen Gesellschaft für Medizinische Informatik, Biometrie und Epidemiologie","['data', 'cql', 'graph', 'graph database', 'Graph Database']"
c2528e88d5554e9df9f9d482ad46cb5331c4d794,Benchmarking Graph Database Backends - What Works Well with Wikidata?,https://www.semanticscholar.org/paper/c2528e88d5554e9df9f9d482ad46cb5331c4d794,JournalArticle,"Knowledge bases often utilize graphs as logical model. RDF-based knowledge bases (KB) are prime examples, as RDF (Resource Description Framework) does use graph as logical model. Graph databases are an emerging breed of NoSQL-type databases, offering graph as the logical model. Although there are specialized databases, the so-called triple stores, for storing RDF data, graph databases can also be promising candidates for storing knowledge. In this paper, we benchmark different graph database implementations loaded with Wikidata, a real-life, large-scale knowledge base. Graph databases come in all shapes and sizes, offer different APIs and graph models. Hence we used a measurement system, that can abstract away the API differences. For the modeling aspect, we made measurements with different graph encodings previously suggested in the literature, in order to observe the impact of the encoding aspect on the overall performance. 
 ",2019,8,24,"[2058456649.0, 2057440722.0, 144521155.0]",2058456649.0,Tallinn,Acta Cybernetica,"['graph', 'knowledge', 'model', 'graph database', 'Graph Database']"
f295157f37cfb43cd8d8d2690ea124edc5ea59c2,Beyond Macrobenchmarks: Microbenchmark-based Graph Database Evaluation,https://www.semanticscholar.org/paper/f295157f37cfb43cd8d8d2690ea124edc5ea59c2,JournalArticle,"
 Despite the increasing interest in graph databases their requirements and specifications are not yet fully understood by everyone, leading to a great deal of variation in the supported functionalities and the achieved performances. In this work, we provide a comprehensive study of the existing graph database systems. We introduce a novel microbenchmarking framework that provides insights on their performance that go beyond what macro-benchmarks can offer. The framework includes the largest set of queries and operators so far considered. The graph database systems are evaluated on synthetic and real data, from different domains, and at scales much larger than any previous work. The framework is materialized as an open-source suite and is easily extended to new datasets, systems, and queries
 1
 .
",2018,47,12,"[2574504.0, 3415440.0, 2163752.0]",2574504.0,Ljubljana,Proceedings of the VLDB Endowment,"['graph', 'systems', 'framework', 'graph database', 'Graph Database']"
dcbaf58b16ac7ef947879ea37c021466357b291a,Use of Graph Database for the Integration of Heterogeneous Biological Data,https://www.semanticscholar.org/paper/dcbaf58b16ac7ef947879ea37c021466357b291a,JournalArticle,"Understanding complex relationships among heterogeneous biological data is one of the fundamental goals in biology. In most cases, diverse biological data are stored in relational databases, such as MySQL and Oracle, which store data in multiple tables and then infer relationships by multiple-join statements. Recently, a new type of database, called the graph-based database, was developed to natively represent various kinds of complex relationships, and it is widely used among computer science communities and IT industries. Here, we demonstrate the feasibility of using a graph-based database for complex biological relationships by comparing the performance between MySQL and Neo4j, one of the most widely used graph databases. We collected various biological data (protein-protein interaction, drug-target, gene-disease, etc.) from several existing sources, removed duplicate and redundant data, and finally constructed a graph database containing 114,550 nodes and 82,674,321 relationships. When we tested the query execution performance of MySQL versus Neo4j, we found that Neo4j outperformed MySQL in all cases. While Neo4j exhibited a very fast response for various queries, MySQL exhibited latent or unfinished responses for complex queries with multiple-join statements. These results show that using graph-based databases, such as Neo4j, is an efficient way to store complex biological relationships. Moreover, querying a graph database in diverse ways has the potential to reveal novel relationships among heterogeneous biological data.",2017,76,15,"[3515169.0, 2248549493.0, 2249671765.0]",3515169.0,Prague,Genomics & Informatics,"['relationships', 'data', 'mysql', 'graph database', 'Graph Database']"
4cd033a56b19f87f6adfefeef5fcc990306ecf40,Neo4j graph database realizes efficient storage performance of oilfield ontology,https://www.semanticscholar.org/paper/4cd033a56b19f87f6adfefeef5fcc990306ecf40,JournalArticle,"The integration of oilfield multidisciplinary ontology is increasingly important for the growth of the Semantic Web. However, current methods encounter performance bottlenecks either in storing data and searching for information when processing large amounts of data. To overcome these challenges, we propose a domain-ontology process based on the Neo4j graph database. In this paper, we focus on data storage and information retrieval of oilfield ontology. We have designed mapping rules from ontology files to regulate the Neo4j database, which can greatly reduce the required storage space. A two-tier index architecture, including object and triad indexing, is used to keep loading times low and match with different patterns for accurate retrieval. Therefore, we propose a retrieval method based on this architecture. Based on our evaluation, the retrieval method can save 13.04% of the storage space and improve retrieval efficiency by more than 30 times compared with the methods of relational databases.",2018,34,13,"[51417783.0, 2115736466.0, 2510306.0, 2108756985.0, 145510499.0, 51917504.0]",51417783.0,Paris,PLoS ONE,"['retrieval', 'ontology', 'data', 'graph database', 'Graph Database']"
9a0965beef113cc37491004b1848149e00300561,A Graph Database Model for Knowledge Extracted from Place Descriptions,https://www.semanticscholar.org/paper/9a0965beef113cc37491004b1848149e00300561,JournalArticle,"Everyday place descriptions provide a rich source of knowledge about places and their relative locations. This research proposes a place graph model for modelling this spatial, non-spatial, and contextual knowledge from place descriptions. The model extends a prior place graph, and overcomes a number of limitations. The model is implemented using a graph database, and a management system has also been developed that allows operations including querying, mapping, and visualizing the stored knowledge in an extended place graph. Then three experimental tasks, namely georeferencing, reasoning, and querying, are selected to demonstrate the superiority of the extended model.",2018,22,7,"[2149052018.0, 2074432.0, 145285033.0, 1686534.0]",2149052018.0,Stockholm,ISPRS Int. J. Geo Inf.,"['place', 'graph', 'model', 'graph database', 'Graph Database']"
5f7f10f913ecc478ff7ba304c265fd3c700b47d7,Modeling Graph Database Schema,https://www.semanticscholar.org/paper/5f7f10f913ecc478ff7ba304c265fd3c700b47d7,JournalArticle,"The authors present a new method for creating a graph database schema (GDBS) based on an entity-relationship diagram (ERD) of the application domain, which is mapped to a GDBS in a two-step process. First, the original ERD is adjusted to a semantically equivalent ERD (enabling it to be mapped in step two). In the second step, the adjusted ERD is mapped to a GDBS according to specific rules. The resulting GDBS includes integrity constraints that enrich existing graph databases.",2017,35,19,"[1414019666.0, 1732091.0, 1762969.0, 1753089.0]",1414019666.0,Stockholm,IT Professional,"['gdbs', 'erd', 'graph', 'graph database', 'Graph Database']"
ce3285bf1853f00c00535325851df5c33a0fc5d6,The Fragment Network: A Chemistry Recommendation Engine Built Using a Graph Database.,https://www.semanticscholar.org/paper/ce3285bf1853f00c00535325851df5c33a0fc5d6,JournalArticle,"The hit validation stage of a fragment-based drug discovery campaign involves probing the SAR around one or more fragment hits. This often requires a search for similar compounds in a corporate collection or from commercial suppliers. The Fragment Network is a graph database that allows a user to efficiently search chemical space around a compound of interest. The result set is chemically intuitive, naturally grouped by substitution pattern and meaningfully sorted according to the number of observations of each transformation in medicinal chemistry databases. This paper describes the algorithms used to construct and search the Fragment Network and provides examples of how it may be used in a drug discovery context.",2017,29,60 14,"[145366422.0, 32682297.0, 2462060.0]",145366422.0,Copenhagen,Journal of Medicinal Chemistry,"['fragment', 'drug', 'discovery', 'graph database', 'Graph Database']"
448959eff044f02040ded5afd483b7c4e811b0ac,Engineering Knowledge Graph from Patent Database,https://www.semanticscholar.org/paper/448959eff044f02040ded5afd483b7c4e811b0ac,JournalArticle,"
 We propose a large, scalable engineering knowledge graph, comprising sets of real-world engineering “facts” as < entity, relationship, entity > triples that are found in the patent database. We apply a set of rules based on the syntactic and lexical properties of claims in a patent document to extract facts. We aggregate these facts within each patent document and integrate the aggregated sets of facts across the patent database to obtain an engineering knowledge graph. Such a knowledge graph is expected to support inference, reasoning, and recalling in various engineering tasks. The knowledge graph has a greater size and coverage in comparison with the previously used knowledge graphs and semantic networks in the engineering literature.",2021,41,22,"[51471694.0, 1906960.0, 2053320598.0, 145990580.0]",51471694.0,Monaco,Journal of Computing and Information Science in Engineering,"['engineering', 'knowledge', 'graph', 'graph database', 'Graph Database']"
e89c37b7c2ff465db43c4b9f674867ec4b98aa8b,EpiGeNet: A Graph Database of Interdependencies Between Genetic and Epigenetic Events in Colorectal Cancer,https://www.semanticscholar.org/paper/e89c37b7c2ff465db43c4b9f674867ec4b98aa8b,JournalArticle,"The development of colorectal cancer (CRC)-the third most common cancer type-has been associated with deregulations of cellular mechanisms stimulated by both genetic and epigenetic events. StatEpigen is a manually curated and annotated database, containing information on interdependencies between genetic and epigenetic signals, and specialized currently for CRC research. Although StatEpigen provides a well-developed graphical user interface for information retrieval, advanced queries involving associations between multiple concepts can benefit from more detailed graph representation of the integrated data. This can be achieved by using a graph database (NoSQL) approach. Data were extracted from StatEpigen and imported to our newly developed EpiGeNet, a graph database for storage and querying of conditional relationships between molecular (genetic and epigenetic) events observed at different stages of colorectal oncogenesis. We illustrate the enhanced capability of EpiGeNet for exploration of different queries related to colorectal tumor progression; specifically, we demonstrate the query process for (i) stage-specific molecular events, (ii) most frequently observed genetic and epigenetic interdependencies in colon adenoma, and (iii) paths connecting key genes reported in CRC and associated events. The EpiGeNet framework offers improved capability for management and visualization of data on molecular events specific to CRC initiation and progression.",2017,19,24 10,"[8613913.0, 50701671.0, 47631235.0, 36994619.0, 48107154.0, 31590259.0, 1679216.0, 48662628.0]",8613913.0,Bratislava,J. Comput. Biol.,"['events', 'statepigen', 'database', 'graph database', 'Graph Database']"
ce51eff5b529ee572dab1c1f38f20adc8e89bab2,System G Distributed Graph Database,https://www.semanticscholar.org/paper/ce51eff5b529ee572dab1c1f38f20adc8e89bab2,JournalArticle,"Motivated by the need to extract knowledge and value frominterconnected data, graph analytics on big data is a veryactive area of research in both industry and academia. Tosupport graph analytics efficiently a large number of in mem-ory graph libraries, graph processing systems and graphdatabases have emerged. Projects in each of these cate-gories focus on particular aspects such as static versus dy-namic graphs, off line versus on line processing, small versuslarge graphs, etc.While there has been much advance in graph processingin the past decades, there is still a need for a fast graph pro-cessing, using a cluster of machines with distributed storage.In this paper, we discuss a novel distributed graph databasecalled System G designed for efficient graph data storage andprocessing on modern computing architectures. In particu-lar we describe a single node graph database and a runtimeand communication layer that allows us to compose a dis-tributed graph database from multiple single node instances.From various industry requirements, we find that fast inser-tions and large volume concurrent queries are critical partsof the graph databases and we optimize our database forsuch features. We experimentally show the efficiency ofSystem G for storing data and processing graph queries onstate-of-the-art platforms.",2018,4,abs/1802.03057,"[49573525.0, 2231831.0, 2108602524.0, 48239920.0, 2059069849.0, 3166546.0, 2108965391.0, 35638374.0]",49573525.0,San Marino,arXiv.org,"['graph', 'data', 'database', 'graph database', 'Graph Database']"
bbb52447f2f38aad9613ba026f88b57637ffcbea,Towards FAIRer Biological Knowledge Networks Using a Hybrid Linked Data and Graph Database Approach,https://www.semanticscholar.org/paper/bbb52447f2f38aad9613ba026f88b57637ffcbea,JournalArticle,"Abstract The speed and accuracy of new scientific discoveries – be it by humans or artificial intelligence – depends on the quality of the underlying data and on the technology to connect, search and share the data efficiently. In recent years, we have seen the rise of graph databases and semi-formal data models such as knowledge graphs to facilitate software approaches to scientific discovery. These approaches extend work based on formalised models, such as the Semantic Web. In this paper, we present our developments to connect, search and share data about genome-scale knowledge networks (GSKN). We have developed a simple application ontology based on OWL/RDF with mappings to standard schemas. We are employing the ontology to power data access services like resolvable URIs, SPARQL endpoints, JSON-LD web APIs and Neo4j-based knowledge graphs. We demonstrate how the proposed ontology and graph databases considerably improve search and access to interoperable and reusable biological knowledge (i.e. the FAIRness data principles).",2018,20,15,"[2507242.0, 2117776059.0, 31590259.0, 1398327640.0]",2507242.0,Kiev,Journal of Integrative Bioinformatics,"['data', 'knowledge', 'search', 'graph database', 'Graph Database']"
cf523942d56e90db182c5788845f6502da9a307d,3D Mapping Database Aided GNSS Based Collaborative Positioning Using Factor Graph Optimization,https://www.semanticscholar.org/paper/cf523942d56e90db182c5788845f6502da9a307d,JournalArticle,"The recent development in vehicle-to-everything (V2X) communication opens a new opportunity to improve the positioning performance of the road users. We explore the benefit of connecting the raw data of the global navigation satellite system (GNSS) from the agents. In urban areas, GNSS positioning is highly degraded due to signal blockage and reflection. 3D building model can play a major role in mitigating the GNSS multipath and non-line-of-sight (NLOS) effects. To combine the benefits of 3D models and V2X, we propose a novel 3D mapping aided (3DMA) GNSS-based collaborative positioning method that makes use of the available surrounding GNSS receivers’ measurements. By complementarily integrating the ray-tracing based 3DMA GNSS and the double difference technique, the random errors (such as multipath and NLOS) are mitigated while eliminating the systematic errors (such as atmospheric delay and satellite clock/orbit biases) between road user. To improve the accuracy and robustness of the collaborative algorithm, factor graph optimization (FGO) is employed to optimize the positioning solutions among agents. Multiple low-cost GNSS receivers are used to collect both static and dynamic data in Hong Kong and to evaluate the proposed algorithm by post-processing. We reduce the GNSS positioning error from over 30 meters to less than 10 meters for road users in a deep urban canyon.",2021,28,22,"[46266394.0, 1410897553.0, 41022905.0, 2631044.0]",46266394.0,Nicosia,IEEE transactions on intelligent transportation systems (Print),"['gnss', 'road', '3d', 'graph database', 'Graph Database']"
2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0,Topology Modeling and Analysis of a Power Grid Network Using a Graph Database,https://www.semanticscholar.org/paper/2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0,JournalArticle,"We introduce a new method for storing, modeling, and analyzing power grid data. First, we present an architecture for building the network model for a power grid using the open source graph database Neo4j. Second, we design singleand multi-threading systems for initial energization analysis of the power grid network. We design the shortest path search function and conditional search function based on Neo4j. Finally, we compare the functionality and efficiency of our graph database with a traditional relational database in system initial energization analysis and the shortest path function problems on small to large data sets. The results demonstrate the efficiency and effectiveness of topology modeling and analysis using graph database for a power grid network.",2017,14,10,"[2204964.0, 3256994.0, 121865955.0, 2145308743.0, 2064971594.0, 143627522.0]",2204964.0,Helsinki,International Journal of Computational Intelligence Systems,"['power', 'grid', 'database', 'graph database', 'Graph Database']"
78b2d392ebb100a220ceab6529d26909b27eaa32,"From Relational Database to Big Data: Converting Relational to Graph Database, MOOC Database as Example",https://www.semanticscholar.org/paper/78b2d392ebb100a220ceab6529d26909b27eaa32,JournalArticle,"Existing graph database management systems provide an efficient solution to data storage where graph models are widely used, the conversion of an existing application from a relational to graph data can be convenient but it is usually a hard task for database administrators. In this work, we propose a conversion tool from a relational to graph database. The approach supports the conversion of the schema and the data.",2017,5,8,"[2981793.0, 1596123047.0, 1596120258.0, 1732949.0]",2981793.0,Belgrade,J. Ubiquitous Syst. Pervasive Networks,"['graph', 'database', 'data', 'graph database', 'Graph Database']"
aa6c2afadd660fe4efbac699f7854e8f6f240c38,GenCoNet – A Graph Database for the Analysis of Comorbidities by Gene Networks,https://www.semanticscholar.org/paper/aa6c2afadd660fe4efbac699f7854e8f6f240c38,JournalArticle,"Abstract The prevalence of comorbid diseases poses a major health issue for millions of people worldwide and an enormous socio-economic burden for society. The molecular mechanisms for the development of comorbidities need to be investigated. For this purpose, a workflow system was developed to aggregate data on biomedical entities from heterogeneous data sources. The process of integrating and merging all data sources of the workflow system was implemented as a semi-automatic pipeline that provides the import, fusion, and analysis of the highly connected biomedical data in a Neo4j database GenCoNet. As a starting point, data on the common comorbid diseases essential hypertension and bronchial asthma was integrated. GenCoNet (https://genconet.kalis-amts.de) is a curated database that provides a better understanding of hereditary bases of comorbidities.",2018,9,15,"[38421496.0, 2040118.0, 4373076.0, 144294520.0, 2080135160.0, 2513276.0, 144649232.0, 144098801.0]",38421496.0,Lisbon,Journal of Integrative Bioinformatics,"['data', 'comorbid', 'diseases', 'graph database', 'Graph Database']"
8dd0c1e955c66092ff951941a151336211e6e171,PhyloSuite: An integrated and scalable desktop platform for streamlined molecular sequence data management and evolutionary phylogenetics studies,https://www.semanticscholar.org/paper/8dd0c1e955c66092ff951941a151336211e6e171,JournalArticle,"Multigene and genomic data sets have become commonplace in the field of phylogenetics, but many existing tools are not designed for such data sets, which often makes the analysis time‐consuming and tedious. Here, we present PhyloSuite, a (cross‐platform, open‐source, stand‐alone Python graphical user interface) user‐friendly workflow desktop platform dedicated to streamlining molecular sequence data management and evolutionary phylogenetics studies. It uses a plugin‐based system that integrates several phylogenetic and bioinformatic tools, thereby streamlining the entire procedure, from data acquisition to phylogenetic tree annotation (in combination with iTOL). It has the following features: (a) point‐and‐click and drag‐and‐drop graphical user interface; (b) a workplace to manage and organize molecular sequence data and results of analyses; (c) GenBank entry extraction and comparative statistics; and (d) a phylogenetic workflow with batch processing capability, comprising sequence alignment (mafft and macse), alignment optimization (trimAl, HmmCleaner and Gblocks), data set concatenation, best partitioning scheme and best evolutionary model selection (PartitionFinder and modelfinder), and phylogenetic inference (MrBayes and iq‐tree). PhyloSuite is designed for both beginners and experienced researchers, allowing the former to quick‐start their way into phylogenetic analysis, and the latter to conduct, store and manage their work in a streamlined way, and spend more time investigating scientific questions instead of wasting it on transferring files from one software program to another.",2019,1581,20,"[113087456.0, 3817732.0, 5989466.0, 2286418480.0, 2108930648.0, 2109050852.0, 150116413.0]",113087456.0,Vilnius,Molecular Ecology Resources,"['data', 'sequence', 'sets', 'data management', 'Data Management']"
91bda0785eaf642515eefc9ff2ecd7ddbacaccae,FDM: Fuzzy-Optimized Data Management Technique for Improving Big Data Analytics,https://www.semanticscholar.org/paper/91bda0785eaf642515eefc9ff2ecd7ddbacaccae,JournalArticle,"Big data analytics and processing require complex architectures and sophisticated techniques for extracting useful information from the accumulated information. Visualizing the extracted data for real-time solutions is demanding in accordance with the semantics and the classification employed by the processing models. This article introduces fuzzy-optimized data management (FDM) technique for classifying and improving coalition of accumulated information based semantics and constraints. The dependency of the information is classified on the basis of the relationships modeled between the data based on the attributes. This technique segregates the considered attributes based on similarity index boundaries to process complex data in a controlled time. The performance of the proposed FDM is analyzed using a real-time weather forecast dataset consisting of sensor data (observed) and image data (captured). With this dataset, the functions of FDM such as input semantics analytics and classification based on similarity are performed. The metrics classification and processing time and similarity index are analyzed for the varying data sizes, classification instances, and dataset records. The proposed FDM is found to achieve 36.28% less processing time for varying classification instances, and 12.57% high similarity index.",2021,56,29,"[10148761.0, 2006530617.0, 46916453.0, 145377105.0, 152571594.0, 18180865.0, 27648952.0, 90943712.0]",10148761.0,Amsterdam,IEEE transactions on fuzzy systems,"['data', 'classification', 'processing', 'data management', 'Data Management']"
02fa2389b1b64b661192e224bed8af6df0ce80f6,Deep Reinforcement Learning Assisted Federated Learning Algorithm for Data Management of IIoT,https://www.semanticscholar.org/paper/02fa2389b1b64b661192e224bed8af6df0ce80f6,JournalArticle,"The continuous expanded scale of the industrial Internet of Things (IIoT) leads to IIoT equipments generating massive amounts of user data every moment. According to the different requirement of end users, these data usually have high heterogeneity and privacy, while most of users are reluctant to expose them to the public view. How to manage these time series data in an efficient and safe way in the field of IIoT is still an open issue, such that it has attracted extensive attention from academia and industry. As a new machine learning paradigm, federated learning (FL) has great advantages in training heterogeneous and private data. This article studies the FL technology applications to manage IIoT equipment data in wireless network environments. In order to increase the model aggregation rate and reduce communication costs, we apply deep reinforcement learning (DRL) to IIoT equipment selection process, specifically to select those IIoT equipment nodes with accurate models. Therefore, we propose a FL algorithm assisted by DRL, which can take into account the privacy and efficiency of data training of IIoT equipment. By analyzing the data characteristics of IIoT equipments, we use MNIST, fashion MNIST, and CIFAR-10 datasets to represent the data generated by IIoT. During the experiment, we employ the deep neural network model to train the data, and experimental results show that the accuracy can reach more than 97%, which corroborates the effectiveness of the proposed algorithm.",2021,93,17,"[40075749.0, 2144447082.0, 1750017.0, 145169163.0]",40075749.0,Helsinki,IEEE Transactions on Industrial Informatics,"['iiot', 'data', 'equipment', 'data management', 'Data Management']"
9f9afed22cdc73be627932270a9bcae341df99d4,Towards Observability Data Management at Scale,https://www.semanticscholar.org/paper/9f9afed22cdc73be627932270a9bcae341df99d4,JournalArticle,"Observability has been gaining importance as a key capability in today's large-scale software systems and services. Motivated by current experience in industry exemplified by Slack and as a call to arms for database research, this paper outlines the challenges and opportunities involved in designing and building Observability Data Management Systems (ODMSs) to handle this emerging workload at scale.",2021,24,49,"[2678944.0, 8347899.0, 2031287.0, 1773620.0]",2678944.0,Bucharest,SIGMOD record,"['observability', 'systems', 'importance', 'data management', 'Data Management']"
cd5b4b113d5fa90b5dc5aa372111b89d18df88fb,Automating Research Data Management Using Machine-Actionable Data Management Plans,https://www.semanticscholar.org/paper/cd5b4b113d5fa90b5dc5aa372111b89d18df88fb,JournalArticle,"Many research funders mandate researchers to create and maintain data management plans (DMPs) for research projects that describe how research data is managed to ensure its reusability. A DMP, being a static textual document, is difficult to act upon and can quickly become obsolete and impractical to maintain. A new generation of machine-actionable DMPs (maDMPs) was therefore proposed by the Research Data Alliance to enable automated integration of information and updates. maDMPs open up a variety of use cases enabling interoperability of research systems and automation of data management tasks. In this article, we describe a system for machine-actionable data management planning in an institutional context. We identify common use cases within research that can be automated to benefit from machine-actionability of DMPs. We propose a reference architecture of an maDMP support system that can be embedded into an institutional research data management infrastructure. The system semi-automates creation and maintenance of DMPs, and thus eases the burden for the stakeholders responsible for various DMP elements. We evaluate the proposed system in a case study conducted at the largest technical university in Austria and quantify to what extent the DMP templates provided by the European Commission and a national funding body can be pre-filled. The proof-of-concept implementation shows that maDMP workflows can be semi-automated, thus workload on involved parties can be reduced and quality of information increased. The results are especially relevant to decision makers and infrastructure operators who want to design information systems in a systematic way that can utilize the full potential of maDMPs.",2021,11,13,"[1752597.0, 2124473614.0, 145494588.0]",1752597.0,Ljubljana,ACM Transactions on Management Information Systems,"['research', 'data', 'management', 'data management', 'Data Management']"
1ff76ab0fcf22110df62337d462e15d79a2a2593,A Blockchain-Based Trusted Data Management Scheme in Edge Computing,https://www.semanticscholar.org/paper/1ff76ab0fcf22110df62337d462e15d79a2a2593,JournalArticle,"With rapid development of computing technologies, large amount of data are gathered from edge terminals or Internet of Things (IoT) devices, however data trust and security in edge computing environment are very important issues to be considered, especially when the gathered data are fraud or dishonest, or the data are misused or spread without any authorization, which may lead to serious problems. In this article, a blockchain-based trusted data management scheme (called BlockTDM) in edge computing is proposed to solve the above problems, in which we proposed a flexible and configurable blockchain architecture that includes mutual authentication protocol, flexible consensus, smart contract, block and transaction data management, blockchain nodes management, and deployment. The BlockTDM scheme can support matrix-based multichannel data segment and isolation for sensitive or privacy data protection, and moreover, we have designed user-defined sensitive data encryption before the transaction payload stores in blockchain system, and have implemented conditional access and decryption query of the protected blockchain data and transactions through smart contract. Finally, we have evaluated the proposed BlockTDM scheme security, availability, and efficiency with large amount of experiments. Analysis and evaluations manifest that the proposed BlockTDM scheme provides a general, flexible, and configurable blockchain-based paradigm for trusted data management with tamper-resistance, which is suitable for edge computing with high-level security and creditability.",2020,130,16,"[31303748.0, 12717353.0, 32069527.0, 47224454.0, 9195315.0, 2061881331.0]",31303748.0,San Marino,IEEE Transactions on Industrial Informatics,"['data', 'edge', 'management', 'data management', 'Data Management']"
b795c74a0150ec091003ffbaa5bd7d74487c137b,Responsible data management,https://www.semanticscholar.org/paper/b795c74a0150ec091003ffbaa5bd7d74487c137b,JournalArticle,"The need for responsible data management intensifies with the growing impact of data on society. One central locus of the societal impact of data are Automated Decision Systems (ADS), socio-legal-technical systems that are used broadly in industry, non-profits, and government. ADS process data about people, help make decisions that are consequential to people's lives, are designed with the stated goals of improving efficiency and promoting equitable access to opportunity, involve a combination of human and automated decision making, and are subject to auditing for legal compliance and to public disclosure. They may or may not use AI, and may or may not operate with a high degree of autonomy, but they rely heavily on data. In this article, we argue that the data management community is uniquely positioned to lead the responsible design, development, use, and oversight of ADS. We outline a technical research agenda that requires that we step outside our comfort zone of engineering for efficiency and accuracy, to also incorporate reasoning about values and beliefs. This seems high-risk, but one of the upsides is being able to explain to our children what we do and why it matters.",2020,84,13,"[1682824.0, 1686294.0, 145531067.0]",1682824.0,Sofia,Proceedings of the VLDB Endowment,"['data', 'ads', 'management', 'data management', 'Data Management']"
799d5a8271887adede035644d878c7bd555576df,Geospatial Data Management Research: Progress and Future Directions,https://www.semanticscholar.org/paper/799d5a8271887adede035644d878c7bd555576df,JournalArticle,"Without geospatial data management, today’s challenges in big data applications such as earth observation, geographic information system/building information modeling (GIS/BIM) integration, and 3D/4D city planning cannot be solved. Furthermore, geospatial data management plays a connecting role between data acquisition, data modelling, data visualization, and data analysis. It enables the continuous availability of geospatial data and the replicability of geospatial data analysis. In the first part of this article, five milestones of geospatial data management research are presented that were achieved during the last decade. The first one reflects advancements in BIM/GIS integration at data, process, and application levels. The second milestone presents theoretical progress by introducing topology as a key concept of geospatial data management. In the third milestone, 3D/4D geospatial data management is described as a key concept for city modelling, including subsurface models. Progress in modelling and visualization of massive geospatial features on web platforms is the fourth milestone which includes discrete global grid systems as an alternative geospatial reference framework. The intensive use of geosensor data sources is the fifth milestone which opens the way to parallel data storage platforms supporting data analysis on geosensors. In the second part of this article, five future directions of geospatial data management research are presented that have the potential to become key research fields of geospatial data management in the next decade. Geo-data science will have the task to extract knowledge from unstructured and structured geospatial data and to bridge the gap between modern information technology concepts and the geo-related sciences. Topology is presented as a powerful and general concept to analyze GIS and BIM data structures and spatial relations that will be of great importance in emerging applications such as smart cities and digital twins. Data-streaming libraries and “in-situ” geo-computing on objects executed directly on the sensors will revolutionize geo-information science and bridge geo-computing with geospatial data management. Advanced geospatial data visualization on web platforms will enable the representation of dynamically changing geospatial features or moving objects’ trajectories. Finally, geospatial data management will support big geospatial data analysis, and graph databases are expected to experience a revival on top of parallel and distributed data stores supporting big geospatial data analysis.",2020,71,9,"[3248270.0, 145663597.0, 46581566.0, 145216339.0, 1576996670.0, 2071346808.0, 1403759150.0, 2418044.0, 1483744000.0]",3248270.0,Oslo,ISPRS Int. J. Geo Inf.,"['data', 'management', 'analysis', 'data management', 'Data Management']"
98b9086750f08a21c8778ab986339321e9caf790,The Best of Both Worlds: A General Architecture for Data Management in Blockchain-enabled Internet-of-Things,https://www.semanticscholar.org/paper/98b9086750f08a21c8778ab986339321e9caf790,JournalArticle,"The rapid proliferation of Internet-of-Things (IoT) devices has brought great challenges of data management, i.e., storing, retrieving and manipulating a large volume of IoT data. Conventional IoT systems rely on centralized architectures to manage IoT data, hence suffering from limited scalability, lack of transparency, and single point of failure issues. As such, we employ blockchain as a distributed ledger to support the decentralized approach of data management in IoT systems, where IoT data are stored in the deployed blockchain for further utilization, e.g., retrieve and audit. A general architecture combining blockchain and IoT systems is presented. Nevertheless, as the resource constraints of IoT devices may still exist during the process of data transmissions from IoT devices to the blockchain network, we propose a case study of a learning-assisted resource allocation method to support intelligent data management. The numerical results show that the proposed scheme achieves superior performance compared with baseline solutions.",2020,67,34,"[2943819.0, 2145954062.0, 3046954.0, 1713586.0, 144442055.0, 2554658.0]",2943819.0,Andorra,IEEE Network,"['iot', 'data', 'management', 'data management', 'Data Management']"
2019cf49b51021a376f9833a53565513f0d8107b,InFeMo: Flexible Big Data Management Through a Federated Cloud System,https://www.semanticscholar.org/paper/2019cf49b51021a376f9833a53565513f0d8107b,JournalArticle,"This paper introduces and describes a novel architecture scenario based on Cloud Computing and counts on the innovative model of Federated Learning. The proposed model is named Integrated Federated Model, with the acronym InFeMo. InFeMo incorporates all the existing Cloud models with a federated learning scenario, as well as other related technologies that may have integrated use with each other, offering a novel integrated scenario. In addition to this, the proposed model is motivated to deliver a more energy efficient system architecture and environment for the users, which aims to the scope of data management. Also, by applying the InFeMo the user would have less waiting time in every procedure queue. The proposed system was built on the resources made available by Cloud Service Providers (CSPs) and by using the PaaS (Platform as a Service) model, in order to be able to handle user requests better and faster. This research tries to fill a scientific gap in the field of federated Cloud systems. Thus, taking advantage of the existing scenarios of FedAvg and CO-OP, we were keen to end up with a new federated scenario that merges these two algorithms, and aiming for a more efficient model that is able to select, depending on the occasion, if it “trains” the model locally in client or globally in server.",2020,56,22,"[144717855.0, 41037178.0, 144901889.0]",144717855.0,Vilnius,ACM Trans. Internet Techn.,"['model', 'scenario', 'cloud', 'data management', 'Data Management']"
447884e7da189102189a156966623335c72199b0,ACTION-EHR: Patient-Centric Blockchain-Based Electronic Health Record Data Management for Cancer Care,https://www.semanticscholar.org/paper/447884e7da189102189a156966623335c72199b0,JournalArticle,"Background With increased specialization of health care services and high levels of patient mobility, accessing health care services across multiple hospitals or clinics has become very common for diagnosis and treatment, particularly for patients with chronic diseases such as cancer. With informed knowledge of a patient’s history, physicians can make prompt clinical decisions for smarter, safer, and more efficient care. However, due to the privacy and high sensitivity of electronic health records (EHR), most EHR data sharing still happens through fax or mail due to the lack of systematic infrastructure support for secure, trustable health data sharing, which can also cause major delays in patient care. Objective Our goal was to develop a system that will facilitate secure, trustable management, sharing, and aggregation of EHR data. Our patient-centric system allows patients to manage their own health records across multiple hospitals. The system will ensure patient privacy protection and guarantee security with respect to the requirements for health care data management, including the access control policy specified by the patient. Methods We propose a permissioned blockchain-based system for EHR data sharing and integration. Each hospital will provide a blockchain node integrated with its own EHR system to form the blockchain network. A web-based interface will be used for patients and doctors to initiate EHR sharing transactions. We take a hybrid data management approach, where only management metadata will be stored on the chain. Actual EHR data, on the other hand, will be encrypted and stored off-chain in Health Insurance Portability and Accountability Act–compliant cloud-based storage. The system uses public key infrastructure–based asymmetric encryption and digital signatures to secure shared EHR data. Results In collaboration with Stony Brook University Hospital, we developed ACTION-EHR, a system for patient-centric, blockchain-based EHR data sharing and management for patient care, in particular radiation treatment for cancer. The prototype was built on Hyperledger Fabric, an open-source, permissioned blockchain framework. Data sharing transactions were implemented using chaincode and exposed as representational state transfer application programming interfaces used for the web portal for patients and users. The HL7 Fast Healthcare Interoperability Resources standard was adopted to represent shared EHR data, making it easy to interface with hospital EHR systems and integrate a patient’s EHR data. We tested the system in a distributed environment at Stony Brook University using deidentified patient data. Conclusions We studied and developed the critical technology components to enable patient-centric, blockchain-based EHR sharing to support cancer care. The prototype demonstrated the feasibility of our approach as well as some of the major challenges. The next step will be a pilot study with health care providers in both the United States and Switzerland. Our work provides an exemplar testbed to build next-generation EHR sharing infrastructures.",2020,94,22,"[2142586.0, 3419650.0, 2145164466.0, 2059003208.0, 1753286463.0, 2104195713.0, 1920698256.0, 2097767549.0, 1753285996.0, 1753093575.0, 46779638.0, 1751802.0, 2173602.0, 2113586661.0, 1409895301.0]",2142586.0,Podgorica,Journal of Medical Internet Research,"['ehr', 'data', 'health', 'data management', 'Data Management']"
e359e8960b0b09e8685a32927b7818f4b06ef881,Intelligent Data Management and Security in Cloud Computing,https://www.semanticscholar.org/paper/e359e8960b0b09e8685a32927b7818f4b06ef881,JournalArticle,"This paper will present the authors’ own techniques of secret data management and protection, with particular attention paid to techniques securing data services. Among the solutions discussed, there will be information-sharing protocols dedicated to the tasks of secret (confidential) data sharing. Such solutions will be presented in an algorithmic form, aimed at solving the tasks of protecting and securing data against unauthorized acquisition. Data-sharing protocols will execute the tasks of securing a special type of information, i.e., data services. The area of data protection will be defined for various levels, within which will be executed the tasks of data management and protection. The authors’ solution concerning securing data with the use of cryptographic threshold techniques used to split the secret among a specified group of secret trustees, simultaneously enhanced by the application of linguistic methods of description of the shared secret, forms a new class of protocols, i.e., intelligent linguistic threshold schemes. The solutions presented in this paper referring to the service management and securing will be dedicated to various levels of data management. These levels could be differentiated both in the structure of a given entity and in its environment. There is a special example thereof, i.e., the cloud management processes. These will also be subject to the assessment of feasibility of application of the discussed protocols in these areas. Presented solutions will be based on the application of an innovative approach, in which we can use a special formal graph for the creation of a secret representation, which can then be divided and transmitted over a distributed network.",2020,34,20,"[2388293.0, 1790345.0, 145655733.0]",2388293.0,London,Italian National Conference on Sensors,"['data', 'management', 'solutions', 'data management', 'Data Management']"
ff74bfbd9ebf4c54809873aecb04be27e9402cb8,Data management for developing digital twin ontology model,https://www.semanticscholar.org/paper/ff74bfbd9ebf4c54809873aecb04be27e9402cb8,JournalArticle,"Digital Twin (DT) is the imitation of the real world product, process or system. Digital Twin is the ideal solution for data-driven optimisations in different phases of the product lifecycle. With the rapid growth in DT research, data management for digital twin is a challenging field for both industries and academia. The challenges for DT data management are analysed in this article are data variety, big data & data mining and DT dynamics. The current research proposes a novel concept of DT ontology model and methodology to address these data management challenges. The DT ontology model captures and models the conceptual knowledge of the DT domain. Using the proposed methodology, such domain knowledge is transformed into a minimum data model structure to map, query and manage databases for DT applications. The proposed research is further validated using a case study based on Condition-Based Monitoring (CBM) DT application. The query formulation around minimum data model structure further shows the effectiveness of the current approach by returning accurate results, along with maintaining semantics and conceptual relationships along DT lifecycle. The method not only provides flexibility to retain knowledge along DT lifecycle but also helps users and developers to design, maintain and query databases effectively for DT applications and systems of different scale and complexities.",2020,31,235,"[2109039649.0, 1891038.0, 1891038.0, 123445664.0, 144782078.0, 2064304090.0, 1911359.0, 98725327.0]",2109039649.0,Belgrade,"Proceedings of the Institution of mechanical engineers. Part B, journal of engineering manufacture","['dt', 'data', 'model', 'data management', 'Data Management']"
d7b820af40a9e2660ef700d39f7b2e27b43435c5,Analysis of Data Management in Blockchain-Based Systems: From Architecture to Governance,https://www.semanticscholar.org/paper/d7b820af40a9e2660ef700d39f7b2e27b43435c5,JournalArticle,"In a blockchain-based system, data and the consensus-based process of recording and updating them over distributed nodes are central to enabling the trustless multi-party transactions. Thus, properly understanding what and how the data are stored and manipulated ultimately determines the degree of utility, performance, and cost of a blockchain-based application. While blockchains enhance the quality of the data by providing a transparent, immutable, and consistent data store, the technology also brings new challenges from a data management perspective. In this paper, we analyse blockchains from the viewpoint of a developer to highlight important concepts and considerations when incorporating a blockchain into a larger software system as a data store. The work aims to increase the level of understanding of blockchain technology as a data store and to promote a methodical approach in applying it to large software systems. First, we identify the common architectural layers of a typical software system with data stores and conceptualise each layer in blockchain terms. Second, we examine the placement and flow of data in blockchain-based applications. Third, we explore data administration aspects for blockchains, especially as a distributed data store. Fourth, we discuss the analytics of blockchain data and trustable data analytics enabled by blockchain. Lastly, we examine the data governance issues in blockchains in terms of privacy and quality assurance.",2019,93,7,"[33548540.0, 3087664.0, 134047389.0, 2248812173.0, 35529345.0]",33548540.0,Nicosia,IEEE Access,"['data', 'store', 'system', 'data management', 'Data Management']"
9e66ae24a541255c2d931184498ee116ce81478a,A Cross-Chain Solution to Integrating Multiple Blockchains for IoT Data Management,https://www.semanticscholar.org/paper/9e66ae24a541255c2d931184498ee116ce81478a,JournalArticle,"With the rapid development of the internet of things (IoT), traditional industries are setting off a massive wave of digitization. In the era of the Internet of Everything, millions of devices and links in IoT pose more significant challenges to data management. Most existing solutions employ centralized systems to control IoT devices, which brings about the privacy and security issues in IoT data management. Recently, blockchain has attracted much attention in the field of IoT due to its decentralization, traceability, and non-tamperability. However, it is non-trivial to apply the current blockchain techniques to IoT due to the lack of scalability and high resource costs. Different blockchain platforms have their particular advantages in the scenario of IoT data management. In this paper, we propose a cross-chain framework to integrate multiple blockchains for efficient and secure IoT data management. Our solution builds an interactive decentralized access model which employs a consortium blockchain as the control station. Other blockchain platforms customized for specific IoT scenarios run as the backbone of all IoT devices. It is equivalent to opening the off-chain channels on the consortium blockchain. Our model merges transactions in these channels for confirmation based on the notary mechanism. Finally, we implement a prototype of the proposed model based on hyperledge Fabric and IOTA Tangle. We evaluate the performance of our method through extensive experiments. The results demonstrate the effectiveness and efficiency of our framework.",2019,86,19,"[2146420685.0, 143813668.0, 2115626527.0, 2112264042.0]",2146420685.0,Skopje,Italian National Conference on Sensors,"['iot', 'data', 'management', 'data management', 'Data Management']"
3be6a57d6db95bba2962a1f3476414a0a9b230b5,EdgeCare: Leveraging Edge Computing for Collaborative Data Management in Mobile Healthcare Systems,https://www.semanticscholar.org/paper/3be6a57d6db95bba2962a1f3476414a0a9b230b5,JournalArticle,"With the wide application of mobile healthcare systems, the total amount of healthcare data is ever increasing rapidly as users interact with healthcare service providers frequently. This leads to a challenging task to manage healthcare data. Existing work mainly pay attention to centralized and blockchain-based mechanisms. But they cannot adapt to the increasing amount of global healthcare data and suffer from complex application challenges, respectively. Decentralized and collaborative data management assisted by edge computing exhibits major advantages in improving overall system performance. We present a secure and efficient data management system named as EdgeCare for mobile healthcare systems. Local authorities are established to schedule edge servers for processing healthcare data and facilitating data trading. A hierarchical architecture with collaboration is designed for feasible implementation of EdgeCare. After that, we investigate secure data uploading and sharing in the system. We use an electronic medical record to show how healthcare data is processed with security considerations. We also conduct the Stackelberg game-based optimization algorithm to approach the optimal incentive mechanism for a data collector and users in the fair decentralized data trading. The numerical results with security analysis are provided to demonstrate that EdgeCare offers effective solutions to protect healthcare data, and support efficient data trading.",2019,64,7,"[2118890103.0, 3116552.0, 2109490422.0, 143791274.0, 1731449.0]",2118890103.0,Lisbon,IEEE Access,"['data', 'healthcare', 'system', 'data management', 'Data Management']"
5687c9e8da574453fd873662b95caec70dac9d1e,Framework of an IoT-based Industrial Data Management for Smart Manufacturing,https://www.semanticscholar.org/paper/5687c9e8da574453fd873662b95caec70dac9d1e,JournalArticle,"The Internet of Things (IoT) is the global network of interrelated physical devices such as sensors, actuators, smart applications, objects, computing devices, mechanical machines, and people that are becoming an essential part of the internet. In an industrial environment, these devices are the source of data which provide abundant information in manufacturing processes. Nevertheless, the massive, heterogeneous, and time-sensitive nature of the data brings substantial challenges to the real-time collection, processing, and decision making. Therefore, this paper presents a framework of an IoT-based Industrial Data Management System (IDMS) which can manage the huge industrial data, support online monitoring, and control smart manufacturing. The framework contains five basic layers such as physical, network, middleware, database, and application layers to provide a service-oriented architecture for the end users. Experimental results from a smart factory case study demonstrate that the framework can manage the regular data and urgent events generated from various factory devices in the distributed industrial environment through state-of-the-art communication protocols. The collected data is converted into useful information which improves productivity and the prognosis of production lines.",2019,67,8,"[145429147.0, 1784862.0, 33018846.0, 2108614875.0]",145429147.0,Moscow,J. Sens. Actuator Networks,"['data', 'devices', 'framework', 'data management', 'Data Management']"
676664ee7471738577f641e6159e7596625b7fdb,Ten principles for machine-actionable data management plans,https://www.semanticscholar.org/paper/676664ee7471738577f641e6159e7596625b7fdb,JournalArticle,"Data management plans (DMPs) are documents accompanying research proposals and project outputs. DMPs are created as free-form text and describe the data and tools employed in scientific investigations. They are often seen as an administrative exercise and not as an integral part of research practice. There is now widespread recognition that the DMP can have more thematic, machine-actionable richness with added value for all stakeholders: researchers, funders, repository managers, research administrators, data librarians, and others. The research community is moving toward a shared goal of making DMPs machine-actionable to improve the experience for all involved by exchanging information across research tools and systems and embedding DMPs in existing workflows. This will enable parts of the DMP to be automatically generated and shared, thus reducing administrative burdens and improving the quality of information within a DMP. This paper presents 10 principles to put machine-actionable DMPs (maDMPs) into practice and realize their benefits. The principles contain specific actions that various stakeholders are already undertaking or should undertake in order to work together across research communities to achieve the larger aims of the principles themselves. We describe existing initiatives to highlight how much progress has already been made toward achieving the goals of maDMPs as well as a call to action for those who wish to get involved.",2019,49,15,"[1752597.0, 145730688.0, 1772144.0, 144783437.0]",1752597.0,Minsk,PLoS Comput. Biol.,"['research', 'data', 'dmp', 'data management', 'Data Management']"
6ba00c2386f2edc0b43eec442cd1923b5d964633,"""Data Stewardship Wizard"": A Tool Bringing Together Researchers, Data Stewards, and Data Experts around Data Management Planning",https://www.semanticscholar.org/paper/6ba00c2386f2edc0b43eec442cd1923b5d964633,JournalArticle,"The Data Stewardship Wizard is a tool for data management planning that is focused on getting the most value out of data management planning for the project itself rather than on fulfilling obligations. It is based on FAIR Data Stewardship, in which each data-related decision in a project acts to optimize the Findability, Accessibility, Interoperability and/or Reusability of the data. The background to this philosophy is that the first reuser of the data is the researcher themselves. The tool encourages the consulting of expertise and experts, can help researchers avoid risks they did not know they would encounter by confronting them with practical experience from others, and can help them discover helpful technologies they did not know existed. In this paper, we discuss the context and motivation for the tool, we explain its architecture and we present key functions, such as the knowledge model evolvability and migrations, assembling data management plans, metrics and evaluation of data management plans.",2019,36,18,"[2810600.0, 50371588.0, 69513411.0, 1394243406.0, 1394243478.0]",2810600.0,Lisbon,Data Science Journal,"['data', 'management', 'tool', 'data management', 'Data Management']"
7af07490da518c8ef3cf2ae106071df2c2d0101e,Patient-generated health data management and quality challenges in remote patient monitoring,https://www.semanticscholar.org/paper/7af07490da518c8ef3cf2ae106071df2c2d0101e,JournalArticle,"Abstract Background Patient-Generated Health Data (PGHD) in remote monitoring programs is a promising source of precise, personalized data, encouraged by expanding growth in the health technologies market. However, PGHD utilization in clinical settings is low. One of the critical challenges that impedes confident clinical use of PGHD is that these data are not managed according to any recognized approach for data quality assurance. Objective This article aims to identify the PGHD management and quality challenges that such an approach must address, as these are expressed by key PGHD stakeholder groups. Materials and Methods In-depth interviews were conducted with 20 experts who have experience in the use of PGHD in remote patient monitoring, including: healthcare providers, health information professionals within clinical settings, and commercial providers of remote monitoring solutions. Participants were asked to describe PGHD management processes in the remote monitoring programs in which they are involved, and to express their perspectives on PGHD quality challenges during the data management stages. Results The remote monitoring programs in the study did not follow clear PGHD management or quality assurance approach. Participants were not fully aware of all the considerations of PGHD quality. Digital health literacy, wearable accuracy, difficulty in data interpretation, and lack of PGHD integration with electronic medical record systems were among the key challenges identified that impact PGHD quality. Conclusion Co-development of PGHD quality guidelines with relevant stakeholders, including patients, is needed to ensure that quality remote monitoring data from wearables is available for use in more precise and personalized patient care.",2019,75,2,"[5840536.0, 144370168.0, 2189206.0, 144132699.0]",5840536.0,Bratislava,JAMIA Open,"['quality', 'data', 'pghd', 'data management', 'Data Management']"
3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8,Big Data Management Canvas: A Reference Model for Value Creation from Data,https://www.semanticscholar.org/paper/3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8,JournalArticle,"Many big data projects are technology-driven and thus, expensive and inefficient. It is often unclear how to exploit existing data resources and map data, systems and analytics results to actual use cases. Existing big data reference models are mostly either technological or business-oriented in nature, but do not consequently align both aspects. To address this issue, a reference model for big data management is proposed that operationalizes value creation from big data by linking business targets with technical implementation. The purpose of this model is to provide a goal- and value-oriented framework to effectively map and plan purposeful big data systems aligned with a clear value proposition. Based on an epistemic model that conceptualizes big data management as a cognitive system, the solution space of data value creation is divided into five layers: preparation, analysis, interaction, effectuation, and intelligence. To operationalize the model, each of these layers is subdivided into corresponding business and IT aspects to create a link from use cases to technological implementation. The resulting reference model, the big data management canvas, can be applied to classify and extend existing big data applications and to derive and plan new big data solutions, visions, and strategies for future projects. To validate the model in the context of existing information systems, the paper describes three cases of big data management in existing companies.",2019,27,3,[143918410.0],143918410.0,Kiev,Big Data and Cognitive Computing,"['data', 'model', 'management', 'data management', 'Data Management']"
6dd3e404aac097f63d42dc19fd08c7ec281f90b4,Qualitative Data Management and Analysis within a Data Repository,https://www.semanticscholar.org/paper/6dd3e404aac097f63d42dc19fd08c7ec281f90b4,JournalArticle,"Data repositories can support secure data management for multi-institutional and geographically dispersed research teams. Primarily designed to provide secure access, storage, and sharing of quantitative data, limited focus has been given to the unique considerations of data repositories for qualitative research. We share our experiences of using a data repository in a large qualitative nursing research study. Over a 27-month period, data collected by this 15-member team from 83 participants included photos, audio recordings and transcripts of interviews, and field notes. The data repository supported the secure collection, storage, and management of over 1,800 files with data. However, challenges were introduced during analysis that required negotiations about the structure and processes of the data repository. We discuss strengths and limitations of data repositories, and introduce practical strategies for developing a data management plan for qualitative research, which is supported through a data repository.",2019,20,42,"[48232006.0, 1390140002.0, 5973699.0, 3971602.0, 14746788.0, 6429295.0]",48232006.0,Tirana,Western Journal of Nursing Research,"['data', 'research', 'repositories', 'data management', 'Data Management']"
d7fddafbbc372da4fa884f67bdc32db71b888806,IMG/VR v.2.0: an integrated data management and analysis system for cultivated and environmental viral genomes,https://www.semanticscholar.org/paper/d7fddafbbc372da4fa884f67bdc32db71b888806,JournalArticle,"Abstract The Integrated Microbial Genome/Virus (IMG/VR) system v.2.0 (https://img.jgi.doe.gov/vr/) is the largest publicly available data management and analysis platform dedicated to viral genomics. Since the last report published in the 2016, NAR Database Issue, the data has tripled in size and currently contains genomes of 8389 cultivated reference viruses, 12 498 previously published curated prophages derived from cultivated microbial isolates, and 735 112 viral genomic fragments computationally predicted from assembled shotgun metagenomes. Nearly 60% of the viral genomes and genome fragments are clustered into 110 384 viral Operational Taxonomic Units (vOTUs) with two or more members. To improve data quality and predictions of host specificity, IMG/VR v.2.0 now separates prokaryotic and eukaryotic viruses, utilizes known prophage sequences to improve taxonomic assignments, and provides viral genome quality scores based on the estimated genome completeness. New features also include enhanced BLAST search capabilities for external queries. Finally, geographic map visualization to locate user-selected viral genomes or genome fragments has been implemented and download options have been extended. All of these features make IMG/VR v.2.0 a key resource for the study of viruses.",2018,142,47,"[1396822935.0, 52161096.0, 2956683.0, 2412941.0, 39517703.0, 2285301474.0, 2078394.0, 144896762.0, 2254214902.0, 1702316.0, 1396822962.0, 2241382793.0, 2805081.0]",1396822935.0,Minsk,Nucleic Acids Res.,"['data', 'viruses', 'fragments', 'data management', 'Data Management']"
f6dab84c2c00ab92d8ee9d9359d7e530512114f9,"Finance Big Data: Management, Analysis, and Applications",https://www.semanticscholar.org/paper/f6dab84c2c00ab92d8ee9d9359d7e530512114f9,JournalArticle,"Big Data is an emerging paradigm in almost all industries. Finance big data (FBD) is becoming one of the most promising areas of management and governance in the financial sector. It is significantly changing business models in financial companies. Many researchers argue that Big Data is fueling the transformation of finance and business at-large in the ways that we cannot as yet assess. A new research area is evolving to study quantitative models and econometric approaches for financial studies that can bridge the gap between empirical finance research and data science. In this fascinating area, experts and scientists can propose novel finance business models by using the Big Data methods, present sophisticated methods for risk control with machine learning tools, provide visualization tools for financial markets analysis, create new finance sentiment indexes by mining public feelings from the massive textual data from social networks, and deploy the information-based tools in other creative ways. Due to the 4V characteristics of Big Data—volume (large data scale), velocity (real-time data streaming), variety (different data formats), and veracity (data uncertainty)—a long list of challenges for FBD management, analytics, and applications exists. These challenges include (1) to organize and manage FBD in effective and efficient ways; (2) to find novel business models from FBD analytics; (3) to handle traditional finance issues like high-frequency trading, sentiments, credit risk, financial analysis, risk management and regulation, and others, in creative Big Data–driven ways; (4) to integrate the variety of heterogeneous data from different sources; and (5) to ensure the security and safety of finance systems and to protect the individual privacy in view of the availability of Big Data. To meet these challenges, we need fundamental research on both data analytics technology and finance business. This special issue, “Finance Big Data: Management, Analysis, and Applications,” of International Journal of Electronic Commerce, is motivated by the need to meet the challenges of the fast development of finance big data. The papers brought together in this special issue highlight research efforts focused on the development of methods, tools, and techniques for the handling of various aspects of FBD from academia and industries. Viktor Manahov and Hanxiong Zhang, in “Forecasting Financial Markets Using High-Frequency Trading Data: Examination with Strongly Typed Genetic Programming,” develop an artificial futures market populated with high-frequency (HF) traders and institutional traders using Strongly Typed Genetic Programming trading algorithm. The authors simulate real-life futures trading at the millisecond time frame by applying Strongly Typed",2019,30,23,"[152354805.0, 48081155.0, 2148904286.0]",152354805.0,Valletta,International Journal of Electronic Commerce,"['data', 'finance', 'fbd', 'data management', 'Data Management']"
b7f5973dbf2ef76fcc3aea616a7f72ca79f09020,Towards high-precision data modeling of SHM measurements using an improved sparse Bayesian learning scheme with strong generalization ability,https://www.semanticscholar.org/paper/b7f5973dbf2ef76fcc3aea616a7f72ca79f09020,JournalArticle,"Central to structural health monitoring (SHM) is data modeling, manipulation, and interpretation on the basis of a sophisticated SHM system. Despite continuous evolution of SHM technology, the precise modeling and forecasting of SHM measurements under various uncertainties to extract structural condition-relevant knowledge remains a challenge. Aiming to resolve this problem, a novel application of a fully probabilistic and high-precision data modeling method was proposed in the context of an improved Sparse Bayesian Learning (iSBL) scheme. The proposed iSBL data modeling framework features the following merits. It can remove the need to specify the number of terms in the data-fitting function, and automatize sparsity of the Bayesian model based on the features of SHM monitoring data, which will enhance the generalization ability and then improve the data prediction accuracy. Embedded in a Bayesian framework which exhibits built-in protection against over-fitting problems, the proposed iSBL scheme has high robustness to data noise, especially for data forecasting. The model is verified to be effective on SHM vibration field monitoring data collected from a real-world large-scale cable-stayed bridge. The recorded acceleration data with two different vibration patterns, that is, stationary ambient vibration data and non-stationary decay vibration data, are investigated, returning accurate probabilistic predictions in both the time and frequency domains.",2023,12,23,"[2153282879.0, 2181284180.0, 2160872910.0, 2171114745.0, 100841270.0, 50040606.0, 3124770.0, 2204699413.0, 2218326293.0, 2217936172.0]",2153282879.0,Berlin,Structural Health Monitoring,"['data', 'shm', 'modeling', 'data modeling', 'Data Modeling']"
a2ec47b9bcc95d2456a8a42199233e5d9129ef18,TabTransformer: Tabular Data Modeling Using Contextual Embeddings,https://www.semanticscholar.org/paper/a2ec47b9bcc95d2456a8a42199233e5d9129ef18,JournalArticle,"We propose TabTransformer, a novel deep tabular data modeling architecture for supervised and semi-supervised learning. The TabTransformer is built upon self-attention based Transformers. The Transformer layers transform the embeddings of categorical features into robust contextual embeddings to achieve higher prediction accuracy. Through extensive experiments on fifteen publicly available datasets, we show that the TabTransformer outperforms the state-of-the-art deep learning methods for tabular data by at least 1.0% on mean AUC, and matches the performance of tree-based ensemble models. Furthermore, we demonstrate that the contextual embeddings learned from TabTransformer are highly robust against both missing and noisy data features, and provide better interpretability. Lastly, for the semi-supervised setting we develop an unsupervised pre-training procedure to learn data-driven contextual embeddings, resulting in an average 2.1% AUC lift over the state-of-the-art methods.",2020,227,abs/2012.06678,"[2152663837.0, 3083159.0, 2008176653.0, 3386660.0]",2152663837.0,Nicosia,arXiv.org,"['embeddings', 'data', 'learning', 'data modeling', 'Data Modeling']"
73cbaf5f2441ef3478266b5438c0e90d1ae71652,Unsupervised Grouped Axial Data Modeling via Hierarchical Bayesian Nonparametric Models With Watson Distributions,https://www.semanticscholar.org/paper/73cbaf5f2441ef3478266b5438c0e90d1ae71652,JournalArticle,"This paper aims at proposing an unsupervised hierarchical nonparametric Bayesian framework for modeling axial data (i.e., observations are axes of direction) that can be partitioned into multiple groups, where each observation within a group is sampled from a mixture of Watson distributions with an infinite number of components that are allowed to be shared across different groups. First, we propose a hierarchical nonparametric Bayesian model for modeling grouped axial data based on the hierarchical Pitman-Yor process mixture model of Watson distributions. Then, we demonstrate that by setting the discount parameters of the proposed model to 0, another hierarchical nonparametric Bayesian model based on hierarchical Dirichlet process can be derived for modeling axial data. To learn the proposed models, we systematically develop a closed-form optimization algorithm based on the collapsed variational Bayes (CVB) inference. Furthermore, to ensure the convergence of the proposed learning algorithm, an annealing mechanism is introduced to the framework of CVB inference, leading to an averaged collapsed variational Bayes inference strategy. The merits of the proposed models for modeling grouped axial data are demonstrated through experiments on both synthetic data and real-world applications involving gene expression data clustering and depth image analysis.",2021,36,44,"[2038786.0, 2155558174.0, 1729109.0]",2038786.0,Rome,IEEE Transactions on Pattern Analysis and Machine Intelligence,"['data', 'model', 'inference', 'data modeling', 'Data Modeling']"
d916776e0c6a04b0def4c22257c188776c2edab2,Industrial Big Data Modeling and Monitoring Framework for Plant-Wide Processes,https://www.semanticscholar.org/paper/d916776e0c6a04b0def4c22257c188776c2edab2,JournalArticle,"This article proposes a distributed parallel modeling and monitoring framework for plant-wide processes with big data. The “distributed” contains two layers of meaning. One is the spatially distributed modeling and hierarchical monitoring for the plant-wide process with multiple operating units. The other represents the distributed parallel modeling for big process data with various features. Under the framework, the distributed parallel mixture probabilistic latent variable model is proposed based on the stochastic variational inference algorithm and the parameter server architecture to cope with the big process data. Then, the model is utilized to develop the plant-wide hierarchical and distributed process monitoring algorithms, where the multilevel monitoring indexes and fault contribution indexes are established based on the Bayesian fusion algorithm for process fault detection and diagnosis. The performance comparison and visualization for the industrial plant-wide process case has demonstrated the reliability and superiority of the proposed algorithm and framework.",2021,24,17,"[48028582.0, 145619185.0]",48028582.0,Berlin,IEEE Transactions on Industrial Informatics,"['process', 'monitoring', 'plantwide', 'data modeling', 'Data Modeling']"
8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78,Multidimensional Data Modeling and Model Validation for Digital Twin Workshop,https://www.semanticscholar.org/paper/8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78,JournalArticle,"
 Digital twin workshop (DTW) is an important embodiment of intelligent manufacturing in the workshop level, which enables the smart production control and management of the workshop. However, there still exist problems including data modeling and verification of digital model in the process of DTW construction. To solve these problem, multidimensional data modeling and model validation methods of DTW are proposed in this article. First, five-order tensor models for representing manufacturing elements are established to unify the data from physical workshop (PW) and virtual workshop (VW). Then, the mathematical method for verifying DTW twin model is proposed from the recessive and explicit perspective. Finally, a case study of an aerospace machining workshop is carried out to verify the operability and effectiveness of the proposed method. The case analysis shows that the proposed methods can effectively evaluate whether the twin model accurately provides the description of the actual behavior process of physical workshop, and the proposed methods have good performance.",2021,12,21,"[2105608191.0, 2143379026.0, 2051655406.0, 2052219438.0, 9319875.0, 1768360511.0]",2105608191.0,Nicosia,Journal of Computing and Information Science in Engineering,"['workshop', 'dtw', 'model', 'data modeling', 'Data Modeling']"
b266510f5f9b40d42b51884ad13a1867fb3284fd,Conceptual Data Modeling: Entity-Relationship Models as Thinging Machines,https://www.semanticscholar.org/paper/b266510f5f9b40d42b51884ad13a1867fb3284fd,JournalArticle,"Data modeling is a process of developing a model to design and develop a data system that supports an organization s various business processes. A conceptual data model represents a technology-independent specification of structure of data to be stored within a database. The model aims at providing richer expressiveness and incorporating a set of semantics to (a) support the design, control, and integrity parts of the data stored in data management structures and (b) coordinate viewing of connections and ideas on a database. The described structure of the data is often represented in an entity-relationship (ER) model, which was one of the first data-modeling techniques and is likely to continue to be a popular way of characterizing entity classes, attributes and relationships. This paper is an attempt to examine the basic ER modeling notions to analyze the concepts to which they refer as well as ways to represent them. In such a mission, we apply a new modeling methodology (thinging machine; TM) to ER in terms of its fundamental building constructs, representation entities, relationships and attributes. The goal of this venture is to further the understanding of data models and enrich their semantics. Three specific contributions to modeling in this context are incorporated: (a) using the TM model s five generic actions to inject processing in the ER structure; (b) relating the single ontological element of TM modeling (i.e., a thing/machine or thimac) to ER entities and relationships; and (c) proposing a high-level integrated, extended ER model that includes structural and time-oriented notions (e.g., events or behavior).",2021,8,abs/2109.14717,[1405319717.0],1405319717.0,Tallinn,arXiv.org,"['data', 'model', 'modeling', 'data modeling', 'Data Modeling']"
244e08e109f6f4fa0f846977e1aef1e7b6a9e816,Robust Online Sequential RVFLNs for Data Modeling of Dynamic Time-Varying Systems With Application of an Ironmaking Blast Furnace,https://www.semanticscholar.org/paper/244e08e109f6f4fa0f846977e1aef1e7b6a9e816,JournalArticle,"By dealing with robust modeling and online learning together in a unified random vector functional-link networks (RVFLNs) framework, this paper presents a novel robust online sequential RVFLNs for data modeling of dynamic time-varying systems together with its application for a blast furnace (BF) ironmaking process. First, to overcome the difficulties caused by the nonlinear time-varying dynamics of process and to enable the RVFLNs to learn online and to avoid data saturation, an improved online sequential version of RVFLNs (OS-RVFLNs) is presented by sequential learning with forgetting factor. It has been shown that the improved OS-RVFLNs with forgetting factor is not only suitable for the large-scale and real-time data transfer situation but also can adjust the sensitivity of the algorithm to different samples. Second, in order to solve the issue of modeling robustness when the dataset is contaminated with various outliers, a Cauchy distribution function weighted M-estimator is introduced to strengthen the robustness of the improved OS-RVFLNs. The non-Gaussian Cauchy distribution function is used to estimate the weights of different data and thus the corresponding contribution on modeling can be properly distinguished. Experiments using actual industrial data of a large BF ironmaking process have demonstrated that the proposed algorithm produces a much stronger robustness and better estimation accuracy than other algorithms.",2020,28,50,"[144255239.0, 2108700974.0, 39483422.0, 50651835.0, 145011556.0]",144255239.0,Copenhagen,IEEE Transactions on Cybernetics,"['data', 'rvflns', 'process', 'data modeling', 'Data Modeling']"
417326e51d78ba8bd2621f23e539b41bbdd336d6,Dynamic Probabilistic Latent Variable Model for Process Data Modeling and Regression Application,https://www.semanticscholar.org/paper/417326e51d78ba8bd2621f23e539b41bbdd336d6,JournalArticle,"Dynamic and uncertainty are two main features of the industrial process data which should be paid attention when carrying out process data modeling and analytics. In this paper, the dynamical and uncertain data characteristics are both taken into consideration for the regression modeling purpose. Based on the probabilistic latent variable modeling framework, the linear dynamic system is introduced for incorporation of the dynamical data feature. The expectation–maximization Algorithm is introduced for parameter learning of the dynamical probabilistic latent variable model, based on which a new soft sensing scheme is then formulated for online prediction of key/quality variables in the process. An industrial case study illustrates the necessity and effectiveness of introducing the dynamical data information into the probabilistic latent variable model.",2019,53,27,"[145619185.0, 2109110276.0]",145619185.0,Minsk,IEEE Transactions on Control Systems Technology,"['data', 'process', 'modeling', 'data modeling', 'Data Modeling']"
627d7d631fd4e0e2179f82199f014deb7ff0ea0b,Bayesian synthesis of probabilistic programs for automatic data modeling,https://www.semanticscholar.org/paper/627d7d631fd4e0e2179f82199f014deb7ff0ea0b,JournalArticle,"We present new techniques for automatically constructing probabilistic programs for data analysis, interpretation, and prediction. These techniques work with probabilistic domain-specific data modeling languages that capture key properties of a broad class of data generating processes, using Bayesian inference to synthesize probabilistic programs in these modeling languages given observed data. We provide a precise formulation of Bayesian synthesis for automatic data modeling that identifies sufficient conditions for the resulting synthesis procedure to be sound. We also derive a general class of synthesis algorithms for domain-specific languages specified by probabilistic context-free grammars and establish the soundness of our approach for these languages. We apply the techniques to automatically synthesize probabilistic programs for time series data and multivariate tabular data. We show how to analyze the structure of the synthesized programs to compute, for key qualitative properties of interest, the probability that the underlying data generating process exhibits each of these properties. Second, we translate probabilistic programs in the domain-specific language into probabilistic programs in Venture, a general-purpose probabilistic programming system. The translated Venture programs are then executed to obtain predictions of new time series data and new multivariate data records. Experimental results show that our techniques can accurately infer qualitative structure in multiple real-world data sets and outperform standard data analysis methods in forecasting and predicting new data.",2019,46,3,"[145044852.0, 1397895859.0, 1922848.0, 1720971.0, 1735083.0]",145044852.0,Helsinki,Proc. ACM Program. Lang.,"['data', 'programs', 'techniques', 'data modeling', 'Data Modeling']"
72a6a5d5864eb915231c7128b90977f1d2acf6e9,Deep spatio-temporal residual neural networks for road-network-based data modeling,https://www.semanticscholar.org/paper/72a6a5d5864eb915231c7128b90977f1d2acf6e9,JournalArticle,"ABSTRACT Recently, researchers have introduced deep learning methods such as convolutional neural networks (CNN) to model spatio-temporal data and achieved better results than those with conventional methods. However, these CNN-based models employ a grid map to represent spatial data, which is unsuitable for road-network-based data. To address this problem, we propose a deep spatio-temporal residual neural network for road-network-based data modeling (DSTR-RNet). The proposed model constructs locally-connected neural network layers (LCNR) to model road network topology and integrates residual learning to model the spatio-temporal dependency. We test the DSTR-RNet by predicting the traffic flow of Didi cab service, in an 8-km2 region with 2,616 road segments in Chengdu, China. The results demonstrate that the DSTR-RNet maintains the spatial precision and topology of the road network as well as improves the prediction accuracy. We discuss the prediction errors and compare the prediction results to those of grid-based CNN models. We also explore the sensitivity of the model to its parameters; this will aid the application of this model to network-based data modeling.",2019,33,33,"[2115242952.0, 49829302.0, 46866973.0]",2115242952.0,Dublin,International Journal of Geographical Information Science,"['model', 'data', 'network', 'data modeling', 'Data Modeling']"
6dd481d5eb8d76d4b61a1829c7687d008e0937ab,A Unified Framework for Plasma Data Modeling in Dynamic Positron Emission Tomography Studies,https://www.semanticscholar.org/paper/6dd481d5eb8d76d4b61a1829c7687d008e0937ab,JournalArticle,"Objective: Full quantification of dynamic positron emission tomography (PET) data requires the knowledge of tracer concentration in the arterial plasma. However, its accurate measurement is challenging due to the presence of radiolabeled metabolites and measurement noise. Mathematical models are fitted to the plasma data for both radiometabolite correction and data denoising. However, the models used are generally not physiologically informed and not consistently applied across studies even when quantifying the kinetics of the same radiotracer, introducing methodological variability affecting the results interpretation. The aim of this study was to develop and validate a unified framework for the arterial data modeling to achieve an accurate and fully automated description of the plasma tracer kinetics. Methods: The proposed pipeline employs basis pursuit techniques for estimating both radiometabolites and parent concentration models from the raw plasma measurements, allowing the resulting algorithm to be both robust and flexible to the different quality of data available. The pipeline was tested on four PET tracers ([11C]PBR28, [11C]MePPEP, [11C]WAY-100635, and [11C]PIB) with continuous and discrete blood sampling. Results: Compared to the standard procedure, the pipeline provided similar fit of the parent fraction but yielded a better description of the total plasma radioactivity, which in turn allowed a more accurate fit of the tissue PET data. Conclusion: The new method showed superior fits compared to the standard pipeline, for both continuous and discrete arterial sampling protocol, yielding to better description of PET data. Significance: The proposed pipeline has the potential to standardize the blood data modeling in dynamic PET studies given its robustness, flexibility and easiness of use.",2019,16,66,"[2744698.0, 38121795.0, 144666389.0, 3589089.0, 6315093.0, 46174961.0, 2991080.0]",2744698.0,Warsaw,IEEE Transactions on Biomedical Engineering,"['data', 'plasma', 'pipeline', 'data modeling', 'Data Modeling']"
eeac4411ae119c6c7ac33a11f762f2495b4dd960,Predicting the Risk of Heart Failure With EHR Sequential Data Modeling,https://www.semanticscholar.org/paper/eeac4411ae119c6c7ac33a11f762f2495b4dd960,JournalArticle,"Electronic health records (EHRs) contain patient diagnostic records, physician records, and records of hospital departments. For heart failure, we can obtain mass unstructured data from EHR time series. By analyzing and mining these time-based EHRs, we can identify the links between diagnostic events and ultimately predict when a patient will be diagnosed. However, it is difficult to use the existing EHR data directly, because they are sparse and non-standardized. Thus, this paper proposes an effective and robust architecture for heart failure prediction. The main contribution of this paper is to predict heart failure using a neural network (i.e., to predict the possibility of cardiac illness based on patient’s electronic medical data). Specifically, we employed one-hot encoding and word vectors to model the diagnosis events and predicted heart failure events using the basic principles of a long short-term memory network model. Evaluations based on a real-world data set demonstrate the promising utility and efficacy of the proposed architecture in the prediction of the risk of heart failure.",2018,99,6,"[144732658.0, 2106075012.0, 2109343347.0, 5818674.0, 5028442.0, 2115493235.0]",144732658.0,Copenhagen,IEEE Access,"['heart', 'failure', 'records', 'data modeling', 'Data Modeling']"
b6becea767675ea6ee43c78ce747077a5050019c,Fuzzy Spatiotemporal Data Modeling Based on UML,https://www.semanticscholar.org/paper/b6becea767675ea6ee43c78ce747077a5050019c,JournalArticle,"With the wide application of spatiotemporal data, more and more data need to be modeled. Fuzziness is one of the important characteristics of spatiotemporal data, but most of the existing spatiotemporal data models are regarded as accurate data and most of the spatiotemporal data models are static. The purpose of this paper is to build a fuzzy spatiotemporal data model based on UML by expanding the standard modeling language UML. On this basis, the historical topological state, effective time, and transaction time are added to make the model dynamic, that is, the states of multiple times are combined together to form a timeline to represent the development process of a fuzzy spatiotemporal entity. Considering the fuzziness of data and the dynamic development of objects, the model can describe the fuzzy spatiotemporal objects better, and make the model dynamic. At the same time, the operation of the UML class diagram is added to make the model data exercisable and compared according to their own needs, which increases the practicability of the model. Finally, an example of land desertification in Alashan is given, to prove the practicability of the model constructed in this paper.",2019,11,7,"[2155516106.0, 1779967.0]",2155516106.0,Prague,IEEE Access,"['data', 'model', 'time', 'data modeling', 'Data Modeling']"
ddc6e677715c03fe574319d3f80a3e1577bdbdd3,Dynamic Multivariate Functional Data Modeling via Sparse Subspace Learning,https://www.semanticscholar.org/paper/ddc6e677715c03fe574319d3f80a3e1577bdbdd3,JournalArticle,"ABSTRACT Multivariate functional data from a complex system are naturally high-dimensional and have a complex cross-correlation structure. The complexity of data structure can be observed as that (1) some functions are strongly correlated with similar features, while some others may have almost no cross-correlations with quite diverse features; and (2) the cross-correlation structure may also change over time due to the system evolution. With this regard, this article presents a dynamic subspace learning method for multivariate functional data modeling. In particular, we consider that different functions come from different subspaces, and only functions of the same subspace have cross-correlations with each other. The subspaces can be automatically formulated and learned by reformatting the problem as a sparse regression. By allowing but regularizing the regression change over time, we can describe the cross-correlation dynamics. The model can be efficiently estimated by the fast iterative shrinkage-thresholding algorithm, and the features of each subspace can be extracted using the smooth multi-channel functional principal component analysis. Some theoretical properties of the model are presented. Numerical studies, together with case studies, demonstrate the efficiency and applicability of the proposed methodology.",2018,27,63,"[2111572794.0, 144303419.0, 2117177710.0, 153533172.0]",2111572794.0,Warsaw,Technometrics,"['data', 'crosscorrelation', 'structure', 'data modeling', 'Data Modeling']"
63adc1e5086481e36b19b62707a96b799da51e59,Data modeling versus simulation modeling in the big data era: case study of a greenhouse control system,https://www.semanticscholar.org/paper/63adc1e5086481e36b19b62707a96b799da51e59,JournalArticle,"Recently, big data has received greater attention in diverse research fields, including medicine, science, engineering, management, defense, politics, and others. Such research uses big data to predict target systems, thereby constructing a model of the system in two ways: data modeling and simulation modeling. Data modeling is a method in which a model represents correlation relationships between one set of data and the other set of data. On the other hand, physics-based simulation modeling (or simply simulation modeling) is a more classical, but more powerful, method in which a model represents causal relationships between a set of controlled inputs and corresponding outputs. This paper (i) clarifies the difference between the two modeling approaches, (ii) explains their advantages and limitations and compares each characteristic, and (iii) presents a complementary cooperation modeling approach. Then, we apply the proposed modeling to develop a greenhouse control system in the real world. Finally, we expect that this modeling approach will be an alternative modeling approach in the big data era.",2017,61,93,"[2608638.0, 48418805.0, 3160707.0, 1761763.0]",2608638.0,Stockholm,International Conference on Advances in System Simulation,"['data', 'modeling', 'model', 'data modeling', 'Data Modeling']"
fdc57c18f3b636c3273542327ae540217972558f,"DART: Recent Advances in Remote Sensing Data Modeling With Atmosphere, Polarization, and Chlorophyll Fluorescence",https://www.semanticscholar.org/paper/fdc57c18f3b636c3273542327ae540217972558f,JournalArticle,"To better understand the life-essential cycles and processes of our planet and to further develop remote sensing (RS) technology, there is an increasing need for models that simulate the radiative budget (RB) and RS acquisitions of urban and natural landscapes using physical approaches and considering the three-dimensional (3-D) architecture of Earth surfaces. Discrete anisotropic radiative transfer (DART) is one of the most comprehensive physically based 3-D models of Earth-atmosphere radiative transfer, covering the spectral domain from ultraviolet to thermal infrared wavelengths. It simulates the optical 3-D RB and optical signals of proximal, aerial, and satellite imaging spectrometers and laser scanners, for any urban and/or natural landscapes and for any experimental and instrumental configurations. It is freely available for research and teaching activities. In this paper, we briefly introduce DART theory and present recent advances in simulated sensors (LiDAR and cameras with finite field of view) and modeling mechanisms (atmosphere, specular reflectance with polarization and chlorophyll fluorescence). A case study demonstrating a novel application of DART to investigate urban landscapes is also presented.",2017,148,10,"[1412902270.0, 2327930.0, 144868747.0, 7471201.0, 144579083.0, 48538041.0, 2990847.0, 32719983.0, 31140405.0, 3394506.0, 114824980.0, 46771083.0, 8599371.0, 33617898.0, 3420619.0, 2660652.0, 143717263.0]",1412902270.0,Berlin,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"['radiative', 'landscapes', '3d', 'data modeling', 'Data Modeling']"
e3b94a5f28522e6825aff16ff07d56bd70d26c96,The YANG 1.1 Data Modeling Language,https://www.semanticscholar.org/paper/e3b94a5f28522e6825aff16ff07d56bd70d26c96,JournalArticle,"YANG is a data modeling language used to model configuration data,
state data, Remote Procedure Calls, and notifications for network
management protocols. This document describes the syntax and semantics
of version 1.1 of the YANG language. YANG version 1.1 is a maintenance
release of the YANG language, addressing ambiguities and defects in
the original specification. There are a small number of backward
incompatibilities from YANG version 1. This document also specifies
the YANG mappings to the Network Configuration Protocol (NETCONF).",2016,127,7950,[103682810.0],103682810.0,Bratislava,Request for Comments,"['yang', 'data', 'language', 'data modeling', 'Data Modeling']"
56ae2837b6cd4d143bbfa4a4b06811d70126106f,Unevenly Sampled Dynamic Data Modeling and Monitoring With an Industrial Application,https://www.semanticscholar.org/paper/56ae2837b6cd4d143bbfa4a4b06811d70126106f,JournalArticle,"In this paper, a dynamic modeling method for unevenly sampled data is proposed for the monitoring of bi-layer (i.e., a process layer and a quality layer) dynamic processes. First, a novel uneven data dynamic canonical correlation analysis method with an integrated dynamic time window is proposed for interlayer latent structure modeling, which captures the dynamic relations between regularly sampled process data and quality data with slow and irregular sampling. The new model is a step toward big data modeling to deal with data irregularity and diversity. Second, after extracting covariations using an interlayer model, intralayer variations are extracted using subsequent principal component analysis on the residual subspaces of the original process data and quality data, respectively. Third, a concurrent monitoring method for unevenly sampled bi-layer data is proposed. Finally, the proposed method is demonstrated using an illustrative simulation example and applied successfully to a real blast furnace iron-making process.",2017,33,13,"[46581019.0, 143609842.0, 145011556.0]",46581019.0,Vienna,IEEE Transactions on Industrial Informatics,"['data', 'method', 'process', 'data modeling', 'Data Modeling']"
b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0,Bayesian Random Vector Functional-Link Networks for Robust Data Modeling,https://www.semanticscholar.org/paper/b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0,JournalArticle,"Random vector functional-link (RVFL) networks are randomized multilayer perceptrons with a single hidden layer and a linear output layer, which can be trained by solving a linear modeling problem. In particular, they are generally trained using a closed-form solution of the (regularized) least-squares approach. This paper introduces several alternative strategies for performing full Bayesian inference (BI) of RVFL networks. Distinct from standard or classical approaches, our proposed Bayesian training algorithms allow to derive an entire probability distribution over the optimal output weights of the network, instead of a single pointwise estimate according to some given criterion (e.g., least-squares). This provides several known advantages, including the possibility of introducing additional prior knowledge in the training process, the availability of an uncertainty measure during the test phase, and the capability of automatically inferring hyper-parameters from given data. In this paper, two BI algorithms for regression are first proposed that, under some practical assumptions, can be implemented by a simple iterative process with closed-form computations. Simulation results show that one of the proposed algorithms, Bayesian RVFL, is able to outperform standard training algorithms for RVFL networks with a proper regularization factor selected carefully via a line search procedure. A general strategy based on variational inference is also presented, with an application to data modeling problems with noisy outputs or outliers. As we discuss in this paper, using recent advances in automatic differentiation this strategy can be applied to a wide range of additional situations in an immediate fashion.",2018,26,48,"[1752983.0, 70129091.0, 1737292.0]",1752983.0,Nicosia,IEEE Transactions on Cybernetics,"['rvfl', 'algorithms', 'networks', 'data modeling', 'Data Modeling']"
2141334fad7248fc707607bc9453d44686ae07a7,How to Make Sense of Team Sport Data: From Acquisition to Data Modeling and Research Aspects,https://www.semanticscholar.org/paper/2141334fad7248fc707607bc9453d44686ae07a7,JournalArticle,"Automatic and interactive data analysis is instrumental in making use of increasing amounts of complex data. Owing to novel sensor modalities, analysis of data generated in professional team sport leagues such as soccer, baseball, and basketball has recently become of concern, with potentially high commercial and research interest. The analysis of team ball games can serve many goals, e.g., in coaching to understand effects of strategies and tactics, or to derive insights improving performance. Also, it is often decisive to trainers and analysts to understand why a certain movement of a player or groups of players happened, and what the respective influencing factors are. We consider team sport as group movement including collaboration and competition of individuals following specific rule sets. Analyzing team sports is a challenging problem as it involves joint understanding of heterogeneous data perspectives, including high-dimensional, video, and movement data, as well as considering team behavior and rules (constraints) given in the particular team sport. We identify important components of team sport data, exemplified by the soccer case, and explain how to analyze team sport data in general. We identify challenges arising when facing these data sets and we propose a multi-facet view and analysis including pattern detection, context-aware analysis, and visual explanation. We also present applicable methods and technologies covering the heterogeneous aspects in team sport data.",2017,66,2,"[40617842.0, 2024783.0, 3471557.0, 51169038.0, 40106679.0, 2064855598.0, 2942725.0, 1706471.0, 9106757.0, 1786155.0]",40617842.0,Nicosia,International Conference on Data Technologies and Applications,"['data', 'team', 'sport', 'data modeling', 'Data Modeling']"
8ba8a0d18a06752f5a39996ccf1e914da0941443,Combining first-principles and data modeling for the accurate prediction of the refractive index of organic polymers.,https://www.semanticscholar.org/paper/8ba8a0d18a06752f5a39996ccf1e914da0941443,JournalArticle,"Organic materials with a high index of refraction (RI) are attracting considerable interest due to their potential application in optic and optoelectronic devices. However, most of these applications require an RI value of 1.7 or larger, while typical carbon-based polymers only exhibit values in the range of 1.3-1.5. This paper introduces an efficient computational protocol for the accurate prediction of RI values in polymers to facilitate in silico studies that can guide the discovery and design of next-generation high-RI materials. Our protocol is based on the Lorentz-Lorenz equation and is parametrized by the polarizability and number density values of a given candidate compound. In the proposed scheme, we compute the former using first-principles electronic structure theory and the latter using an approximation based on van der Waals volumes. The critical parameter in the number density approximation is the packing fraction of the bulk polymer, for which we have devised a machine learning model. We demonstrate the performance of the proposed RI protocol by testing its predictions against the experimentally known RI values of 112 optical polymers. Our approach to combine first-principles and data modeling emerges as both a successful and a highly economical path to determining the RI values for a wide range of organic polymers.",2018,33,148 24,"[16009412.0, 3480094.0, 40471981.0]",16009412.0,Brussels,Journal of Chemical Physics,"['ri', 'polymers', 'values', 'data modeling', 'Data Modeling']"
54a26f5ef4b0524c60249fe98d0fc3646b2791ad,Sampled-Data Modeling of Switched- Capacitor Voltage Regulator With Frequency-Modulation Control,https://www.semanticscholar.org/paper/54a26f5ef4b0524c60249fe98d0fc3646b2791ad,JournalArticle,The development of systems-on-chip requires embedded power management solutions due to the large number of power domains. The switched-capacitor voltage regulator is a suitable candidate as capacitors may be integrated whereas inductors still suffer limitations in that respect. Literature covers proposals of optimized power stages and several dedicated controllers for switched-capacitor DC-DC converters. Unfortunately the results do not cover systematic stability analyses. The paper proposes an original and systematic approach for the stability analysis of a switched-capacitor voltage regulator using sampled-data modeling. An application is given for a one-phase converter with a frequency-modulation based controller. The stability of the open-loop and closed-loop model is proposed and can be extended to multi-phase configurations.,2015,21,62,"[112878157.0, 145511872.0, 1399086860.0]",112878157.0,Berlin,IEEE Transactions on Circuits and Systems Part 1: Regular Papers,"['power', 'switchedcapacitor', 'stability', 'data modeling', 'Data Modeling']"
00bbc94806ca0821a9c82d8aedf16f0e6263b89f,Agreement between PRE2DUP register data modeling method and comprehensive drug use interview among older persons,https://www.semanticscholar.org/paper/00bbc94806ca0821a9c82d8aedf16f0e6263b89f,JournalArticle,"Background PRE2DUP is a modeling method that generates drug use periods (ie, when drug use started and ended) from drug purchases recorded in dispensing-based register data. It is based on the evaluation of personal drug purchasing patterns and considers hospital stays, possible stockpiling of drugs, and package information. Objective The objective of this study was to investigate person-level agreement between self-reported drug use in the interview and drug use modeled from dispensing data with PRE2DUP method for various drug classes used by older persons. Methods Self-reported drug use was assessed from the GeMS Study including a random sample of persons aged ≥75 years from the city of Kuopio, Finland, in 2006. Drug purchases recorded in the Prescription register data of these persons were modeled to determine drug use periods with PRE2DUP modeling method. Agreement between self-reported drug use on the interview date and drug use calculated from register-based data was compared in order to find the frequently used drugs and drug classes, which was evaluated by Cohen’s kappa. Kappa values 0.61–0.80 were considered to represent good and 0.81–1.00 as very good agreement. Results Among 569 participants with mean age of 82 years, the agreement between interview and register data was very good for 75% and very good or good for 93% of the studied drugs or drug classes. Good or very good agreement was observed for drugs that are typically used on regular bases, whereas “as needed” drugs represented poorer results. Conclusion PRE2DUP modeling method validly describes regular drug use among older persons. For most of drug classes investigated, PRE2DUP-modeled register data described drug use as well as interview-based data which are more time-consuming to collect. Further studies should be conducted by comparing it with other methods and in different drug user populations.",2016,58,8,"[2167301.0, 2638813.0, 2529812.0, 2628180.0, 7478280.0, 2119993.0]",2167301.0,Reykjavik,Clinical Epidemiology,"['drug', 'use', 'data', 'data modeling', 'Data Modeling']"
f0f1627db35b4942e0f83069f20dd0948fc35d28,A Novel Data-Driven Situation Awareness Approach for Future Grids—Using Large Random Matrices for Big Data Modeling,https://www.semanticscholar.org/paper/f0f1627db35b4942e0f83069f20dd0948fc35d28,JournalArticle,"Data-driven approaches, when tasked with situation awareness, are suitable for complex grids with <italic>massive datasets</italic>. It is a challenge, however, to efficiently turn these massive datasets into useful big data analytics. To address such a challenge, this paper, based on random matrix theory, proposes a data-driven approach. The approach models massive datasets as large random matrices; it is model-free and requires no knowledge about physical model parameters. In particular, the large data dimension <inline-formula> <tex-math notation=""LaTeX"">$N$ </tex-math></inline-formula> and the large time span <inline-formula> <tex-math notation=""LaTeX"">$T$ </tex-math></inline-formula>, from the spatial aspect and the temporal aspect, respectively, lead to favorable results. The beautiful thing lies in that these linear eigenvalue statistics (LESs) are built from data matrices to follow Gaussian distributions for very general conditions, due to the <italic>latest breakthroughs</italic> in probability on the central limit theorems of those LESs. Numerous case studies, with both simulated data and field data, are given to validate the proposed new algorithms.",2016,43,6,"[3026184.0, 144248374.0, 1709488.0, 1816765.0, 152355659.0]",3026184.0,Minsk,IEEE Access,"['data', 'challenge', 'datasets', 'data modeling', 'Data Modeling']"
b3f7359c6d5780972c5ea8db016a01f0c705aa01,glmmTMB Balances Speed and Flexibility Among Packages for Zero-inflated Generalized Linear Mixed Modeling,https://www.semanticscholar.org/paper/b3f7359c6d5780972c5ea8db016a01f0c705aa01,JournalArticle,"Count data can be analyzed using generalized linear mixed models when observations are correlated in ways that require random effects. However, count data are often zero-inflated, containing more zeros than would be expected from the typical error distributions. We present a new package, glmmTMB, and compare it to other R packages that fit zero-inflated mixed models. The glmmTMB package fits many types of GLMMs and extensions, including models with continuously distributed responses, but here we focus on count responses. glmmTMB is faster than glmmADMB, MCMCglmm, and brms, and more flexible than INLA and mgcv for zero-inflated modeling. One unique feature of glmmTMB (among packages that fit zero-inflated mixed models) is its ability to estimate the Conway-Maxwell-Poisson distribution parameterized by the mean. Overall, its most appealing features for new users may be the combination of speed, flexibility, and its interface’s similarity to lme4.",2017,4831,9,"[32710347.0, 46980587.0, 5210567.0, 35493487.0, 10705679.0, 2079275650.0, 2128088.0, 144985567.0, 2254255.0]",32710347.0,Podgorica,The R Journal,"['models', 'glmmtmb', 'count', 'data modeling', 'Data Modeling']"
7ed665355ac78bf0c394602dd9d26075195ce2f2,A Novel Big Data Modeling Method for Improving Driving Range Estimation of EVs,https://www.semanticscholar.org/paper/7ed665355ac78bf0c394602dd9d26075195ce2f2,JournalArticle,"In this paper, we address a big-data analysis method for estimating the driving range of an electric vehicle (EV), allowing drivers to overcome range anxiety. First, we present an estimating approach to project the life of battery pack for 1600 cycles (i.e., 8 years/160 000 km) based on the data collected from a cycle-life test. This approach has the merit of simplicity. In addition, it considers several critical issues that occur inside battery packs, such as the dependence of internal resistance and the state-of-health. Subsequently, we describe our work on driving pattern analysis of an EV, using a machine-learning approach, namely growing hierarchical self-organizing maps, to cluster the collected EV big data. This paper contains the analysis of energy consumption and driving range estimation for EVs, including powertrain simulation and driving behavior analysis. The experimental results, including both simulating battery degradation and analysis of driving behaviors, demonstrate a feasible solution for improving driving range estimation by the EV big data.",2015,58,3,"[1783205.0, 2146335468.0]",1783205.0,Luxembourg,IEEE Access,"['analysis', 'range', 'battery', 'data modeling', 'Data Modeling']"
c665003881c3c35589d1e48da1ee7234b48f2ac8,Fuzzy Spatiotemporal Data Modeling and Operations in XML,https://www.semanticscholar.org/paper/c665003881c3c35589d1e48da1ee7234b48f2ac8,JournalArticle,"Because increasing requirements of fuzzy spatiotemporal applications are attracting much attention from both academia and industry, it is challenging to model fuzzy spatiotemporal data and effectively operate them. However, various researches are studied in traditional databases that impose strict restrictions, and relatively little work has been carried out in modeling and operating fuzzy spatiotemporal data in XML. In this paper, we propose a novel approach to model fuzzy spatiotemporal data based on XML. On the basis of the model, we investigate how to represent fuzzy spatiotemporal data in XML documents and extend the XML schema so that it is possible to describe fuzzy spatiotemporal data and capture the structural information of fuzzy spatiotemporal XML documents. Furthermore, we give algorithms for fuzzy operations, containing node operations and topological relationship operations. Finally, we apply our model in meteorological events.",2015,15,29,"[1779967.0, 47002265.0, 2200379863.0]",1779967.0,London,Applied Artificial Intelligence,"['data', 'xml', 'model', 'data modeling', 'Data Modeling']"
633e2fbfc0b21e959a244100937c5853afca4853,Score-Based Generative Modeling through Stochastic Differential Equations,https://www.semanticscholar.org/paper/633e2fbfc0b21e959a244100937c5853afca4853,JournalArticle,"Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",2020,2851,abs/2011.13456,"[115504645.0, 1407546424.0, 1726807.0, 2109224633.0, 2490652.0, 16443937.0]",115504645.0,Andorra,International Conference on Learning Representations,"['sde', 'distribution', 'data', 'data modeling', 'Data Modeling']"
91b9d3ab7532ea24ae70cd726355f25235b1fe8b,Ten simple rules for the computational modeling of behavioral data,https://www.semanticscholar.org/paper/91b9d3ab7532ea24ae70cd726355f25235b1fe8b,JournalArticle,"Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.",2019,309,8,"[153160317.0, 50593332.0]",153160317.0,Dublin,eLife,"['modeling', 'data', 'rules', 'data modeling', 'Data Modeling']"
2e965b5d97c2d6fb4af284307735be39283792ba,Extracting Training Data from Diffusion Models,https://www.semanticscholar.org/paper/2e965b5d97c2d6fb4af284307735be39283792ba,JournalArticle,"Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.",2023,266,abs/2301.13188,"[2483738.0, 9200194.0, 3490923.0, 40844378.0, 3482535.0, 2444919.0, 1718064.0, 7975935.0, 145217343.0]",2483738.0,Chisinau,USENIX Security Symposium,"['models', 'diffusion', 'training', 'data modeling', 'Data Modeling']"
4b06c7e29280b1c6bc05c9df39023b48fef02c93,Escaping the Big Data Paradigm with Compact Transformers,https://www.semanticscholar.org/paper/4b06c7e29280b1c6bc05c9df39023b48fef02c93,JournalArticle,"With the rise of Transformers as the standard for language processing, and their advancements in computer vision, there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolutional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outperforms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers. Our code and pre-trained models are publicly available at https://github.com/SHI-Labs/Compact-Transformers.",2021,314,abs/2104.05704,"[2855934.0, 102471415.0, 2087066452.0, 133551208.0, 2125031571.0, 48667025.0]",2855934.0,Amsterdam,arXiv.org,"['transformers', 'size', 'data', 'big data', 'Big Data']"
2a56bd89fd1a9457f0705142540ffc4396fad4f7,Variational LSTM Enhanced Anomaly Detection for Industrial Big Data,https://www.semanticscholar.org/paper/2a56bd89fd1a9457f0705142540ffc4396fad4f7,JournalArticle,"With the increasing population of Industry 4.0, industrial big data (IBD) has become a hotly discussed topic in digital and intelligent industry field. The security problem existing in the signal processing on large scale of data stream is still a challenge issue in industrial internet of things, especially when dealing with the high-dimensional anomaly detection for intelligent industrial application. In this article, to mitigate the inconsistency between dimensionality reduction and feature retention in imbalanced IBD, we propose a variational long short-term memory (VLSTM) learning model for intelligent anomaly detection based on reconstructed feature representation. An encoder–decoder neural network associated with a variational reparameterization scheme is designed to learn the low-dimensional feature representation from high-dimensional raw data. Three loss functions are defined and quantified to constrain the reconstructed hidden variable into a more explicit and meaningful form. A lightweight estimation network is then fed with the refined feature representation to identify anomalies in IBD. Experiments using a public IBD dataset named UNSW-NB15 demonstrate that the proposed VLSTM model can efficiently cope with imbalance and high-dimensional issues, and significantly improve the accuracy and reduce the false rate in anomaly detection for IBD according to F1, area under curve (AUC), and false alarm rate (FAR).",2021,202,17,"[2504776.0, 1752768069.0, 2088921907.0, 1695160.0, 46876083.0]",2504776.0,Vaduz,IEEE Transactions on Industrial Informatics,"['ibd', 'feature', 'data', 'big data', 'Big Data']"
8894d431a768a35dc7ca4d762ebdba4f407b978c,The ProteomeXchange consortium in 2020: enabling ‘big data’ approaches in proteomics,https://www.semanticscholar.org/paper/8894d431a768a35dc7ca4d762ebdba4f407b978c,JournalArticle,"Abstract The ProteomeXchange (PX) consortium of proteomics resources (http://www.proteomexchange.org) has standardized data submission and dissemination of mass spectrometry proteomics data worldwide since 2012. In this paper, we describe the main developments since the previous update manuscript was published in Nucleic Acids Research in 2017. Since then, in addition to the four PX existing members at the time (PRIDE, PeptideAtlas including the PASSEL resource, MassIVE and jPOST), two new resources have joined PX: iProX (China) and Panorama Public (USA). We first describe the updated submission guidelines, now expanded to include six members. Next, with current data submission statistics, we demonstrate that the proteomics field is now actively embracing public open data policies. At the end of June 2019, more than 14 100 datasets had been submitted to PX resources since 2012, and from those, more than 9 500 in just the last three years. In parallel, an unprecedented increase of data re-use activities in the field, including ‘big data’ approaches, is enabling novel research and new data resources. At last, we also outline some of our future plans for the coming years.",2019,537,48,"[1763674.0, 2865038.0, 33582313.0, 1390051934.0, 31411209.0, 1605658912.0, 1402417041.0, 4007395.0, 13084208.0, 39268701.0, 151067942.0, 14948487.0, 46994990.0, 48576550.0, 49701119.0, 1785886.0, 144252544.0, 2167765.0, 2762241.0, 91596213.0, 3245770.0]",1763674.0,Oslo,Nucleic Acids Res.,"['data', 'px', 'resources', 'big data', 'Big Data']"
b34fc78de28be598e21118d7cb9d84d63374addc,Analysis of Dimensionality Reduction Techniques on Big Data,https://www.semanticscholar.org/paper/b34fc78de28be598e21118d7cb9d84d63374addc,JournalArticle,"Due to digitization, a huge volume of data is being generated across several sectors such as healthcare, production, sales, IoT devices, Web, organizations. Machine learning algorithms are used to uncover patterns among the attributes of this data. Hence, they can be used to make predictions that can be used by medical practitioners and people at managerial level to make executive decisions. Not all the attributes in the datasets generated are important for training the machine learning algorithms. Some attributes might be irrelevant and some might not affect the outcome of the prediction. Ignoring or removing these irrelevant or less important attributes reduces the burden on machine learning algorithms. In this work two of the prominent dimensionality reduction techniques, Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are investigated on four popular Machine Learning (ML) algorithms, Decision Tree Induction, Support Vector Machine (SVM), Naive Bayes Classifier and Random Forest Classifier using publicly available Cardiotocography (CTG) dataset from University of California and Irvine Machine Learning Repository. The experimentation results prove that PCA outperforms LDA in all the measures. Also, the performance of the classifiers, Decision Tree, Random Forest examined is not affected much by using PCA and LDA.To further analyze the performance of PCA and LDA the eperimentation is carried out on Diabetic Retinopathy (DR) and Intrusion Detection System (IDS) datasets. Experimentation results prove that ML algorithms with PCA produce better results when dimensionality of the datasets is high. When dimensionality of datasets is low it is observed that the ML algorithms without dimensionality reduction yields better results.",2020,420,8,"[38608914.0, 49247239.0, 51017707.0, 46234199.0, 32336300.0, 2176030144.0, 2287603163.0, 2176030144.0]",38608914.0,Vaduz,IEEE Access,"['machine', 'algorithms', 'datasets', 'big data', 'Big Data']"
391a5f286f814d852dddcab1b2b68e5c1af6c79e,Data mining with big data,https://www.semanticscholar.org/paper/391a5f286f814d852dddcab1b2b68e5c1af6c79e,JournalArticle,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",2016,1674,26,"[2271668292.0, 2283098327.0, 2286440807.0, 2286232238.0]",2271668292.0,Skopje,IEEE Transactions on Knowledge and Data Engineering,"['data', 'model', 'sources', 'big data', 'Big Data']"
156609022dd6258c60238859622da0a1683bd062,Big Data Analysis Based Network Behavior Insight of Cellular Networks for Industry 4.0 Applications,https://www.semanticscholar.org/paper/156609022dd6258c60238859622da0a1683bd062,JournalArticle,"In this article, we propose a big data based analysis framework to analyze and extract network behaviors in cellular networks for Industry 4.0 applications from a big data perspective, using Hadoop, Hive, HBase, and so on. The data prehandling and traffic flow extraction approaches are presented to construct effective traffic matrices. Accordingly, we can capture network behaviors in cellular networks from a networkwide perspective. Although there have been a number of prior studies on cellular network usage, to the best of our knowledge, this article is a first study that characterizes network behaviors using the big data analytics to analyze a network big data of call detail records over a longer duration (five months), with more users (five million), more records (several hundred million lines) and nationwide coverage. The call pattern analysis and network behavior extraction approaches are designed to perform big data analysis and feature extractions. Then, the corresponding algorithms are proposed to characterize network behaviors, i.e., cellular call patterns and network resource usage. The detailed evaluation is proposed to validate our method. For example, we find that some unpopular calls can last longer time and thus consume more network resources.",2020,137,16,"[33367477.0, 2108035003.0, 1792647.0, 6353833.0, 143978827.0]",33367477.0,Kiev,IEEE Transactions on Industrial Informatics,"['network', 'data', 'behaviors', 'big data', 'Big Data']"
0e33833f5e2e2719edfba1d142eb4d27f96e799f,Big data analytics and enterprises: a bibliometric synthesis of the literature,https://www.semanticscholar.org/paper/0e33833f5e2e2719edfba1d142eb4d27f96e799f,JournalArticle,"ABSTRACT Understanding the developmental trajectories of big data analytics in the corporate context is highly relevant for information systems research and practice. This study presents a comprehensive bibliometric analysis of applications of big data analytics in enterprises. The sample for this study contained a total of 1727 articles from the Scopus database. The sample was analyzed with techniques such as bibliographic coupling, citation analysis, co-word analysis, and co-authorship analysis. Findings from the co-citation analysis identified four major thematic areas in the extant literature. The evolution of these thematic areas was documented with dynamic co-citation analysis.",2020,141,14,"[30757622.0, 1889986.0, 3062994.0]",30757622.0,Kiev,Enterprise Information Systems,"['analysis', 'data', 'analytics', 'big data', 'Big Data']"
18d87bff073687c025f9bd23ab2dfb20d5f72a66,BIM Big Data Storage in WebVRGIS,https://www.semanticscholar.org/paper/18d87bff073687c025f9bd23ab2dfb20d5f72a66,JournalArticle,"In the context of big data and the Internet of Things, with the advancement of geospatial data acquisition and retrieval, the volume of available geospatial data is increasing every minute. Thus, new data-management architecture is needed. We proposed a building information model (BIM) big data-storage-management solution with hybrid storage architecture based on web virtual reality geographical information system (WebVRGIS). BIM is associated with the integration of spatial and semantic information on the various stages of urban building. In this paper, based on the spatial distribution characteristics of BIM geospatial big data, a data storage and management model is proposed for BIM geospatial big data management. The architecture primarily includes Not only Structured Query Language (NoSQL) database and distributed peer-to-peer storage. The evaluation of the proposed storage method is conducted on the same software platform as our previous research about WebVR. The experimental results show that the hybrid storage architecture proposed in this research has a lower response time compared to the traditional relational database in geospatial big data searches. The integration and fusion of BIM big data in WebVRGIS realizes a revolutionary transformation of city information management during a full lifecycle. The system also has great promise for the storage of other geospatial big data, such as traffic data.",2020,129,16,"[145772858.0, 2116431916.0, 38929912.0, 9138305.0]",145772858.0,Skopje,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'bim', 'big data', 'Big Data']"
18d87bff073687c025f9bd23ab2dfb20d5f72a66,BIM Big Data Storage in WebVRGIS,https://www.semanticscholar.org/paper/18d87bff073687c025f9bd23ab2dfb20d5f72a66,JournalArticle,"In the context of big data and the Internet of Things, with the advancement of geospatial data acquisition and retrieval, the volume of available geospatial data is increasing every minute. Thus, new data-management architecture is needed. We proposed a building information model (BIM) big data-storage-management solution with hybrid storage architecture based on web virtual reality geographical information system (WebVRGIS). BIM is associated with the integration of spatial and semantic information on the various stages of urban building. In this paper, based on the spatial distribution characteristics of BIM geospatial big data, a data storage and management model is proposed for BIM geospatial big data management. The architecture primarily includes Not only Structured Query Language (NoSQL) database and distributed peer-to-peer storage. The evaluation of the proposed storage method is conducted on the same software platform as our previous research about WebVR. The experimental results show that the hybrid storage architecture proposed in this research has a lower response time compared to the traditional relational database in geospatial big data searches. The integration and fusion of BIM big data in WebVRGIS realizes a revolutionary transformation of city information management during a full lifecycle. The system also has great promise for the storage of other geospatial big data, such as traffic data.",2020,129,16,"[145772858.0, 2116431916.0, 38929912.0, 9138305.0]",145772858.0,Skopje,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'bim', 'data storage', 'Big Data']"
18d87bff073687c025f9bd23ab2dfb20d5f72a66,BIM Big Data Storage in WebVRGIS,https://www.semanticscholar.org/paper/18d87bff073687c025f9bd23ab2dfb20d5f72a66,JournalArticle,"In the context of big data and the Internet of Things, with the advancement of geospatial data acquisition and retrieval, the volume of available geospatial data is increasing every minute. Thus, new data-management architecture is needed. We proposed a building information model (BIM) big data-storage-management solution with hybrid storage architecture based on web virtual reality geographical information system (WebVRGIS). BIM is associated with the integration of spatial and semantic information on the various stages of urban building. In this paper, based on the spatial distribution characteristics of BIM geospatial big data, a data storage and management model is proposed for BIM geospatial big data management. The architecture primarily includes Not only Structured Query Language (NoSQL) database and distributed peer-to-peer storage. The evaluation of the proposed storage method is conducted on the same software platform as our previous research about WebVR. The experimental results show that the hybrid storage architecture proposed in this research has a lower response time compared to the traditional relational database in geospatial big data searches. The integration and fusion of BIM big data in WebVRGIS realizes a revolutionary transformation of city information management during a full lifecycle. The system also has great promise for the storage of other geospatial big data, such as traffic data.",2020,129,16,"[145772858.0, 2116431916.0, 38929912.0, 9138305.0]",145772858.0,Reykjavik,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'bim', 'big data', 'Data Storage']"
18d87bff073687c025f9bd23ab2dfb20d5f72a66,BIM Big Data Storage in WebVRGIS,https://www.semanticscholar.org/paper/18d87bff073687c025f9bd23ab2dfb20d5f72a66,JournalArticle,"In the context of big data and the Internet of Things, with the advancement of geospatial data acquisition and retrieval, the volume of available geospatial data is increasing every minute. Thus, new data-management architecture is needed. We proposed a building information model (BIM) big data-storage-management solution with hybrid storage architecture based on web virtual reality geographical information system (WebVRGIS). BIM is associated with the integration of spatial and semantic information on the various stages of urban building. In this paper, based on the spatial distribution characteristics of BIM geospatial big data, a data storage and management model is proposed for BIM geospatial big data management. The architecture primarily includes Not only Structured Query Language (NoSQL) database and distributed peer-to-peer storage. The evaluation of the proposed storage method is conducted on the same software platform as our previous research about WebVR. The experimental results show that the hybrid storage architecture proposed in this research has a lower response time compared to the traditional relational database in geospatial big data searches. The integration and fusion of BIM big data in WebVRGIS realizes a revolutionary transformation of city information management during a full lifecycle. The system also has great promise for the storage of other geospatial big data, such as traffic data.",2020,129,16,"[145772858.0, 2116431916.0, 38929912.0, 9138305.0]",145772858.0,Reykjavik,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'bim', 'data storage', 'Data Storage']"
cc1e82125f7f8636b25ccdcdb63e8f812add7f87,A Big Data Enabled Channel Model for 5G Wireless Communication Systems,https://www.semanticscholar.org/paper/cc1e82125f7f8636b25ccdcdb63e8f812add7f87,JournalArticle,"The standardization process of the fifth generation (5G) wireless communications has recently been accelerated and the first commercial 5G services would be provided as early as in 2018. The increasing of enormous smartphones, new complex scenarios, large frequency bands, massive antenna elements, and dense small cells will generate big datasets and bring 5G communications to the era of big data. This paper investigates various applications of big data analytics, especially machine learning algorithms in wireless communications and channel modeling. We propose a big data and machine learning enabled wireless channel model framework. The proposed channel model is based on artificial neural networks (ANNs), including feed-forward neural network (FNN) and radial basis function neural network (RBF-NN). The input parameters are transmitter (Tx) and receiver (Rx) coordinates, Tx–Rx distance, and carrier frequency, while the output parameters are channel statistical properties, including the received power, root mean square (RMS) delay spread (DS), and RMS angle spreads (ASs). Datasets used to train and test the ANNs are collected from both real channel measurements and a geometry based stochastic model (GBSM). Simulation results show good performance and indicate that machine learning algorithms can be powerful analytical tools for future measurement-based wireless channel modeling.",2020,82,6,"[2144794493.0, 50096877.0, 2075396941.0, 2152916916.0, 2149869431.0, 1735048.0, 2236529.0]",2144794493.0,Berlin,IEEE Transactions on Big Data,"['channel', 'g', 'communications', 'big data', 'Big Data']"
0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1,Big Data and Its Applications in Smart Real Estate and the Disaster Management Life Cycle: A Systematic Analysis,https://www.semanticscholar.org/paper/0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1,JournalArticle,"Big data is the concept of enormous amounts of data being generated daily in different fields due to the increased use of technology and internet sources. Despite the various advancements and the hopes of better understanding, big data management and analysis remain a challenge, calling for more rigorous and detailed research, as well as the identifications of methods and ways in which big data could be tackled and put to good use. The existing research lacks in discussing and evaluating the pertinent tools and technologies to analyze big data in an efficient manner which calls for a comprehensive and holistic analysis of the published articles to summarize the concept of big data and see field-specific applications. To address this gap and keep a recent focus, research articles published in last decade, belonging to top-tier and high-impact journals, were retrieved using the search engines of Google Scholar, Scopus, and Web of Science that were narrowed down to a set of 139 relevant research articles. Different analyses were conducted on the retrieved papers including bibliometric analysis, keywords analysis, big data search trends, and authors’ names, countries, and affiliated institutes contributing the most to the field of big data. The comparative analyses show that, conceptually, big data lies at the intersection of the storage, statistics, technology, and research fields and emerged as an amalgam of these four fields with interlinked aspects such as data hosting and computing, data management, data refining, data patterns, and machine learning. The results further show that major characteristics of big data can be summarized using the seven Vs, which include variety, volume, variability, value, visualization, veracity, and velocity. Furthermore, the existing methods for big data analysis, their shortcomings, and the possible directions were also explored that could be taken for harnessing technology to ensure data analysis tools could be upgraded to be fast and efficient. The major challenges in handling big data include efficient storage, retrieval, analysis, and visualization of the large heterogeneous data, which can be tackled through authentication such as Kerberos and encrypted files, logging of attacks, secure communication through Secure Sockets Layer (SSL) and Transport Layer Security (TLS), data imputation, building learning models, dividing computations into sub-tasks, checkpoint applications for recursive tasks, and using Solid State Drives (SDD) and Phase Change Material (PCM) for storage. In terms of frameworks for big data management, two frameworks exist including Hadoop and Apache Spark, which must be used simultaneously to capture the holistic essence of the data and make the analyses meaningful, swift, and speedy. Further field-specific applications of big data in two promising and integrated fields, i.e., smart real estate and disaster management, were investigated, and a framework for field-specific applications, as well as a merger of the two areas through big data, was highlighted. The proposed frameworks show that big data can tackle the ever-present issues of customer regrets related to poor quality of information or lack of information in smart real estate to increase the customer satisfaction using an intermediate organization that can process and keep a check on the data being provided to the customers by the sellers and real estate managers. Similarly, for disaster and its risk management, data from social media, drones, multimedia, and search engines can be used to tackle natural disasters such as floods, bushfires, and earthquakes, as well as plan emergency responses. In addition, a merger framework for smart real estate and disaster risk management show that big data generated from the smart real estate in the form of occupant data, facilities management, and building integration and maintenance can be shared with the disaster risk management and emergency response teams to help prevent, prepare, respond to, or recover from the disasters.",2020,84,4,"[71940750.0, 1657644010.0, 48633912.0, 90426770.0]",71940750.0,Zagreb,Big Data and Cognitive Computing,"['data', 'management', 'analysis', 'big data', 'Big Data']"
8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6,Applying big data based deep learning system to intrusion detection,https://www.semanticscholar.org/paper/8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6,JournalArticle,"With vast amounts of data being generated daily and the ever increasing interconnectivity of the world's internet infrastructures, a machine learning based Intrusion Detection Systems (IDS) has become a vital component to protect our economic and national security. Previous shallow learning and deep learning strategies adopt the single learning model approach for intrusion detection. The single learning model approach may experience problems to understand increasingly complicated data distribution of intrusion patterns. Particularly, the single deep learning model may not be effective to capture unique patterns from intrusive attacks having a small number of samples. In order to further enhance the performance of machine learning based IDS, we propose the Big Data based Hierarchical Deep Learning System (BDHDLS). BDHDLS utilizes behavioral features and content features to understand both network traffic characteristics and information stored in the payload. Each deep learning model in the BDHDLS concentrates its efforts to learn the unique data distribution in one cluster. This strategy can increase the detection rate of intrusive attacks as compared to the previous single learning model approaches. Based on parallel training strategy and big data techniques, the model construction time of BDHDLS is reduced substantially when multiple machines are deployed.",2020,77,3,"[49408448.0, 2052212942.0, 144310754.0]",49408448.0,Chisinau,Big Data Mining and Analytics,"['learning', 'model', 'data', 'big data', 'Big Data']"
ed9e7821b3e51c7e59183300d6c8cf90c8de0f26,COVID-19 is spatial: Ensuring that mobile Big Data is used for social good,https://www.semanticscholar.org/paper/ed9e7821b3e51c7e59183300d6c8cf90c8de0f26,JournalArticle,"The mobility restrictions related to COVID-19 pandemic have resulted in the biggest disruption to individual mobilities in modern times. The crisis is clearly spatial in nature, and examining the geographical aspect is important in understanding the broad implications of the pandemic. The avalanche of mobile Big Data makes it possible to study the spatial effects of the crisis with spatiotemporal detail at the national and global scales. However, the current crisis also highlights serious limitations in the readiness to take the advantage of mobile Big Data for social good, both within and beyond the interests of health sector. We propose two strategical pathways for the future use of mobile Big Data for societal impact assessment, addressing access to both raw mobile Big Data as well as aggregated data products. Both pathways require careful considerations of privacy issues, harmonized and transparent methodologies, and attention to the representativeness, reliability and continuity of data. The goal is to be better prepared to use mobile Big Data in future crises.",2020,64,7,"[101768561.0, 4902441.0, 32343498.0, 4478199.0]",101768561.0,Ljubljana,Big Data & Society,"['data', 'crisis', 'pathways', 'big data', 'Big Data']"
7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca,The value of Big Data in government: The case of ‘smart cities’,https://www.semanticscholar.org/paper/7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca,JournalArticle,"The emergence of Big Data has added a new aspect to conceptualizing the use of digital technologies in the delivery of public services and for realizing digital governance. This article explores, via the ‘value-chain’ approach, the evolution of digital governance research, and aligns it with current developments associated with data analytics, often referred to as ‘Big Data’. In many ways, the current discourse around Big Data reiterates and repeats established commentaries within the eGovernment research community. This body of knowledge provides an opportunity to reflect on the ‘promise’ of Big Data, both in relation to service delivery and policy formulation. This includes, issues associated with the quality and reliability of data, from mixing public and private sector data, issues associated with the ownership of raw and manipulated data, and ethical issues concerning surveillance and privacy. These insights and the issues raised help assess the value of Big Data in government and smart city environments.",2020,58,7,"[2235459675.0, 2285816475.0, 2285813122.0]",2235459675.0,Monaco,Big Data & Society,"['data', 'issues', 'delivery', 'big data', 'Big Data']"
02b1607af35b48f0bd716367caf6a7428b969369,AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty,https://www.semanticscholar.org/paper/02b1607af35b48f0bd716367caf6a7428b969369,JournalArticle,"Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.",2019,988,abs/1912.02781,"[3422872.0, 52227748.0, 8132903.0, 2368067.0, 2058362.0, 40627523.0]",3422872.0,Luxembourg,International Conference on Learning Representations,"['accuracy', 'distribution', 'test', 'data processing', 'Data Processing']"
41d04aa3c25dcfbf1b44ce666c48759e03c216c7,tf.data: A Machine Learning Data Processing Framework,https://www.semanticscholar.org/paper/41d04aa3c25dcfbf1b44ce666c48759e03c216c7,JournalArticle,"Training machine learning models requires feeding input data for models to ingest. Input pipelines for machine learning jobs are often challenging to implement efficiently as they require reading large volumes of data, applying complex transformations, and transferring data to hardware accelerators while overlapping computation and communication to achieve optimal performance. We present tf.data, a framework for building and executing efficient input pipelines for machine learning jobs. The tf.data API provides operators which can be parameterized with user-defined computation, composed, and reused across different machine learning domains. These abstractions allow users to focus on the application logic of data processing, while tf.data's runtime ensures that pipelines run efficiently. 
We demonstrate that input pipeline performance is critical to the end-to-end training time of state-of-the-art machine learning models. tf.data delivers the high performance required, while avoiding the need for manual tuning of performance knobs. We show that tf.data features, such as parallelism, caching, static optimizations, and non-deterministic execution are essential for high performance. Finally, we characterize machine learning input pipelines for millions of jobs that ran in Google's fleet, showing that input data processing is highly diverse and consumes a significant fraction of job resources. Our analysis motivates future research directions, such as sharing computation across jobs and pushing data projection to the storage layer.",2021,58,14,"[20154699.0, 38300863.0, 2285439911.0, 120658773.0]",20154699.0,Chisinau,Proceedings of the VLDB Endowment,"['machine', 'input', 'data', 'data processing', 'Data Processing']"
f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa,Cloud-Edge Orchestration for the Internet of Things: Architecture and AI-Powered Data Processing,https://www.semanticscholar.org/paper/f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa,JournalArticle,"The Internet of Things (IoT) has been deeply penetrated into a wide range of important and critical sectors, including smart city, water, transportation, manufacturing, and smart factory. Massive data are being acquired from a fast growing number of IoT devices. Efficient data processing is a necessity to meet diversified and stringent requirements of many emerging IoT applications. Due to the constrained computation and storage resources, IoT devices have resorted to the powerful cloud computing to process their data. However, centralized and remote cloud computing may introduce unacceptable communication delay since its physical location is far away from IoT devices. Edge cloud has been introduced to overcome this issue by moving the cloud in closer proximity to IoT devices. The orchestration and cooperation between the cloud and the edge provides a crucial computing architecture for IoT applications. Artificial intelligence (AI) is a powerful tool to enable the intelligent orchestration in this architecture. This article first introduces such a kind of computing architecture from the perspective of IoT applications. It then investigates the state-of-the-art proposals on AI-powered cloud-edge orchestration for the IoT. Finally, a list of potential research challenges and open issues is provided and discussed, which can provide useful resources for carrying out future research in this area.",2020,89,8,[2947357.0],2947357.0,Warsaw,IEEE Internet of Things Journal,"['iot', 'cloud', 'data', 'data processing', 'Data Processing']"
febe776e285dc5e72c7e3ee697a87a794e1c00ff,Privacy-Preserving Data Processing with Flexible Access Control,https://www.semanticscholar.org/paper/febe776e285dc5e72c7e3ee697a87a794e1c00ff,JournalArticle,"Cloud computing provides an efficient and convenient platform for cloud users to store, process and control their data. Cloud overcomes the bottlenecks of resource-constrained user devices and greatly releases their storage and computing burdens. However, due to the lack of full trust in cloud service providers, the cloud users generally prefer to outsource their sensitive data in an encrypted form, which, however, seriously complicates data processing, analysis, as well as access control. Homomorphic encryption (HE) as a single key system cannot flexibly control data sharing and access after encrypted data processing. How to realize various computations over encrypted data in an efficient way and at the same time flexibly control the access to data processing results has been an important challenging issue. In this paper, we propose a privacy-preserving data processing scheme with flexible access control. With the cooperation of a data service provider (DSP) and a computation party (CP), our scheme, based on Paillier's partial homomorphic encryption (PHE), realizes seven basic operations, i.e., Addition, Subtraction, Multiplication, Sign Acquisition, Absolute, Comparison, and Equality Test, over outsourced encrypted data. In addition, our scheme, based on the homomorphism of attribute-based encryption (ABE), is also designed to support flexible access control over processing results of encrypted data. We further prove the security of our scheme and demonstrate its efficiency and advantages through simulations and comparisons with existing work.",2020,65,17,"[2234493.0, 145843577.0, 1713669.0]",2234493.0,Tallinn,IEEE Transactions on Dependable and Secure Computing,"['data', 'cloud', 'processing', 'data processing', 'Data Processing']"
22c141b489e6e189f5996537b0a908fc10f90de7,Adaptive and Fault-Tolerant Data Processing in Healthcare IoT Based on Fog Computing,https://www.semanticscholar.org/paper/22c141b489e6e189f5996537b0a908fc10f90de7,JournalArticle,"In recent years, healthcare IoT have been helpful in mitigating pressures of hospital and medical resources caused by aging population to a large extent. As a safety-critical system, the rapid response from the health care system is extremely important. To fulfill the low latency requirement, fog computing is a competitive solution by deploying healthcare IoT devices on the edge of clouds. However, these fog devices generate huge amount of sensor data. Designing a specific framework for fog devices to ensure reliable data transmission and rapid data processing becomes a topic of utmost significance. In this paper, a Reduced Variable Neighborhood Search (RVNS)-based sEnsor Data Processing Framework (REDPF) is proposed to enhance reliability of data transmission and processing speed. Functionalities of REDPF include fault-tolerant data transmission, self-adaptive filtering and data-load-reduction processing. Specifically, a reliable transmission mechanism, managed by a self-adaptive filter, will recollect lost or inaccurate data automatically. Then, a new scheme is designed to evaluate the health status of the elderly people. Through extensive simulations, we show that our proposed scheme improves network reliability, and provides a faster processing speed.",2020,68,7,"[28014924.0, 2112573464.0, 144206960.0, 39006765.0, 144123438.0]",28014924.0,Dublin,IEEE Transactions on Network Science and Engineering,"['data', 'transmission', 'fog', 'data processing', 'Data Processing']"
d6bc29a897fd85e7187dc33c3c974b8879462237,BloCkEd: Blockchain-Based Secure Data Processing Framework in Edge Envisioned V2X Environment,https://www.semanticscholar.org/paper/d6bc29a897fd85e7187dc33c3c974b8879462237,JournalArticle,"There has been an increasing trend of moving computing activities closer to the edge of the network, particularly in smart city applications (e.g., vehicle-to-everything – V2X). Such a paradigm allows the end user’s requests to be handled/processed by nodes at the edge of the network; thus, reducing latency, and preserving privacy of user data/activities. However, there are a number of challenges in such an edge computing ecosystem. Examples include (1) potential inappropriate utilization of resources at the edge nodes, (2) operational challenges in cache management and data integrity due to data migration between edge nodes, particularly when dealing with vehicular mobility in a V2X application, and (3) high energy consumption due to continuous link breakage and subsequent reestablishment of link(s). Therefore in this paper, we design a blockchain-based secure data processing framework for an edge envisioned V2X environment (hereafter referred to as BloCkEd). Specifically, a multi-layered edge-enabled V2X system model for BloCkEd is presented, which includes the formulation of a multi-objective optimization problem. In addition, BloCkEd comprises an optimal container-based data processing scheme, and a blockchain-based data integrity management scheme, designed to minimize link breakage and reducing latency. Using Chandigarh City, India, as the scenario, we implement and evaluate the proposed approach in terms of its latency, energy consumption, and service level agreement compliance.",2020,58,69,"[8880101.0, 48775545.0, 2110675251.0, 144996075.0, 116022805.0]",8880101.0,Dublin,IEEE Transactions on Vehicular Technology,"['edge', 'data', 'latency', 'data processing', 'Data Processing']"
8d942a3b52e2ad16ff8e5970be59591970d89fae,Big Data Processing Workflows Oriented Real-Time Scheduling Algorithm using Task-Duplication in Geo-Distributed Clouds,https://www.semanticscholar.org/paper/8d942a3b52e2ad16ff8e5970be59591970d89fae,JournalArticle,"Scheduling big data processing workflows involves both large-scale tasks and transmission of massive intermediate data among tasks, thus optimizing their completion time and monetary cost becomes a challenging issue. Besides, data streams are continuously generated, and dynamically submitted to clouds for real-time or near real-time processing. Naturally, responsive schedules are required to keep pace with such dynamic environments and this further aggravates the difficulty of the workflow scheduling problem. To address these issues, we first derive two theorems to minimize the completion time of a set of parallel workflow tasks and the start time of each workflow task, and then define the latest finish time for workflow tasks, which is also proved its advantage in reducing costs without delaying the completion of workflows. On the basis of these theorems, we propose a novel real-time scheduling algorithm using task-duplication, RTSATD, such that minimizing both the completion time and monetary cost of processing big data workflows in clouds. The performance of RTSATD is analyzed by using both synthesized and real-world workflows. The experimental results demonstrate the superiority of the proposed algorithm with respect to completion time (up to 28.73 percent) and resource utilization (up to 46.31 percent) over two existing approaches.",2020,43,6,"[2382347.0, 2090538077.0, 1731634.0, 46532001.0]",2382347.0,Ljubljana,IEEE Transactions on Big Data,"['time', 'completion', 'data', 'data processing', 'Data Processing']"
4e746359afd6f81705b875d71cc499b904a320df,ScienceEarth: A Big Data Platform for Remote Sensing Data Processing,https://www.semanticscholar.org/paper/4e746359afd6f81705b875d71cc499b904a320df,JournalArticle,"Mass remote sensing data management and processing is currently one of the most important topics. In this study, we introduce ScienceEarth, a cluster-based data processing framework. The aim of ScienceEarth is to store, manage, and process large-scale remote sensing data in a cloud-based cluster-computing environment. The platform consists of the following three main parts: ScienceGeoData, ScienceGeoIndex, and ScienceGeoSpark. ScienceGeoData stores and manages remote sensing data. ScienceGeoIndex is an index and query system, a spatial index based on quad-tree and Hilbert curve which is combined for heterogeneous tiled remote sensing data that makes efficient data retrieval in ScienceGeoData. ScienceGeoSpark is an easy-to-use computing framework in which we use Apache Spark as the analytics engine for big remote sensing data processing. The result of tests proves that ScienceEarth can efficiently store, retrieve, and process remote sensing data. The results reveal ScienceEarth has the potential and capabilities of efficient big remote sensing data processing.",2020,34,12,"[2153077994.0, 121682029.0, 49356798.0, 1697984.0]",2153077994.0,Madrid,Remote Sensing,"['data', 'processing', 'scienceearth', 'data processing', 'Data Processing']"
5a32ebacd5c32d52734f9d2a2cfb5d0cdbe469e2,Toward Big Data Processing in IoT: Path Planning and Resource Management of UAV Base Stations in Mobile-Edge Computing System,https://www.semanticscholar.org/paper/5a32ebacd5c32d52734f9d2a2cfb5d0cdbe469e2,JournalArticle,"Heavy data load and wide cover range have always been crucial problems for big data processing in Internet of Things (IoT). Recently, mobile-edge computing (MEC) and unmanned aerial vehicle base stations (UAV-BSs) have emerged as promising techniques in IoT. In this article, we propose a three-layer online data processing network based on the MEC technique. On the bottom layer, raw data are generated by distributed sensors with local information. Upon them, UAV-BSs are deployed as moving MEC servers, which collect data and conduct initial steps of data processing. On top of them, a center cloud receives processed results and conducts further evaluation. For online processing requirements, the edge nodes should stabilize delay to ensure data freshness. Furthermore, limited onboard energy poses constraints to edge processing capability. In this article, we propose an online edge processing scheduling algorithm based on Lyapunov optimization. In cases of low data rate, it tends to reduce edge processor frequency for saving energy. In the presence of a high data rate, it will smartly allocate bandwidth for edge data offloading. Meanwhile, hovering UAV-BSs bring a large and flexible service coverage, which results in a path planning issue. In this article, we also consider this problem and apply deep reinforcement learning to develop an online path planning algorithm. Taking observations of around environment as an input, a CNN network is trained to predict action rewards. By simulations, we validate its effectiveness in enhancing service coverage. The result will contribute to big data processing in future IoT.",2019,70,7,"[49725081.0, 2605688.0, 7469995.0, 145142172.0]",49725081.0,Paris,IEEE Internet of Things Journal,"['data', 'processing', 'edge', 'data processing', 'Data Processing']"
02a1e8e77f501675945890df45fbdc11726cb0ba,An Open-Source Monitor-Independent Movement Summary for Accelerometer Data Processing.,https://www.semanticscholar.org/paper/02a1e8e77f501675945890df45fbdc11726cb0ba,JournalArticle,"Background
Physical behavior researchers using motion sensors often use acceleration summaries to visualize, clean, and interpret data. Such output is dependent on device specifications (e.g., dynamic range, sampling rate) and/or are proprietary, which invalidate cross-study comparison of findings when using different devices. This limits flexibility in selecting devices to measure physical activity, sedentary behavior, and sleep.


Purpose
Develop an open-source, universal acceleration summary metric that accounts for discrepancies in raw data among research and consumer devices.


Methods
We used signal processing techniques to generate a Monitor-Independent Movement Summary unit (MIMS-unit) optimized to capture normal human motion. Methodological steps included raw signal harmonization to eliminate inter-device variability (e.g., dynamic g-range, sampling rate), bandpass filtering (0.2-5.0 Hz) to eliminate non-human movement, and signal aggregation to reduce data to simplify visualization and summarization. We examined the consistency of MIMS-units using orbital shaker testing on eight accelerometers with varying dynamic range (±2 to ±8 g) and sampling rates (20-100 Hz), and human data (N = 60) from an ActiGraph GT9X.


Results
During shaker testing, MIMS-units yielded lower between-device coefficient of variations than proprietary ActiGraph and ENMO acceleration summaries. Unlike the widely used ActiGraph activity counts, MIMS-units were sensitive in detecting subtle wrist movements during sedentary behaviors.


Conclusions
Open-source MIMS-units may provide a means to summarize high-resolution raw data in a device-independent manner, thereby increasing standardization of data cleaning and analytical procedures to estimate selected attributes of physical behavior across studies.",2019,81,2 4,"[144403874.0, 2869719.0, 3072621.0, 1705903.0]",144403874.0,Vaduz,Journal for the Measurement of Physical Behaviour,"['data', 'mimsunits', 'behavior', 'data processing', 'Data Processing']"
15370f51d666ab8ef17185679553c6a8647b2a15,Interunit Reliability and Effect of Data-Processing Methods of Global Positioning Systems.,https://www.semanticscholar.org/paper/15370f51d666ab8ef17185679553c6a8647b2a15,JournalArticle,"PURPOSE
To establish the interunit reliability of a range of global positioning system (GPS)-derived movement indicators, to determine the variation between manufacturers, and to investigate the difference between software-derived and raw data.


METHODS
A range of movement variables were obtained from 27 GPS units from 3 manufacturers (GPSports EVO, 10 Hz, n = 10; STATSports Apex, 10 Hz, n = 10; and Catapult S5, 10 Hz, n = 7) that measured the same team-sport simulation session while positioned on a sled. The interunit reliability was determined using the coefficient of variation (%) and 90% confidence limits, whereas between-manufacturers comparisons and comparisons of software versus raw processed data were established using standardized effect sizes and 90% confidence limits.


RESULTS
The interunit reliability for both software and raw processed data ranged from good to poor (coefficient of variation = 0.2%; ±1.5% to 78.2%; ±1.5%), with distance, speed, and maximal speed exhibiting the best reliability. There were substantial differences between manufacturers, particularly for threshold-based acceleration and deceleration variables (effect sizes; ±90% confidence limits: -2.0; ±0.1 to 1.9; ±0.1), and there were substantial differences between data-processing methods for a range of movement indicators.


CONCLUSIONS
The interunit reliability of most movement indicators was deemed as good regardless of processing method, suggesting that practitioners can have confidence within systems. Standardized data-processing methods are recommended, due to the large differences between data outputs from various manufacturer-derived software.",2019,75,14 4,"[40068409.0, 29886701.0, 8579299.0, 5521724.0, 145356043.0]",40068409.0,Warsaw,International Journal of Sports Physiology and Performance,"['reliability', 'interunit', 'movement', 'data processing', 'Data Processing']"
665b0c776ff7507c32793f10ce9edf90bc2f674a,Specifics of the data processing of precession electron diffraction tomography data and their implementation in the program PETS2.0.,https://www.semanticscholar.org/paper/665b0c776ff7507c32793f10ce9edf90bc2f674a,JournalArticle,"Electron diffraction tomography (EDT) data are in many ways similar to X-ray diffraction data. However, they also present certain specifics. One of the most noteworthy is the specific rocking curve observed for EDT data collected using the precession electron diffraction method. This double-peaked curve (dubbed `the camel') may be described with an approximation based on a circular integral of a pseudo-Voigt function and used for intensity extraction by profile fitting. Another specific aspect of electron diffraction data is the high likelihood of errors in the estimation of the crystal orientation, which may arise from the inaccuracies of the goniometer reading, crystal deformations or crystal movement during the data collection. A method for the refinement of crystal orientation for each frame individually is proposed based on the least-squares optimization of simulated diffraction patterns. This method provides typical angular accuracy of the frame orientations of less than 0.05°. These features were implemented in the computer program PETS 2.0. The implementation of the complete data processing workflow in the program PETS and the incorporation of the features specific for electron diffraction data is also described.",2019,80,75 Pt 4,"[6667699.0, 34279810.0, 32574880.0, 13587113.0, 12098632.0, 8303781.0]",6667699.0,Tirana,"Acta Crystallographica. Section B: Structural Science, Crystal Engineering and Materials","['data', 'diffraction', 'electron', 'data processing', 'Data Processing']"
bb5d26da72bfe7030dbc6650b686b210ae661f2c,A New Data Processing Architecture for Multi-Scenario Applications in Aviation Manufacturing,https://www.semanticscholar.org/paper/bb5d26da72bfe7030dbc6650b686b210ae661f2c,JournalArticle,"The development of industry 4.0 has spurred the transformation of traditional manufacturing into modern industrial Internet-of-Things. The most notable feature during this transition is the improvement of digitization and intelligence based on the massive data drives. In such a data-driven environment, the processing, storage, and utilization of the industry data get more and more important. Usually, the traditional data processing architecture runs as a one-way streamline, which cannot adapt to the different requirements of the multi-scenario application. This paper proposed a new industrial big data processing architecture called Phi architecture, which can realize many functions such as batch data processing and stream data processing, distributed storage and access, and real-time control. Compared with other data processing architecture, the Phi architecture combined with edge computing and feedback control has the ability to deal with the different demands in aviation manufacturing. Next, the new architecture is designed for microservices pattern, which improves the flexibility and stability of the architecture, and makes it independent operated in multi-scenarios, such as state monitoring of workshop, adaptive data acquisition, feedback control, and user-oriented information classification. As a proof of concept, the architecture has been tested in a simulation digital manufacturing workshop. The results verify the improved effectiveness of the Phi architecture on the data feedback control and real-time processing. And, the development of microservices architecture greatly improves the efficiency, adaptability, and extensibility of the manufacturing process.",2019,23,7,"[2158624007.0, 2147261221.0, 2119407396.0, 103474705.0]",2158624007.0,Luxembourg,IEEE Access,"['architecture', 'data', 'processing', 'data processing', 'Data Processing']"
c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b,"Performance Analysis of IoT-Based Sensor, Big Data Processing, and Machine Learning Model for Real-Time Monitoring System in Automotive Manufacturing",https://www.semanticscholar.org/paper/c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b,JournalArticle,"With the increase in the amount of data captured during the manufacturing process, monitoring systems are becoming important factors in decision making for management. Current technologies such as Internet of Things (IoT)-based sensors can be considered a solution to provide efficient monitoring of the manufacturing process. In this study, a real-time monitoring system that utilizes IoT-based sensors, big data processing, and a hybrid prediction model is proposed. Firstly, an IoT-based sensor that collects temperature, humidity, accelerometer, and gyroscope data was developed. The characteristics of IoT-generated sensor data from the manufacturing process are: real-time, large amounts, and unstructured type. The proposed big data processing platform utilizes Apache Kafka as a message queue, Apache Storm as a real-time processing engine and MongoDB to store the sensor data from the manufacturing process. Secondly, for the proposed hybrid prediction model, Density-Based Spatial Clustering of Applications with Noise (DBSCAN)-based outlier detection and Random Forest classification were used to remove outlier sensor data and provide fault detection during the manufacturing process, respectively. The proposed model was evaluated and tested at an automotive manufacturing assembly line in Korea. The results showed that IoT-based sensors and the proposed big data processing system are sufficiently efficient to monitor the manufacturing process. Furthermore, the proposed hybrid prediction model has better fault prediction accuracy than other models given the sensor data as input. The proposed system is expected to support management by improving decision-making and will help prevent unexpected losses caused by faults during the manufacturing process.",2018,224,18,"[2913270.0, 3212733.0, 31380410.0, 3353468.0]",2913270.0,Stockholm,Italian National Conference on Sensors,"['data', 'manufacturing', 'process', 'data processing', 'Data Processing']"
c2a448bb511ebae41a87e69891da8bbf17ddba3d,KAMO: towards automated data processing for microcrystals,https://www.semanticscholar.org/paper/c2a448bb511ebae41a87e69891da8bbf17ddba3d,JournalArticle,An automated data-processing pipeline for protein microcrystals is presented. The processing of multiple small-wedge data sets was made dramatically easier by this pipeline.,2018,150,74,"[13736628.0, 47781577.0, 2243646160.0]",13736628.0,Budapest,Acta Crystallographica Section D: Structural Biology,"['pipeline', 'dataprocessing', 'protein', 'data processing', 'Data Processing']"
1051abf1e3dae90241ad15b3f98f2e41197ee611,A Personalized Healthcare Monitoring System for Diabetic Patients by Utilizing BLE-Based Sensors and Real-Time Data Processing,https://www.semanticscholar.org/paper/1051abf1e3dae90241ad15b3f98f2e41197ee611,JournalArticle,"Current technology provides an efficient way of monitoring the personal health of individuals. Bluetooth Low Energy (BLE)-based sensors can be considered as a solution for monitoring personal vital signs data. In this study, we propose a personalized healthcare monitoring system by utilizing a BLE-based sensor device, real-time data processing, and machine learning-based algorithms to help diabetic patients to better self-manage their chronic condition. BLEs were used to gather users’ vital signs data such as blood pressure, heart rate, weight, and blood glucose (BG) from sensor nodes to smartphones, while real-time data processing was utilized to manage the large amount of continuously generated sensor data. The proposed real-time data processing utilized Apache Kafka as a streaming platform and MongoDB to store the sensor data from the patient. The results show that commercial versions of the BLE-based sensors and the proposed real-time data processing are sufficiently efficient to monitor the vital signs data of diabetic patients. Furthermore, machine learning–based classification methods were tested on a diabetes dataset and showed that a Multilayer Perceptron can provide early prediction of diabetes given the user’s sensor data as input. The results also reveal that Long Short-Term Memory can accurately predict the future BG level based on the current sensor data. In addition, the proposed diabetes classification and BG prediction could be combined with personalized diet and physical activity suggestions in order to improve the health quality of patients and to avoid critical conditions in the future.",2018,149,18,"[3212733.0, 2913270.0, 48761825.0, 2060154.0, 31380410.0, 3353468.0]",3212733.0,Minsk,Italian National Conference on Sensors,"['data', 'sensor', 'signs', 'data processing', 'Data Processing']"
1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7,When Computation Hugs Intelligence: Content-Aware Data Processing for Industrial IoT,https://www.semanticscholar.org/paper/1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7,JournalArticle,"Data service has been considered as one the most prominent characteristics for Industrial Internet of Things (IIoT). This paper studies how to design an optimal computing manner for a general IIoT system. On the theory end, we analyze the relationship between the data processing and the energy consumption through investigating the content correlation of the captured data. Importantly, we derive an exact expression for the performance of IIoT by combining computation with intelligence. On the application end, we design an efficient way to obtain a threshold by approximating the performances of different computing manners, and show how to apply it to practical IIoT applications. We believe that the proposed computation rules hold great significance for the IIoT designer, that is, it is better to use distributed computing manner when the content correlation is high, otherwise, centralized computing manner is better.",2018,112,5,"[2865824.0, 40259011.0, 2115526972.0, 34665139.0]",2865824.0,Lisbon,IEEE Internet of Things Journal,"['iiot', 'data', 'manner', 'data processing', 'Data Processing']"
14fe35149aed6a47b6ebfd207deb7681b9446bb6,Urban Planning and Smart City Decision Management Empowered by Real-Time Data Processing Using Big Data Analytics,https://www.semanticscholar.org/paper/14fe35149aed6a47b6ebfd207deb7681b9446bb6,JournalArticle,"The Internet of Things (IoT), inspired by the tremendous growth of connected heterogeneous devices, has pioneered the notion of smart city. Various components, i.e., smart transportation, smart community, smart healthcare, smart grid, etc. which are integrated within smart city architecture aims to enrich the quality of life (QoL) of urban citizens. However, real-time processing requirements and exponential data growth withhold smart city realization. Therefore, herein we propose a Big Data analytics (BDA)-embedded experimental architecture for smart cities. Two major aspects are served by the BDA-embedded smart city. Firstly, it facilitates exploitation of urban Big Data (UBD) in planning, designing, and maintaining smart cities. Secondly, it occupies BDA to manage and process voluminous UBD to enhance the quality of urban services. Three tiers of the proposed architecture are liable for data aggregation, real-time data management, and service provisioning. Moreover, offline and online data processing tasks are further expedited by integrating data normalizing and data filtering techniques to the proposed work. By analyzing authenticated datasets, we obtained the threshold values required for urban planning and city operation management. Performance metrics in terms of online and offline data processing for the proposed dual-node Hadoop cluster is obtained using aforementioned authentic datasets. Throughput and processing time analysis performed with regard to existing works guarantee the performance superiority of the proposed work. Hence, we can claim the applicability and reliability of implementing proposed BDA-embedded smart city architecture in the real world.",2018,83,18,"[7992672.0, 2109215851.0, 2703536.0, 2111508316.0, 51455282.0, 1845785166.0, 3310138.0, 152742012.0]",7992672.0,Reykjavik,Italian National Conference on Sensors,"['data', 'city', 'architecture', 'data processing', 'Data Processing']"
780c725848aac1118d00c8bb306719ec803369cd,Model-free Control for Distributed Stream Data Processing using Deep Reinforcement Learning,https://www.semanticscholar.org/paper/780c725848aac1118d00c8bb306719ec803369cd,JournalArticle,"In this paper, we focus on general-purpose Distributed Stream Data Processing Systems (DSDPSs), which deal with processing of unbounded streams of continuous data at scale distributedly in real or near-real time. A fundamental problem in a DSDPS is the scheduling problem with the objective of minimizing average end-to-end tuple processing time. A widely-used solution is to distribute workload evenly over machines in the cluster in a round-robin manner, which is obviously not efficient due to lack of consideration for communication delay. Model-based approaches do not work well either due to the high complexity of the system environment. We aim to develop a novel model-free approach that can learn to well control a DSDPS from its experience rather than accurate and mathematically solvable system models, just as a human learns a skill (such as cooking, driving, swimming, etc). Specifically, we, for the first time, propose to leverage emerging Deep Reinforcement Learning (DRL) for enabling model-free control in DSDPSs; and present design, implementation and evaluation of a novel and highly effective DRL-based control framework, which minimizes average end-to-end tuple processing time by jointly learning the system environment via collecting very limited runtime statistics data and making decisions under the guidance of powerful Deep Neural Networks. To validate and evaluate the proposed framework, we implemented it based on a widely-used DSDPS, Apache Storm, and tested it with three representative applications. Extensive experimental results show 1) Compared to Storm's default scheduler and the state-of-the-art model-based method, the proposed framework reduces average tuple processing by 33.5% and 14.0% respectively on average. 2) The proposed framework can quickly reach a good scheduling solution during online learning, which justifies its practicability for online control in DSDPSs.",2018,75,11,"[2115646383.0, 48559420.0, 2115854503.0, 46393431.0]",2115646383.0,Tallinn,Proceedings of the VLDB Endowment,"['time', 'control', 'framework', 'data processing', 'Data Processing']"
5371896313ac227eb819038dd55f213cb42b99e2,RHEEM: Enabling Cross-Platform Data Processing - May The Big Data Be With You! -,https://www.semanticscholar.org/paper/5371896313ac227eb819038dd55f213cb42b99e2,JournalArticle,"
 Solving business problems increasingly requires going beyond the limits of a single data processing platform (platform for short), such as Hadoop or a DBMS. As a result, organizations typically perform tedious and costly tasks to juggle their code and data across different platforms. Addressing this pain and achieving automatic cross-platform data processing is quite challenging: finding the most efficient platform for a given task requires quite good expertise for all the available platforms. We present R
 heem
 , a general-purpose cross-platform data processing system that decouples applications from the underlying platforms. It not only determines the best platform to run an incoming task, but also splits the task into subtasks and assigns each subtask to a specific platform to minimize the overall cost (e.g., runtime or monetary cost). It features (i) an interface to easily compose data analytic tasks; (ii) a novel cost-based optimizer able to find the most efficient platform in almost all cases; and (iii) an executor to efficiently orchestrate tasks over different platforms. As a result, it allows users to focus on the business logic of their applications rather than on the mechanics of how to compose and execute them. Using different real-world applications with R
 heem
 , we demonstrate how cross-platform data processing can accelerate performance by more than one order of magnitude compared to single-platform data processing.
",2018,62,11,"[143970078.0, 50793091.0, 1404359012.0, 145188857.0, 3416143.0, 2827559.0, 143924672.0, 2081346.0, 1801187.0, 2168047.0, 1802817.0, 1399355221.0, 8669763.0, 2934941.0, 1410159156.0]",143970078.0,Budapest,Proceedings of the VLDB Endowment,"['data', 'platform', 'processing', 'data processing', 'Data Processing']"
b4c9c134ad5bd4a037115df65411b4c49abe1322,Framework for Mobile Internet of Things Security Monitoring Based on Big Data Processing and Machine Learning,https://www.semanticscholar.org/paper/b4c9c134ad5bd4a037115df65411b4c49abe1322,JournalArticle,"The paper discusses a new framework combining the possibilities of Big Data processing and machine leaning developed for security monitoring of mobile Internet of Things. The mathematical foundations and the problem statement are considered. The description of the used data set and the architecture of proposed security monitoring framework are provided. The framework specifies several machine learning mechanisms intended for solving classification tasks. The classifier operation results are exposed to plurality voting, weighted voting, and soft voting. The framework performance and accuracy is assessed experimentally.",2018,53,6,"[16777650.0, 145683384.0, 2636647.0]",16777650.0,Oslo,IEEE Access,"['framework', 'data', 'machine', 'data processing', 'Data Processing']"
6eb8caebbffc7e5b301b66dc36acad46b4dca5c9,3-D Data Processing to Extract Vehicle Trajectories from Roadside LiDAR Data,https://www.semanticscholar.org/paper/6eb8caebbffc7e5b301b66dc36acad46b4dca5c9,JournalArticle,"High-resolution vehicle data including location, speed, and direction is significant for new transportation systems, such as connected-vehicle applications, micro-level traffic performance evaluation, and adaptive traffic control. This research developed a data processing procedure for detection and tracking of multi-lane multi-vehicle trajectories with a roadside light detection and ranging (LiDAR) sensor. Different from existing methods for vehicle onboard sensing systems, this procedure was developed specifically to extract high-resolution vehicle trajectories from roadside LiDAR sensors. This procedure includes preprocessing of the raw data, statistical outlier removal, a Least Median of Squares based ground estimation method to accurately remove the ground points, vehicle data clouds clustering, a principle component-based oriented bounding box method to estimate the location of the vehicle, and a geometrically-based tracking algorithm. The developed procedure has been applied to a two-way-stop-sign intersection and an arterial road in Reno, Nevada. The data extraction procedure has been validated by comparing tracking results and speeds logged from a testing vehicle through the on-board diagnostics interface. This data processing procedure could be applied to extract high-resolution trajectories of connected and unconnected vehicles for connected-vehicle applications, and the data will be valuable to practices in traffic safety, traffic mobility, and fuel efficiency estimation.",2018,52,2672,"[2108706355.0, 39911449.0, 2109184366.0, 48510386.0, 2071260355.0]",2108706355.0,Copenhagen,Transportation Research Record,"['data', 'vehicle', 'procedure', 'data processing', 'Data Processing']"
2dafea864f74a477414c3b71b742f7997e216102,Energy-Aware Mobile Edge Computing and Routing for Low-Latency Visual Data Processing,https://www.semanticscholar.org/paper/2dafea864f74a477414c3b71b742f7997e216102,JournalArticle,"New paradigms such as Mobile Edge Computing (MEC) are becoming feasible for use in, e.g., real-time decision-making during disaster incident response to handle the data deluge occurring in the network edge. However, MEC deployments today lack flexible IoT device data handling such as handling user preferences for real-time versus energy-efficient processing. Moreover, MEC can also benefit from a policy-based edge routing to handle sustained performance levels with efficient energy consumption. In this paper, we study the potential of MEC to address application issues related to energy management on constrained IoT devices with limited power sources, while also providing low-latency processing of visual data being generated at high resolutions. Using a facial recognition application that is important in disaster incident response scenarios, we propose a novel “offload decision-making” algorithm that analyzes the tradeoffs in computing policies to offload visual data processing (i.e., to an edge cloud or a core cloud) at low-to-high workloads. This algorithm also analyzes the impact on energy consumption in the decision-making under different visual data consumption requirements (i.e., users with thick clients or thin clients). To address the processing-throughput versus energy-efficiency tradeoffs, we propose a “Sustainable Policy-based Intelligence-Driven Edge Routing” algorithm that uses machine learning within Mobile Ad hoc Networks. This algorithm is energy aware and improves the geographic routing baseline performance (i.e., minimizes impact of local minima) for throughput performance sustainability, while also enabling flexible policy specification. We evaluate our proposed algorithms by conducting experiments on a realistic edge and core cloud testbed in the GENI Cloud infrastructure, and recreate disaster scenes of tornado damages within simulations. Our empirical results show how MEC can provide flexibility to users who desire energy conservation over low latency or vice versa in the visual data processing with a facial recognition application. In addition, our simulation results show that our routing approach outperforms existing solutions under diverse user preferences, node mobility, and severe node failure conditions.",2018,54,20,"[2064711347.0, 3174714.0, 2729273.0, 134771108.0, 2066206022.0, 2112228538.0, 1921638.0]",2064711347.0,Bucharest,IEEE transactions on multimedia,"['edge', 'data', 'energy', 'data processing', 'Data Processing']"
86dbd884043eb5807c61d2c65b813e673b4a04fa,Blockchain-Based Secure Data Storage Protocol for Sensors in the Industrial Internet of Things,https://www.semanticscholar.org/paper/86dbd884043eb5807c61d2c65b813e673b4a04fa,JournalArticle,"The Industrial Internet of Things (IIoT) that introduces Internet of Things (IoT) technology into industrial environments is beneficial to construct smart factories. It utilizes various sensors to collect the data of industrial devices. These data are analyzed to improve the manufacturing efficiency and product quality. Cloud storage provides a solution for storing data outsourced, especially for sensors that have limited local storage and computational capacity. To ensure the privacy preserving of devices, the collected data should be stored in the formal ciphertext. Therefore, encrypted data sharing should be implemented to analyze the devices’ data. In this article, the cloud storage solution for sensors is considered. To achieve a secure and efficient data storage and sharing, a novel group signature scheme, which has less computation overhead and communication overhead, is designed to realize anonymous authentication first. And then, a novel blockchain-based cloud storage protocol for sensors in IIoT is constructed on basis of the proposed group signature scheme. Smart contract and proxy re-encryption are utilized in this protocol to realize secure data sharing with a less computational overhead. Furthermore, security proofs and performance evaluations demonstrate that this protocol is secure, privacy-preserving, and has at least 40% and 20% performance improvement in data storage and sharing phase, respectively.",2022,34,18,"[2049071558.0, 143808691.0, 46342348.0, 2082427140.0]",2049071558.0,Brussels,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'sensors', 'data storage', 'Data Storage']"
24ab4e99e582c9770281eee0a39cbeb70ddd891a,Information-Theoretic Foundations of DNA Data Storage,https://www.semanticscholar.org/paper/24ab4e99e582c9770281eee0a39cbeb70ddd891a,JournalArticle,"Due to its longevity and enormous information density, DNA is an attractive medium for archival data storage. Natural DNA more than 700.000 years old has been recovered, and about 5 grams of DNA can in principle hold a Zetabyte of digital information, orders of magnitude more than what is achieved on conventional storage media. Thanks to rapid technological advances, DNA storage is becoming practically feasible, as demonstrated by a number of experimental storage systems, making it a promising solution for our society’s increasing need of data storage. While in living things, DNA molecules can consist of millions of nucleotides, due to technological constraints, in practice, data is stored on many short DNA molecules, which are preserved in a DNA pool and cannot be spatially ordered. Moreover, imperfections in sequencing, synthesis, and handling, as well as DNA decay during storage, introduce random noise into the system, making the task of reliably storing and retrieving information in DNA challenging. This unique setup raises a natural information-theoretic question: how much information can be reliably stored on and reconstructed from millions of short noisy sequences? The goal of this monograph is to address this question by discussing the fundamental limits of storing information on DNA. Motivated by current technological constraints on DNA synthesis and sequencing, we propose a probabilistic channel model that captures three key distinctive aspects of the DNA storage systems: (1) the data is written onto many short DNA molecules that are stored in an unordered fashion; (2) the molecules are corrupted by noise and (3) the data is read by randomly sampling from the DNA pool. Our goal is to investigate the impact of each of these key aspects on the capacity of the DNA storage system. Rather than focusing on coding-theoretic considerations and computationally efficient encoding and decoding, we aim to build an information-theoretic foundation for the analysis of these channels, developing tools for achievability and converse arguments. This is a preprint of the following publication: Ilan Shomorony and Reinhard Heckel (2022),“Information-Theoretic Foundations of DNA Data Storage”, Foundations and Trends in Communications and Information Theory: Vol. 19, No. 1, pp 1-106. DOI: 10.1561/0100000117. ar X iv :2 21 1. 05 55 2v 1 [ cs .I T ] 1 0 N ov 2 02 2",2022,22,abs/2211.05552,"[1800004.0, 145639495.0]",1800004.0,Prague,Foundations and Trends in Communications and Information Theory,"['dna', 'storage', 'information', 'data storage', 'Data Storage']"
b904dcdbd7c7b33938583f2f57d05ca70e121ea9,An Efficient and Secure Big Data Storage in Cloud Environment by Using Triple Data Encryption Standard,https://www.semanticscholar.org/paper/b904dcdbd7c7b33938583f2f57d05ca70e121ea9,JournalArticle,"In recent decades, big data analysis has become the most important research topic. Hence, big data security offers Cloud application security and monitoring to host highly sensitive data to support Cloud platforms. However, the privacy and security of big data has become an emerging issue that restricts the organization to utilize Cloud services. The existing privacy preserving approaches showed several drawbacks such as a lack of data privacy and accurate data analysis, a lack of efficiency of performance, and completely rely on third party. In order to overcome such an issue, the Triple Data Encryption Standard (TDES) methodology is proposed to provide security for big data in the Cloud environment. The proposed TDES methodology provides a relatively simpler technique by increasing the sizes of keys in Data Encryption Standard (DES) to protect against attacks and defend the privacy of data. The experimental results showed that the proposed TDES method is effective in providing security and privacy to big healthcare data in the Cloud environment. The proposed TDES methodology showed less encryption and decryption time compared to the existing Intelligent Framework for Healthcare Data Security (IFHDS) method.",2022,24,6,"[2186480691.0, 2200758545.0, 2150706913.0, 65965701.0, 2200528122.0, 2143272549.0]",2186480691.0,Ljubljana,Big Data and Cognitive Computing,"['data', 'security', 'privacy', 'data storage', 'Data Storage']"
287a7da1801a07cf7fd85ffcc23c79504876ecc0,An artificial chromosome for data storage,https://www.semanticscholar.org/paper/287a7da1801a07cf7fd85ffcc23c79504876ecc0,JournalArticle,"Abstract DNA digital storage provides an alternative for information storage with high density and long-term stability. Here, we report the de novo design and synthesis of an artificial chromosome that encodes two pictures and a video clip. The encoding paradigm utilizing the superposition of sparsified error correction codewords and pseudo-random sequences tolerates base insertions/deletions and is well suited to error-prone nanopore sequencing for data retrieval. The entire 254 kb sequence was 95.27% occupied by encoded data. The Transformation-Associated Recombination method was used in the construction of this chromosome from DNA fragments and necessary autonomous replication sequences. The stability was demonstrated by transmitting the data-carrying chromosome to the 100th generation. This study demonstrates a data storage method using encoded artificial chromosomes via in vivo assembly for write-once and stable replication for multiple retrievals, similar to a compact disc, with potential in economically massive data distribution.",2021,58,8,"[2154742781.0, 1635504630.0, 48129278.0, 2089054636.0, 2108355738.0, 2108098765.0, 2110018952.0, 2040448821.0, 2158141874.0]",2154742781.0,Belgrade,National Science Review,"['data', 'storage', 'chromosome', 'data storage', 'Data Storage']"
1e1cf81a1113482be3f0c280db994a832cb9426a,Secure Data Storage and Recovery in Industrial Blockchain Network Environments,https://www.semanticscholar.org/paper/1e1cf81a1113482be3f0c280db994a832cb9426a,JournalArticle,"The massive redundant data storage and communication in network 4.0 environments have issues of low integrity, high cost, and easy tampering. To address these issues, in this article, a secure data storage and recovery scheme in the blockchain-based network is proposed by improving the decentration, tampering-proof, real-time monitoring, and management of storage systems, as such design supports the dynamic storage, fast repair, and update of distributed data in the data storage system of industrial nodes. A local regenerative code technology is used to repair and store data between failed nodes while ensuring the privacy of user data. That is, as the data stored are found to be damaged, multiple local repair groups constructed by vector code can simultaneously yet efficiently repair multiple distributed data storage nodes. Based on the unique chain storage structure, such as data consensus mechanism and smart contract, the storage structure of blockchain distributed coding not only quickly repair the nearby local regenerative codes in the blockchain but also reduce the resource overhead in the data storage process of industrial nodes. Experimental results show that the proposed scheme improves the repair rate of multinode data by 9% and data storage rate increased by 8.6%, indicating to be promising with good security and real-time performance.",2020,165,16,"[97233415.0, 34548938.0, 69486668.0, 40452934.0, 144086037.0]",97233415.0,Valletta,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'repair', 'data storage', 'Data Storage']"
6544259ff6b335b1dcec75e031b6d57e5b9509f4,Scaling DNA data storage with nanoscale electrode wells,https://www.semanticscholar.org/paper/6544259ff6b335b1dcec75e031b6d57e5b9509f4,JournalArticle,A demonstration of DNA synthesis control at over 1000x higher density shows the path to large scale DNA data storage.,2021,31,7,"[38697713.0, 35479753.0, 2061294792.0, 2119125110.0, 2142147919.0, 2142142532.0, 1729804.0, 2056913832.0, 29865576.0, 2387195.0, 119935593.0, 2058303089.0, 52129431.0, 1717411.0, 145033446.0]",38697713.0,Kiev,Science Advances,"['dna', 'demonstration', 'synthesis', 'data storage', 'Data Storage']"
fc61c7221350806c25379f385c27b2102ff8eb57,Uncertainties in synthetic DNA-based data storage,https://www.semanticscholar.org/paper/fc61c7221350806c25379f385c27b2102ff8eb57,JournalArticle,"Abstract Deoxyribonucleic acid (DNA) has evolved to be a naturally selected, robust biomacromolecule for gene information storage, and biological evolution and various diseases can find their origin in uncertainties in DNA-related processes (e.g. replication and expression). Recently, synthetic DNA has emerged as a compelling molecular media for digital data storage, and it is superior to the conventional electronic memory devices in theoretical retention time, power consumption, storage density, and so forth. However, uncertainties in the in vitro DNA synthesis and sequencing, along with its conjugation chemistry and preservation conditions can lead to severe errors and data loss, which limit its practical application. To maintain data integrity, complicated error correction algorithms and substantial data redundancy are usually required, which can significantly limit the efficiency and scale-up of the technology. Herein, we summarize the general procedures of the state-of-the-art DNA-based digital data storage methods (e.g. write, read, and preservation), highlighting the uncertainties involved in each step as well as potential approaches to correct them. We also discuss challenges yet to overcome and research trends in the promising field of DNA-based data storage.",2021,25,49,"[1515428357.0, 151480423.0, 2084599260.0, 2118902636.0]",1515428357.0,Skopje,Nucleic Acids Research,"['data', 'storage', 'dna', 'data storage', 'Data Storage']"
82663577cf1d08235bb56ad648c9dad36343ccfb,Electrochemical DNA synthesis and sequencing on a single electrode with scalability for integrated data storage,https://www.semanticscholar.org/paper/82663577cf1d08235bb56ad648c9dad36343ccfb,JournalArticle,DNA synthesis and sequencing on a single electrode enables integrated data storage using a sliding microarray chip.,2021,27,7,"[1515428357.0, 2084599260.0, 97709033.0, 93610956.0, 2112728651.0, 2118902636.0]",1515428357.0,Minsk,Science Advances,"['dna', 'synthesis', 'electrode', 'data storage', 'Data Storage']"
3b230f14c46e7e177e9bebb2ebc9f46b346b646d,Secure and efficient data storage and sharing scheme for blockchain‐based mobile‐edge computing,https://www.semanticscholar.org/paper/3b230f14c46e7e177e9bebb2ebc9f46b346b646d,JournalArticle,"With the rapid development of Internet of Things (IoT) technology, IoT devices have been widely used to collect physiological health data and provide diversified services to the terminal users. However, traditional data storage and sharing scheme cloud computing based in IoT face many challenges. For example, IoT devices are usually resource‐constrained (storage, computing power, battery capacity, etc.), data signed by IoT devices to ensure data integrity and authenticity will consume a lot of computing resources of IoT devices. At the same time, there is the challenge of high latency and unsafe data storage and sharing. To overcome these challenges, we propose a secure and efficient data storage and sharing scheme for blockchain‐based mobile‐edge computing. In our scheme, we construct the unique signature private key in a region into multiple key shares. IoT devices only need to submit the data and the random key shares allocated to the edge node. Edge node uses the recovered signature private key to realize data signature and homomorphic encryption. At the same time, the edge node will process timely data and return to the user. For data that need to be uploaded to the cloud for analysis, we use backup uploads to avoid data floods. Through experiments, it was found that our scheme can not only realize low‐latency message response for the terminal users, but also realize anonymous identity verification while ensuring data integrity and authenticity. The key shares of the signature private key are stored in different blocks of the blockchain to improve fault tolerance. The content extraction signature algorithm ensures that the key shares stored in different blocks are publicly verifiable. Safety analysis and performance analysis verify the feasibility and effectiveness of our scheme.",2021,27,32,"[2108781632.0, 2055642882.0, 2108438707.0, 2481509.0, 2269644.0, 2155550922.0]",2108781632.0,San Marino,Transactions on Emerging Telecommunications Technologies,"['data', 'iot', 'scheme', 'data storage', 'Data Storage']"
bb1118fb9fd86da6a2f65770353d8fb4362d9883,An Empirical Comparison of Preservation Methods for Synthetic DNA Data Storage.,https://www.semanticscholar.org/paper/bb1118fb9fd86da6a2f65770353d8fb4362d9883,JournalArticle,"Synthetic DNA has recently risen as a viable alternative for long-term digital data storage. To ensure that information is safely recovered after storage, it is essential to appropriately preserve the physical DNA molecules encoding the data. While preservation of biological DNA has been studied previously, synthetic DNA differs in that it is typically much shorter in length, it has different sequence profiles with fewer, if any, repeats (or homopolymers), and it has different contaminants. In this paper, nine different methods used to preserve data files encoded in synthetic DNA are evaluated by accelerated aging of nearly 29 000 DNA sequences. In addition to a molecular count comparison, the DNA is also sequenced and analyzed after aging. These findings show that errors and erasures are stochastic and show no practical distribution difference between preservation methods. Finally, the physical density of these methods is compared and a stability versus density trade-offs discussion provided.",2021,24,5 5,"[23757022.0, 38697713.0, 1957273475.0, 2111830587.0, 13163128.0, 29865576.0, 48560944.0, 1717411.0, 145033446.0]",23757022.0,Zagreb,Small Methods,"['dna', 'data', 'methods', 'data storage', 'Data Storage']"
44786a2c2a8ba8cf5c74a8fb10098c220e924c56,A Blockchain-Based Incremental Update Supported Data Storage System for Intelligent Vehicles,https://www.semanticscholar.org/paper/44786a2c2a8ba8cf5c74a8fb10098c220e924c56,JournalArticle,"With the development of autonomous driving and the Internet of Vehicles, vehicle data communication and data security become more and more important. Blockchain which has transparency, decentralization and immutability nature is treated as a promising approach to support intelligent vehicle systems. However, due to the high data update overhead, vulnerable raw data storage policy and inflexible consensus algorithm, traditional blockchain technologies are not suitable in modern vehicular systems. Hence, we propose, a blockchain data storage system that supports incremental data updating. Specifically, the system reduces the re-uploaded data size through smart contract and data partition to decrease the overhead. Besides, data replica and multi-data source addressing of index on the chain enhance the data reliability. In addition, an adaptive proof-of-work algorithm is developed, whose execution cost is dynamically adjusted based on nodes’ behavior. It greatly improves the data record and updating efficiency. Comprehensive experimental results show that BUS can effectively improve the data updating efficiency with low overhead and fewer resources in intelligent vehicle scenarios.",2021,21,70,"[1785971.0, 49034115.0, 2109971162.0, 34905515.0, 2116611035.0]",1785971.0,Nicosia,IEEE Transactions on Vehicular Technology,"['data', 'vehicle', 'systems', 'data storage', 'Data Storage']"
649c3497e3b34b15a5011259fcb837cf6c1ac04a,Lensless phase retrieval based on deep learning used in holographic data storage.,https://www.semanticscholar.org/paper/649c3497e3b34b15a5011259fcb837cf6c1ac04a,JournalArticle,"This paper proposes a lensless phase retrieval method based on deep learning (DL) used in holographic data storage. By training an end-to-end convolutional neural network between the phase-encoded data pages and the corresponding near-field diffraction intensity images, the new unknown phase data page can be predicted directly from the intensity image by the network model without any iterations. The DL-based phase retrieval method has a higher storage density, lower bit-error-rate (BER), and higher data transfer rate compared to traditional iterative methods. The retrieval optical system is simple, stable, and robust to environment fluctuations which is suitable for holographic data storage. Besides, we studied and demonstrated that the DL method has a good suppression effect on the dynamic noise of the holographic data storage system.",2021,14,46 17,"[1752784087.0, 2117689925.0, 2108139284.0, 92401367.0, 2037478325.0, 2128089506.0, 2119043585.0, 48391667.0]",1752784087.0,Dublin,Optics Letters,"['data', 'storage', 'phase', 'data storage', 'Data Storage']"
f7b69b8babfa607f2e8b0371f24cb720a7a827d6,Blockchain for Large-Scale Internet of Things Data Storage and Protection,https://www.semanticscholar.org/paper/f7b69b8babfa607f2e8b0371f24cb720a7a827d6,JournalArticle,"With the dramatically increasing deployment of IoT devices, storing and protecting the large volume of IoT data has become a significant issue. Traditional cloud-based IoT structures impose extremely high computation and storage demands on the cloud servers. Meanwhile, the strong dependencies on the centralized servers bring significant trust issues. To mitigate these problems, we propose a distributed data storage scheme employing blockchain and cetrificateless cryptography. Our scheme eliminates the traditional centralized servers by leveraging the blockchain miners who perform “transaction” verifications and records audit with the help of certificateless cryptography. We present a clear definition of the transactions in a non-cryptocurrency system and illustrate how the transactions are processed. To the best of our knowledge, this is the first work designing a secure and accountable IoT storage system using blockchain. Additionally, we extend our scheme to enable data trading and elaborate how data trading can be efficiently and effectively achieved.",2019,210,12,"[2336789.0, 2746913.0, 34312176.0, 46382448.0, 143651788.0, 2110651504.0]",2336789.0,Bern,IEEE Transactions on Services Computing,"['data', 'iot', 'storage', 'data storage', 'Data Storage']"
03aeb4520e760a906393aaf9c1bf4e526483d081,Capacity-Approaching Constrained Codes With Error Correction for DNA-Based Data Storage,https://www.semanticscholar.org/paper/03aeb4520e760a906393aaf9c1bf4e526483d081,JournalArticle,"We propose coding techniques that simultaneously limit the length of homopolymers runs, ensure the <inline-formula> <tex-math notation=""LaTeX"">${\tt G}{\tt C}$ </tex-math></inline-formula>-content constraint, and are capable of correcting a single edit error in strands of nucleotides in DNA-based data storage systems. In particular, for given <inline-formula> <tex-math notation=""LaTeX"">$\ell, \epsilon >0$ </tex-math></inline-formula>, we propose simple and efficient encoders/decoders that transform binary sequences into DNA base sequences (codewords), namely sequences of the symbols <inline-formula> <tex-math notation=""LaTeX"">${\tt A}, {\tt T}, {\tt C}$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=""LaTeX"">${\tt G}$ </tex-math></inline-formula>, that satisfy all of the following properties: 1) runlength constraint: the maximum homopolymer run in each codeword is at most <inline-formula> <tex-math notation=""LaTeX"">$\ell $ </tex-math></inline-formula>; 2) <inline-formula> <tex-math notation=""LaTeX"">${\tt G}{\tt C}$ </tex-math></inline-formula>-content constraint: the <inline-formula> <tex-math notation=""LaTeX"">${\tt G}{\tt C}$ </tex-math></inline-formula>-content of each codeword is within <inline-formula> <tex-math notation=""LaTeX"">$[0.5-\epsilon,0.5+\epsilon]$ </tex-math></inline-formula>; 3) error-correction: each codeword is capable of correcting a single deletion, or single insertion, or single substitution error. While various combinations of these properties have been considered in the literature, this work provides generalizations of codes constructions that satisfy all the properties with arbitrary parameters of <inline-formula> <tex-math notation=""LaTeX"">$\ell $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=""LaTeX"">$\epsilon $ </tex-math></inline-formula>. Furthermore, for practical values of <inline-formula> <tex-math notation=""LaTeX"">$\ell $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=""LaTeX"">$\epsilon $ </tex-math></inline-formula>, we show that our encoders achieve higher rates than existing results in the literature and approach capacity. Our methods have low encoding/decoding complexity and limited error propagation.",2020,44,67,"[27445282.0, 3680574.0, 1741737.0, 1977220.0]",27445282.0,Paris,IEEE Transactions on Information Theory,"['texmath', 'inlineformula', 'notationlatextt', 'data storage', 'Data Storage']"
147c868b721c8d29df7c61db7f2360114c760614,Digital Data Storage Using DNA Nanostructures and Solid-State Nanopores.,https://www.semanticscholar.org/paper/147c868b721c8d29df7c61db7f2360114c760614,JournalArticle,"Solid-state nanopores are powerful tools for reading the three-dimensional shape of molecules, allowing for the translation of molecular structure information into electric signals. Here, we show a high-resolution integrated nanopore system for identifying DNA nanostructures that has the capability of distinguishing attached short DNA hairpins with only a stem length difference of 8 bp along a DNA double strand named the DNA carrier. Using our platform, we can read up to 112 DNA hairpins with a separating distance of 114 bp attached on a DNA carrier that carries digital information. Our encoding strategy allows for the creation of a library of molecules with a size of up to 5 × 1033 (2112) that is only built from a few hundred types of base molecules for data storage and has the potential to be extended by linking multiple DNA carriers. Our platform provides a nanopore- and DNA nanostructure-based data storage method with convenient access and the potential for miniature-scale integration.",2019,105,19 2,"[8336569.0, 50596731.0, 5499803.0, 14869064.0, 48627323.0, 3356500.0]",8336569.0,Reykjavik,Nano letters (Print),"['dna', 'molecules', 'information', 'data storage', 'Data Storage']"
8fcbd1cd1ee2211bd183b986900042470ee7f440,Secure data storage based on blockchain and coding in edge computing.,https://www.semanticscholar.org/paper/8fcbd1cd1ee2211bd183b986900042470ee7f440,JournalArticle,"Edge computing is an important tool for smart computing, which brings convenience to data processing as well as security problems. In particular, the security of data storage under edge computing has become an obstacle to its widespread use. To solve the problem, the mechanism combing blockchain with regeneration coding is proposed to improve the security and reliability of stored data under edge computing. Our contribution is as follows. 1) According to the three-tier edge computing architecture and data security storage requirements, we proposed hybrid storage architecture and model specifically adapted to edge computing. 2) Making full use of the data storage advantages of edge network devices and cloud storage servers, we build a global blockchain in the cloud service layer and local blockchain is built on the terminals of the Internet of things. Moreover, the regeneration coding is utilized to further improve the reliability of data storage in blockchains. 3) Our scheme provides a mechanism for periodically validating hash values of data to ensure the integrity of data stored in global blockchain.",2019,103,16 4,"[2955771.0, 1491631727.0, 8259409.0, 2143717442.0]",2955771.0,Tirana,Mathematical biosciences and engineering : MBE,"['data', 'storage', 'edge', 'data storage', 'Data Storage']"
3adb779bb37d22e3aa299364c2a337603801ca5c,DNA Micro‐Disks for the Management of DNA‐Based Data Storage with Index and Write‐Once–Read‐Many (WORM) Memory Features,https://www.semanticscholar.org/paper/3adb779bb37d22e3aa299364c2a337603801ca5c,JournalArticle,"DNA‐based data storage has attracted attention because of its higher physical density of the data and longer retention time than those of conventional digital data storage. However, previous DNA‐based data storage lacked index features and the data quality of storage after a single access was not preserved, obstructing its industrial use. Here, DNA micro‐disks, QR‐coded micro‐sized disks that harbor data‐encoded DNA molecules for the efficient management of DNA‐based data storage, are proposed. The two major features that previous DNA‐based data‐storage studies could not achieve are demonstrated. One feature is accessing data items efficiently by indexing the data‐encoded DNA library. Another is achieving write‐once–read‐many (WORM) memory through the immobilization of DNA molecules on the disk and their enrichment through in situ DNA production. Through these features, the reliability of DNA‐based data storage is increased by allowing selective and multiple accession of data‐encoded DNA with lower data loss than previous DNA‐based data storage methods.",2020,33,32,"[73509301.0, 15994978.0, 31399475.0, 2111389723.0, 50403503.0, 5430731.0, 2058870027.0, 2109559620.0, 114449847.0, 51163989.0, 2110007025.0, 145720154.0, 88396204.0]",73509301.0,Vilnius,Advances in Materials,"['data', 'storage', 'dna', 'data storage', 'Data Storage']"
fac67bf55456b52ac6e4f280ad953d0250c74ebc,2D MXene–TiO2 Core–Shell Nanosheets as a Data‐Storage Medium in Memory Devices,https://www.semanticscholar.org/paper/fac67bf55456b52ac6e4f280ad953d0250c74ebc,JournalArticle,"MXenes, an emerging class of 2D transition metal carbides and nitrides with the general formula Mn+1XnTx (n = 1–4), have potential for application as floating gates in memory devices because of their intrinsic properties of a 2D structure, high density‐of‐states, and high work function. In this study, a series of MXene–TiO2 core–shell nanosheets are synthesized by deterministic control of the surface oxidation of MXene. The floating gate (multilayer MXene) and tunneling layer (TiO2) in a nano‐floating‐gate transistor memory (NFGTM) device are prepared simultaneously by a facile, low‐cost, and water‐based process. The memory performance is optimized via adjustment of the thickness of the oxidation layer formed on the MXene surface. The fabricated MXene NFGTMs exhibit excellent nonvolatile memory characteristics, including a large memory window (>35.2 V), high programming/erasing current ratio (≈106), low off‐current (<1 pA), long retention (>104 s), and cyclic endurance (300 cycles). Furthermore, synaptic functions, including the excitatory postsynaptic current/inhibitory postsynaptic current, paired‐pulse facilitation, and synaptic plasticity (long‐term potentiation/depression), are successfully emulated using the MXene NFGTMs. The successful control of MXene oxidation and its application to NFGTMs are expected to inspire the application of MXene as a data‐storage medium in future memory devices.",2020,73,32,"[1380082069.0, 49146822.0, 94314332.0, 47703819.0, 2115457464.0, 153311051.0, 5559947.0]",1380082069.0,Paris,Advances in Materials,"['mxene', 'memory', 'application', 'data storage', 'Data Storage']"
4bd3c9e1bb1ca2df62b66201616b8740300efd0a,Integrating encryption techniques for secure data storage in the cloud,https://www.semanticscholar.org/paper/4bd3c9e1bb1ca2df62b66201616b8740300efd0a,JournalArticle,"Cloud computing has emerged as one of the most groundbreaking technologies to have redefined the bounds of conventional computing techniques. It has ushered in a paradigm shift and pushed the frontiers of how computing assets, inclusive of infrastructure resources, software, and applications can be used, adopted, and purchased. The economic benefits or rather the fundamental economic shift offered by cloud computing in reducing capital expenditure and converting it to operational expenditure has been a primary motivating factor for early adopters. However, despite its inherent advantages that include better access and control, there exist several reservations around cloud computing that have impeded its growth. The control, elasticity, and ease of use that cloud computing is associated with also engender many security issues. Security is considered to be the topmost hurdle out of the nine identified challenges of cloud computing as underlined by the study conducted by the International Data Corporation. It therefore follows that an exceedingly secure system is essential for the safeguarding of an organizational entity, its resources, and assets. In this article, it is our endeavor to offer insights into the implementation of a novel architecture that can deliver an enhanced degree of security for outsourcing information in a cloud computing environment while involving numerous independent cloud providers. The framework comprises of dual encryption and data fragmentation techniques that envision the secure distribution of information in a multicloud environment. The various concerns surrounding this area, specifically, the challenges of integrity, security, confidentiality, and authentication have been addressed. All simulations and scrutiny have been accomplished on an Oracle virtual machine Virtual‐Box and a Fog environment on an Ubuntu 16.04 platform. Extensive safety measures and performance analysis that take into account diverse parameters, especially execution time, integrity, throughput, entropy, transfer rate, and delay demonstrate that our projected proposal is vastly proficient and satisfies the security prerequisites of secure data sharing and can efficiently withstand security attacks.",2020,56,33,"[84156264.0, 38298800.0, 66519890.0, 34956744.0, 150302778.0, 144369609.0]",84156264.0,Nicosia,Transactions on Emerging Telecommunications Technologies,"['cloud', 'computing', 'security', 'data storage', 'Data Storage']"
9a60d3c9d3a5b7f165325fca45bd418d651682d3,Boafft: Distributed Deduplication for Big Data Storage in the Cloud,https://www.semanticscholar.org/paper/9a60d3c9d3a5b7f165325fca45bd418d651682d3,JournalArticle,"As data progressively grows within data centers, the cloud storage systems continuously facechallenges in saving storage capacity and providing capabilities necessary to move big data within an acceptable time frame. In this paper, we present the Boafft, a cloud storage system with distributed deduplication. The Boafft achieves scalable throughput and capacity usingmultiple data servers to deduplicate data in parallel, with a minimal loss of deduplication ratio. Firstly, the Boafft uses an efficient data routing algorithm based on data similarity that reduces the network overhead by quickly identifying the storage location. Secondly, the Boafft maintains an in-memory similarity indexing in each data server that helps avoid a large number of random disk reads and writes, which in turn accelerates local data deduplication. Thirdly, the Boafft constructs hot fingerprint cache in each data server based on access frequency, so as to improve the data deduplication ratio. Our comparative analysis with EMC's stateful routing algorithm reveals that the Boafft can provide a comparatively high deduplication ratio with a low network bandwidth overhead. Moreover, the Boafft makes better usage of the storage space, with higher read/write bandwidth and good load balance.",2020,44,8,"[35365766.0, 8274453.0, 1980434.0, 1740261.0, 2181606.0]",35365766.0,Stockholm,IEEE Transactions on Cloud Computing,"['data', 'boafft', 'storage', 'data storage', 'Data Storage']"
08be4e23951a0def1c5d235befbb39c8d8d373a3,A Thorough Trust and Reputation Based RBAC Model for Secure Data Storage in the Cloud,https://www.semanticscholar.org/paper/08be4e23951a0def1c5d235befbb39c8d8d373a3,JournalArticle,"Cloud computing is a widespread technology, which has attracted much attention nowadays. Among the many criteria that must be considered for data storage in the cloud, access control plays a vital role. Role-based access control (RBAC) is a well-known technique for secure data storage in the cloud. Since the traditional RBAC models are improper for open and decentralized environments, recently, some works have integrated the trust concept into the RBAC model. Nevertheless, they have not fully addressed the required security metrics of a trust-based system. Therefore, in this paper, we first introduce the security goals that should be considered in an efficient trust-based system. Second, we propose a novel trust and reputation based RBAC model that not only can properly withstand the security threats of trust-based RBAC models, but also is scalable as it has reasonable execution time. Third, we evaluate the proposed model using the famous trust network of advogato dataset. Eventually, we compare the proposed model with recently-published ones in terms of mean absolute error, execution time of indirect trust computation, and provided features. The achieved results are indicative of the priority of the proposed model to be employed in real cloud environments.",2019,47,30,"[1411256149.0, 1410127739.0, 11531589.0]",1411256149.0,Ljubljana,IEEE Transactions on Parallel and Distributed Systems,"['rbac', 'model', 'cloud', 'data storage', 'Data Storage']"
82a09f72d7b9587e3b519b1cd9640a5a611f3480,Data Storage Mechanism Based on Blockchain with Privacy Protection in Wireless Body Area Network,https://www.semanticscholar.org/paper/82a09f72d7b9587e3b519b1cd9640a5a611f3480,JournalArticle,"Wireless body area networks (WBANs) are expected to play a vital role in the field of patient-health monitoring shortly. They provide a convenient way to collect patient data, but they also bring serious problems which are mainly reflected in the safe storage of the collected data. The privacy and security of data storage in WBAN devices cannot meet the needs of WBAN users. Therefore, this paper adopts blockchain technology to store data, which improves the security of the collected data. Moreover, a storage model based on blockchain in WBAN is proposed in our solution. However, blockchain storage brings new problems, for example, that the storage space of blockchain is small, and the stored content is open to unauthorized attackers. To solve the problems above, this paper proposed a sequential aggregate signature scheme with a designated verifier (DVSSA) to ensure that the user’s data can only be viewed by the designated person and to protect the privacy of the users of WBAN. In addition, the new signature scheme can also compress the size of the blockchain storage space.",2019,61,19,"[2955771.0, 1491631727.0, 108621123.0, 2143717442.0, 79987966.0]",2955771.0,Podgorica,Italian National Conference on Sensors,"['data', 'storage', 'blockchain', 'data storage', 'Data Storage']"
1986318d8a565fbff8fde545b8d0c2012c6462d8,Construction of Bio-Constrained Code for DNA Data Storage,https://www.semanticscholar.org/paper/1986318d8a565fbff8fde545b8d0c2012c6462d8,JournalArticle,"With extremely high density and durable preservation, DNA data storage has become one of the most cutting-edge techniques for long-term data storage. Similar to traditional storage which impose restrictions on the form of encoded data, data stored in DNA storage systems are also subject to two biochemical constraints, i.e., maximum homopolymer run limit and balanced GC content limit. Previous studies used successive process to satisfy these two constraints. As a result, the process suffers low efficiency and high complexity. In this letter, we propose a novel content-balanced run-length limited code with an efficient code construction method, which generates short DNA sequences that satisfy both constraints at one time. Besides, we develop an encoding method to map binary data into long DNA sequences for DNA data storage, which ensures both local and global stability in terms of satisfying the biochemical constraints. The proposed encoding method has high effective code rate of 1.917 bits per nucleotide and low coding complexity.",2019,47,23,"[2108734743.0, 1398730731.0, 143884284.0, 1737045.0, 10310238.0]",2108734743.0,Sofia,IEEE Communications Letters,"['data', 'dna', 'storage', 'data storage', 'Data Storage']"
4895c430c7810b45840b58cc9182f12143013a43,Magnetic Nanofiber Mats for Data Storage and Transfer,https://www.semanticscholar.org/paper/4895c430c7810b45840b58cc9182f12143013a43,JournalArticle,"Electrospun nanofiber mats may serve as new hardware for neuromorphic computing. To enable data storage and transfer in them, they should be magnetic, possibly electrically conductive and able to respond to further external impulses. Here we report on creating magnetic nanofiber mats, consisting of magnetically doped polymer nanofibers for data transfer and polymer beads containing larger amounts of magnetic nanoparticles for storage purposes. Using magnetite and iron nickel oxide nanoparticles, a broad range of doping ratios could be electrospun with a needleless technique, resulting in magnetic nanofiber mats with varying morphologies and different amounts of magnetically doped beads.",2019,45,9,"[66770881.0, 50353334.0, 65827603.0, 66552339.0, 51312398.0, 66760366.0, 49746834.0, 32559865.0]",66770881.0,Madrid,Nanomaterials,"['nanofiber', 'mats', 'data', 'data storage', 'Data Storage']"
0599f45e03ac2016321df0dd653ba4c0034c79d5,Fuzzy-Folded Bloom Filter-as-a-Service for Big Data Storage in the Cloud,https://www.semanticscholar.org/paper/0599f45e03ac2016321df0dd653ba4c0034c79d5,JournalArticle,"With the ongoing trend of smart and Internet-connected objects being deployed across a broad range of applications, there is also a corresponding increase in the amount of data movement across different geographical regions. This, in turn, poses a number of challenges with respect to big data storage across multiple locations, including cloud computing platform. For example, the underlying distributed file system has a large number of directories and files in the form of gigantic trees, which are difficult to parse in polynomial time. Moreover, with the exponential increase of big data streams (i.e., unbounded sets of continuous data flows), challenges associated with indexing and membership queries are compounded. The capability to process such significant amount of data with high accuracy can have significant impact on decision-making and formulation of business and risk-related strategies, particularly in our current Industrial Internet of Things environment (IIoT). However, existing storage solutions are deterministic in nature. In other words, they tend to consume considerable memory and CPU time to yield accurate results. This necessitates the design of efficient quality of service-aware IIoT applications that are able to deal with the challenges of data storage and retrieval in the cloud computing environment. In this paper, we present an effective space-effective strategy for massive data storage using bloom filter (BF). Specifically, in the proposed scheme, the standard BF is extended to incorporate fuzzy-enabled folding approach, hereafter referred to as fuzzy folded BF (FFBF). In FFBF, fuzzy operations are used to accommodate the hashed data of one BF into another to reduce storage requirements. Evaluations on UCI ML AReM and Facebook datasets demonstrate the efficacy of FFBF, in terms of dealing with approximately 1.9 times more data as compared to using the standard BF. This is also achieved without affecting the false positive rate and query time.",2019,40,15,"[48775545.0, 144529280.0, 46555602.0, 2818166.0, 144996075.0, 2840539.0]",48775545.0,Oslo,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'bf', 'data storage', 'Data Storage']"
742747c7a453b293352b772d0d99541c96a351c3,Heterogeneous Data Storage Management with Deduplication in Cloud Computing,https://www.semanticscholar.org/paper/742747c7a453b293352b772d0d99541c96a351c3,JournalArticle,"Cloud storage as one of the most important services of cloud computing helps cloud users break the bottleneck of restricted resources and expand their storage without upgrading their devices. In order to guarantee the security and privacy of cloud users, data are always outsourced in an encrypted form. However, encrypted data could incur much waste of cloud storage and complicate data sharing among authorized users. We are still facing challenges on encrypted data storage and management with deduplication. Traditional deduplication schemes always focus on specific application scenarios, in which the deduplication is completely controlled by either data owners or cloud servers. They cannot flexibly satisfy various demands of data owners according to the level of data sensitivity. In this paper, we propose a heterogeneous data storage management scheme, which flexibly offers both deduplication management and access control at the same time across multiple Cloud Service Providers (CSPs). We evaluate its performance with security analysis, comparison and implementation. The results show its security, effectiveness and efficiency towards potential practical usage.",2019,40,5,"[145843577.0, 2108025636.0, 2234493.0, 50320297.0]",145843577.0,Bern,IEEE Transactions on Big Data,"['data', 'cloud', 'storage', 'data storage', 'Data Storage']"
624b2f14be4287d6a400cdf88a6f911b434b182e,Enabling Ternary Hash Tree Based Integrity Verification for Secure Cloud Data Storage,https://www.semanticscholar.org/paper/624b2f14be4287d6a400cdf88a6f911b434b182e,JournalArticle,"Cloud Computing enables the remote users to access data, services, and applications in on-demand from the shared pool of configurable computing resources, without the consideration of storage, hardware, and software management. On the other hand, it is not easy for cloud users to identify whether Cloud Service Provider's (CSP) tag along with the data security legal expectations. So, cloud users could not rely on CSP's in terms of trust. So, it is significant to build a secure and efficient data auditing framework for increasing and maintaining cloud users trust with CSP. Researchers suggested introducing Third Party Auditor (TPA) on behalf of cloud user for verifying the outsourced data integrity, which may reduce the computation overhead of cloud users. In this work, we proposed a novel integrity verification framework for securing cloud storage based on Ternary Hash Tree (THT) and Replica based Ternary Hash Tree (R-THT), which will be used by TPA to perform data auditing. Differing from existing work, the proposed framework performs Block-level, File-level and Replica-level auditing with tree block ordering, storage block ordering for verifying the data integrity and ensuring data availability in the cloud. We further extend our framework to support error localization with data correctness, dynamic updates with block update, insert, and delete operations in the cloud. The structure of THT and R-THT will reduce the computation cost and provide efficiency in data updates compared to the existing schemes. The security analysis of the proposed public auditing framework indicates the achievement of desired properties and performance has been evaluated with the detailed experiment set. The results show that the proposed secure cloud auditing framework is highly secure and efficient in storage, communication, and computation costs.",2020,21,32,"[2288759936.0, 48919600.0]",2288759936.0,Minsk,IEEE Transactions on Knowledge and Data Engineering,"['cloud', 'data', 'framework', 'data storage', 'Data Storage']"
54814b0669f20f07ec8d8c7e4fabddfadfe66b1a,DNA Data Storage and Hybrid Molecular–Electronic Computing,https://www.semanticscholar.org/paper/54814b0669f20f07ec8d8c7e4fabddfadfe66b1a,JournalArticle,"Moore’s law may be slowing, but our ability to manipulate molecules is improving faster than ever. DNA could provide alternative substrates for computing and storage as existing ones approach physical limits. In this paper, we explore the implications of this trend in computer architecture. We present a computer systems perspective on molecular processing and storage, positing a hybrid molecular–electronic architecture that plays to the strengths of both domains. We cover the design and implementation of all stages of the pipeline: encoding, DNA synthesis, system integration with digital microfluidics, DNA sequencing (including emerging technologies such as nanopores), and decoding. We first draw on our experience designing a DNA-based archival storage system, which includes the largest demonstration to date of DNA digital data storage of over three billion nucleotides encoding over 400 MB of data. We then propose a more ambitious hybrid–electronic design that uses a molecular form of near-data processing for massive parallelism. We present a model that demonstrates the feasibility of these systems in the near future. We think the time is ripe to consider molecular storage seriously and explore system designs and architectural implications.",2019,39,107,"[52129431.0, 1717411.0, 49997612.0, 145966834.0, 145033446.0, 8810543.0]",52129431.0,Reykjavik,Proceedings of the IEEE,"['storage', 'dna', 'system', 'data storage', 'Data Storage']"
11c3154d709c74dbbe702e7f7c46a37224f9cc36,Secure Data Storage and Searching for Industrial IoT by Integrating Fog Computing and Cloud Computing,https://www.semanticscholar.org/paper/11c3154d709c74dbbe702e7f7c46a37224f9cc36,JournalArticle,"With the fast development of industrial Internet of things (IIoT), a large amount of data is being generated continuously by different sources. Storing all the raw data in the IIoT devices locally is unwise considering that the end devices’ energy and storage spaces are strictly limited. In addition, the devices are unreliable and vulnerable to many threats because the networks may be deployed in remote and unattended areas. In this paper, we discuss the emerging challenges in the aspects of data processing, secure data storage, efficient data retrieval and dynamic data collection in IIoT. Then, we design a flexible and economical framework to solve the problems above by integrating the fog computing and cloud computing. Based on the time latency requirements, the collected data are processed and stored by the edge server or the cloud server. Specifically, all the raw data are first preprocessed by the edge server and then the time-sensitive data (e.g., control information) are used and stored locally. The non-time-sensitive data (e.g., monitored data) are transmitted to the cloud server to support data retrieval and mining in the future. A series of experiments and simulation are conducted to evaluate the performance of our scheme. The results illustrate that the proposed framework can greatly improve the efficiency and security of data storage and retrieval in IIoT.",2018,211,14,"[1843181.0, 2144443688.0, 144861189.0, 1712275.0, 1831104.0]",1843181.0,Tirana,IEEE Transactions on Industrial Informatics,"['data', 'server', 'iiot', 'data storage', 'Data Storage']"
94de6bab96ad533169dbc3bf9e6557581e59cb6f,Data Storage in the Decentralized World: Blockchain and Derivatives,https://www.semanticscholar.org/paper/94de6bab96ad533169dbc3bf9e6557581e59cb6f,JournalArticle,"We have entered an era where the importance of decentralized solutions has become more obvious. Blockchain technology and its derivatives are distributed ledger technologies that keep the registry of data between peers of a network. This ledger is secured within a successive over looping cryptographic chain. The accomplishment of the Bitcoin cryptocurrency proved that blockchain technology and its derivatives could be used to eliminate intermediaries and provide security for cyberspace. However, there are some challenges in the implementation of blockchain technology. This chapter first explains the concept of blockchain technology and the data that we can store therein. The main advantage of blockchain is the security services that it provides. This section continues by describing these services.. The challenges of blockchain; blockchain anomalies, energy consumption, speed, scalability, interoperability, privacy and cryptology in the age of quantum computing are described. Selected solutions for these challenges are given. Remarkable derivatives of blockchain, which use different solutions (directed acyclic graph, distributed hash table, gossip consensus protocol) to solve some of these challenges are described. Then the data storage in blockchain and evolving data solutions are explained. The comparison of decentralized solutions with the lcentralized database systems is given. A multi-platform interoperable scalable architecture (MPISA) is proposed. In the conclusion we include the evolution assumptions of data storage in a decentralized world.",2020,19,abs/2012.10253,"[40631192.0, 1573532302.0]",40631192.0,Dublin,arXiv.org,"['solutions', 'blockchain', 'data', 'data storage', 'Data Storage']"
afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84,IoT-Based Big Data Storage Systems in Cloud Computing: Perspectives and Challenges,https://www.semanticscholar.org/paper/afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84,JournalArticle,"Internet of Things (IoT) related applications have emerged as an important field for both engineers and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations especially in cloud computing. This paper first provides a functional framework that identifies the acquisition, management, processing and mining areas of IoT big data, and several associated technical modules are defined and described in terms of their key characteristics and capabilities. Then current research in IoT application is analyzed, moreover, the challenges and opportunities associated with IoT big data research are identified. We also report a study of critical IoT application publications and research topics based on related academic and industry publications. Finally, some open issues and some typical examples are given under the proposed IoT-related research framework.",2017,441,4,"[1720869.0, 2219053320.0, 30000987.0, 1747034.0]",1720869.0,Madrid,IEEE Internet of Things Journal,"['research', 'iot', 'framework', 'data storage', 'Data Storage']"
9d788cfe4a0991d3b1a266c8329f6e903840b82f,Deep Learning-Based Data Storage for Low Latency in Data Center Networks,https://www.semanticscholar.org/paper/9d788cfe4a0991d3b1a266c8329f6e903840b82f,JournalArticle,"Low-latency data access is becoming an upcoming and increasingly important challenge. The proper placement of data blocks can reduce data travel among distributed storage systems, which contributes significantly to the latency reduction. However, the dominant data placement optimization has primarily relied on prior known data requests or static initial data distribution, which ignores the dynamics of clients’ data access requests and networks. The learning technology can help the data center networks (DCNs) learn from historical access information and make optimal data storage decision. Consider a more practical DCNs with fat-tree topology, we utilize a deep-learning technology <inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula>-means to help store data blocks and then improve the read and write latency of the DCN, where <inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> is the number of cores in the fat-tree. The evaluation results demonstrate that the average write and read latency of the whole system can be lowered by 33% and 45%, respectively. And the best set of parameter <inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> is analyzed and recommended to provide guidance to the real application, which is equal to the number of cores in the DCNs.",2019,33,7,"[2431843.0, 2109960523.0, 32434251.0, 1796706.0, 2143716755.0, 79987966.0]",2431843.0,Tallinn,IEEE Access,"['data', 'access', 'latency', 'data storage', 'Data Storage']"
224c11bc51b4959bc787d6681c2b152468294b11,Multi-Touch Querying on Data Physicalizations in Immersive AR,https://www.semanticscholar.org/paper/224c11bc51b4959bc787d6681c2b152468294b11,JournalArticle,"Data physicalizations (3D printed terrain models, anatomical scans, or even abstract data) can naturally engage both the visual and haptic senses in ways that are difficult or impossible to do with traditional planar touch screens and even immersive digital displays. Yet, the rigid 3D physicalizations produced with today's most common 3D printers are fundamentally limited for data exploration and querying tasks that require dynamic input (e.g., touch sensing) and output (e.g., animation), functions that are easily handled with digital displays. We introduce a novel style of hybrid virtual + physical visualization designed specifically to support interactive data exploration tasks. Working toward a ""best of both worlds"" solution, our approach fuses immersive AR, physical 3D data printouts, and touch sensing through the physicalization. We demonstrate that this solution can support three of the most common spatial data querying interactions used in scientific visualization (streamline seeding, dynamic cutting places, and world-in-miniature visualization). Finally, we present quantitative performance data and describe a first application to exploratory visualization of an actively studied supercomputer climate simulation data with feedback from domain scientists.",2021,7,5,"[153475895.0, 2089478126.0, 1825752990.0, 2141634264.0, 3307286.0, 144027436.0, 145735309.0]",153475895.0,Berlin,Proc. ACM Hum. Comput. Interact.,"['data', '3d', 'visualization', 'data querying', 'Data Querying']"
557ee73ff4e87ca0cb1b6c78af0730a2d94b710f,Panorama: A Data System for Unbounded Vocabulary Querying over Video,https://www.semanticscholar.org/paper/557ee73ff4e87ca0cb1b6c78af0730a2d94b710f,JournalArticle,"Deep convolutional neural networks (CNNs) achieve state-of-the-art accuracy for many computer vision tasks. But using them for video monitoring applications incurs high computational cost and inference latency. Thus, recent works have studied how to improve system efficiency. But they largely focus on small ""closed world"" prediction vocabularies even though many applications in surveillance security, traffic analytics, etc. have an ever-growing set of target entities. We call this the ""unbounded vocabulary"" issue, and it is a key bottleneck for emerging video monitoring applications. We present the first data system for tacking this issue for video querying, Panorama. Our design philosophy is to build a unified and domain-agnostic system that lets application users generalize to unbounded vocabularies in an out-of-the-box manner without tedious manual re-training. To this end, we synthesize and innovate upon an array of techniques from the ML, vision, databases, and multimedia systems literature to devise a new system architecture. We also present techniques to ensure Panorama has high inference efficiency. Experiments with multiple real-world datasets show that Panorama can achieve between 2x to 20x higher efficiency than baseline approaches on in-vocabulary queries, while still yielding comparable accuracy and also generalizing well to unbounded vocabularies.",2019,29,13,"[2108545581.0, 2119303314.0]",2108545581.0,Prague,Proceedings of the VLDB Endowment,"['system', 'video', 'applications', 'data querying', 'Data Querying']"
1fa4936fb06319c3f4536c26a447d5507c92bd48,A Privacy-Preserving and Verifiable Querying Scheme in Vehicular Fog Data Dissemination,https://www.semanticscholar.org/paper/1fa4936fb06319c3f4536c26a447d5507c92bd48,JournalArticle,"Vehicular fog has attracted considerable attention recently, as the densely deployed fog devices are in proximity to vehicular end-users, and they are particularly suitable for the latency-sensitive and location-aware vehicular services. In this paper, we propose a secure querying scheme in vehicular fog data dissemination, in which the roadside units (RSUs) act as fog storage devices to cache data at network edge and disseminate data upon querying. To disrupt the association between a specific data request and its origin vehicle, the proposed scheme exploits an invertible matrix to structure multiple data requests from different vehicles, and aggregates the ciphertexts of data requests at the RSU side with the homomorphic Paillier cryptosystem. Meanwhile, given the invertible matrix and decryption result, the RSU can recover each individual data request without identifying its origin vehicle. In addition, the RSU can verify the correctness of the recovered data requests with an identity-based batch verification scheme. Through security analysis, we demonstrate that the proposed scheme can achieve the security goals of unlinkability, confidentiality, and verifiability. Performance evaluations are also conducted, in which the obtained results show that the proposed scheme can be adaptive to the fluctuating number of the data querying vehicles, and significantly reduce the computation complexity and communication overhead.",2019,30,68,"[2984224.0, 103073143.0, 144637915.0, 3326354.0]",2984224.0,Valletta,IEEE Transactions on Vehicular Technology,"['data', 'scheme', 'fog', 'data querying', 'Data Querying']"
99ae62cca0b15275d9ed1528f345f9dcefe50dd7,Temporal Data Representation and Querying Based on RDF,https://www.semanticscholar.org/paper/99ae62cca0b15275d9ed1528f345f9dcefe50dd7,JournalArticle,"With the explosive growth of temporal data, how to query and manage temporal data has become an important research issue. Resource description framework (RDF), as the standard data and knowledge description language of the semantic web, has been widely used to represent various domain data. Aiming at the representation and querying of temporal data, this paper proposes a temporal data representation model based on RDF and its corresponding querying method. First, a representation model called RDFt is proposed, which can represent temporal data with both the time information and the update count information, and the syntax and semantics of the RDFt model are given. Then, we propose a query language called SPARQL[t] for RDFt, and we give the query syntax and operations of SPARQL[t] in detail. In addition, a querying transformation algorithm from SPARQL[t] to SPARQL is proposed, in order to achieve compatibility with the existing RDF query engines. Finally, we implemented a prototype system that can support RDFt temporal data representation and querying, and the case studies and experimental results verify the feasibility of the proposed approach.",2019,15,7,"[2686365.0, 2133843731.0, 9248001.0, 4671421.0]",2686365.0,Bratislava,IEEE Access,"['data', 'query', 'representation', 'data querying', 'Data Querying']"
8babcaf89f8537dc628a029ebf932100f57289fd,Lightweight Indexing and Querying Services for Big Spatial Data,https://www.semanticscholar.org/paper/8babcaf89f8537dc628a029ebf932100f57289fd,JournalArticle,"With the widespread use of GPS-equipped smartphones and Internet of Things devices, a huge amount of data with location information is being generated at an unprecedented rate. To gain a deeper insight into such a plethora of spatial data, scientists and engineers are widely using spatial queries for their big data applications. However, because of not only the massive spatial data size but also the complexity of spatial query processing, they are struggling to efficiently process the spatial queries. In this paper, we propose lightweight and scalable indexing and querying services for big spatial data stored in distributed storage systems or graph-based systems. Our spatial services have several advantages over existing approaches. First, our services can be easily applied to existing storage systems or graph-based models without modifying the internal implementation of existing systems/models. Second, our services achieve high pruning power by efficiently selecting only relevant spatial objects based on a simple yet effective filter. Third, our services support a customizable and easy-to-use control of index data size by adjusting the precision of indexed geometries. Lastly, our services support efficient updates of spatial data. Our experimental results using real-world datasets validate the effectiveness and efficiency of our spatial services.",2019,14,12,"[1899648.0, 46458150.0, 143642366.0, 1718467.0, 2145907057.0, 144586053.0, 2115979102.0]",1899648.0,Belgrade,IEEE Transactions on Services Computing,"['data', 'services', 'systems', 'data querying', 'Data Querying']"
d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b,BimSPARQL: Domain-specific functional SPARQL extensions for querying RDF building data,https://www.semanticscholar.org/paper/d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b,JournalArticle,"In this paper, we propose to extend SPARQL functions for querying Industry Foundation Classes (IFC) building data. The official IFC documentation and BIM requirement checking use cases are used to drive the development of the proposed functionality. By extending these functions, we aim to (1) simplify writing queries and (2) retrieve useful information implied in 3D geometry data according to requirement checking use cases. Extended functions are modelled as RDF vocabularies and classified into groups for further extensions. We combine declarative rules with procedural programming to implement extended functions. Realistic requirement checking scenarios are used to evaluate and demonstrate the effectiveness of this approach and indicate query performance. Compared with query techniques developed in the conventional Building Information Modeling domain, we show the added value of such approach by providing an application example of querying building and regulatory data, where spatial and logic reasoning can be applied and data from multiple sources are required. Based on the implementation and evaluation work, we discuss the advantages and applicability of this approach, current issues and future challenges.",2018,66,9,"[2115811945.0, 144204299.0, 35195456.0]",2115811945.0,London,Semantic Web,"['functions', 'data', 'requirement', 'data querying', 'Data Querying']"
b9f190339ce0b4cdad3464c45ec3266a7369fb7c,Querying Log Data with Metric Temporal Logic,https://www.semanticscholar.org/paper/b9f190339ce0b4cdad3464c45ec3266a7369fb7c,JournalArticle,"
 
 
We propose a novel framework for ontology-based access to temporal log data using a datalog extension datalogMTL of the Horn fragment of the metric temporal logic MTL. We show that datalogMTL is EXPSPACE-complete even with punctual intervals, in which case full MTL is known to be undecidable. We also prove that nonrecursive datalogMTL is PSPACE-complete for combined complexity and in AC0 for data complexity. We demonstrate by two real-world use cases that nonrecursive datalogMTL programs can express complex temporal concepts from typical user queries and thereby facilitate access to temporal log data. Our experiments with Siemens turbine data and MesoWest weather data show that datalogMTL ontology-mediated queries are efficient and scale on large datasets. 
 
 
",2017,60,62,"[144105942.0, 3449760.0, 145500489.0, 47490276.0, 1749505.0]",144105942.0,Riga,Journal of Artificial Intelligence Research,"['data', 'datalogmtl', 'access', 'data querying', 'Data Querying']"
ac67d5f9c89d8d72fbd074f94079608220348f3f,ATHENA: An Ontology-Driven System for Natural Language Querying over Relational Data Stores,https://www.semanticscholar.org/paper/ac67d5f9c89d8d72fbd074f94079608220348f3f,JournalArticle,"In this paper, we present ATHENA, an ontology-driven system for natural language querying of complex relational databases. Natural language interfaces to databases enable users easy access to data, without the need to learn a complex query language, such as SQL. ATHENA uses domain specific ontologies, which describe the semantic entities, and their relationships in a domain. We propose a unique two-stage approach, where the input natural language query (NLQ) is first translated into an intermediate query language over the ontology, called OQL, and subsequently translated into SQL. Our two-stage approach allows us to decouple the physical layout of the data in the relational store from the semantics of the query, providing physical independence. Moreover, ontologies provide richer semantic information, such as inheritance and membership relations, that are lost in a relational schema. By reasoning over the ontologies, our NLQ engine is able to accurately capture the user intent. We study the effectiveness of our approach using three different workloads on top of geographical (GEO), academic (MAS) and financial (FIN) data. ATHENA achieves 100% precision on the GEO and MAS workloads, and 99% precision on the FIN workload which operates on a complex financial ontology. Moreover, ATHENA attains 87.2%, 88.3%, and 88.9% recall on the GEO, MAS, and FIN workloads, respectively.",2016,153,9,"[39376048.0, 2327080.0, 145590185.0, 1856878.0, 3441349.0, 143972996.0]",39376048.0,Berlin,Proceedings of the VLDB Endowment,"['language', 'athena', 'query', 'data querying', 'Data Querying']"
736ef8a32d6c5f76a21d61299300cf796480d507,Toward sustainable publishing and querying of distributed Linked Data archives,https://www.semanticscholar.org/paper/736ef8a32d6c5f76a21d61299300cf796480d507,JournalArticle,"Purpose 
 
 
 
 
The purpose of this paper is to detail a low-cost, low-maintenance publishing strategy aimed at unlocking the value of Linked Data collections held by libraries, archives and museums (LAMs). 
 
 
 
 
Design/methodology/approach 
 
 
 
 
The shortcomings of commonly used Linked Data publishing approaches are identified, and the current lack of substantial collections of Linked Data exposed by LAMs is considered. To improve on the discussed status quo, a novel approach for publishing Linked Data is proposed and demonstrated by means of an archive of DBpedia versions, which is queried in combination with other Linked Data sources. 
 
 
 
 
Findings 
 
 
 
 
The authors show that the approach makes publishing Linked Data archives easy and affordable, and supports distributed querying without causing untenable load on the Linked Data sources. 
 
 
 
 
Research limitations/implications 
 
 
 
 
The proposed approach significantly lowers the barrier for publishing, maintaining, and making Linked Data collections queryable. As such, it offers the potential to substantially grow the distributed network of queryable Linked Data sources. Because the approach supports querying without causing unacceptable load on the sources, the queryable interfaces are expected to be more reliable, allowing them to become integral building blocks of robust applications that leverage distributed Linked Data sources. 
 
 
 
 
Originality/value 
 
 
 
 
The novel publishing strategy significantly lowers the technical and financial barriers that LAMs face when attempting to publish Linked Data collections. The proposed approach yields Linked Data sources that can reliably be queried, paving the way for applications that leverage distributed Linked Data sources through federated querying.",2018,23,74,"[2701137.0, 1723397.0, 2911425.0, 8050194.0]",2701137.0,London,J. Documentation,"['data', 'sources', 'publishing', 'data querying', 'Data Querying']"
fa77a44f3f1857361a50c3137d623c35ef8a5739,Querying Graphs with Data,https://www.semanticscholar.org/paper/fa77a44f3f1857361a50c3137d623c35ef8a5739,JournalArticle,"Graph databases have received much attention as of late due to numerous applications in which data is naturally viewed as a graph; these include social networks, RDF and the Semantic Web, biological databases, and many others. There are many proposals for query languages for graph databases that mainly fall into two categories. One views graphs as a particular kind of relational data and uses traditional relational mechanisms for querying. The other concentrates on querying the topology of the graph. These approaches, however, lack the ability to combine data and topology, which would allow queries asking how data changes along paths and patterns enveloping it. In this article, we present a comprehensive study of languages that enable such combination of data and topology querying. These languages come in two flavors. The first follows the standard approach of path queries, which specify how labels of edges change along a path, but now we extend them with ways of specifying how both labels and data change. From the complexity point of view, the right type of formalisms are subclasses of register automata. These, however, are not well suited for querying. To overcome this, we develop several types of extended regular expressions to specify paths with data and study their querying power and complexity. The second approach adopts the popular XML language XPath and extends it from XML documents to graphs. Depending on the exact set of allowed features, we have a family of languages, and our study shows that it includes efficient and highly expressive formalisms for querying both the structure of the data and the data itself.",2016,105,63,"[2285196231.0, 2243335778.0, 2434366.0]",2285196231.0,Oslo,Journal of the ACM,"['data', 'graph', 'languages', 'data querying', 'Data Querying']"
7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2,"Storing, Tracking, and Querying Provenance in Linked Data",https://www.semanticscholar.org/paper/7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2,JournalArticle,"The proliferation of heterogeneous Linked Data on the Web poses new challenges to database systems. In particular, the capacity to store, track, and query provenance data is becoming a pivotal feature of modern triplestores. We present methods extending a native RDF store to efficiently handle the storage, tracking, and querying of provenance in RDF data. We describe a reliable and understandable specification of the way results were derived from the data and how particular pieces of data were combined to answer a query. Subsequently, we present techniques to tailor queries with provenance data. We empirically evaluate the presented methods and show that the overhead of storing and tracking provenance is acceptable. Finally, we show that tailoring a query with provenance information can also significantly improve the performance of query execution.",2017,28,29,"[2108533.0, 1393644275.0, 1733076.0, 1727784.0]",2108533.0,Stockholm,IEEE Transactions on Knowledge and Data Engineering,"['data', 'provenance', 'query', 'data querying', 'Data Querying']"
b3dbe1b460a3b66df1653508c9eed7dd51dee4d2,Lusail: A System for Querying Linked Data at Scale,https://www.semanticscholar.org/paper/b3dbe1b460a3b66df1653508c9eed7dd51dee4d2,JournalArticle,"The RDF data model allows publishing interlinked RDF datasets, where each dataset is independently maintained and is queryable via a SPARQL endpoint. Many applications would benefit from querying the resulting large, decentralized, geo-distributed graph through a federated SPARQL query processor. A crucial factor for good performance in federated query processing is pushing as much computation as possible to the local endpoints. Surprisingly, existing federated SPARQL engines are not effective at this task since they rely only on schema information. Consequently, they cause unnecessary data retrieval and communication, leading to poor scalability and response time. This paper addresses these limitations and presents Lusail, a scalable and efficient federated SPARQL system for querying large RDF graphs that are geo-distributed on different endpoints. Lusail uses a novel query rewriting algorithm to push computation to the local endpoints by relying on information about the RDF instances and not only the schema. The query rewriting algorithm has the additional advantage of exposing parallelism in query processing, which Lusail exploits through advanced scheduling at query run time. Our experiments on billions of triples of real and synthetic data show that Lusail outperforms state-of-the-art systems by orders of magnitude in terms of scalability and response time.",2017,24,11,"[145749443.0, 1801187.0, 2168047.0, 1704622.0, 2000187.0]",145749443.0,Chisinau,Proceedings of the VLDB Endowment,"['query', 'rdf', 'sparql', 'data querying', 'Data Querying']"
2374106a32169c07703599ff3f6f4b31e8067b89,Querying biomedical Linked Data with natural language questions,https://www.semanticscholar.org/paper/2374106a32169c07703599ff3f6f4b31e8067b89,JournalArticle,"Recent and intensive research in the biomedical area enabled to accumulate and disseminate biomedical knowledge through various knowledge bases increasingly available on the Web. The exploitation of this knowledge requires to create links between these bases and to use them jointly. Linked Data, the SPARQL language and interfaces in natural language question answering provide interesting solutions for querying such knowledge bases. However, while using biomedical Linked Data is crucial, life-science researchers may have difficulties using the SPARQL language. Interfaces based on natural language question answering are recognized to be suitable for querying knowledge bases. In this paper, we propose a method for translating natural language questions into SPARQL queries. We use Natural Language Processing tools, semantic resources and RDF triple descriptions. We designed a four-step method which allows to linguistically and semantically annotate questions, to perform an abstraction of these questions, then to build a representation of the SPARQL queries, and finally to generate the queries. The method is designed on 50 questions over three biomedical knowledge bases used in the task 2 of the QALD-4 challenge framework and evaluated on 27 new questions. It achieves good performance with 0.78 F-measure on the test set. The method for translating questions into SPARQL queries is implemented as a Perl module and is available at http://search.cpan.org/~thhamon/ RDF-NLP-SPARQLQuery/.",2017,24,8,"[1690504.0, 1732198.0, 1791253.0]",1690504.0,San Marino,Semantic Web,"['knowledge', 'language', 'questions', 'data querying', 'Data Querying']"
da3f33d858586d24cb265e79eb54f3746e998f57,Querying industrial stream-temporal data: An ontology-based visual approach,https://www.semanticscholar.org/paper/da3f33d858586d24cb265e79eb54f3746e998f57,JournalArticle,"An increasing number of sensors are being deployed in business-critical environments, systems, and equipment; and stream a vast amount of data. The operational efficiency and effectiveness of business processes rely on domain experts’ agility in interpreting data into actionable business information. A domain expert has extensive domain knowledge but not necessarily skills and knowledge on databases and formal query languages. Therefore, centralised approaches are often preferred. These require IT experts to translate the information needs of domain experts into extract-transform-load (ETL) processes in order to extract and integrate data and then let domain experts apply predefined analytics. Since such a workflow is too time intensive, heavy-weight and inflexible given the high volume and velocity of data, domain experts need to extract and analyse the data of interest directly. Ontologies, i.e., semantically rich conceptual domain models, present an intelligible solution by describing the domain of interest on a higher level of abstraction closer to the reality. Moreover, recent ontology-based data access (OBDA) technologies enable end users to formulate their information needs into queries using a set of terms defined in an ontology. Ontological queries could then be translated into SQL or some other database query languages, and executed over the data in its original place and format automatically. To this end, this article reports an ontology-based visual query system (VQS), namely OptiqueVQS, how it is extended for a stream-temporal query language called STARQL, a user experiment with the domain experts at Siemens AG, and STARQL’s query answering performance over a proof of concept implementation for PostgreSQL.",2017,30,9,"[153750193.0, 144911780.0, 1783242.0, 1402158435.0, 1697928.0, 3342377.0, 2028218.0, 144105942.0]",153750193.0,Sarajevo,Journal of Ambient Intelligence and Smart Environments,"['domain', 'data', 'experts', 'data querying', 'Data Querying']"
5b34752817bc0d6aa96466dabcbc24a83dd071fe,SPARQLByE: Querying RDF data by example,https://www.semanticscholar.org/paper/5b34752817bc0d6aa96466dabcbc24a83dd071fe,JournalArticle,"Semantic Web technologies such as RDF and its query language, SPARQL, offer the possibility of opening up the use of public datasets to a great variety of ordinary users. But a key obstacle to the use of open data is the unfamiliarity of users with the structure of data or with SPARQL. To deal with these issues, we introduce a system for querying RDF data by example. At its core is a technique for reverse-engineering SPARQL queries by example. We demonstrate how reverse engineering along with other techniques, such as query relaxation, enables our system, SPARQLByE, to guide users who are unfamiliar with both the dataset and with SPARQL to the desired query and result set.",2016,48,9,"[2899295.0, 144658846.0, 1750856.0]",2899295.0,Bern,Proceedings of the VLDB Endowment,"['sparql', 'query', 'users', 'data querying', 'Data Querying']"
eb76aabdb294814d36b11f1d961f3a0fa2d04e57,Integrated Querying of SQL database data and S3 data in Amazon Redshift,https://www.semanticscholar.org/paper/eb76aabdb294814d36b11f1d961f3a0fa2d04e57,JournalArticle,"Amazon Redshift features integrated, in-place access to data residing in a relational database and to data residing in the Amazon S3 object storage. In this paper we discuss associated query planning and processing aspects. Redshift plays the role of the integration query processor, in addition to the usual processor of queries over Redshift tables. In particular, during query execution, every compute node of a Redshift cluster issues (sub)queries over S3 objects, employing a novel multi-tenant (sub)query execution layer, called Amazon Redshift Spectrum, and merges/joins the results in an streaming and parallel fashion. The Spectrum layer offers massive scalability, with independent scaling of storage and computation. Redshifts optimizer determines how to minimize the amount of data scanned by Spectrum, the amount of data communicated to Redshift and the number of Spectrum nodes to be used. In particular, Redshifts query processor dynamically prunes partitions and pushes subqueries to Spectrum, recognizing which objects are relevant and restricting the subqueries to a subset of SQL that is amenable to Spectrums massively scalable processing. Furthermore, Redshift employs novel dynamic optimization techniques in order to formulate the subqueries. One such technique is a variant of semijoin reduction, which is combined in Redshift with join-aggregation query rewritings. Other optimizations and rewrit-ings relate to the memory footprint of query processing in Spectrum, and the SQL functionality that it is being supported by the Spectrum layer. The users of Redshift use the same SQL syntax to access scalar Redshift and external tables.",2018,14,41,"[39527078.0, 2062848.0, 2110760082.0, 12421844.0, 2754078.0, 1786049.0, 1835963.0]",39527078.0,Valletta,IEEE Data Engineering Bulletin,"['redshift', 'query', 'spectrum', 'data querying', 'Data Querying']"
e4788ee4f5e90c6f42cedc5116acd2d6475c3180,QUIS: InSitu Heterogeneous Data Source Querying,https://www.semanticscholar.org/paper/e4788ee4f5e90c6f42cedc5116acd2d6475c3180,JournalArticle,"Existing data integration frameworks are poorly suited for the special requirements of scientists. To answer a specific research question, often, excerpts of data from different sources need to be integrated. The relevant parts and the set of underlying sources may differ from query to query. The analyses also oftentimes involve frequently changing data and exploratory querying. Additionally, The data sources not only store data in different formats, but also provide inconsistent data access functionality. The classic Extract-Transform-Load (ETL) approach seems too complex and time-consuming and does not fit well with interest and expertise of the scientists. 
 
With QUIS (QUery In-Situ), we provide a solution for this problem. QUIS is an open source heterogeneous in-situ data querying system. It utilizes a federated query virtualization approach that is built upon plugged-in adapters. QUIS takes a user query and transforms appropriate portions of it into the corresponding computation model on individual data sources and executes it. It complements the segments of the query that the target data sources can not execute. Hence, it guarantees full syntax and semantic support for its language on all data sources. QUIS's in-situ querying facility almost eliminates the time to prepare the data while maintaining a competitive performance and steady scalability. 
 
The present demonstration illustrates interesting features of the system: virtual Schemas, heterogeneous joins, and visual query results. We provide a realistic data processing scenario to examine the system's features. Users can interact with QUIS using its desktop workbench, command line interface, or from any R client including RStudio Server.",2017,9,10,"[2840853.0, 1389035902.0, 145531067.0]",2840853.0,Zagreb,Proceedings of the VLDB Endowment,"['data', 'query', 'sources', 'data querying', 'Data Querying']"
5e4141d4be74fe673f092036822f1312103b9294,Mixed-instance querying: a lightweight integration architecture for data journalism,https://www.semanticscholar.org/paper/5e4141d4be74fe673f092036822f1312103b9294,JournalArticle,"As the world's affairs get increasingly more digital, timely production and consumption of news require to efficiently and quickly exploit heterogeneous data sources. Discussions with journalists revealed that content management tools currently at their disposal fall very short of expectations. We demonstrate Tatooine, a lightweight data integration prototype, which allows to quickly set up integration queries across (very) heterogeneous data sources, capitalizing on the many data links (joins) available in this application domain. Our demonstration is based on scenarios we study in collaboration with Le Monde, France's major newspaper.",2016,38,9,"[3361826.0, 152612731.0, 2772242.0, 2273645.0, 2086632521.0, 1739309.0, 1412353532.0, 3099593.0, 2404850.0, 2812731.0]",3361826.0,Bucharest,Proceedings of the VLDB Endowment,"['data', 'sources', 'integration', 'data querying', 'Data Querying']"
a620ea8654f86b282e0d2c1fbc6616b90ca45bd1,"A Data Analytic Algorithm for Managing, Querying, and Processing Uncertain Big Data in Cloud Environments",https://www.semanticscholar.org/paper/a620ea8654f86b282e0d2c1fbc6616b90ca45bd1,JournalArticle,"Big data are everywhere as high volumes of varieties of valuable precise and uncertain data can be easily collected or generated at high velocity in various real-life applications. Embedded in these big data are rich sets of useful information and knowledge. To mine these big data and to discover useful information and knowledge, we present a data analytic algorithm in this article. Our algorithm manages, queries, and processes uncertain big data in cloud environments. More specifically, it manages transactions of uncertain big data, allows users to query these big data by specifying constraints expressing their interests, and processes the user-specified constraints to discover useful information and knowledge from the uncertain big data. As each item in every transaction in these uncertain big data is associated with an existential probability value expressing the likelihood of that item to be present in a particular transaction, computation could be intensive. Our algorithm uses the MapReduce model on a cloud environment for effective data analytics on these uncertain big data. Experimental results show the effectiveness of our data analytic algorithm for managing, querying, and processing uncertain big data in cloud environments.",2015,45,8,"[2069981213.0, 1726081.0]",2069981213.0,Minsk,Algorithms,"['data', 'algorithm', 'information', 'data querying', 'Data Querying']"
d280cf82a6144b3e168840802b1a8a14d4eaccb9,"A Hierarchical Tensor-Based Approach to Compressing, Updating and Querying Geospatial Data",https://www.semanticscholar.org/paper/d280cf82a6144b3e168840802b1a8a14d4eaccb9,JournalArticle,"With the rapid development of data observation and model simulation in geoscience, spatial-temporal data have become increasingly multidimensional, massive and are consistently being updated. As a result, the integrated maintenance of these data is becoming a challenge. This paper presents a blocked hierarchical tensor representation within the split-and-merge paradigm for the compressed storage, continuously updating and data querying of multidimensional geospatial field data. The original multidimensional geospatial field data are split into small blocks according to their spatial-temporal references. These blocks are represented and compressed hierarchically, and then combined into a single hierarchical tree as the representation of original data. With a buffered binary tree data structure and corresponding optimized operation algorithms, the original multidimensional geospatial field data can be continuously compressed, appended, and queried. Data from the 20th Century Reanalysis Monthly Mean Composites are used to evaluate the performance of this approach. Compared to traditional methods, the new approach is shown to retain the quality of the original data with much lower storage costs and faster computational performance. The result suggests that the blocked hierarchical tensor representation provides an effective structure for integrated storage, presentation and computation of multidimensional geospatial field data.",2015,19,27,"[2100621.0, 2660835.0, 145773524.0, 98150490.0, 2931093.0, 145555027.0]",2100621.0,Oslo,IEEE Transactions on Knowledge and Data Engineering,"['data', 'field', 'representation', 'data querying', 'Data Querying']"
f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f,Price-Optimal Querying with Data APIs,https://www.semanticscholar.org/paper/f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f,JournalArticle,"Data is increasingly being purchased online in data markets and REST APIs have emerged as a favored method to acquire such data. Typically, sellers charge buyers based on how much data they purchase. In many scenarios, buyers need to make repeated calls to the seller's API. The challenge is then for buyers to keep track of the data they purchase and avoid purchasing the same data twice. In this paper, we propose lightweight modifications to data APIs to achieve optimal history-aware pricing so that buyers are only charged once for data that they have purchased and that has not been updated. The key idea behind our approach is the notion of refunds: buyers buy data as needed but have the ability to ask for refunds of data that they had already purchased before. We show that our techniques can provide significant data cost savings while reducing overheads by two orders of magnitude as compared to the state-of-the-art competing approaches.",2016,23,9,"[1758301.0, 1718134.0, 144823759.0]",1758301.0,Berlin,Proceedings of the VLDB Endowment,"['data', 'buyers', 'apis', 'data querying', 'Data Querying']"
5cdb31c5e3e0aa4cc2d10229793b4911f69fd78f,Ontology for Querying Heterogeneous Data Sources in Freight Transportation,https://www.semanticscholar.org/paper/5cdb31c5e3e0aa4cc2d10229793b4911f69fd78f,JournalArticle,"AbstractNavigating through multiple heterogeneous freight data sources to find the ones that are relevant to answering a question can be a challenging task when performed manually. It is highly dependent on the individual’s knowledge of all available data sources and the information contained in each one. Multiple factors contribute to freight data heterogeneity, including differences in data element definitions, level of disaggregation, and classification systems. This paper proposes a standardized knowledge representation of freight data sources using an ontology that both computer systems and domain experts can understand. The ontology is developed from a formal representation of data elements found to exist in freight data sources. The paper also presents a querying algorithm for searching through the ontology and determining relevant freight data sources that can be used for answering user queries. The proposed ontology and querying algorithm facilitate the querying and identification of relevant fre...",2016,10,30,"[51413028.0, 72448716.0, 1483964179.0]",51413028.0,Bucharest,Journal of computing in civil engineering,"['data', 'freight', 'sources', 'data querying', 'Data Querying']"
343500e0052eb1b683f32b00efbbd1331c94184a,Sapphire: Querying RDF Data Made Simple,https://www.semanticscholar.org/paper/343500e0052eb1b683f32b00efbbd1331c94184a,JournalArticle,"There is currently a large amount of publicly accessible structured data available as RDF data sets. For example, the Linked Open Data (LOD) cloud now consists of thousands of RDF data sets with over 30 billion triples, and the number and size of the data sets is continuously growing. Many of the data sets in the LOD cloud provide public SPARQL endpoints to allow issuing queries over them. These end-points enable users to retrieve data using precise and highly expressive SPARQL queries. However, in order to do so, the user must have sufficient knowledge about the data sets that she wishes to query, that is, the structure of data, the vocabulary used within the data set, the exact values of literals, their data types, etc. Thus, while SPARQL is powerful, it is not easy to use. An alternative to SPARQL that does not require as much prior knowledge of the data is some form of keyword search over the structured data. Keyword search queries are easy to use, but inherently ambiguous in describing structured queries. 
 
This demonstration introduces Sapphire, a system for querying RDF data that strikes a middle ground between ambiguous keyword search and difficult-to-use SPARQL. Our system does not replace either, but utilizes both where they are most effective. Sapphire helps the user construct expressive SPARQL queries that represent her information needs without requiring detailed knowledge about the queried data sets. These queries are then executed over public SPARQL endpoints from the LOD cloud. Sapphire guides the user in the query writing process by showing suggestions of query terms based on the queried data, and by recommending changes to the query based on a predictive user model.",2016,14,9,"[90981528.0, 2073283133.0, 1704622.0, 145580839.0]",90981528.0,Skopje,Proceedings of the VLDB Endowment,"['data', 'sparql', 'queries', 'data querying', 'Data Querying']"
dd2deed2ce6e110236a1280db765fa02c7488eb1,Indexing Fuzzy Spatiotemporal Data for Efficient Querying: A Meteorological Application,https://www.semanticscholar.org/paper/dd2deed2ce6e110236a1280db765fa02c7488eb1,JournalArticle,"Spatiotemporal data, in particular fuzzy and complex spatial objects representing geographic entities and relations, is a topic of great importance in geographic information systems and environmental data management systems. For database researchers, modeling and designing a database of fuzzy spatiotemporal data and querying such a database efficiently have been challenging issues due to complex spatial features and uncertainty involved. This paper presents an integrated approach to modeling, indexing, and efficiently querying spatiotemporal data related to fuzzy spatial and complex objects and spatial relations. As our case study, we design and implement a meteorological database application that involves fuzzy spatial and complex objects, and a spatiotemporal index structure, and supports various types of spatial queries including fuzzy spatiotemporal queries. Our implementation is based on an intelligent database system architecture that combines a fuzzy object-oriented database with a fuzzy knowledge base.",2015,10,23,"[3262907.0, 144502489.0, 1712420.0]",3262907.0,Sofia,IEEE transactions on fuzzy systems,"['database', 'data', 'relations', 'data querying', 'Data Querying']"
8ab93bee04cd5bdbd96002b2e325d02f61ba695a,TUSQ: Targeted High-Utility Sequence Querying,https://www.semanticscholar.org/paper/8ab93bee04cd5bdbd96002b2e325d02f61ba695a,JournalArticle,"Significant efforts have been expended in the research and development of a database management system (DBMS) that has a wide range of applications for managing an enormous collection of multisource, heterogeneous, complex, or growing data. Besides the primary function (i.e., create, delete, and update), a practical and impeccable DBMS can interact with users through information selection, that is, querying with their targets. Previous querying algorithms, such as frequent itemset querying and sequential pattern querying (SPQ) have focused on the measurement of frequency, which does not involve the concept of utility, which is helpful for users to discover more informative patterns. To apply the querying technology for wider applications, we incorporate utility into target-oriented SPQ and formulate the task of targeted utility-oriented sequence querying. To address the proposed problem, we develop a novel algorithm, namely targeted high-utility sequence querying (TUSQ), based on two novel upper bounds (suffix remain utility and terminated descendants utility) as well as a vertical last instance table. For further efficiency, TUSQ relies on a projection technology utilizing a compact data structure called the targeted chain. An extensive experimental study conducted on several real and synthetic datasets shows that the proposed algorithm outperformed the designed baseline algorithm in terms of runtime, memory consumption, and candidate filtering.",2021,16,9,"[3114478.0, 35061398.0, 2061514417.0, 3045042.0, 2111880710.0, 144019071.0]",3114478.0,Podgorica,IEEE Transactions on Big Data,"['utility', 'algorithm', 'applications', 'data querying', 'Data Querying']"
1a37223175138bc1aa53b425ea2fdd0b382405a5,Massively Distributed Time Series Indexing and Querying,https://www.semanticscholar.org/paper/1a37223175138bc1aa53b425ea2fdd0b382405a5,JournalArticle,"Indexing is crucial for many data mining tasks that rely on efficient and effective similarity query processing. Consequently, indexing large volumes of time series, along with high performance similarity query processing, have became topics of high interest. For many applications across diverse domains though, the amount of data to be processed might be intractable for a single machine, making existing centralized indexing solutions inefficient. We propose a parallel indexing solution that gracefully scales to billions of time series, and a parallel query processing strategy that, given a batch of queries, efficiently exploits the index. Our experiments, on both synthetic and real world data, illustrate that our index creation algorithm works on four billion time series in less than five hours, while the state of the art centralized algorithms do not scale and have their limit on 1 billion time series, where they need more than five days. Also, our distributed querying algorithm is able to efficiently process millions of queries over collections of billions of time series, thanks to an effective load balancing mechanism.",2020,40,32,"[32212787.0, 1709023.0, 2028901.0, 1725167.0]",32212787.0,Tallinn,IEEE Transactions on Knowledge and Data Engineering,"['time', 'series', 'data', 'data querying', 'Data Querying']"
a18f02e5c24e1f924aea268dd343bbdea234f2bb,"ChIP-Atlas 2021 update: a data-mining suite for exploring epigenomic landscapes by fully integrating ChIP-seq, ATAC-seq and Bisulfite-seq data",https://www.semanticscholar.org/paper/a18f02e5c24e1f924aea268dd343bbdea234f2bb,JournalArticle,"Abstract ChIP-Atlas (https://chip-atlas.org) is a web service providing both GUI- and API-based data-mining tools to reveal the architecture of the transcription regulatory landscape. ChIP-Atlas is powered by comprehensively integrating all data sets from high-throughput ChIP-seq and DNase-seq, a method for profiling chromatin regions accessible to DNase. In this update, we further collected all the ATAC-seq and whole-genome bisulfite-seq data for six model organisms (human, mouse, rat, fruit fly, nematode, and budding yeast) with the latest genome assemblies. These together with ChIP-seq data can be visualized with the Peak Browser tool and a genome browser to explore the epigenomic landscape of a query genomic locus, such as its chromatin accessibility, DNA methylation status, and protein–genome interactions. This epigenomic landscape can also be characterized for multiple genes and genomic loci by querying with the Enrichment Analysis tool, which, for example, revealed that inflammatory bowel disease-associated SNPs are the most significantly hypo-methylated in neutrophils. Therefore, ChIP-Atlas provides a panoramic view of the whole epigenomic landscape. All datasets are free to download via either a simple button on the web page or an API.",2022,91,50,"[2151226838.0, 5548191.0, 49030422.0, 3960565.0]",2151226838.0,Vaduz,Nucleic Acids Res.,"['landscape', 'chipatlas', 'data', 'data querying', 'Data Querying']"
6db0f8d396371078590faa7b34ae2e0e1b154a60,Building and Querying an Enterprise Knowledge Graph,https://www.semanticscholar.org/paper/6db0f8d396371078590faa7b34ae2e0e1b154a60,JournalArticle,"Information providers are faced with a critical challenge to process, retrieve and present information to their users in order to satisfy their complex information needs, because data has been increasing in an unprecedented manner, coming from diverse sources, and covering a variety of domains in heterogeneous formats. In this paper, we present Thomson Reuters’ effort in developing a family of services for building and querying an enterprise knowledge graph in order to address this challenge. We first acquire data from various sources via different approaches. Furthermore, we mine useful information from the data by adopting a variety of techniques, including Named Entity Recognition and Relation Extraction; such mined information is further integrated with existing structured data (e.g., via Entity Linking techniques) in order to obtain relatively comprehensive descriptions of the entities. By modeling the data as an RDF graph model, we enable easy data management and the embedding of rich semantics in our data. Finally, in order to facilitate the querying of this mined and integrated data, i.e., the knowledge graph, we propose TR Discover, a natural language interface that allows users to ask questions of our knowledge graph in their own words; such natural language questions are then translated into executable queries for answer retrieval. We evaluate our services, i.e., named entity recognition, relation extraction, entity linking and natural language interface, on real-world datasets, and demonstrate and discuss their practicability and limitations.",2019,36,12,"[34695819.0, 52222067.0, 3159223.0, 1412465368.0, 37722032.0, 1414195584.0, 1414195614.0, 32244429.0, 1410506555.0, 2135403.0, 1403903208.0, 5486617.0, 2061706386.0]",34695819.0,Luxembourg,IEEE Transactions on Services Computing,"['data', 'information', 'graph', 'data querying', 'Data Querying']"
8b7b2e88207fc2dd849f5d83c9b6e890f0174abc,Expressive Languages for Querying the Semantic Web,https://www.semanticscholar.org/paper/8b7b2e88207fc2dd849f5d83c9b6e890f0174abc,JournalArticle,"The problem of querying RDF data is a central issue for the development of the Semantic Web. The query language SPARQL has become the standard language for querying RDF since its W3C standardization in 2008. However, the 2008 version of this language missed some important functionalities: reasoning capabilities to deal with RDFS and OWL vocabularies, navigational capabilities to exploit the graph structure of RDF data, and a general form of recursion much needed to express some natural queries. To overcome these limitations, a new version of SPARQL, called SPARQL 1.1, was released in 2013, which includes entailment regimes for RDFS and OWL vocabularies, and a mechanism to express navigation patterns through regular expressions. Unfortunately, there are a number of useful navigation patterns that cannot be expressed in SPARQL 1.1, and the language lacks a general mechanism to express recursive queries. To the best of our knowledge, no efficient RDF query language that combines the above functionalities is known. It is the aim of this work to fill this gap. To this end, we focus on a core fragment of the OWL 2 QL profile of OWL 2 and show that every SPARQL query enriched with the above features can be naturally translated into a query expressed in a language that is based on an extension of Datalog, which allows for value invention and stratified negation. However, the query evaluation problem for this language is highly intractable, which is not surprising since it is expressive enough to encode some inherently hard queries. We identify a natural fragment of it, and we show it to be tractable and powerful enough to define SPARQL queries enhanced with the desired functionalities.",2018,68,43,"[144658846.0, 1684745.0, 1771740.0]",144658846.0,Luxembourg,TODS,"['language', 'sparql', 'query', 'data querying', 'Data Querying']"
88a724083b2cfcc096448c28e6973c8f761ee463,S2RDF: RDF Querying with SPARQL on Spark,https://www.semanticscholar.org/paper/88a724083b2cfcc096448c28e6973c8f761ee463,JournalArticle,"RDF has become very popular for semantic data publishing due to its flexible and universal graph-like data model. Thus, the ever-increasing size of RDF data collections raises the need for scalable distributed approaches. We endorse the usage of existing infrastructures for Big Data processing like Hadoop for this purpose. Yet, SPARQL query performance is a major challenge as Hadoop is not intentionally designed for RDF processing. Existing approaches often favor certain query pattern shapes while performance drops significantly for other shapes. In this paper, we introduce a novel relational partitioning schema for RDF data called ExtVP that uses a semi-join based preprocessing, akin to the concept of Join Indices in relational databases, to efficiently minimize query input size regardless of its pattern shape and diameter. Our prototype system S2RDF is built on top of Spark and uses SQL to execute SPARQL queries over ExtVP. We demonstrate its superior performance in comparison to state of the art SPARQL-on-Hadoop approaches.",2015,194,abs/1512.07021,"[2131714.0, 1403683697.0, 2772470.0, 1809410.0]",2131714.0,Vienna,Proceedings of the VLDB Endowment,"['data', 'rdf', 'approaches', 'data querying', 'Data Querying']"
f3eb8b6b836c0ef419e1fa565476e6892c8717ff,CoV-Spectrum: analysis of globally shared SARS-CoV-2 data to identify and characterize new variants,https://www.semanticscholar.org/paper/f3eb8b6b836c0ef419e1fa565476e6892c8717ff,JournalArticle,"Abstract Summary The CoV-Spectrum website supports the identification of new SARS-CoV-2 variants of concern and the tracking of known variants. Its flexible amino acid and nucleotide mutation search allows querying of variants before they are designated by a lineage nomenclature system. The platform brings together SARS-CoV-2 data from different sources and applies analyses. Results include the proportion of different variants over time, their demographic and geographic distributions, common mutations, hospitalization and mortality probabilities, estimates for transmission fitness advantage and insights obtained from wastewater samples. Availability and implementation CoV-Spectrum is available at https://cov-spectrum.org. The code is released under the GPL-3.0 license at https://github.com/cevo-public/cov-spectrum-website.",2021,186,38,"[4385113.0, 11684279.0, 2112425054.0, 2020052361.0, 2057542266.0, 2142494085.0, 2580290.0]",4385113.0,Berlin,Bioinform.,"['variants', 'covspectrum', 'sarscov2', 'data querying', 'Data Querying']"
519ffd9744de5638d8c950090f065923e0793a93,Reachability Querying: Can It Be Even Faster?,https://www.semanticscholar.org/paper/519ffd9744de5638d8c950090f065923e0793a93,JournalArticle,"As an important graph operator, reachability query has been extensively studied over decades, which is to check whether a vertex can reach another vertex over a large directed graph <inline-formula><tex-math notation=""LaTeX"">$G$ </tex-math><alternatives><inline-graphic xlink:href=""zhu-ieq1-2631160.gif""/></alternatives></inline-formula> with <inline-formula><tex-math notation=""LaTeX"">$n$</tex-math><alternatives> <inline-graphic xlink:href=""zhu-ieq2-2631160.gif""/></alternatives></inline-formula> vertices and <inline-formula> <tex-math notation=""LaTeX"">$m$</tex-math><alternatives><inline-graphic xlink:href=""zhu-ieq3-2631160.gif""/> </alternatives></inline-formula> edges. The efforts made in the reported studies have greatly improved the query time of answering reachability queries online, while reducing the offline index construction time to construct an index with a reasonable size given the approach taken, where an entry in an index for a vertex is called a label of the vertex. Among all the work, the recent development of <italic>IP</italic> (Independent Permutation) employs randomness using <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives> <inline-graphic xlink:href=""zhu-ieq4-2631160.gif""/></alternatives></inline-formula>-min-wise independent permutations to process reachability queries, and shows the advantages for both query time and index construction time. In this paper, we propose a new Bloom filter Labeling, denoted as <italic>BFL</italic>. We show that the probability to answer reachability queries by <italic>BFL</italic> can be bounded, and <italic>BFL</italic> has high pruning power to answer more reachability queries directly. We give algorithms and analyze the pruning power of <italic>BFL</italic>. We conduct extensive studies using 19 large datasets. We show that <italic>BFL</italic> with an interval label performs best in the index construction time for all 19 cases, and performs best in query time for 16 out of 19 cases.",2017,74,29,"[2116956834.0, 2152202354.0, 2109393608.0, 144666776.0]",2116956834.0,Valletta,IEEE Transactions on Knowledge and Data Engineering,"['time', 'reachability', 'index', 'data querying', 'Data Querying']"
af13a92977d4f4dc5b28b13746d86111d42939e8,F1 Query: Declarative Querying at Scale,https://www.semanticscholar.org/paper/af13a92977d4f4dc5b28b13746d86111d42939e8,JournalArticle,"F1 Query is a stand-alone, federated query processing platform that executes SQL queries against data stored in different file-based formats as well as different storage systems at Google (e.g., Bigtable, Spanner, Google Spreadsheets, etc.). F1 Query eliminates the need to maintain the traditional distinction between different types of data processing workloads by simultaneously supporting: (i) OLTP-style point queries that affect only a few records; (ii) low-latency OLAP querying of large amounts of data; and (iii) large ETL pipelines. F1 Query has also significantly reduced the need for developing hard-coded data processing pipelines by enabling declarative queries integrated with custom business logic. F1 Query satisfies key requirements that are highly desirable within Google: (i) it provides a unified view over data that is fragmented and distributed over multiple data sources; (ii) it leverages datacenter resources for performant query processing with high throughput and low latency; (iii) it provides high scalability for large data sizes by increasing computational parallelism; and (iv) it is extensible and uses innovative approaches to integrate complex business logic in declarative query processing. This paper presents the end-to-end design of F1 Query. Evolved out of F1, the distributed database originally built to manage Google's advertising data, F1 Query has been in production for multiple years at Google and serves the querying needs of a large number of users and systems.",2018,36,11,"[2922886.0, 1958664.0, 2762838.0, 3001926.0, 1734332.0, 1410051210.0, 1726031123.0, 2083106.0, 2254406.0, 2709144.0, 1774311.0, 144317729.0, 2144495208.0, 2152986683.0, 153154328.0, 2112339870.0, 33857494.0, 2114309525.0, 89387275.0, 2090205657.0, 1414760032.0, 2111133570.0, 1772311.0, 2140961.0, 2324910.0, 2328103.0, 34842481.0, 2112558225.0, 145182862.0, 1410635059.0, 1799376.0, 1918781.0, 145204762.0, 143970078.0, 50179113.0, 1710223.0]",2922886.0,Zagreb,Proceedings of the VLDB Endowment,"['query', 'data', 'f1', 'data querying', 'Data Querying']"
0fa554d981809c5eb78956c779f75092c4f6c16b,psrqpy: a python interface for querying the ATNF pulsar catalogue,https://www.semanticscholar.org/paper/0fa554d981809c5eb78956c779f75092c4f6c16b,JournalArticle,"This Python module provides an interface for querying the Australia Telescope National Facility (ATNF) pulsar catalogue (Manchester et al. 2005). The intended users are astronomers wanting to extract data from the catalogue through a script rather than having to download and parse text tables output using the standard web interface. It allows users to access information, such as pulsar frequencies and sky locations, on all pulsars in the catalogue. Querying of the catalogue can easily be incorporated into Python scripts.",2018,20,3,[119136574.0],119136574.0,Vienna,Journal of Open Source Software,"['catalogue', 'interface', 'pulsar', 'data querying', 'Data Querying']"
0bca61986b8edeaf33018d0203b44110f2480110,haploR: an R package for querying web-based annotation tools,https://www.semanticscholar.org/paper/0bca61986b8edeaf33018d0203b44110f2480110,JournalArticle,"We developed haploR, an R package for querying web based genome annotation tools HaploReg and RegulomeDB. haploR gathers information in a data frame which is suitable for downstream bioinformatic analyses. This will facilitate post-genome wide association studies streamline analysis for rapid discovery and interpretation of genetic associations.",2017,33,6,"[29629766.0, 2183217.0, 2040119.0]",29629766.0,Andorra,F1000Research,"['haplor', 'r', 'package', 'data querying', 'Data Querying']"
819f2778eba0d4c9eea86307bedaaeed94dc751d,"Pathway Commons 2019 Update: integration, analysis and exploration of pathway data",https://www.semanticscholar.org/paper/819f2778eba0d4c9eea86307bedaaeed94dc751d,JournalArticle,"Abstract Pathway Commons (https://www.pathwaycommons.org) is an integrated resource of publicly available information about biological pathways including biochemical reactions, assembly of biomolecular complexes, transport and catalysis events and physical interactions involving proteins, DNA, RNA, and small molecules (e.g. metabolites and drug compounds). Data is collected from multiple providers in standard formats, including the Biological Pathway Exchange (BioPAX) language and the Proteomics Standards Initiative Molecular Interactions format, and then integrated. Pathway Commons provides biologists with (i) tools to search this comprehensive resource, (ii) a download site offering integrated bulk sets of pathway data (e.g. tables of interactions and gene sets), (iii) reusable software libraries for working with pathway information in several programming languages (Java, R, Python and Javascript) and (iv) a web service for programmatically querying the entire dataset. Visualization of pathways is supported using the Systems Biological Graphical Notation (SBGN). Pathway Commons currently contains data from 22 databases with 4794 detailed human biochemical processes (i.e. pathways) and ∼2.3 million interactions. To enhance the usability of this large resource for end-users, we develop and maintain interactive web applications and training materials that enable pathway exploration and advanced analysis.",2019,190,48,"[2485039.0, 2039003.0, 2804763.0, 6562624.0, 48455738.0, 73460467.0, 1724282.0, 8847603.0, 1384224631.0, 1384221174.0, 1382043577.0, 1384221131.0, 1384221034.0, 1381370960.0, 2061150843.0, 1725976794.0, 1381997561.0, 48293468.0, 35805107.0, 49248672.0, 34762869.0, 2276851.0, 2807929.0, 144937305.0, 144882390.0]",2485039.0,Copenhagen,Nucleic Acids Res.,"['pathway', 'interactions', 'commons', 'data querying', 'Data Querying']"
