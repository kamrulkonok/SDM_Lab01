field_query,paperId,title,url,publicationTypes,abstract,year,citationCount,journal_volume,authorIds,corresponding_author,venue,number_reviewers,reviewers,approved,comments,proceedings,edition,journal_name,keywords
NLP,29ddc1f43f28af7c846515e32cc167bc66886d0c,Parameter-Efficient Transfer Learning for NLP,https://www.semanticscholar.org/paper/29ddc1f43f28af7c846515e32cc167bc66886d0c,Conference,"Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.",2019,2453,,"[2815290.0, 1911881.0, 40569328.0, 68973833.0, 51985388.0, 2813347.0, 2809991.0, 1802148.0]",2815290.0,Nicosia,2,"[50052368.0, 115300694.0]",Y,"['In the videos, it seems like the robot could get a slightly better view if it took another couple of steps.', 'Algorithm 1 does not make clear the relationship between the model learned in step 2 and the algorithms in steps 4 to 6.']",International Conference on Machine Learning,19.0,,"['task', 'parameters', 'transfer', 'nlp', 'NLP']"
NLP,58ed1fbaabe027345f7bb3a6312d41c5aac63e22,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,https://www.semanticscholar.org/paper/58ed1fbaabe027345f7bb3a6312d41c5aac63e22,JournalArticle,"Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",2020,1686,abs/2005.11401,"[145222654.0, 3439053.0, 1716179427.0, 40052301.0, 2067091563.0, 39589154.0, 103131985.0, 35084211.0, 144105277.0, 2620211.0, 48662861.0, 1743722.0]",145222654.0,Moscow,3,"[2151900144.0, 46307329.0, 2542999.0]",Y,"['2. The experimental results are fairly weak compared to the other methods that also uses many layers.', '3. This paper lack original technical contribution from themselves.', 'The paper briefly mentions supervising attention models using such boxes, but it isn’t clear how bounding boxes for data points could be used.']",,,Neural Information Processing Systems,"['models', 'tasks', 'language', 'nlp', 'NLP']"
NLP,d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea,Energy and Policy Considerations for Deep Learning in NLP,https://www.semanticscholar.org/paper/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea,Conference,"Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",2019,2113,,"[2268272.0, 47079359.0, 143753639.0]",2268272.0,Bratislava,3,"[38608914.0, 2285816475.0, 2109716565.0]",Y,"['1. The authors claimed that this is the first end-to-end trainable hierarchical attention model, but there is a previous work that also addressed the similar task: Seo et al, Progressive Attention Networks for Visual Attribute Prediction, in Arxiv preprint:1606.02393, 2016', 'on the positive side: The paper is well written, quality and clarity of the work are good.', 'There are many typos (e.g., ""This advantage is also its difficulty"", ""Much previous work on language modeling has evaluated "", ""We focus in on the task"", and others) so the writing needs to be significantly improved for it to be a conference paper, preferably with some help from a native English speaker.']",Annual Meeting of the Association for Computational Linguistics,19.0,,"['nlp', 'hardware', 'networks', 'models', 'NLP']"
NLP,d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea,Energy and Policy Considerations for Deep Learning in NLP,https://www.semanticscholar.org/paper/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea,Conference,"Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",2019,2113,,"[2268272.0, 47079359.0, 143753639.0]",2268272.0,Bratislava,3,"[38608914.0, 2285816475.0, 2109716565.0]",Y,"['1. The authors claimed that this is the first end-to-end trainable hierarchical attention model, but there is a previous work that also addressed the similar task: Seo et al, Progressive Attention Networks for Visual Attribute Prediction, in Arxiv preprint:1606.02393, 2016', 'on the positive side: The paper is well written, quality and clarity of the work are good.', 'There are many typos (e.g., ""This advantage is also its difficulty"", ""Much previous work on language modeling has evaluated "", ""We focus in on the task"", and others) so the writing needs to be significantly improved for it to be a conference paper, preferably with some help from a native English speaker.']",Annual Meeting of the Association for Computational Linguistics,19.0,,"['hardware', 'nlp', 'networks', 'deep learning', 'NLP']"
Deep Learning,d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea,Energy and Policy Considerations for Deep Learning in NLP,https://www.semanticscholar.org/paper/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea,Conference,"Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",2019,2113,,"[2268272.0, 47079359.0, 143753639.0]",2268272.0,Bucharest,2,"[2420527.0, 3062937.0]",Y,"['The graphs plot number of noisy labels per clean label so a alpha of 100 would imply 1 right label and 100 noisy labels for total 101 labels.', 'Is this implying that the gradients wrt off-diagonal entries of the controller weight matrix are 0 under the diagonal initialization, hence the off-diagonal entries remain zero after learning?']",Annual Meeting of the Association for Computational Linguistics,19.0,,"['nlp', 'hardware', 'networks', 'models', 'Deep Learning']"
Deep Learning,d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea,Energy and Policy Considerations for Deep Learning in NLP,https://www.semanticscholar.org/paper/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea,Conference,"Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",2019,2113,,"[2268272.0, 47079359.0, 143753639.0]",2268272.0,Bucharest,2,"[2420527.0, 3062937.0]",Y,"['The graphs plot number of noisy labels per clean label so a alpha of 100 would imply 1 right label and 100 noisy labels for total 101 labels.', 'Is this implying that the gradients wrt off-diagonal entries of the controller weight matrix are 0 under the diagonal initialization, hence the off-diagonal entries remain zero after learning?']",Annual Meeting of the Association for Computational Linguistics,19.0,,"['hardware', 'nlp', 'networks', 'deep learning', 'Deep Learning']"
NLP,06d7cb8c8816360feb33c3367073e0ef66d7d0b0,Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks,https://www.semanticscholar.org/paper/06d7cb8c8816360feb33c3367073e0ef66d7d0b0,Conference,"How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions—training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.",2022,432,,"[1705260.0, 1817207.0, 1805993128.0, 2156538832.0, 2162779320.0, 1667817604.0, 2063938464.0, 2162779517.0, 2064353087.0, 1689466093.0, 2074097046.0, 8458211.0, 2162779839.0, 93841942.0, 9377739.0, 2110871234.0, 2158994272.0, 2162779709.0, 81331041.0, 50494955.0, 40879549.0, 1423660254.0, 1576655836.0, 2067056655.0, 2162781785.0, 39765564.0, 2159207824.0, 72254820.0, 51479145.0, 1380075136.0, 2112127876.0, 2150098899.0, 19255781.0, 2126503480.0, 2144058688.0, 2064619864.0, 1699545.0, 144365875.0, 2548384.0, 1783281.0]",1705260.0,Vienna,2,"[153475895.0, 1753223.0]",Y,"['Good contribution, paper needs to be made clearer This paper thoroughly analyzes an algorithmic task (determining if two points in a maze are connected, which requires BFS to solve) by constructing an explicit ConvNet solution and analytically deriving properties of the loss surface around this analytical solution.', 'Considering representative synthetic problems is a good idea, but it is not clear to me why this particular choice is useful for the purpose.']",Conference on Empirical Methods in Natural Language Processing,22.0,,"['instructions', 'nlp', 'models', 'tasks', 'NLP']"
NLP,5471114e37448bea2457b74894b1ecb92bbcfdf6,From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models,https://www.semanticscholar.org/paper/5471114e37448bea2457b74894b1ecb92bbcfdf6,Conference,"Language models (LMs) are pretrained on diverse data sources—news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness.",2023,72,,"[2114887261.0, 50487261.0, 2169159066.0, 2073587169.0]",2114887261.0,Andorra,3,"[47540245.0, 2113511266.0, 2152050780.0]",Y,"['The particular structural elements also induce certain inductive biases which make learning or generalization easier in many cases.', 'The manuscript mainly presents a cheap pruning algorithm for dense layers of DNNs.', 'The main novelty in this paper is that it uses the label as a third view of a multi-view model and make use of cross moments.']",Annual Meeting of the Association for Computational Linguistics,23.0,,"['lms', 'biases', 'data', 'nlp', 'NLP']"
NLP,13c4e5a6122f3fa2663f63e49537091da6532f35,Are NLP Models really able to Solve Simple Math Word Problems?,https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35,Conference,"The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered “solved” with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.",2021,385,,"[1443788809.0, 92954142.0, 144260125.0]",1443788809.0,Dublin,3,"[2805081.0, 2146671969.0, 2328103.0]",Y,"['If this is not the case for some reason, more detailed explanation is needed.', '6. Figure 4 seems a bit misleading.', 'This seems at odds with models, such as DRAW, evaluate the likelihood -- once at the end of the generative drawing process.']",North American Chapter of the Association for Computational Linguistics,21.0,,"['mwps', 'solvers', 'mwp', 'nlp', 'NLP']"
NLP,33ec7eb2168e37e3007d1059aa96b9a63254b4da,Beyond Accuracy: Behavioral Testing of NLP Models with CheckList,https://www.semanticscholar.org/paper/33ec7eb2168e37e3007d1059aa96b9a63254b4da,Conference,"Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",2020,866,,"[78846919.0, 35232494.0, 1730156.0, 34650964.0]",78846919.0,San Marino,3,"[2868254.0, 89843190.0, 1707355.0]",Y,"['The paper is based on the cyclical learning rates proposed by Smith (2015, 2017).', 'In Tables 7 and 8, the human beings agree with the LeNet in >= 58% of cases.', 'However, batch normalization only sees the variation in the activations given to it by a SPECIFIC set of weights.']",Annual Meeting of the Association for Computational Linguistics,20.0,,"['nlp', 'models', 'checklist', 'test', 'NLP']"
NLP,f4c4e148546089123f8da5db4fb246ab4062bd40,Evaluation of ChatGPT for NLP-based Mental Health Applications,https://www.semanticscholar.org/paper/f4c4e148546089123f8da5db4fb246ab4062bd40,JournalArticle,"Large language models (LLM) have been successful in several natural language understanding tasks and could be relevant for natural language processing (NLP)-based mental health application research. In this work, we report the performance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three text-based mental health classification tasks: stress detection (2-class classification), depression detection (2-class classification), and suicidality detection (5-class classification). We obtained annotated social media posts for the three classification tasks from public datasets. Then ChatGPT API classified the social media posts with an input prompt for classification. We obtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression detection, and suicidality detection, respectively. A baseline model that always predicted the dominant class resulted in F1 scores of 0.35, 0.60, and 0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a potential use of language models for mental health classification tasks.",2023,35,abs/2303.15727,[2120664.0],2120664.0,Reykjavik,3,"[46429989.0, 1891554.0, 144248374.0]",Y,"['Without experiments over other datasets and wall clock time results, it is hard to appreciate the significance of this improvement.', ""I don't find the reconstructions demonstrated particularly compelling (they are generally pretty different from the original input)."", ""It's possible some of the issues arise from the particular architectures they choose to investigate and demonstrate on (eg I have mostly seen ResNets in the context of CNNs but they analyze on FC topologies, the form of the loss, etc) but that's a guess and there are some further analysis in the supp material for these networks which I haven't looked at in detail.""]",,,arXiv.org,"['classification', 'detection', 'language', 'nlp', 'NLP']"
NLP,97906df07855b029b7aae7c2a1c6c5e8df1d531c,BERT Rediscovers the Classical NLP Pipeline,https://www.semanticscholar.org/paper/97906df07855b029b7aae7c2a1c6c5e8df1d531c,Conference,"Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",2019,1193,,"[6117577.0, 143790066.0, 2949185.0]",6117577.0,Stockholm,3,"[6413143.0, 51488437.0, 151213231.0]",Y,"['All components are trained with dense supervision (e.g. loop closure, ego motion with orientation-position, and the ground truth local overhead map).', 'Summary: This work is about model evaluation for molecule generation and design.', 'I found the formulation of the \\alpha to be non-intuitive and confusing at times.']",Annual Meeting of the Association for Computational Linguistics,19.0,,"['nlp', 'model', 'information', 'pipeline', 'NLP']"
NLP,d47a682723f710395454687319bb55635e653105,Language (Technology) is Power: A Critical Survey of “Bias” in NLP,https://www.semanticscholar.org/paper/d47a682723f710395454687319bb55635e653105,Conference,"We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process. We further find that these papers’ proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “bias” in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of “bias”---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements—and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",2020,838,,"[3422038.0, 2881033.0, 2065041692.0, 1831395.0]",3422038.0,Monaco,3,"[2108724347.0, 9162688.0, 31347453.0]",Y,"['Their abstract also claims to utilize a convex programming formulation.', 'Hence inferring a structured latent space is a challenge.', 'Follow up experiments extend the basic setup significantly.']",Annual Meeting of the Association for Computational Linguistics,20.0,,"['bias', 'nlp', 'systems', 'papers', 'NLP']"
NLP,c9b56cb026a38e39bb0228faac57accd6f65e6f7,"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP",https://www.semanticscholar.org/paper/c9b56cb026a38e39bb0228faac57accd6f65e6f7,Conference,"While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack’s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.",2020,522,,"[153769695.0, 1453652787.0, 1693182792.0, 1829303908.0, 2068347799.0, 121817403.0]",153769695.0,Vilnius,3,"[3010457.0, 1410115257.0, 48919600.0]",Y,"['It might be good to emphasize that you don’t train on the IWAE bound in any experiments.', 'But the current system is a great start.', 'When citing Shen et al., 2017, consider also mentioning the following: Controllable Invariance through Adversarial Feature Learning; Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, Graham Neubig; NIPS 2017; https://arxiv.org/abs/1705.11122 Response read -- thanks.']",Conference on Empirical Methods in Natural Language Processing,20.0,,"['attacks', 'nlp', 'textattack', 'code', 'NLP']"
NLP,0e141942fa265142f41a2a26eb17b6005d3af29e,The State and Fate of Linguistic Diversity and Inclusion in the NLP World,https://www.semanticscholar.org/paper/0e141942fa265142f41a2a26eb17b6005d3af29e,Conference,"Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the “language agnostic” status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.",2020,532,,"[47070483.0, 50074956.0, 2410839.0, 3086996.0, 143990839.0]",47070483.0,Chisinau,2,"[32830771.0, 2163313042.0]",Y,"['Do you get comparable inception scores?', 'The main signal I lack from reading the paper is whether the proposed model actually does better than a reasonable baseline.']",Annual Meeting of the Association for Computational Linguistics,20.0,,"['language', 'languages', 'technologies', 'nlp', 'NLP']"
NLP,579476d19566efc842929ea6bdd18ab760c8cfa2,Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?,https://www.semanticscholar.org/paper/579476d19566efc842929ea6bdd18ab760c8cfa2,Conference,"With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is “defined” by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.",2020,408,,"[41016275.0, 79775260.0]",41016275.0,Berlin,2,"[145201124.0, 2309967.0]",Y,"['Interesting set of ideas and direction, but lack of quantitative analysis supporting the results.', 'In this paper, the authors propose deep architecture that preserves mutual information between the input and the hidden representation and show that the loss of information can only occur at the final layer.']",Annual Meeting of the Association for Computational Linguistics,20.0,,"['faithfulness', 'interpretation', 'evaluation', 'nlp', 'NLP']"
NLP,4fcfe83c05402b5c5fb6e853082e74af6379d7f9,"Missing Information, Unresponsive Authors, Experimental Flaws: The Impossibility of Assessing the Reproducibility of Previous Human Evaluations in NLP",https://www.semanticscholar.org/paper/4fcfe83c05402b5c5fb6e853082e74af6379d7f9,Workshop,"We report our efforts in identifying a set of previous human evaluations in NLP that would be suitable for a coordinated study examining what makes human evaluations in NLP more/less reproducible. We present our results and findings, which include that just 13% of papers had (i) sufficiently low barriers to reproduction, and (ii) enough obtainable information, to be considered for reproduction, and that all but one of the experiments we selected for reproduction was discovered to have flaws that made the meaningfulness of conducting a reproduction questionable. As a result, we had to change our coordinated study design from a reproduce approach to a standardise-then-reproduce-twice approach. Our overall (negative) finding that the great majority of human evaluations in NLP is not repeatable and/or not reproducible and/or too flawed to justify reproduction, paints a dire picture, but presents an opportunity for a rethink about how to design and report human evaluations in NLP.",2023,20,,"[41052836.0, 2122535749.0, 2113922820.0, 17038002.0, 2151319597.0, 9364658.0, 3159752.0, 2648584.0, 40684993.0, 10708829.0, 72115354.0, 2544049.0, 2620186.0, 1720986506.0, 1700894.0, 2921637.0, 2215866480.0, 2022288.0, 2165661802.0, 119804885.0, 1380281888.0, 3201315.0, 66376493.0, 50521235.0, 3192572.0, 2142159995.0, 2221260.0, 2921990.0, 2742475.0, 2326758.0, 2201327940.0, 1681799.0, 2150388522.0, 2086973507.0, 2065048323.0, 9714242.0, 9092408.0, 145148787.0, 2143919864.0]",41052836.0,Reykjavik,3,"[2152209915.0, 40497400.0, 145465286.0]",Y,"['A lookup table storing the training data generates samples containing objects and perfect details, but obviously has not learned anything about either objects or the low-level statistics of natural images.', 'I would like to see the same spectra included.', 'It it thus hard to properly evaluate your method against other proposed methods.']",First Workshop on Insights from Negative Results in NLP,23.0,,"['reproduction', 'evaluations', 'study', 'nlp', 'NLP']"
NLP,03532123ccffae8d411264320e8a5ae2b6eddea0,Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP,https://www.semanticscholar.org/paper/03532123ccffae8d411264320e8a5ae2b6eddea0,JournalArticle,"Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple""retrieve-then-read""pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp",2022,124,abs/2212.14024,"[144112155.0, 50818255.0, 32551341.0, 145385471.0, 145419642.0, 144922861.0, 143834867.0]",144112155.0,Vilnius,2,"[2931093.0, 3055912.0]",Y,"['Some reasons below: * There are no specific results on properties of the divergences, or axioms that justify them.', 'Revision of the review in light of the author response: The authors have adequately addressed my main remarks, and while doing so have improved both the positioning of the paper amongst relevant literature and the somewhat limited empirical comparisons.']",,,arXiv.org,"['dsp', 'rm', 'incontext', 'nlp', 'NLP']"
NLP,77a096d80eb4dd4ccd103d1660c5a5498f7d026b,Dynabench: Rethinking Benchmarking in NLP,https://www.semanticscholar.org/paper/77a096d80eb4dd4ccd103d1660c5a5498f7d026b,Conference,"We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",2021,256,,"[1743722.0, 153408953.0, 40383658.0, 9264826.0, 80833908.0, 47039337.0, 2737827.0, 119869488.0, 1422035486.0, 1736107.0, 1500242049.0, 48662861.0, 2138053020.0, 1918552.0, 3422908.0, 143977268.0, 144922861.0, 2110032535.0]",1743722.0,Dublin,2,"[2729273.0, 82614785.0]",Y,"['I feel that the authors should give a more prominent disclaimer to potential users of the test.', ""If so, doesn't that correspond to evaluating the model under a different generative assumption?""]",North American Chapter of the Association for Computational Linguistics,21.0,,"['model', 'creation', 'platform', 'nlp', 'NLP']"
NLP,087dd95e13efd47aef2a6582e6801b39fc0f83d8,ERASER: A Benchmark to Evaluate Rationalized NLP Models,https://www.semanticscholar.org/paper/087dd95e13efd47aef2a6582e6801b39fc0f83d8,Conference,"State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",2019,473,,"[48727916.0, 49837811.0, 8937909.0, 51172373.0, 2228109.0, 2166511.0, 1912476.0]",48727916.0,Lisbon,2,"[41020222.0, 2066229431.0]",Y,"['I welcome and are grateful for any theory in the area.', 'Indeed, the authors have commented: ""AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures.""']",Annual Meeting of the Association for Computational Linguistics,19.0,,"['models', 'benchmark', 'rationales', 'nlp', 'NLP']"
NLP,a747e8f2659df479c0092301b9658fc582423df1,"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia",https://www.semanticscholar.org/paper/a747e8f2659df479c0092301b9658fc582423df1,Conference,"NLP research is impeded by a lack of resources and awareness of the challenges presented by underrepresented languages and dialects. Focusing on the languages spoken in Indonesia, the second most linguistically diverse and the fourth most populous nation of the world, we provide an overview of the current state of NLP research for Indonesia’s 700+ languages. We highlight challenges in Indonesian NLP and how these affect the performance of current NLP systems. Finally, we provide general recommendations to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.",2022,56,,"[8129718.0, 9162688.0, 2789148.0, 66986482.0, 2279712392.0, 1935324.0, 46199596.0, 35722593.0, 2368148.0, 145465286.0, 1800564.0, 2124014463.0]",8129718.0,Oslo,2,"[5840536.0, 47224454.0]",Y,"['It would be nice to provide some sort of analysis of it, even an empirical one.', 'Moreover, the authors proposed updating the parameter \\theta of the generator g_\\theta.']",Annual Meeting of the Association for Computational Linguistics,22.0,,"['nlp', 'languages', 'indonesia', 'research', 'NLP']"
NLP,598231eb906b183f7a2a408ef4536127e11e3de9,Challenges and Strategies in Cross-Cultural NLP,https://www.semanticscholar.org/paper/598231eb906b183f7a2a408ef4536127e11e3de9,Conference,"Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.",2022,72,,"[2064295987.0, 37922370.0, 49568895.0, 3295381.0, 30671790.0, 6547490.0, 83574123.0, 2093582149.0, 2125376289.0, 1717462692.0, 50110151.0, 82259306.0, 1660797358.0, 1700187.0]",2064295987.0,Podgorica,2,"[1803520.0, 2043402.0]",Y,"['Are the test environments sufficiently different from the training ones?', 'The model is based on both source/target syntax trees and performs an attentional encoder-decoder style network over the tree structure.']",Annual Meeting of the Association for Computational Linguistics,22.0,,"['nlp', 'language', 'efforts', 'speakers', 'NLP']"
NLP,3a7bbc46795929f0eace82b64c44c92a48682fb5,FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP,https://www.semanticscholar.org/paper/3a7bbc46795929f0eace82b64c44c92a48682fb5,Conference,"We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to “mix and match” various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a “model zoo” of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",2019,679,,"[2403712.0, 2077245166.0, 2725328.0, 4565995.0, 134757625.0, 2742129.0]",2403712.0,Warsaw,3,"[143651788.0, 2243348413.0, 5486617.0]",Y,"['The fact that the proposed technique is simple yet yields such speedups is encouraging.', 'In light of this, I think it is reasonable to ignore the fact that the proposed initialization does not provide benefits on Penn Treebank and text8.', 'The motivation is difficult to grasp and the contributions do not seem compelling.']",North American Chapter of the Association for Computational Linguistics,19.0,,"['framework', 'nlp', 'models', 'training', 'NLP']"
NLP,ce9ca56036307217ea565644d3d3bd74b879e045,Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP,https://www.semanticscholar.org/paper/ce9ca56036307217ea565644d3d3bd74b879e045,JournalArticle,"Abstract ⚠ This paper contains prompts and model outputs that are offensive in nature. When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model’s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1",2021,244,9,"[32246932.0, 40941940.0, 144418438.0]",32246932.0,San Marino,2,"[2220637583.0, 2150277971.0]",Y,"['* A thorough discussion on mixing in feature space, as well as a baseline which mizes in feature space.', 'A tensor compiler (as opposed to a DSL inside a general purpose language) The success of Deep Learning is, in no small part, due the development of libraries and frameworks which have made building novel models much easier, faster and less error prone and also make taking advantage of modern hardware (such as GPUs) more accessible.']",,,Transactions of the Association for Computational Linguistics,"['language', 'models', 'model', 'nlp', 'NLP']"
NLP,be7cb8f79bc018e57467168fc0c7f8ad59bba04f,Adaptive Testing and Debugging of NLP Models,https://www.semanticscholar.org/paper/be7cb8f79bc018e57467168fc0c7f8ad59bba04f,Conference,"Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor, or only work for a very restrictive class of bugs. We present AdaTest, a process which uses large scale language models (LMs) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model. Such bugs are then addressed through an iterative text-fix-retest loop, inspired by traditional software development. In experiments with expert and non-expert users and commercial / research models for 8 different tasks, AdaTest makes users 5-10x more effective at finding bugs than current approaches, and helps users effectively fix bugs without adding new bugs.",2022,53,,"[78846919.0, 23451726.0]",78846919.0,Bratislava,3,"[1505828520.0, 103131985.0, 1996394.0]",Y,"['After reading the paper, it’s not clear to me what the components of the model are, what each of them take as input and produce as output, what these modules do and how they are combined.', '* There are many estimators for f-divergences (like the ones cited above and many others based e.g. on nearest-neighbors) that are sample-based and thus correspond to the ""implicit"" case that the authors discuss.', 'It is not clear how a softmax output of a CNN, which is trained in a supervised way, follow such assumptions.']",Annual Meeting of the Association for Computational Linguistics,22.0,,"['bugs', 'models', 'users', 'nlp', 'NLP']"
NLP,3caf34532597683c980134579b156cd0d7db2f40,Universal Adversarial Triggers for Attacking and Analyzing NLP,https://www.semanticscholar.org/paper/3caf34532597683c980134579b156cd0d7db2f40,Conference,"Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94% to 0.55%, 72% of “why” questions in SQuAD to be answered “to kill american people”, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.",2019,625,,"[145217343.0, 2113511266.0, 1380266797.0, 40642935.0, 34650964.0]",145217343.0,Vaduz,3,"[1864353.0, 1830383266.0, 47149500.0]",Y,"['How much can change between the goal images and the environment before the system fails?', 'What is LSS in figure 4?', 'The WAGE parameters (i.e., the quantized no. of bits used) are 2-8-8-8, respectively.']",Conference on Empirical Methods in Natural Language Processing,19.0,,"['model', 'triggers', 'models', 'nlp', 'NLP']"
NLP,b651d67502790e1d6d41c589e1d93e996ba7b935,Pretrained Biomedical Language Models for Clinical NLP in Spanish,https://www.semanticscholar.org/paper/b651d67502790e1d6d41c589e1d93e996ba7b935,Workshop,"This work presents the first large-scale biomedical Spanish language models trained from scratch, using large biomedical corpora consisting of a total of 1.1B tokens and an EHR corpus of 95M tokens. We compared them against general-domain and other domain-specific models for Spanish on three clinical NER tasks. As main results, our models are superior across the NER tasks, rendering them more convenient for clinical NLP applications. Furthermore, our findings indicate that when enough data is available, pre-training from scratch is better than continual pre-training when tested on clinical tasks, raising an exciting research question about which approach is optimal. Our models and fine-tuning scripts are publicly available at HuggingFace and GitHub.",2022,34,,"[1416319999.0, 2165226605.0, 1850527789.0, 2078772072.0, 1405319696.0, 2119543279.0, 2168853963.0, 1403836100.0, 2066499928.0]",1416319999.0,Podgorica,2,"[9051130.0, 1414195584.0]",Y,"['It tries to provide an explanation for the phenomenon and a procedure to test when it happens.', 'Proposes a simple yet effective way to sample walks from large graphs.']",Workshop on Biomedical Natural Language Processing,22.0,,"['models', 'scratch', 'tokens', 'nlp', 'NLP']"
NLP,d235a9085e0543fcbe502fbc269f9a8ee01dcbab,AdaPrompt: Adaptive Model Training for Prompt-based NLP,https://www.semanticscholar.org/paper/d235a9085e0543fcbe502fbc269f9a8ee01dcbab,Conference,"Prompt-based learning, with its capability to tackle zero-shot and few-shot NLP tasks, has gained much attention in community. The main idea is to bridge the gap between NLP downstream tasks and language modeling (LM), by mapping these tasks into natural language prompts, which are then filled by pre-trained language models (PLMs). However, for prompt learning, there are still two salient gaps between NLP tasks and pretraining. First, prompt information is not necessarily sufficiently present during LM pretraining. Second, task-specific data are not necessarily well represented during pretraining. We address these two issues by proposing AdaPrompt, adaptively retrieving external data for continual pretraining of PLMs by making use of both task and prompt characteristics. In addition, we make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers. Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings. In addition, in zero-shot settings, our method outperforms standard prompt-based methods by up to 26.35\% relative error reduction.",2022,33,,"[2109404730.0, 39798499.0, 2152290059.0, 2992833.0, 8652308.0, 48262024.0, 2145913600.0]",2109404730.0,Chisinau,2,"[77790220.0, 2772242.0]",Y,"['The idea has some novelty and the results on several tasks attempting to prove its effectiveness against systems that handle named entities in a static way.', 'For the application of images, using text description to refine the representation is a natural and important research question.']",Conference on Empirical Methods in Natural Language Processing,22.0,,"['tasks', 'language', 'nlp', 'plms', 'NLP']"
NLP,011095a0082e5e301f9bf30267b193c1c9e7e370,Perturbation Augmentation for Fairer NLP,https://www.semanticscholar.org/paper/011095a0082e5e301f9bf30267b193c1c9e7e370,Conference,"Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and train a neural perturbation model, which we show outperforms heuristic alternatives. We find that (i) language models (LMs) pre-trained on demographically perturbed corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE datasets exhibit less demographic bias on downstream tasks, and (iii) fairness improvements do not come at the expense of performance on downstream tasks. Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models. We hope that this exploration of neural demographic perturbation will help drive more improvement towards fairer NLP.",2022,34,,"[2149798086.0, 51519704.0, 2166312768.0, 51324296.0, 2111313627.0, 2110032535.0]",2149798086.0,Helsinki,3,"[2118438836.0, 2282113562.0, 2113619066.0]",Y,"['It would be interesting to see the results of the new activation function on LSTM.', 'After all, reward seems to play a very important role for the proposed system.', ""To start, why not plot the empirical histogram of p(z|x) (for some fixed x's) to get a sense of how well-behaved it is as a distribution.""]",Conference on Empirical Methods in Natural Language Processing,22.0,,"['models', 'nlp', 'language', 'datasets', 'NLP']"
NLP,e57aeb158a38ccf33d2f0f5a8f63f1209497e329,What Do NLP Researchers Believe? Results of the NLP Community Metasurvey,https://www.semanticscholar.org/paper/e57aeb158a38ccf33d2f0f5a8f63f1209497e329,Conference,"We present the results of the NLP Community Metasurvey. Run from May to June 2022, it elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split in half on the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In addition, the survey posed meta-questions, asking respondents to predict the distribution of survey responses. This allows us to uncover false sociological beliefs where the community’s predictions don’t match reality. Among other results, we find that the community greatly overestimates its own belief in the usefulness of benchmarks and the potential for scaling to solve real-world problems, while underestimating its belief in the importance of linguistic structure, inductive bias, and interdisciplinary science.",2022,26,,"[38614754.0, 14487640.0, 119389860.0, 49355602.0, 144906624.0, 13336152.0, 93811144.0, 10666396.0, 46230016.0, 80842917.0, 1799822.0]",38614754.0,Valletta,3,"[48738717.0, 1919541.0, 2157681212.0]",Y,"['This is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial NLL loss and a reconstruction loss at the last hidden layer.', 'The explanation of the cause of ""super-convergence"" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments.', 'This paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values (possibly different for positive and negative weights).']",Annual Meeting of the Association for Computational Linguistics,22.0,,"['results', 'nlp', 'community', 'respondents', 'NLP']"
NLP,7fa273f450251523e6b7fcc2eb3fdbdfd4a30493,CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP,https://www.semanticscholar.org/paper/7fa273f450251523e6b7fcc2eb3fdbdfd4a30493,Conference,"Humans can learn a new language task efficiently with only few examples, by leveraging their knowledge obtained when learning prior tasks. In this paper, we explore whether and how such cross-task generalization ability can be acquired, and further applied to build better few-shot learners across diverse NLP tasks. We introduce CrossFit, a problem setup for studying cross-task generalization ability, which standardizes seen/unseen task partitions, data access during different learning stages, and the evaluation protocols. To instantiate different seen/unseen task partitions in CrossFit and facilitate in-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse few-shot NLP tasks created from open-access NLP datasets and converted to a unified text-to-text format. Our analysis reveals that the few-shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks. We also observe that the selection of upstream learning tasks can significantly influence few-shot performance on unseen tasks, asking further analysis on task similarity and transferability.",2021,141,,"[1557391091.0, 51583409.0, 145201124.0]",1557391091.0,Zagreb,3,"[145993598.0, 9543395.0, 2805081.0]",Y,"['It is somewhat alarming how the analysis has little to do with the neural networks and how dropout works, let along RNNs, while the strength of the empirical results are all on RNNs.', 'In fact, linear PCA can be viewed as an autoencoder model with linear encoder and decoder (so that the squared error reconstruction loss between a given sample and the sample reconstructed by the autoencoder is minimal (Bishop, 2006)).', 'After introducing the idea and related work, the authors in Section 3 give details about how to perform the quantization.']",Conference on Empirical Methods in Natural Language Processing,21.0,,"['tasks', 'nlp', 'task', 'ability', 'NLP']"
NLP,185e7d2a761594451b02ace240356dadad2aef78,Dice Loss for Data-imbalanced NLP Tasks,https://www.semanticscholar.org/paper/185e7d2a761594451b02ace240356dadad2aef78,Conference,"Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sørensen--Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.",2019,366,,"[2845020.0, 2109329406.0, 65844131.0, 2115780454.0, 144894837.0, 49298465.0]",2845020.0,Riga,3,"[2063938464.0, 2051536212.0, 2073587169.0]",Y,"['However, the presentation of the method is not clear, the experiment is not sufficient, and the paper is not polished.', 'In other words, the evaluation is a bit lazy somewhat in the same sense as the writing and treatment of related work; the authors implemented the model and ran it on a collection of public data sets, but did not venture further into scientific reporting of the merits and limitations of the approach.', 'As such the paper would be a nice contribution to ICLR.']",Annual Meeting of the Association for Computational Linguistics,19.0,,"['examples', 'training', 'tasks', 'nlp', 'NLP']"
NLP,d6f002d88638de71114dab083f0ea8ceea6b6a5a,Benchmarking Intersectional Biases in NLP,https://www.semanticscholar.org/paper/d6f002d88638de71114dab083f0ea8ceea6b6a5a,Conference,"There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.",2022,32,,"[9051130.0, 2143685866.0, 2152595932.0, 3047212.0, 144849629.0]",9051130.0,Athens,2,"[48469973.0, 2231158981.0]",Y,"['In the experiment, the proposed method shows no or slight degradation of accuracy with big savings in communication cost.', 'Concerning the lack of mathematical rigour in the statistical physics literature on which the authors comment, they might want to relate to a very recent work https://arxiv.org/pdf/1708.03395.pdf work that sets all the past statistical physics results on optimal generalization in single layer neural networks on fully rigorous basis by proving that the corresponding formulas stemming from the replica method are indeed correct.']",North American Chapter of the Association for Computational Linguistics,22.0,,"['models', 'work', 'nlp', 'bias', 'NLP']"
NLP,f72053903270d9a7f41108461ad04d5aa075218d,SHAP-Based Explanation Methods: A Review for NLP Interpretability,https://www.semanticscholar.org/paper/f72053903270d9a7f41108461ad04d5aa075218d,Conference,"Model explanations are crucial for the transparent, safe, and trustworthy deployment of machine learning models. The SHapley Additive exPlanations (SHAP) framework is considered by many to be a gold standard for local explanations thanks to its solid theoretical background and general applicability. In the years following its publication, several variants appeared in the literature—presenting adaptations in the core assumptions and target applications. In this work, we review all relevant SHAP-based interpretability approaches available to date and provide instructive examples as well as recommendations regarding their applicability to NLP use cases.",2022,32,,"[1786389.0, 2178446.0, 2187454523.0, 2187454784.0, 146800020.0]",1786389.0,Dublin,3,"[2108691840.0, 144717855.0, 93421340.0]",Y,"['If the baselines are stronger and this point is more convincing, I am happy to raise my rating of the paper.', '3) and extracting 2D slices from the compressed space and computing 2D histograms per slice.', 'In other words, the evaluation is a bit lazy somewhat in the same sense as the writing and treatment of related work; the authors implemented the model and ran it on a collection of public data sets, but did not venture further into scientific reporting of the merits and limitations of the approach.']",International Conference on Computational Linguistics,22.0,,"['explanations', 'applicability', 'nlp', 'model', 'NLP']"
NLP,322d91190acd8ac8c64598f5126947b0485ba249,Quantified Reproducibility Assessment of NLP Results,https://www.semanticscholar.org/paper/322d91190acd8ac8c64598f5126947b0485ba249,Conference,"This paper describes and tests a method for carrying out quantified reproducibility assessment (QRA) that is based on concepts and definitions from metrology. QRA produces a single score estimating the degree of reproducibility of a given system and evaluation measure, on the basis of the scores from, and differences between, different reproductions. We test QRA on 18 different system and evaluation measure combinations (involving diverse NLP tasks and types of evaluation), for each of which we have the original results and one to seven reproduction results. The proposed QRA method produces degree-of-reproducibility scores that are comparable across multiple reproductions not only of the same, but also of different, original studies. We find that the proposed method facilitates insights into causes of variation between reproductions, and as a result, allows conclusions to be drawn about what aspects of system and/or evaluation design need to be changed in order to improve reproducibility.",2022,25,,"[41052836.0, 2162189986.0, 2738095.0]",41052836.0,Podgorica,2,"[2113909888.0, 2115457464.0]",Y,"['I could then identify the influential dimension(s) by taking the largest absolute values in the gradient vector.', 'The paper says Deep Voice 2 (Arik et al., 2017a) is only prior work for multi-speaker TTS.']",Annual Meeting of the Association for Computational Linguistics,22.0,,"['qra', 'method', 'reproducibility', 'nlp', 'NLP']"
NLP,472644c5f4155635cf9e9e37540bfa53c20e7610,Semantically Equivalent Adversarial Rules for Debugging NLP models,https://www.semanticscholar.org/paper/472644c5f4155635cf9e9e37540bfa53c20e7610,Conference,"Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) – semantic-preserving perturbations that induce changes in the model’s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) – simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.",2018,440,,"[78846919.0, 34650964.0, 1730156.0]",78846919.0,San Marino,3,"[1912476.0, 47627363.0, 2112425054.0]",Y,"['If the contribution of the paper is the ""output stochastic"" noise model, I think it is worth experimenting with the design options one has with such a model.', 'Inclusion of Assumption 3 would in particular require better justification.', 'The model is further extended for conditional generation and demonstrated on a range of image benchmark data sets.']",Annual Meeting of the Association for Computational Linguistics,18.0,,"['instances', 'adversaries', 'sears', 'nlp', 'NLP']"
NLP,9b54941de1e21826ecc28b32730ac3f69991ede4,Robustness Gym: Unifying the NLP Evaluation Landscape,https://www.semanticscholar.org/paper/9b54941de1e21826ecc28b32730ac3f69991ede4,Conference,"Despite impressive performance on standard benchmarks, natural language processing (NLP) models are often brittle when deployed in real-world systems. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, RG enables practitioners to compare results from disparate evaluation paradigms with a single click, and to easily develop and share novel evaluation methods using a built-in set of abstractions. RG is under active development and we welcome feedback & contributions from the community.",2021,116,,"[1822288.0, 8937909.0, 2056908.0, 145814654.0, 2120253018.0, 3393918.0, 2228109.0, 143977268.0, 2061444681.0]",1822288.0,Lisbon,2,"[35210462.0, 2144633223.0]",Y,"['The authors propose to train a generator network in combination with the classifier and an adversarial discriminator.', 'After all, reward seems to play a very important role for the proposed system.']",North American Chapter of the Association for Computational Linguistics,21.0,,"['evaluation', 'nlp', 'systems', 'rg', 'NLP']"
NLP,8d0f755dea90f35f4b126a01fa3cce96b3bdd344,Towards Climate Awareness in NLP Research,https://www.semanticscholar.org/paper/8d0f755dea90f35f4b126a01fa3cce96b3bdd344,Conference,"The climate impact of AI, and NLP research in particular, has become a serious issue given the enormous amount of energy that is increasingly being used for training and running computational models. Consequently, increasing focus is placed on efficient NLP. However, this important initiative lacks simple guidelines that would allow for systematic climate reporting of NLP research. We argue that this deficiency is one of the reasons why very few publications in NLP report key figures that would allow a more thorough examination of environmental impact, and present a quantitative survey to demonstrate this. As a remedy, we propose a climate performance model card with the primary purpose of being practically usable with only limited information about experiments and the underlying computer hardware. We describe why this step is essential to increase awareness about the environmental impact of NLP research and, thereby, paving the way for more thorough discussions.",2022,22,,"[2064295987.0, 2023644816.0, 2156865999.0, 2006205621.0, 3073566.0]",2064295987.0,Moscow,3,"[2237803694.0, 1505708061.0, 90943712.0]",Y,"['However, that is moved completely to the Appendix.', ""This manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum, which actually has been shown by existing works like Kawaguchi (2016) with some assumptions."", '* Qualitative results on sampling from the model using the CIFAR dataset.']",Conference on Empirical Methods in Natural Language Processing,22.0,,"['nlp', 'climate', 'impact', 'research', 'NLP']"
NLP,5e31fa4e69d1a3587230f5d134c0b7e2ed84a742,Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models,https://www.semanticscholar.org/paper/5e31fa4e69d1a3587230f5d134c0b7e2ed84a742,Conference,"Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at https://github.com/lancopku/Embedding-Poisoning.",2021,97,,"[2120801160.0, 49192881.0, 50317060.0, 19169659.0, 11774802.0, 1631386300.0]",2120801160.0,Berlin,3,"[1743722.0, 2660835.0, 1817207.0]",Y,"['(As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.)', 'The authors claimed that this parameter update is one of the novelty of their method, making it different from the method of Schlegl et al. (2017).', 'The most important idea of the papier (PDE-net) is to learn both differential operators and the function that governs the PDE.']",North American Chapter of the Association for Computational Linguistics,21.0,,"['nlp', 'models', 'security', 'samples', 'NLP']"
NLP,7571ed4cf1bbdcf891b576a0da12c910b1f0c72f,Concealed Data Poisoning Attacks on NLP Models,https://www.semanticscholar.org/paper/7571ed4cf1bbdcf891b576a0da12c910b1f0c72f,Conference,"Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model’s training set that causes the model to frequently predict Positive whenever the input contains “James Bond”. Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (“Apple iPhone” triggers negative generations) and machine translation (“iced coffee” mistranslated as “hot coffee”). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.",2021,104,,"[145217343.0, 145914976.0, 144588144.0, 34650964.0]",145217343.0,Stockholm,2,"[1756679.0, 2072801764.0]",Y,"['This paper proposes a different approach (with could be combined with these methods) based on a new training procedure.', 'I could then identify the influential dimension(s) by taking the largest absolute values in the gradient vector.']",North American Chapter of the Association for Computational Linguistics,21.0,,"['model', 'predictions', 'attack', 'nlp', 'NLP']"
NLP,a0f788f6de0fb83d623c875a98120e3f347f70d1,Biomedical and clinical English model packages for the Stanza Python NLP library,https://www.semanticscholar.org/paper/a0f788f6de0fb83d623c875a98120e3f347f70d1,JournalArticle,"Abstract Objective The study sought to develop and evaluate neural natural language processing (NLP) packages for the syntactic analysis and named entity recognition of biomedical and clinical English text. Materials and Methods We implement and train biomedical and clinical English NLP pipelines by extending the widely used Stanza library originally designed for general NLP tasks. Our models are trained with a mix of public datasets such as the CRAFT treebank as well as with a private corpus of radiology reports annotated with 5 radiology-domain entities. The resulting pipelines are fully based on neural networks, and are able to perform tokenization, part-of-speech tagging, lemmatization, dependency parsing, and named entity recognition for both biomedical and clinical text. We compare our systems against popular open-source NLP libraries such as CoreNLP and scispaCy, state-of-the-art models such as the BioBERT models, and winning systems from the BioNLP CRAFT shared task. Results For syntactic analysis, our systems achieve much better performance compared with the released scispaCy models and CoreNLP models retrained on the same treebanks, and are on par with the winning system from the CRAFT shared task. For NER, our systems substantially outperform scispaCy, and are better or on par with the state-of-the-art performance from BioBERT, while being much more computationally efficient. Conclusions We introduce biomedical and clinical NLP packages built for the Stanza library. These packages offer performance that is similar to the state of the art, and are also optimized for ease of use. To facilitate research, we make all our models publicly available. We also provide an online demonstration (http://stanza.run/bio).",2021,101,28,"[49889487.0, 49889860.0, 50531624.0, 144783904.0, 2356307.0]",49889487.0,Prague,3,"[1422035486.0, 2228185935.0, 9035741.0]",Y,"['However, that is moved completely to the Appendix.', 'Moreover, results on Table 2 are reported as the ones with “the highest test accuracy achieved with each tandem block”.', '3) Can you write the exact dynamics used for Theorem 4.1 ?']",,,J. Am. Medical Informatics Assoc.,"['nlp', 'models', 'systems', 'packages', 'NLP']"
NLP,f91dbd39d4c742ba675e447b04a0b0c70b33e836,Measure and Improve Robustness in NLP Models: A Survey,https://www.semanticscholar.org/paper/f91dbd39d4c742ba675e447b04a0b0c70b33e836,Conference,"As NLP models achieved state-of-the-art performances over benchmarks and gained wide applications, it has been increasingly important to ensure the safe deployment of these models in the real world, e.g., making sure the models are robust against unseen or challenging scenarios. Despite robustness being an increasingly studied topic, it has been separately explored in applications like vision and NLP, with various definitions, evaluation and mitigation strategies in multiple lines of research. In this paper, we aim to provide a unifying survey of how to define, measure and improve robustness in NLP. We first connect multiple definitions of robustness, then unify various lines of work on identifying robustness failures and evaluating models’ robustness. Correspondingly, we present mitigation strategies that are data-driven, model-driven, and inductive-prior-based, with a more systematic view of how to effectively improve robustness in NLP models. Finally, we conclude by outlining open challenges and future directions to motivate further research in this area.",2021,86,,"[1524732527.0, 49528192.0, 2143919864.0]",1524732527.0,Amsterdam,3,"[1753285996.0, 37502184.0, 1633124736.0]",Y,"[""If the loss for the task is proper, then it's well known how to construct a divergence which coincides with the optimal risk."", 'It seems the Blizzard results in Figure 2 are missing no reconstruction loss + full backprop.', 'The evaluation framework is described in enough detail to replicate the results.']",North American Chapter of the Association for Computational Linguistics,21.0,,"['models', 'nlp', 'robustness', 'applications', 'NLP']"
NLP,1109d62ebd2b29a7dc148bc30dd6cfc803a63dec,IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP,https://www.semanticscholar.org/paper/1109d62ebd2b29a7dc148bc30dd6cfc803a63dec,Conference,"Although the Indonesian language is spoken by almost 200 million people and the 10th most spoken language in the world, it is under-represented in NLP research. Previous work on Indonesian has been hampered by a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In this work, we release the IndoLEM dataset comprising seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. We additionally release IndoBERT, a new pre-trained language model for Indonesian, and evaluate it over IndoLEM, in addition to benchmarking it against existing resources. Our experiments show that IndoBERT achieves state-of-the-art performance over most of the tasks in IndoLEM.",2020,157,,"[2789148.0, 2953039.0, 1800564.0, 145465286.0]",2789148.0,Prague,2,"[2288804757.0, 145771261.0]",Y,"['Results are good, some unclear explanation This paper proposes an end-to-end trainable attention module, which takes as input the 2D feature vector map and outputs a 2D matrix of scores for each map.', ""Pros and Cons ============ + good results + interesting idea of using the algorithm for RLfD - weak experiments for an application paper - not clear what's new""]",International Conference on Computational Linguistics,20.0,,"['language', 'indolem', 'nlp', 'work', 'NLP']"
NLP,3cc2f69951cd24fe61be4cf32d62afbac297bc2b,Social Biases in NLP Models as Barriers for Persons with Disabilities,https://www.semanticscholar.org/paper/3cc2f69951cd24fe61be4cf32d62afbac297bc2b,Conference,"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",2020,223,,"[2083807.0, 3331141.0, 40081727.0, 20825661.0, 2112887022.0, 1667883461.0]",2083807.0,Amsterdam,2,"[2154976675.0, 2142159995.0]",Y,"['A new method for weight quantization.', 'The presented results using small networks on the MNIST dataset only show that networks with ISRLU can perform similar to those with other activation functions, but not the speed advantages of ISRLU.']",Annual Meeting of the Association for Computational Linguistics,20.0,,"['biases', 'nlp', 'models', 'disability', 'NLP']"
NLP,11342d45911ee8a7c9e3a94117ce774ad7036172,Neural Unsupervised Domain Adaptation in NLP—A Survey,https://www.semanticscholar.org/paper/11342d45911ee8a7c9e3a94117ce774ad7036172,Conference,"Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future NLP.",2020,209,,"[1723636206.0, 2022124.0]",1723636206.0,Brussels,3,"[32058742.0, 9959840.0, 3410500.0]",Y,"['An encoder is later trained to map real images into the space of latent codes for the generator, allowing the system to be applied to real image segmentation tasks.', 'The core idea is to train the model on pairs of images corresponding to the same content but varying in views, using adversarial training to discriminate such examples from generated pairs.', 'On the theoretical side, the linearly constrained weights are only shown to work for a very special case.']",International Conference on Computational Linguistics,20.0,,"['domain', 'data', 'language', 'nlp', 'NLP']"
NLP,fa7b8acd47631bada5b66049824bfd335ac6bf8f,Towards Improving Adversarial Training of NLP Models,https://www.semanticscholar.org/paper/fa7b8acd47631bada5b66049824bfd335ac6bf8f,Conference,"Adversarial training, a method for learning robust deep neural networks, constructs adversarial examples during training. However, recent methods for generating NLP adversarial examples involve combinatorial search and expensive sentence encoders for constraining the generated instances. As a result, it remains challenging to use vanilla adversarial training to improve NLP models' performance, and the benefits are mainly uninvestigated. This paper proposes a simple and improved vanilla adversarial training process for NLP models, which we name Attacking to Training (A2T). The core part of A2T is a new and cheaper word substitution attack optimized for vanilla adversarial training. We use A2T to train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI datasets. Our results empirically show that it is possible to train robust NLP models using a much cheaper adversary. We demonstrate that vanilla adversarial training with A2T can improve an NLP model's robustness to the attack it was originally trained with and also defend the model against other types of word substitution attacks. Furthermore, we show that A2T can improve NLP models' standard accuracy, cross-domain generalization, and interpretability. Code is available at https://github.com/QData/Textattack-A2T .",2021,80,,"[1693182792.0, 121817403.0]",1693182792.0,Bratislava,2,"[2117315688.0, 145458655.0]",Y,"['Or is similarity to the corpus of interest more important?', ""I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships.""]",Conference on Empirical Methods in Natural Language Processing,21.0,,"['training', 'nlp', 'models', 't', 'NLP']"
NLP,353c88c231ce156d604e074af276422422fc73f7,"A Survey of Race, Racism, and Anti-Racism in NLP",https://www.semanticscholar.org/paper/353c88c231ce156d604e074af276422422fc73f7,Conference,"Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.",2021,83,,"[49713890.0, 3422038.0, 2138053020.0, 145317727.0]",49713890.0,Vaduz,3,"[71790825.0, 145366422.0, 88478180.0]",Y,"['Thus, as such, the novelty or the contributions of this paper are minor.', 'However, the proposed method is still within the same family methods as demonstrated by RARL.', 'For example, when showing that the head direction cells generalize in the new mazes how can we be sure that it is not using a common lighting scheme common to both train and test mazes to orient itself?']",Annual Meeting of the Association for Computational Linguistics,21.0,,"['nlp', 'race', 'work', 'research', 'NLP']"
NLP,28a5a53dafacebad8a7c47773079caeffb9a5baa,Representing Numbers in NLP: a Survey and a Vision,https://www.semanticscholar.org/paper/28a5a53dafacebad8a7c47773079caeffb9a5baa,Conference,"NLP systems rarely give special consideration to numbers found in text. This starkly contrasts with the consensus in neuroscience that, in the brain, numbers are represented differently from words. We arrange recent NLP work on numeracy into a comprehensive taxonomy of tasks and methods. We break down the subjective notion of numeracy into 7 subtasks, arranged along two dimensions: granularity (exact vs approximate) and units (abstract vs grounded). We analyze the myriad representational choices made by over a dozen previously published number encoders and decoders. We synthesize best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.",2021,82,,"[37574242.0, 2634786.0, 144171096.0, 2512264.0]",37574242.0,Zagreb,3,"[2138646471.0, 2059547812.0, 32244429.0]",Y,"['The authors did a very good job presenting the problem, motivation and solution in a coherent fashion and easy to follow.', 'While LINE or SDNE, which the authors cite, may not run on some of the larger datasets they can be tested on the smaller datasets.', 'Since this could be a potential disadvantage, some discussions or empirical study on cross-category generalization seems to be interesting.']",North American Chapter of the Association for Computational Linguistics,21.0,,"['nlp', 'numbers', 'numeracy', 'systems', 'NLP']"
NLP,9b529fe170823f95509585d5aa39fa01a43558fd,How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence,https://www.semanticscholar.org/paper/9b529fe170823f95509585d5aa39fa01a43558fd,Conference,"Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we introduce the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from https://github.com/thunlp/CLAIM.",2020,193,,"[51125639.0, 51131083.0, 2652217.0, 145086110.0, 49293587.0, 1753344.0]",51125639.0,Zagreb,3,"[2135403.0, 3144356.0, 1680740.0]",Y,"['“Customers” randomly appear throughout the task and the taxi agents receive reward for moving to the same square as a customer.', 'identifying bugs in programs where the wrong variable is used, and', ""If the loss for the task is proper, then it's well known how to construct a divergence which coincides with the optimal risk.""]",Annual Meeting of the Association for Computational Linguistics,20.0,,"['legalai', 'professionals', 'tasks', 'nlp', 'NLP']"
NLP,9a0424bdd12cdcdf45b556b0b9dcc6fc5a55520b,FORCES NLP: an efficient implementation of interior-point methods for multistage nonlinear nonconvex programs,https://www.semanticscholar.org/paper/9a0424bdd12cdcdf45b556b0b9dcc6fc5a55520b,JournalArticle,"ABSTRACT Real-time implementation of optimisation-based control and trajectory planning can be very challenging for nonlinear systems. As a result, if an implementation based on a fixed linearisation is not suitable, the nonlinear problems are typically locally approximated online, in order to leverage the speed and robustness of embedded solvers for convex quadratic programs (QP) developed during the last decade. The purpose of this paper is to demonstrate that, using simple standard building blocks from nonlinear programming, combined with a structure-exploiting linear system solver, it is possible to achieve computation times in the range typical of solvers for QPs, while retaining nonlinearities and solving the nonlinear programs (NLP) to local optimality. The implemented algorithm is an interior-point method with approximate Hessians and adaptive barrier rules, and is provided as an extension to the C code generator FORCES. Three detailed examples are provided that illustrate a significant improvement in control performance when solving NLPs, with computation times that are comparable with those achieved by fast approximate schemes and up to an order of magnitude faster than the state-of-the-art interior-point solver IPOPT.",2020,164,93,"[35437368.0, 2057413.0, 30521589.0, 144746954.0]",35437368.0,Chisinau,3,"[2155558174.0, 2047926221.0, 2171106811.0]",Y,"['Classifier is trained to not only maximize classification accuracy on the real training data but also to output a uniform distribution for the generated samples.', 'The proposed method is simple yet powerful.', 'Unclear statements/notations: * end of page 3, notations are not entirely consist with previous notations * I do not understand which distribution is assumed on epsilon and gamma when taking the expectancy in equation (9).']",,,International Journal of Control,"['implementation', 'control', 'order', 'nlp', 'NLP']"
NLP,68a3d32416977e88cf1bfa4ad548d403f5f089d6,Rethinking Stealthiness of Backdoor Attack against NLP Models,https://www.semanticscholar.org/paper/68a3d32416977e88cf1bfa4ad548d403f5f089d6,Conference,"Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack. Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. To address this issue, we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings, making an important step towards achieving stealthy backdoor attacking. Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. Our code is available at https://github.com/lancopku/SOS.",2021,68,,"[2120801160.0, 2149202150.0, 50492525.0, 2108485106.0, 2130282986.0]",2120801160.0,Tirana,3,"[1786639.0, 49528584.0, 50738985.0]",Y,"['On empirical evaluation, I already mentioned that it impossible to understand what the classification problem on TIMIT is. I suspect it might be speaker identification.', '4) Novelty The main contribution of this paper is basically a set of experiments looking into architectural choices.', 'This section could be improved by demonstrating the approach on more datasets.']",Annual Meeting of the Association for Computational Linguistics,21.0,,"['backdoor', 'nlp', 'models', 'performance', 'NLP']"
NLP,10aa2be24951e6de76b630482a645d79354c4cde,Quantifying Social Biases in NLP: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics,https://www.semanticscholar.org/paper/10aa2be24951e6de76b630482a645d79354c4cde,JournalArticle,"Abstract Measuring bias is key for better understanding and addressing unfairness in NLP/ML models. This is often done via fairness metrics, which quantify the differences in a model’s behaviour across a range of demographic groups. In this work, we shed more light on the differences and similarities between the fairness metrics used in NLP. First, we unify a broad range of existing metrics under three generalized fairness metrics, revealing the connections between them. Next, we carry out an extensive empirical comparison of existing metrics and demonstrate that the observed differences in bias measurement can be systematically explained via differences in parameter choices for our generalized metrics.",2021,68,9,"[147783277.0, 2065005.0, 39888194.0]",147783277.0,Ljubljana,2,"[51131083.0, 2966239.0]",Y,"['That requires the method to be used together with network pruning methods, which seems limiting its applicability.', 'Figure 2 is very interesting: it is a very relevant way to compare authors model with the literature.']",,,Transactions of the Association for Computational Linguistics,"['metrics', 'differences', 'fairness', 'nlp', 'NLP']"
NLP,c30c0092bf4eb8a44faec3fc60cdd5006276bcdc,Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking,https://www.semanticscholar.org/paper/c30c0092bf4eb8a44faec3fc60cdd5006276bcdc,JournalArticle,"Graph neural networks (GNNs) have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs (e.g. syntactic trees or co-reference structures) contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected $L_0$ norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions.",2020,154,abs/2010.00577,"[8804828.0, 41019080.0, 144889265.0]",8804828.0,Andorra,3,"[1738190.0, 2062848.0, 2513276.0]",Y,"['* Qualitative results on sampling from the model using the CIFAR dataset.', '1. Comparing to previous work (Mordatch & Abbeel, 2018), the task is relatively simple, only requiring the agent to perform binary prediction.', 'This insight is simple, elegant and valuable in my opinion.']",,,International Conference on Learning Representations,"['model', 'gnns', 'nlp', 'models', 'NLP']"
NLP,8dce62b18bdea587c07cb4769a05ca0a816d09ba,"The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models",https://www.semanticscholar.org/paper/8dce62b18bdea587c07cb4769a05ca0a816d09ba,Conference,"We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models—including classification, seq2seq, and structured prediction—and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.",2020,150,,"[6117577.0, 49556437.0, 1994065972.0, 2843215.0, 1388485917.0, 3159346.0, 122064392.0, 51478016.0, 146419516.0, 49849144.0, 2061016887.0]",6117577.0,Bern,2,"[2285816475.0, 2168853963.0]",Y,"['See e.g. the discussion in ""Variational Lossy Autoencoder"" (Chen et al. 2016) - Experiments are not complete (e.g. for AR, as noted in the paper).', 'I could imagine a network learning to ignore features of objects that tend to wander over time.']",Conference on Empirical Methods in Natural Language Processing,20.0,,"['analysis', 'nlp', 'models', 'model', 'NLP']"
NLP,6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786,BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models,https://www.semanticscholar.org/paper/6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786,JournalArticle,"Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose \Name, the first task-agnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.",2021,62,abs/2110.02467,"[5781518.0, 65844131.0, 2109329406.0, 16042895.0, 2146331573.0, 49298465.0, 2093474992.0]",5781518.0,Prague,3,"[2146335468.0, 1746466.0, 1394550182.0]",Y,"['* I understood that the conclusion of part 3 was that the expectation of eq (9) was elegantly computable for certain non-linearity (including ReLU).', 'In both datasets the number of atoms were globally fixed, which is counter-intuitive.', 'If ruled-based systems were sufficient, there would not be a need for statistical dialogue managers.']",,,International Conference on Learning Representations,"['nlp', 'models', 'tasks', 'backdoor', 'NLP']"
NLP,c50a909e20bd07f4aea09dc6dae539b45b406a96,Evaluation of BERT and ALBERT Sentence Embedding Performance on Downstream NLP Tasks,https://www.semanticscholar.org/paper/c50a909e20bd07f4aea09dc6dae539b45b406a96,Conference,"Contextualized representations from a pre-trained language model are central to achieve a high performance on downstream NLP task. The pre-trained BERT and A Lite BERT (ALBERT) models can be fine-tuned to give state-of-the-art results in sentence-pair regressions such as semantic textual similarity (STS) and natural language inference (NLI). Although BERT-based models yield the [CLS] token vector as a reasonable sentence embedding, the search for an optimal sentence embedding scheme remains an active research area in computational linguistics. This paper explores on sentence embedding models for BERT and ALBERT. In particular, we take a modified BERT network with siamese and triplet network structures called Sentence-BERT (SBERT) and replace BERT with ALBERT to create Sentence-ALBERT (SALBERT). We also experiment with an outer CNN sentence-embedding network for SBERT and SALBERT. We evaluate performances of all sentence-embedding models considered using the STS and NLI datasets. The empirical results indicate that our CNN architecture improves ALBERT models substantially more than BERT models for STS benchmark. Despite significantly fewer model parameters, ALBERT sentence embedding is highly competitive to BERT in downstream NLP evaluations.",2021,66,,"[2111538614.0, 2125022424.0, 2046999993.0, 2163133.0]",2111538614.0,Stockholm,3,"[1492047220.0, 1630331317.0, 20829758.0]",Y,"['Moreover, I like the qualitative experiment (Figure 2) in which the tree is used to vary a latent dimension to change the digit’s class.', 'For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers.', ""2.  They argue that they don't need to separate train and test, but I think it is important to be sure that the generated programs work on test cases that are not a part of the reward function.""]",International Conference on Pattern Recognition,25.0,,"['bert', 'models', 'albert', 'nlp', 'NLP']"
NLP,e5d720767b7a539bb2edaa98eaf572a4506a79c6,Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing,https://www.semanticscholar.org/paper/e5d720767b7a539bb2edaa98eaf572a4506a79c6,Conference,"The use of crowdworkers in NLP research is growing rapidly, in tandem with the exponential increase in research production in machine learning and AI. Ethical discussion regarding the use of crowdworkers within the NLP research community is typically confined in scope to issues related to labor conditions such as fair pay. We draw attention to the lack of ethical considerations related to the various tasks performed by workers, including labeling, evaluation, and production. We find that the Final Rule, the common ethical framework used by researchers, did not anticipate the use of online crowdsourcing platforms for data collection, resulting in gaps between the spirit and practice of human-subjects ethics in NLP research. We enumerate common scenarios where crowdworkers performing NLP tasks are at risk of harm. We thus recommend that researchers evaluate these risks by considering the three ethical principles set up by the Belmont Report. We also clarify some common misconceptions regarding the Institutional Review Board (IRB) application. We hope this paper will serve to reopen the discussion within our community regarding the ethical use of crowdworkers.",2021,63,,"[1380557800.0, 115300694.0, 2072404217.0, 1746959.0]",1380557800.0,Athens,3,"[49356798.0, 2146058962.0, 7617146.0]",Y,"['An image has many attributes: the glint in the corner of a window, the hue of a leaf.', 'Is it better to add attention to lower layers or higher layers?', '2. The proposed attention mechanism seems to be fairly domain (or task ) specific, and may not be beneficial for strong generalization (generalization over unseen category).']",North American Chapter of the Association for Computational Linguistics,21.0,,"['crowdworkers', 'research', 'nlp', 'production', 'NLP']"
NLP,0427110f0e79f41e69a8eb00a3ec8868bac26a4f,Do NLP Models Know Numbers? Probing Numeracy in Embeddings,https://www.semanticscholar.org/paper/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,Conference,"The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens—they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise—ELMo captures numeracy the best for all pre-trained methods—but BERT, which uses sub-word units, is less exact.",2019,219,,"[145217343.0, 1705260.0, 48831399.0, 34650964.0, 40642935.0]",145217343.0,Copenhagen,3,"[2078772072.0, 2110760082.0, 1492047220.0]",Y,"['Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice?', 'The authors show mixup provides improvement over baselines in the following settings: * Image Classification on Imagenet.', 'see Fig.2 —> see Fig.1 page 4just before equation 8: the the']",Conference on Empirical Methods in Natural Language Processing,19.0,,"['numeracy', 'numbers', 'reasoning', 'nlp', 'NLP']"
NLP,2743e66939b30c43affb3c9e31f20cfac2109045,Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks,https://www.semanticscholar.org/paper/2743e66939b30c43affb3c9e31f20cfac2109045,Conference,"Labelled data is the foundation of most natural language processing tasks. However, labelling data is difficult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it. Descriptive annotation allows for the surveying and modelling of different beliefs, whereas prescriptive annotation enables the training of models that consistently apply one belief. We discuss benefits and challenges in implementing both paradigms, and argue that dataset creators should explicitly aim for one or the other to facilitate the intended use of their dataset. Lastly, we conduct an annotation experiment using hate speech data that illustrates the contrast between the two paradigms.",2021,85,,"[2043232919.0, 2737827.0, 2022288.0, 1970864.0]",2043232919.0,Kiev,2,"[2145413970.0, 1596817678.0]",Y,"['I think this is a nice example of a detailed analysis of the representations acquired by a reinforcement learning agent.', ""I don't understand what is offered beyond the original papers.""]",North American Chapter of the Association for Computational Linguistics,21.0,,"['data', 'annotation', 'beliefs', 'nlp', 'NLP']"
NLP,ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8,RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models,https://www.semanticscholar.org/paper/ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8,Conference,"Backdoor attacks, which maliciously control a well-trained model’s outputs of the instances with specific triggers, are recently shown to be serious threats to the safety of reusing deep neural networks (DNNs). In this work, we propose an efficient online defense mechanism based on robustness-aware perturbations. Specifically, by analyzing the backdoor training process, we point out that there exists a big gap of robustness between poisoned and clean samples. Motivated by this observation, we construct a word-based robustness-aware perturbation to distinguish poisoned samples from clean samples to defend against the backdoor attacks on natural language processing (NLP) models. Moreover, we give a theoretical analysis about the feasibility of our robustness-aware perturbation-based defense method. Experimental results on sentiment analysis and toxic detection tasks show that our method achieves better defending performance and much lower computational costs than existing online defense methods. Our code is available at https://github.com/lancopku/RAP.",2021,51,,"[2120801160.0, 2149202150.0, 50492525.0, 2108485135.0, 11774802.0]",2120801160.0,Vienna,3,"[50593332.0, 1630331317.0, 2155516106.0]",Y,"['We have to, however, evaluate the approach on what it is able to do at the moment.', 'Overall, this works seems somewhat too preliminary at this stage.', 'In particular, they aim at generating a complete set that fully specifies the behavior of the oracle.']",Conference on Empirical Methods in Natural Language Processing,21.0,,"['backdoor', 'defense', 'samples', 'nlp', 'NLP']"
NLP,9eea59c34f139f3d2153226c8cf026e975622074,Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation,https://www.semanticscholar.org/paper/9eea59c34f139f3d2153226c8cf026e975622074,Conference,"Public datasets are often used to evaluate the efficacy and generalizability of state-of-the-art methods for many tasks in natural language processing (NLP). However, the presence of overlap between the train and test datasets can lead to inflated results, inadvertently evaluating the model’s ability to memorize and interpreting it as the ability to generalize. In addition, such data sets may not provide an effective indicator of the performance of these methods in real world scenarios. We identify leakage of training data into test data on several publicly available datasets used to evaluate NLP tasks, including named entity recognition and relation extraction, and study them to assess the impact of that leakage on the model’s ability to memorize versus generalize.",2021,56,,"[26940961.0, 150147667.0, 1404100607.0]",26940961.0,Dublin,2,"[2810600.0, 2109184366.0]",Y,"['1.  The paper misses some more recent reference, e.g. [a,b].', 'They apply this architecture on the same task as the original article: document classification; they use a logistic regression on the extracted representation.']",Conference of the European Chapter of the Association for Computational Linguistics,21.0,,"['datasets', 'nlp', 'ability', 'data', 'NLP']"
NLP,fafa541419b3756968fe5b3156c6f0257cb29c23,Visualizing and Understanding Neural Models in NLP,https://www.semanticscholar.org/paper/fafa541419b3756968fe5b3156c6f0257cb29c23,Conference,"While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve {\em compositionality}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's {\em salience}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,",2015,636,,"[49298465.0, 39717886.0, 144547315.0, 1746807.0]",49298465.0,Madrid,2,"[2108706355.0, 2115215055.0]",Y,"['Is this on bAbi as well?', 'But, it would be good if this can be supported with real life examples.']",North American Chapter of the Association for Computational Linguistics,15.0,,"['compositionality', 'nlp', 'methods', 'networks', 'NLP']"
NLP,1cf2e9e198feef3893da2800a7949f6880ddc084,ExplainaBoard: An Explainable Leaderboard for NLP,https://www.semanticscholar.org/paper/1cf2e9e198feef3893da2800a7949f6880ddc084,Conference,"With the rapid development of NLP research, leaderboards have emerged as one tool to track the performance of various systems on various NLP tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of NLP evaluation: the ExplainaBoard, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g. what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g. where does system A outperform system B? What if we combine systems A, B and C?) and (iii) examine prediction results closely (e.g. what are common errors made by multiple systems or in what contexts do particular errors occur?). So far, ExplainaBoard covers more than 400 systems, 50 datasets, 40 languages, and 12 tasks. We not only released an online platform at the website but also make our evaluation tool an API with MIT Licence at Github and PyPi that allows users to conveniently assess their models offline. We additionally release all output files from systems that we have run or collected to motivate “output-driven” research in the future.",2021,50,,"[144118452.0, 41037252.0, 2116642640.0, 30300197.0, 46923811.0, 2087363104.0, 2108176413.0, 1796245019.0, 14199369.0, 1700325.0]",144118452.0,Lisbon,3,"[1739899643.0, 31380410.0, 115504645.0]",Y,"['One direction to strengthen this paper is to examine if RMN can do better than pair-wise interactions (and other baselines) for more complex reasoning tasks.', 'However, according to me, the fact that it provides no model description, no model analysis, no modification of the model to improve the sentiment discovery, prevents this article from being publicized at ICLR.', 'The technique systematically interacts with the oracle using observations, which are abstractions of environment states, and it is guaranteed to produce a data set that completely specifies the oracle.']",Annual Meeting of the Association for Computational Linguistics,21.0,,"['systems', 'nlp', 'system', 'research', 'NLP']"
NLP,d88c1255876b62fb5f5a8b292098ca430710a540,The NLP Cookbook: Modern Recipes for Transformer Based Deep Learning Architectures,https://www.semanticscholar.org/paper/d88c1255876b62fb5f5a8b292098ca430710a540,JournalArticle,"In recent years, Natural Language Processing (NLP) models have achieved phenomenal success in linguistic and semantic tasks like text classification, machine translation, cognitive dialogue systems, information retrieval via Natural Language Understanding (NLU), and Natural Language Generation (NLG). This feat is primarily attributed due to the seminal Transformer architecture, leading to designs such as BERT, GPT (I, II, III), etc. Although these large-size models have achieved unprecedented performances, they come at high computational costs. Consequently, some of the recent NLP architectures have utilized concepts of transfer learning, pruning, quantization, and knowledge distillation to achieve moderate model sizes while keeping nearly similar performances as achieved by their predecessors. Additionally, to mitigate the data size challenge raised by language models from a knowledge extraction perspective, Knowledge Retrievers have been built to extricate explicit data documents from a large corpus of databases with greater efficiency and accuracy. Recent research has also focused on superior inference by providing efficient attention to longer input sequences. In this paper, we summarize and examine the current state-of-the-art (SOTA) NLP models that have been employed for numerous NLP tasks for optimal performance and efficiency. We provide a detailed understanding and functioning of the different architectures, a taxonomy of NLP designs, comparative evaluations, and future directions in NLP.",2021,54,9,"[2141026590.0, 143794144.0]",2141026590.0,Warsaw,3,"[2105608191.0, 3441349.0, 1939487.0]",Y,"['Nothing groundbreaking here but still interesting enough and well explained.', 'The fact that the proposed technique is simple yet yields such speedups is encouraging.', 'So, the main contribution of the paper is to do all the sensitivity calculations with respect to SVM problem and then use the importance sampling theory to obtain bounds on the coreset size.']",,,IEEE Access,"['nlp', 'language', 'models', 'knowledge', 'NLP']"
NLP,d1206ccabd1980848f14472d6548251c2fab7963,Exploring and Predicting Transferability across NLP Tasks,https://www.semanticscholar.org/paper/d1206ccabd1980848f14472d6548251c2fab7963,Conference,"Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling). Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even when the source task is small or differs substantially from the target task (e.g., part-of-speech tagging transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as source data size, task and domain similarity, and task complexity all play a role in determining transferability.",2020,131,,"[144244743.0, 1664686681.0, 2227827.0, 2041695.0, 3382568.0, 1401265033.0, 35208858.0, 2136562.0]",144244743.0,Vilnius,2,"[1524732527.0, 28552618.0]",Y,"['Some reasons below: * There are no specific results on properties of the divergences, or axioms that justify them.', 'On empirical evaluation, I already mentioned that it impossible to understand what the classification problem on TIMIT is. I suspect it might be speaker identification.']",Conference on Empirical Methods in Natural Language Processing,20.0,,"['task', 'tasks', 'source', 'nlp', 'NLP']"
NLP,da5d78b3e3a1544fde98fba86088e1215e97cbe8,All NLP Tasks Are Generation Tasks: A General Pretraining Framework,https://www.semanticscholar.org/paper/da5d78b3e3a1544fde98fba86088e1215e97cbe8,JournalArticle,"There have been various types of pretraining architectures including autoregressive models (e.g., GPT), autoencoding models (e.g., BERT), and encoder-decoder models (e.g., T5). On the other hand, NLP tasks are different in nature, with three main categories being classification, unconditional generation, and conditional generation. However, none of the pretraining frameworks performs the best for all tasks, which introduces inconvenience for model development and selection. We propose a novel pretraining framework GLM (General Language Model) to address this challenge. Compared to previous work, our architecture has three major benefits: (1) it performs well on classification, unconditional generation, and conditional generation tasks with one single pretrained model; (2) it outperforms BERT-like models on classification due to improved pretrain-finetune consistency; (3) it naturally handles variable-length blank filling which is crucial for many downstream tasks. Empirically, GLM substantially outperforms BERT on the SuperGLUE natural language understanding benchmark with the same amount of pre-training data. Moreover, GLM with 1.25× parameters of BERTLarge achieves the best performance in NLU, conditional and unconditional generation at the same time, which demonstrates its generalizability to different downstream tasks.1 Equal contribution Department of Computer Science and Technology, Tsinghua Univerisity, Beijing, China Beijing Academy of Artificial Intelligence, Beijing, China Massachusetts Institute of Technology, Cambridge, U.S.A. Recurrent AI, Ltd.. Correspondence to: Zhilin Yang <kimi_yang@rcrai.com>, Jie Tang <jietang@tsinghua.edu.cn>. The codes and pre-trained models are available at https: //github.com/THUDM/GLM All [START] NLP tasks are generation tasks All NLP tasks [END] are generation tasks",2021,49,abs/2103.10360,"[66395694.0, 5606742.0, 2111312892.0, 2055623340.0, 40125294.0, 2109512754.0, 2109541439.0]",66395694.0,Budapest,3,"[2254124342.0, 2229703159.0, 2035210.0]",Y,"['— Can the authors provide training time comparison of their model and other/baseline models?', 'This is then used to develop a simple bias initialization scheme for the gates when the range of temporal dependencies relevant for a problem can be estimated or are known.', '— Is the “fork” module the main contribution of the paper?']",,,arXiv.org,"['generation', 'tasks', 'models', 'nlp', 'NLP']"
NLP,e02a757617c2c42eb62889cc4d4aee3765928303,The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus,https://www.semanticscholar.org/paper/e02a757617c2c42eb62889cc4d4aee3765928303,JournalArticle,"In order to address increasing demands of real-world applications, the research for knowledge-intensive NLP (KI-NLP) should advance by capturing the challenges of a truly open-domain environment: web-scale knowledge, lack of structure, inconsistent quality and noise. To this end, we propose a new setup for evaluating existing knowledge intensive tasks in which we generalize the background corpus to a universal web snapshot. We investigate a slate of NLP tasks which rely on knowledge - either factual or common sense, and ask systems to use a subset of CCNet - the Sphere corpus - as a knowledge source. In contrast to Wikipedia, otherwise a common background corpus in KI-NLP, Sphere is orders of magnitude larger and better reflects the full diversity of knowledge on the web. Despite potential gaps in coverage, challenges of scale, lack of structure and lower quality, we find that retrieval from Sphere enables a state of the art system to match and even outperform Wikipedia-based models on several tasks. We also observe that while a dense index can outperform a sparse BM25 baseline on Wikipedia, on Sphere this is not yet possible. To facilitate further research and minimise the community's reliance on proprietary, black-box search engines, we share our indices, evaluation metrics and infrastructure.",2021,48,abs/2112.09924,"[120174856.0, 40052301.0, 2067091563.0, 113568063.0, 2966239.0, 1410231361.0, 145222654.0, 1628391446.0, 3024698.0, 2072801764.0, 48662861.0]",120174856.0,Belgrade,2,"[2057642721.0, 1682124.0]",Y,"['In practice, I believe this is not an unreasonable requirement.', 'I found it would have also been interesting and helpful to define and show a new metric that incorporates both accuracy and compression rate into a single metric, e.g., how much accuracy is lost (or gained) per compression rate relatively to the baseline of no compression.']",,,arXiv.org,"['knowledge', 'sphere', 'nlp', 'tasks', 'NLP']"
NLP,30f233eecca2239ee1dd754914324092e53f8f19,Evaluation Examples are not Equally Informative: How should that change NLP Leaderboards?,https://www.semanticscholar.org/paper/30f233eecca2239ee1dd754914324092e53f8f19,Conference,"Leaderboards are widely used in NLP and push the field forward. While leaderboards are a straightforward ranking of NLP models, this simplicity can mask nuances in evaluation items (examples) and subjects (NLP models). Rather than replace leaderboards, we advocate a re-imagining so that they better highlight if and where progress is made. Building on educational testing, we create a Bayesian leaderboard model where latent subject skill and latent item difficulty predict correct responses. Using this model, we analyze the ranking reliability of leaderboards. Afterwards, we show the model can guide what to annotate, identify annotation errors, detect overfitting, and identify informative examples. We conclude with recommendations for future benchmark tasks.",2021,48,,"[145009056.0, 40080808.0, 49462969.0, 9051130.0, 3422908.0, 1389036863.0]",145009056.0,Madrid,3,"[39703662.0, 2254124342.0, 49248672.0]",Y,"['If we consider e.g., a linear 1-layer autoencoder to be equivalent to PCA (without the rnn layers), in essence this formulation is closely related to applying pca to reduce the initial dimensionality and then t-sne.', 'To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER.', 'First, the description of what is the prior used by batch normalization in section 3.3 is unsatisfactory.']",Annual Meeting of the Association for Computational Linguistics,21.0,,"['leaderboards', 'nlp', 'model', 'models', 'NLP']"
NLP,c4cdf2e0d89aadb13bf9c61654ff9e17be2b01b9,BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements,https://www.semanticscholar.org/paper/c4cdf2e0d89aadb13bf9c61654ff9e17be2b01b9,Conference,"Deep neural networks (DNNs) have progressed rapidly during the past decade and have been deployed in various real-world applications. Meanwhile, DNN models have been shown to be vulnerable to security and privacy attacks. One such attack that has attracted a great deal of attention recently is the backdoor attack. Specifically, the adversary poisons the target model’s training set to mislead any input with an added secret trigger to a target class. Previous backdoor attacks predominantly focus on computer vision (CV) applications, such as image classification. In this paper, we perform a systematic investigation of backdoor attack on NLP models, and propose BadNL, a general NLP backdoor attack framework including novel attack methods. Specifically, we propose three methods to construct triggers, namely BadChar, BadWord, and BadSentence, including basic and semantic-preserving variants. Our attacks achieve an almost perfect attack success rate with a negligible effect on the original model’s utility. For instance, using the BadChar, our backdoor attack achieves a 98.9% attack success rate with yielding a utility improvement of 1.5% on the SST-5 dataset when only poisoning 3% of the original set. Moreover, we conduct a user study to prove that our triggers can well preserve the semantics from humans perspective.",2020,124,,"[151257231.0, 66697271.0, 153642281.0, 144588806.0, 2118869261.0, 40238834.0, 2109716565.0, 2145954003.0]",151257231.0,Sofia,2,"[145966834.0, 1407546424.0]",Y,"['Considering that this is a big, complicated system, it is crucial that the authors included both an ablation experiment to see which pieces were most important, and an experiment that indicates the amount of labeled data that would be required to achieve the same results with a supervised system.', ""The theory is not strong and the experiments don't necessary support the intuitive claims made in the paper.""]",Asia-Pacific Computer Systems Architecture Conference,37.0,,"['attack', 'backdoor', 'nlp', 'applications', 'NLP']"
NLP,df7336844a31165db0ae08f1cd0f560c9e3faeea,BadNL: Backdoor Attacks Against NLP Models,https://www.semanticscholar.org/paper/df7336844a31165db0ae08f1cd0f560c9e3faeea,JournalArticle,"Machine learning (ML) has progressed rapidly during the past decade and ML models have been deployed in various real-world applications. Meanwhile, machine learning models have been shown to be vulnerable to various security and privacy attacks. One attack that has attracted a great deal of attention recently is the backdoor attack. Specifically, the adversary poisons the target model training set, to mislead any input with an added secret trigger to a target class, while keeping the accuracy for original inputs unchanged. 
Previous backdoor attacks mainly focus on computer vision tasks. In this paper, we present the first systematic investigation of the backdoor attack against models designed for natural language processing (NLP) tasks. Specifically, we propose three methods to construct triggers in the NLP setting, including Char-level, Word-level, and Sentence-level triggers. Our Attacks achieve an almost perfect success rate without jeopardizing the original model utility. For instance, using the word-level triggers, our backdoor attack achieves 100% backdoor accuracy with only a drop of 0.18%, 1.26%, and 0.19% in the models utility, for the IMDB, Amazon, and Stanford Sentiment Treebank datasets, respectively.",2020,130,abs/2006.01043,"[151257231.0, 66697271.0, 144588806.0, 2026855.0, 2145954003.0]",151257231.0,Stockholm,2,"[32830771.0, 5028442.0]",Y,"['In fact, a VAE would be nicely suited when proposing to work with low-dimensional latent spaces.', 'Minor comments/concerns: * 2nd paragraph in section 4: Are parameters shared between these 3 MLPs (enc,emb,eff)?']",,,arXiv.org,"['models', 'attack', 'backdoor', 'nlp', 'NLP']"
NLP,00cd2650a89734105fa0c0aba3bf07935b318290,GLUECoS: An Evaluation Benchmark for Code-Switched NLP,https://www.semanticscholar.org/paper/00cd2650a89734105fa0c0aba3bf07935b318290,Conference,"Code-switching is the use of more than one language in the same conversation or utterance. Recently, multilingual contextual embedding models, trained on multiple monolingual corpora, have shown promising results on cross-lingual and multilingual tasks. We present an evaluation benchmark, GLUECoS, for code-switched languages, that spans several NLP tasks in English-Hindi and English-Spanish. Specifically, our evaluation benchmark includes Language Identification from text, POS tagging, Named Entity Recognition, Sentiment Analysis, Question Answering and a new task for code-switching, Natural Language Inference. We present results on all these tasks using cross-lingual word embedding models and multilingual models. In addition, we fine-tune multilingual models on artificially generated code-switched data. Although multilingual models perform significantly better than cross-lingual models, our results show that in most tasks, across both language pairs, multilingual models fine-tuned on code-switched data perform best, showing that multilingual models can be further optimized for code-switching tasks.",2020,112,,"[1452678825.0, 34725175.0, 48180698.0, 3010457.0, 143990839.0]",1452678825.0,Bratislava,2,"[2412941.0, 2115853457.0]",Y,"['In Section 3.2, the authors listed a couple of loss functions.', 'Motivation In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle.']",Annual Meeting of the Association for Computational Linguistics,20.0,,"['models', 'tasks', 'language', 'nlp', 'NLP']"
Machine Learning,f9c602cc436a9ea2f9e7db48c77d924e09ce3c32,Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms,https://www.semanticscholar.org/paper/f9c602cc436a9ea2f9e7db48c77d924e09ce3c32,JournalArticle,"We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL",2017,6944,abs/1708.07747,"[145642373.0, 4565995.0, 2742129.0]",145642373.0,Sarajevo,2,"[1682479.0, 93811144.0]",Y,"['In the experiment, the proposed method shows no or slight degradation of accuracy with big savings in communication cost.', 'Missing citations Do Deep Nets Really Need to be Deep?']",,,arXiv.org,"['images', 'training', 'dataset', 'machine learning', 'Machine Learning']"
Machine Learning,4954fa180728932959997a4768411ff9136aac81,TensorFlow: A system for large-scale machine learning,https://www.semanticscholar.org/paper/4954fa180728932959997a4768411ff9136aac81,Conference,"TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous ""parameter server"" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",2016,16886,,"[2057642721.0, 144758007.0, 2108406634.0, 2545358.0, 36347083.0, 49959210.0, 145139947.0, 1780892.0, 2060655766.0, 2090818.0, 1942300.0, 3369421.0, 3089272.0, 144375552.0, 20154699.0, 32163737.0, 2080690.0, 2053781980.0, 47941411.0, 35078078.0, 2117163698.0, 2108113547.0]",2057642721.0,Zagreb,2,"[145071265.0, 1404359012.0]",Y,"['Is it meaningful to perform ADD or SUBSTRACT on the leaned code?', '“We can quantitatively determine how close the posterior is to a FFG distribution by comparing the Optimal FFG bound and the Optimal Flow bound.”: Why not just compare the optimal with the AIS evaluation?']",USENIX Symposium on Operating Systems Design and Implementation,16.0,,"['tensorflow', 'machine', 'state', 'machine learning', 'Machine Learning']"
Machine Learning,9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,JournalArticle,"TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.",2016,10416,abs/1603.04467,"[2057642721.0, 2078528337.0, 144758007.0, 2445241.0, 2545358.0, 48738717.0, 32131713.0, 36347083.0, 49959210.0, 145139947.0, 1780892.0, 153440022.0, 2064102917.0, 2060655766.0, 2090818.0, 39978391.0, 1944541.0, 40527594.0, 1942300.0, 3369421.0, 30415265.0, 3089272.0, 144375552.0, 20154699.0, 37232298.0, 144927151.0, 1789737.0, 32163737.0, 1701686.0, 35210462.0, 2080690.0, 2657155.0, 2053781980.0, 1765169.0, 1689108.0, 47941411.0, 145233583.0, 35078078.0, 2117163698.0, 2152198093.0]",2057642721.0,Belgrade,3,"[30504500.0, 3045593.0, 2110760082.0]",Y,"['In that perspective, it would be interesting to see the comparisons against other attention mechanisms (e.g. Zhou et al 2016) in terms of localization performance.', 'They are the best in terms of precision.', 'An autoencoder is utilized for dimensionality reduction alongside a clustering objective - that is the autoencoder optimizes the mse (using LSTM layers are utilized in the autoencoder for modelling temporal information), while the latent space  is fed into the temporal clustering layer.']",,,arXiv.org,"['interface', 'algorithms', 'implementation', 'machine learning', 'Machine Learning']"
Machine Learning,f9c990b1b5724e50e5632b94fdb7484ece8a6ce7,Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting,https://www.semanticscholar.org/paper/f9c990b1b5724e50e5632b94fdb7484ece8a6ce7,Conference,"The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.",2015,6501,,"[3008587.0, 2192200.0, 49528584.0, 1739816.0, 145771919.0, 2183294.0]",3008587.0,Madrid,3,"[48510386.0, 1825752990.0, 2545358.0]",Y,"['Good contribution The paper is well written, and the authors do an admirable job of motivating their primary contributions throughout the early portions of the paper.', 'The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models.', 'Inclusion of Assumption 3 would in particular require better justification.']",Neural Information Processing Systems,15.0,,"['precipitation', 'problem', 'nowcasting', 'machine learning', 'Machine Learning']"
Machine Learning,597bd2e45427563cdf025e53a3239006aa364cfc,Open Graph Benchmark: Datasets for Machine Learning on Graphs,https://www.semanticscholar.org/paper/597bd2e45427563cdf025e53a3239006aa364cfc,JournalArticle,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .",2020,1841,abs/2005.00687,"[48594758.0, 3410500.0, 2095762.0, 2047998.0, 40046694.0, 2156641189.0, 1754926.0, 1702139.0]",48594758.0,Madrid,2,"[2580290.0, 2660652.0]",Y,"['The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss.', 'The core idea is to train the model on pairs of images corresponding to the same content but varying in views, using adversarial training to discriminate such examples from generated pairs.']",,,Neural Information Processing Systems,"['ogb', 'graph', 'datasets', 'machine learning', 'Machine Learning']"
Machine Learning,53b047e503f4c24602f376a774d653f7ed56c024,Practical Black-Box Attacks against Machine Learning,https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024,Conference,"Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",2016,3188,,"[1967156.0, 144061974.0, 153440022.0, 1680133.0, 144643812.0, 144231976.0]",1967156.0,Bucharest,2,"[2057642721.0, 1829303908.0]",Y,"['This paper introduces a method for regularizing the REINFORCE algorithm by keeping around a small set of known high-quality samples as part of the sample set when performing stochastic gradient estimation.', '1. An important point regarding the reference ambiguity problem and eq.']",ACM Asia Conference on Computer and Communications Security,16.0,,"['dnn', 'examples', 'attack', 'machine learning', 'Machine Learning']"
Machine Learning,e8b30ebe3351680c3b039555ae0a8d0865ad829b,Neural Networks And Machine Learning,https://www.semanticscholar.org/paper/e8b30ebe3351680c3b039555ae0a8d0865ad829b,Conference,"Recent years have seen an increase in the popularity of neural network (NN) research. The mammalian brain, which consists of billions of interconnected neurons, is renowned for its ability to perform computationally hard tasks, such as face recognition, body motion planning, and muscle activity control. In an effort to emulate the effectiveness of biological neural networks in learning, artificial neural networks (ANNs) were developed. The NN technique has been the topic of many studies over the last few decades, with applications in many fields including control engineering, automation, aerospace, psychology, economics, healthcare, and energy science. The objective of the discipline of machine learning is to create computers that can independently learn and improve. In this chapter, we have attempted to depict the types of neural networks and machine learning as well as their applications in different industrial disciplines such as science, commerce, and medicine.",2023,76,,"[102998577.0, 2215913005.0, 2215954607.0, 2215958663.0, 116180161.0, 2275394607.0, 70310638.0]",102998577.0,Vilnius,2,"[2061881331.0, 2074432.0]",Y,"['* There are many estimators for f-divergences (like the ones cited above and many others based e.g. on nearest-neighbors) that are sample-based and thus correspond to the ""implicit"" case that the authors discuss.', 'After equation 5, the authors suggest categorical loss for discrete problems, but cross-entropy loss might work better.']","2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)",5.0,,"['networks', 'control', 'applications', 'machine learning', 'Machine Learning']"
Machine Learning,69a72ff5b30642d11c96635e99aadad3140d33a7,CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation,https://www.semanticscholar.org/paper/69a72ff5b30642d11c96635e99aadad3140d33a7,JournalArticle,"Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.",2021,686,abs/2102.04664,"[2115338656.0, 2278834796.0, 50052368.0, 145505727.0, 47090739.0, 37488446.0, 2064509404.0, 1943097969.0, 71790825.0, 39483833.0, 1410115257.0, 2143359114.0, 24962156.0, 2135918679.0, 40626221.0, 50175330.0, 92660691.0, 46429989.0, 145507437.0, 1702996983.0, 2072784644.0, 1803054.0]",2115338656.0,Tallinn,3,"[8129718.0, 51917504.0, 2257310922.0]",Y,"['So, I wish to see a section on testing with Resnet and GoogleNet.', 'The partitioning of each task must currently be designed by hand.', 'For this purpose, the authors chose several crieteria: the first two moments of the posterior, the log marginal likelihood and the predictive log-likelihood.']",,,NeurIPS Datasets and Benchmarks,"['codexglue', 'datasets', 'research', 'machine learning', 'Machine Learning']"
Machine Learning,e2a85a6766b982ff7c8980e57ca6342d22493827,Adversarial Machine Learning at Scale,https://www.semanticscholar.org/paper/e2a85a6766b982ff7c8980e57ca6342d22493827,JournalArticle,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.",2016,2703,abs/1611.01236,"[145714153.0, 153440022.0, 1751569.0]",145714153.0,Rome,3,"[31590259.0, 3239480.0, 145291669.0]",Y,"['This paper is clearly written, proposes a simple model and seems to outperform current methods.', ""It's possible some of the issues arise from the particular architectures they choose to investigate and demonstrate on (eg I have mostly seen ResNets in the context of CNNs but they analyze on FC topologies, the form of the loss, etc) but that's a guess and there are some further analysis in the supp material for these networks which I haven't looked at in detail."", 'Interesting experiments but lack of model description The authors propose to use a byte level RNN to classify reviews.']",,,International Conference on Learning Representations,"['training', 'examples', 'models', 'machine learning', 'Machine Learning']"
Machine Learning,7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e,CrypTen: Secure Multi-Party Computation Meets Machine Learning,https://www.semanticscholar.org/paper/7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e,JournalArticle,"Secure multi-party computation (MPC) allows parties to perform computations on data while keeping that data private. This capability has great potential for machine-learning applications: it facilitates training of machine-learning models on private data sets owned by different parties, evaluation of one party's private model using another party's private data, etc. Although a range of studies implement machine-learning models via secure MPC, such implementations are not yet mainstream. Adoption of secure MPC is hampered by the absence of flexible software frameworks that""speak the language""of machine-learning researchers and engineers. To foster adoption of secure MPC in machine learning, we present CrypTen: a software framework that exposes popular secure MPC primitives via abstractions that are common in modern machine-learning frameworks, such as tensor computations, automatic differentiation, and modular neural networks. This paper describes the design of CrypTen and measure its performance on state-of-the-art models for text classification, speech recognition, and image classification. Our benchmarks show that CrypTen's GPU support and high-performance communication between (an arbitrary number of) parties allows it to perform efficient private evaluation of modern machine-learning models under a semi-honest threat model. For example, two parties using CrypTen can securely predict phonemes in speech recordings using Wav2Letter faster than real-time. We hope that CrypTen will spur adoption of secure MPC in the machine-learning community.",2021,218,abs/2109.00984,"[2713842.0, 2262405.0, 144479015.0, 2264597.0, 3407874.0, 1803520.0]",2713842.0,Vaduz,2,"[48662861.0, 3055912.0]",Y,"['Also, they argue that batch normalization in this network is replaced with the shift-quantize operations, and what is matter in this case is (1) the relative values (“orientations”) and not the absolute values and (2) small values in errors are negligible.', 'This is a nice result, but I did not feel as though their algorithm was sufficently different from the algorithm used by Liang et.']",,,Neural Information Processing Systems,"['machinelearning', 'mpc', 'parties', 'machine learning', 'Machine Learning']"
Machine Learning,9e27190f2d9b2167d4a66b88696def4585072fd5,SoilGrids250m: Global gridded soil information based on machine learning,https://www.semanticscholar.org/paper/9e27190f2d9b2167d4a66b88696def4585072fd5,JournalArticle,"This paper describes the technical development and accuracy assessment of the most recent and improved version of the SoilGrids system at 250m resolution (June 2016 update). SoilGrids provides global predictions for standard numeric soil properties (organic carbon, bulk density, Cation Exchange Capacity (CEC), pH, soil texture fractions and coarse fragments) at seven standard depths (0, 5, 15, 30, 60, 100 and 200 cm), in addition to predictions of depth to bedrock and distribution of soil classes based on the World Reference Base (WRB) and USDA classification systems (ca. 280 raster layers in total). Predictions were based on ca. 150,000 soil profiles used for training and a stack of 158 remote sensing-based soil covariates (primarily derived from MODIS land products, SRTM DEM derivatives, climatic images and global landform and lithology maps), which were used to fit an ensemble of machine learning methods—random forest and gradient boosting and/or multinomial logistic regression—as implemented in the R packages ranger, xgboost, nnet and caret. The results of 10–fold cross-validation show that the ensemble models explain between 56% (coarse fragments) and 83% (pH) of variation with an overall average of 61%. Improvements in the relative accuracy considering the amount of variation explained, in comparison to the previous version of SoilGrids at 1 km spatial resolution, range from 60 to 230%. Improvements can be attributed to: (1) the use of machine learning instead of linear regression, (2) to considerable investments in preparing finer resolution covariate layers and (3) to insertion of additional soil profiles. Further development of SoilGrids could include refinement of methods to incorporate input uncertainties and derivation of posterior probability distributions (per pixel), and further automation of spatial modeling so that soil maps can be generated for potentially hundreds of soil variables. Another area of future research is the development of methods for multiscale merging of SoilGrids predictions with local and/or national gridded soil products (e.g. up to 50 m spatial resolution) so that increasingly more accurate, complete and consistent global soil information can be produced. SoilGrids are available under the Open Data Base License.",2017,2378,12,"[2856207.0, 7549589.0, 145253407.0, 9583743.0, 30504500.0, 9030720.0, 2228185935.0, 3376186.0, 3018223.0, 1402912902.0, 145669099.0, 145632581.0, 145028966.0, 49399380.0, 100653750.0, 32830771.0, 2924968.0, 2146245.0, 4953836.0]",2856207.0,Oslo,3,"[51225422.0, 1718467.0, 2073283133.0]",Y,"['To accommodate different goal stacking states the authors extend the state representation of DQN.', 'Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks Summary: The paper proposes Neumman optimizer, which makes some adjustments to the idealized Neumman algorithm to improve performance and stability in training.', 'Comments: The paper is badly structured and is sometimes hard to read because it does not present in a linear way the classic ingredients of Machine Learning, expression of the full function to be estimated, equations of each layer, description of the set of parameters to be learned and the loss function.']",,,PLoS ONE,"['soil', 'soilgrids', 'resolution', 'machine learning', 'Machine Learning']"
Machine Learning,2afa490dde7a8c582d889530c7f8b042fef6a8b7,Machine learning–accelerated computational fluid dynamics,https://www.semanticscholar.org/paper/2afa490dde7a8c582d889530c7f8b042fef6a8b7,JournalArticle,"Significance Accurate simulation of fluids is important for many science and engineering problems but is very computationally demanding. In contrast, machine-learning models can approximate physics very quickly but at the cost of accuracy. Here we show that using machine learning inside traditional fluid simulations can improve both accuracy and speed, even on examples very different from the training data. Our approach opens the door to applying machine learning to large-scale physical modeling tasks like airplane design and climate prediction. Numerical simulation of fluids plays an essential role in modeling many physical phenomena, such as weather, climate, aerodynamics, and plasma physics. Fluids are well described by the Navier–Stokes equations, but solving these equations at scale remains daunting, limited by the computational cost of resolving the smallest spatiotemporal features. This leads to unfavorable trade-offs between accuracy and tractability. Here we use end-to-end deep learning to improve approximations inside computational fluid dynamics for modeling two-dimensional turbulent flows. For both direct numerical simulation of turbulence and large-eddy simulation, our results are as accurate as baseline solvers with 8 to 10× finer resolution in each spatial dimension, resulting in 40- to 80-fold computational speedups. Our method remains stable during long simulations and generalizes to forcing functions and Reynolds numbers outside of the flows where it is trained, in contrast to black-box machine-learning approaches. Our approach exemplifies how scientific computing can leverage machine learning and hardware accelerators to improve simulations without sacrificing accuracy or generalization.",2021,536,118,"[40833997.0, 2119124568.0, 2078196952.0, 2286764736.0, 36397553.0, 2257229905.0]",40833997.0,Reykjavik,2,"[3033269.0, 3094352.0]",Y,"['Suggestion: After capturing the motivation of the task, I suspect that the traditional tree-to-tree (also X-to-tree) ""statistical"" machine translation methods still can also work correctly in this task.', 'I feel that the authors should give a more prominent disclaimer to potential users of the test.']",,,Proceedings of the National Academy of Sciences of the United States of America,"['simulation', 'accuracy', 'fluids', 'machine learning', 'Machine Learning']"
Machine Learning,62df84d6a4d26f95e4714796c2337c9848cc13b5,MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems,https://www.semanticscholar.org/paper/62df84d6a4d26f95e4714796c2337c9848cc13b5,JournalArticle,"MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. 
This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.",2015,2146,abs/1512.01274,"[1913774.0, 2124778071.0, 2110420880.0, 1491081747.0, 48246959.0, 1508337194.0, 39102205.0, 2113742783.0, 151505981.0, 38448016.0]",1913774.0,Rome,2,"[2167765.0, 2483738.0]",Y,"['On empirical evaluation, I already mentioned that it impossible to understand what the classification problem on TIMIT is. I suspect it might be speaker identification.', 'Although, a more detailed discussion and a clearer explanation is needed to clarify what SSF is actually doing, based on the provided formulation.']",,,arXiv.org,"['mxnet', 'expression', 'tensor', 'machine learning', 'Machine Learning']"
Machine Learning,c5c4142a01981787a71bf6ebcb791520c458ab5d,FedML: A Research Library and Benchmark for Federated Machine Learning,https://www.semanticscholar.org/paper/c5c4142a01981787a71bf6ebcb791520c458ab5d,JournalArticle,"Federated learning is a rapidly growing research field in the machine learning domain. Although considerable research efforts have been made, existing libraries cannot adequately support diverse algorithmic development (e.g., diverse topology and flexible message exchange), and inconsistent dataset and model usage in experiments make fair comparisons difficult. In this work, we introduce FedML, an open research library and benchmark that facilitates the development of new federated learning algorithms and fair performance comparisons. FedML supports three computing paradigms (distributed training, mobile on-device training, and standalone simulation) for users to conduct experiments in different system environments. FedML also promotes diverse algorithmic research with flexible and generic API design and reference baseline implementations. A curated and comprehensive benchmark dataset for the non-I.I.D setting aims at making a fair comparison. We believe FedML can provide an efficient and reproducible means of developing and evaluating algorithms for the federated learning research community. We maintain the source code, documents, and user community at this https URL.",2020,420,abs/2007.13518,"[31927890.0, 2239461.0, 144491689.0, 2116031156.0, 2109798334.0, 2118775509.0, 2927870.0, 2034349211.0, 49660254.0, 2144035454.0, 144259957.0, 1505828520.0, 1614034792.0, 145711633.0, 153096457.0, 145599558.0, 121011351.0]",31927890.0,Andorra,3,"[93421340.0, 2990847.0, 50064714.0]",Y,"['Considering that this is a big, complicated system, it is crucial that the authors included both an ablation experiment to see which pieces were most important, and an experiment that indicates the amount of labeled data that would be required to achieve the same results with a supervised system.', 'Wasserstein distances for eliminating batch effect, not enough novelty and no thorough comparisons to other methods.', 'Inclusion of Assumption 3 would in particular require better justification.']",,,arXiv.org,"['research', 'fedml', 'development', 'machine learning', 'Machine Learning']"
Machine Learning,696b388ee6221c6dbcfd647a06883b2bfee773d9,Universal Differential Equations for Scientific Machine Learning,https://www.semanticscholar.org/paper/696b388ee6221c6dbcfd647a06883b2bfee773d9,JournalArticle,"
 In the context of science, the well-known adage “a picture is worth a thousand words” might well be “a model is worth a thousand datasets.” Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring ""big data"". In this work demonstrate how a mathematical object, which we denote universal differential equations (UDEs), can be utilized as a theoretical underpinning to a diverse array of problems in scientific machine learning to yield efficient algorithms and generalized approaches. The UDE model augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating the training of physics-informed neural networks and large-eddy simulations, can all be transformed into UDE training problems that are efficiently solved by a single software methodology.",2020,448,abs/2001.04385,"[5365695.0, 20859037.0, 118225834.0, 1482544386.0, 123251938.0, 93421340.0, 2056678977.0, 37288593.0]",5365695.0,Skopje,2,"[49102717.0, 144531567.0]",Y,"['But, apparently, this is not the problem the authors actually solve, according to eq.', 'The authors propose a technique that achieves this aim by borrowing ideas from programming language and abstract interpretation.']",,,arXiv.org,"['data', 'model', 'models', 'machine learning', 'Machine Learning']"
Machine Learning,31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a,Prediction of Heart Disease Using a Combination of Machine Learning and Deep Learning,https://www.semanticscholar.org/paper/31cf4c96c5dd4ac5a6bbb4ac7b6bab763651624a,JournalArticle,"The correct prediction of heart disease can prevent life threats, and incorrect prediction can prove to be fatal at the same time. In this paper different machine learning algorithms and deep learning are applied to compare the results and analysis of the UCI Machine Learning Heart Disease dataset. The dataset consists of 14 main attributes used for performing the analysis. Various promising results are achieved and are validated using accuracy and confusion matrix. The dataset consists of some irrelevant features which are handled using Isolation Forest, and data are also normalized for getting better results. And how this study can be combined with some multimedia technology like mobile devices is also discussed. Using deep learning approach, 94.2% accuracy was obtained.",2021,249,2021,"[2066165404.0, 6806161.0, 2274111800.0, 2758504.0, 1577665701.0, 2218973562.0]",2066165404.0,Vaduz,3,"[9319875.0, 2143272549.0, 145217343.0]",Y,"['The biggest weakness of the paper (and the reason for my final decision) is that the paper completely goes easy on baseline models.', 'Preference elicitation and inverse reinforcement learning.', 'Detailed comments: The problem of unsupervised time series clustering is important and challenging.']",,,Computational Intelligence and Neuroscience,"['results', 'dataset', 'prediction', 'machine learning', 'Machine Learning']"
Machine Learning,4d8f0ae904779a50b2e18fec49e51a5661a98d8a,MRI-Based Brain Tumor Classification Using Ensemble of Deep Features and Machine Learning Classifiers,https://www.semanticscholar.org/paper/4d8f0ae904779a50b2e18fec49e51a5661a98d8a,JournalArticle,"Brain tumor classification plays an important role in clinical diagnosis and effective treatment. In this work, we propose a method for brain tumor classification using an ensemble of deep features and machine learning classifiers. In our proposed framework, we adopt the concept of transfer learning and uses several pre-trained deep convolutional neural networks to extract deep features from brain magnetic resonance (MR) images. The extracted deep features are then evaluated by several machine learning classifiers. The top three deep features which perform well on several machine learning classifiers are selected and concatenated as an ensemble of deep features which is then fed into several machine learning classifiers to predict the final output. To evaluate the different kinds of pre-trained models as a deep feature extractor, machine learning classifiers, and the effectiveness of an ensemble of deep feature for brain tumor classification, we use three different brain magnetic resonance imaging (MRI) datasets that are openly accessible from the web. Experimental results demonstrate that an ensemble of deep features can help improving performance significantly, and in most cases, support vector machine (SVM) with radial basis function (RBF) kernel outperforms other machine learning classifiers, especially for large datasets.",2021,223,21,"[2837279.0, 48740398.0, 1969352.0]",2837279.0,San Marino,3,"[8652308.0, 2109404730.0, 2273657478.0]",Y,"['Practical contributions: The paper introduces a new technique for training DNNs by forming a convex combination between two training data instances, as well as changing the associated label to the corresponding convex combination of the original 2 labels.', ""It's possible some of the issues arise from the particular architectures they choose to investigate and demonstrate on (eg I have mostly seen ResNets in the context of CNNs but they analyze on FC topologies, the form of the loss, etc) but that's a guess and there are some further analysis in the supp material for these networks which I haven't looked at in detail."", '""we use 2 transformation matrixes"" -> if you could please provide more details How is equation 2 related to figure 1?']",,,Italian National Conference on Sensors,"['machine', 'features', 'classifiers', 'machine learning', 'Machine Learning']"
Machine Learning,74b4f16c5ac91e3e7c88ae81cc8c91416b71d151,Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning,https://www.semanticscholar.org/paper/74b4f16c5ac91e3e7c88ae81cc8c91416b71d151,JournalArticle,"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.",2020,311,abs/2002.05651,"[40068904.0, 2143911040.0, 8365320.0, 2563117.0, 1746807.0, 145134886.0]",40068904.0,Riga,3,"[145735309.0, 1679216.0, 2560362.0]",Y,"['The authors evaluate their method based on a mechanical turk survey, where humans are asked to judge the realism of the generated images; additionally, they propose to measure prediction quality by the distance between the manually annotated positions of objects within ground truth and predicted frames.', 'I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations.', 'Many relevant papers could be referenced in the introduction as examples of successes in computer vision tasks,  identity mapping initialization, recent interpretations of ResNets/DensetNets, etc.']",,,arXiv.org,"['energy', 'carbon', 'machine', 'machine learning', 'Machine Learning']"
Machine Learning,9583ac53a19cdf0db81fef6eb0b63e66adbe2324,Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent,https://www.semanticscholar.org/paper/9583ac53a19cdf0db81fef6eb0b63e66adbe2324,Conference,"We study the resilience to Byzantine failures of distributed implementations of Stochastic Gradient Descent (SGD). So far, distributed machine learning frameworks have largely ignored the possibility of failures, especially arbitrary (i.e., Byzantine) ones. Causes of failures include software bugs, network asynchrony, biases in local datasets, as well as attackers trying to compromise the entire system. Assuming a set of n workers, up to f being Byzantine, we ask how resilient can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient aggregation rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the aggregation rule capturing the basic requirements to guarantee convergence despite f Byzantine workers. We propose Krum, an aggregation rule that satisfies our resilience property, which we argue is the first provably Byzantine-resilient algorithm for distributed SGD. We also report on experimental evaluations of Krum.",2017,1176,,"[3094352.0, 9623412.0, 1727558.0, 1718150.0]",3094352.0,Amsterdam,3,"[2089990776.0, 2248549493.0, 2115692258.0]",Y,"['- In Section 4.3, why did you consider the entropy regularizer?', '-  Please also discuss the relationships, connections, and possible applications of your technique to other algorithms used in Bayesian optimization, active learning and/or sequential learning, for instance as M. U. Gutmann and J. Corander, “Bayesian optimization for likelihood-free inference of simulator-based statistical mod- els,” Journal of Machine Learning Research, vol.', 'The authors of this paper propose to use a deterministic policy instead, and apply the deterministic policy gradient DPG (Silver et al., 2014) for optimizing the behavior policy.']",Neural Information Processing Systems,17.0,,"['resilience', 'failures', 'sgd', 'machine learning', 'Machine Learning']"
Machine Learning,573fd2ce97c70bb29097e8efb28a27af791225ca,BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain,https://www.semanticscholar.org/paper/573fd2ce97c70bb29097e8efb28a27af791225ca,JournalArticle,"Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a \emph{BadNet}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of {25}\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.",2017,1296,abs/1708.06733,"[2367353.0, 1398683279.0, 1696125.0]",2367353.0,Helsinki,3,"[2162042348.0, 143813668.0, 7482477.0]",Y,"['The paper considers single-agent problems and tests on Ms Pacman etc.', 'Because of this work seems likely to be of limited interest to the ICLR audience, most of which are potentially interested users rather than compiler experts.', 'Considering that this is a big, complicated system, it is crucial that the authors included both an ablation experiment to see which pieces were most important, and an experiment that indicates the amount of labeled data that would be required to achieve the same results with a supervised system.']",,,arXiv.org,"['networks', 'network', 'sign', 'machine learning', 'Machine Learning']"
Machine Learning,f86f1748d1b6d22870f4347fd5d65314ba800583,Reconciling modern machine-learning practice and the classical bias–variance trade-off,https://www.semanticscholar.org/paper/f86f1748d1b6d22870f4347fd5d65314ba800583,JournalArticle,"Significance While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.",2018,1268,116,"[145520115.0, 143724861.0, 143791100.0, 151213231.0]",145520115.0,Andorra,3,"[2238395310.0, 152125305.0, 145720154.0]",Y,"['Each extension to the Dual Actor-Critic is well motivated and clear in context.', 'Despite the thoroughness of the task and model descriptions, the proposed method is not well motivated.', 'The core idea is to represent a program as a graph that a GGNN can take as input, and train the GGNN to make token-level predictions that depend on the semantic context.']",,,Proceedings of the National Academy of Sciences of the United States of America,"['performance', 'models', 'data', 'machine learning', 'Machine Learning']"
Machine Learning,f70b2f20be241f445a61f33c4b8e76e554760340,Software Engineering for Machine Learning: A Case Study,https://www.semanticscholar.org/paper/f70b2f20be241f445a61f33c4b8e76e554760340,Conference,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",2019,616,,"[1719124.0, 1776779.0, 145193818.0, 1710751.0, 50355692.0, 1783184.0, 1693689.0, 2571049.0, 143609903.0]",1719124.0,Skopje,2,"[2112455515.0, 144027436.0]",Y,"['This would contradict some previously established convergence results for this type of problems: Reddi et al. (2016) Stochastic Variance Reduction for Nonconvex Optimization, ICML and Wang et al. 2013.', 'But in that case much of the theoretical story goes out the window.']",2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),41.0,,"['software', 'teams', 'microsoft', 'machine learning', 'Machine Learning']"
Machine Learning,b7a717233ec3ff37385ab1b06816d0ca375f5bb3,Data Shapley: Equitable Valuation of Data for Machine Learning,https://www.semanticscholar.org/paper/b7a717233ec3ff37385ab1b06816d0ca375f5bb3,Conference,"As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on $n$ data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.",2019,515,,"[27316199.0, 145085305.0]",27316199.0,Budapest,3,"[1793506.0, 1702822.0, 2362078.0]",Y,"['Additional regularization terms are also added to encourage the model to encode longer term dependencies in its latent distributions.', 'The current analysis is too simple.', 'A particularly interesting question would be whether the proposed model actually is a direct GAN-based extension of IBFA, and if not then how does it differ.']",International Conference on Machine Learning,19.0,,"['data', 'shapley', 'value', 'machine learning', 'Machine Learning']"
Machine Learning,d0ab11de3077490c80a08abd0fb8827bac84c454,MoleculeNet: a benchmark for molecular machine learning,https://www.semanticscholar.org/paper/d0ab11de3077490c80a08abd0fb8827bac84c454,JournalArticle,"A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms.",2017,1272,9,"[9957625.0, 2378027.0, 5932099.0, 145986494.0, 9959840.0, 5929246.0, 40867019.0, 1806271.0]",9957625.0,Nicosia,3,"[1764004.0, 145385471.0, 2256558402.0]",Y,"['Why will D answer negatively (or positively) on this example ?', 'Since this is so important to the results, more analysis would be helpful.', 'Overall the work is important, original, well-executed, and should open new directions for deep learning in program analysis.']",,,Chemical Science,"['scale', 'benchmark', 'machine', 'machine learning', 'Machine Learning']"
Machine Learning,71a85e735a3686bef8cce3725ae5ba82e2cabb1b,Underspecification Presents Challenges for Credibility in Modern Machine Learning,https://www.semanticscholar.org/paper/71a85e735a3686bef8cce3725ae5ba82e2cabb1b,JournalArticle,"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.",2020,546,23,"[1396841807.0, 145993598.0, 40497400.0, 1874006.0, 48401711.0, 2638246.0, 2110195795.0, 1695378.0, 144154709.0, 28552618.0, 2420527.0, 2815290.0, 3108448.0, 3451901.0, 6413143.0, 34302129.0, 2146275249.0, 6322777.0, 2007712128.0, 4836115.0, 145071265.0, 81408931.0, 144223091.0, 2065412355.0, 25897803.0, 2035210.0, 88478180.0, 144042306.0, 3212089.0, 6454443.0, 2007741250.0, 46537606.0, 2974320.0, 3316311.0, 1524732527.0, 20825661.0, 2856607.0, 2520251.0, 2743563.0, 1733143.0]",1396841807.0,Dublin,2,"[2006108901.0, 2810600.0]",Y,"['It puts forward a qualitative analogy between some recently observed behaviours in deep learning and results stemming from previous quantitative statistical physics analysis of single and two-layer neural networks.', '(or equivalently, the optimal generator under the density-ratio-estimator interpretation of discriminators proposed by https://arxiv.org/abs/1610.03483).']",,,Journal of machine learning research,"['pipelines', 'domains', 'underspecification', 'machine learning', 'Machine Learning']"
Machine Learning,2bc3644ce4de7fce5812c1455e056649a47c1bbf,Effective Heart Disease Prediction Using Hybrid Machine Learning Techniques,https://www.semanticscholar.org/paper/2bc3644ce4de7fce5812c1455e056649a47c1bbf,JournalArticle,"Heart disease is one of the most significant causes of mortality in the world today. Prediction of cardiovascular disease is a critical challenge in the area of clinical data analysis. Machine learning (ML) has been shown to be effective in assisting in making decisions and predictions from the large quantity of data produced by the healthcare industry. We have also seen ML techniques being used in recent developments in different areas of the Internet of Things (IoT). Various studies give only a glimpse into predicting heart disease with ML techniques. In this paper, we propose a novel method that aims at finding significant features by applying machine learning techniques resulting in improving the accuracy in the prediction of cardiovascular disease. The prediction model is introduced with different combinations of features and several known classification techniques. We produce an enhanced performance level with an accuracy level of 88.7% through the prediction model for heart disease with the hybrid random forest with a linear model (HRFLM).",2019,791,7,"[150302778.0, 9727014.0, 144369609.0]",150302778.0,Rome,2,"[2158585032.0, 3428490.0]",Y,"['The model is evaluated on the following tasks: * Qualitative results on denoising and one-shot generation using the Omniglot dataset.', 'The paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optimized.']",,,IEEE Access,"['disease', 'prediction', 'techniques', 'machine learning', 'Machine Learning']"
LLM,a0a79dad89857a96f8f71b14238e5237cbfc4787,Judging LLM-as-a-judge with MT-Bench and Chatbot Arena,https://www.semanticscholar.org/paper/a0a79dad89857a96f8f71b14238e5237cbfc4787,JournalArticle,"Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",2023,913,abs/2306.05685,"[2149970173.0, 2537924.0, 2209360681.0, 92721493.0, 1390573666.0, 2152482391.0, 143872641.0, 2141335450.0, 2117961435.0, 143977260.0, 145140331.0, 144307989.0, 2055174324.0]",2149970173.0,Tallinn,3,"[1783184.0, 51017707.0, 2108614875.0]",Y,"['(As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.)', 'Presumably, there exists some hyperparameters where the model does not achieve 100% test accuracy, in which case, what are the failure modes?', 'It is also not mentioned whether the other methods that the authors compare to are re-trained on their newly proposed training dataset.']",,,Neural Information Processing Systems,"['preferences', 'benchmarks', 'agreement', 'llm', 'LLM']"
LLM,e2a58fd18961c3941102989e3a3d0d27c615e015,Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic,https://www.semanticscholar.org/paper/e2a58fd18961c3941102989e3a3d0d27c615e015,JournalArticle,"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.",2023,196,abs/2306.15195,"[32811782.0, 2156120640.0, 13886055.0, 2109975984.0, 2075369514.0, 1395873384.0]",32811782.0,Copenhagen,2,"[144666776.0, 2126503480.0]",Y,"['Overall the work is important, original, well-executed, and should open new directions for deep learning in program analysis.', 'An additional heatmap generator component can be further included in the clustering model.']",,,arXiv.org,"['regions', 'language', 'tasks', 'llm', 'LLM']"
LLM,7a1e71cb1310c4a873e7a4e54d1a6dab0553adce,"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",https://www.semanticscholar.org/paper/7a1e71cb1310c4a873e7a4e54d1a6dab0553adce,JournalArticle,"Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.",2023,377,abs/2306.01116,"[2179371701.0, 1739899643.0, 80424302.0, 101370046.0, 2069293685.0, 2219490144.0, 91723794.0, 1967677.0, 143945447.0]",2179371701.0,Prague,2,"[1680133.0, 2155550922.0]",Y,"['The motivation is clear and the idea is simple and effective.', 'The models that are reviewed in the appendix, i.e. the continuous and Ising perceptron and the committee machine are more relevant.']",,,arXiv.org,"['models', 'data', 'web', 'llm', 'LLM']"
LLM,929305892d4ddae575a0fc23227a8139f7681632,Jailbroken: How Does LLM Safety Training Fail?,https://www.semanticscholar.org/paper/929305892d4ddae575a0fc23227a8139f7681632,JournalArticle,"Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of""jailbreak""attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes.",2023,219,abs/2307.02483,"[143797846.0, 3033269.0, 5164568.0]",143797846.0,Bern,3,"[3357166.0, 2257107248.0, 7469995.0]",Y,"['Should these parameters be take out of the n-step advantage function A?', 'The idea is a simple but useful extension of these previous works.', 'The “obverter” technique is quite interesting since it incorporates the concept from the theory of mind which is similar to human alignment or AGI approach.']",,,Neural Information Processing Systems,"['safety', 'models', 'failure', 'llm', 'LLM']"
LLM,2c5ab7d87e3342d2dba7d1d113ca1b16c545e344,AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration,https://www.semanticscholar.org/paper/2c5ab7d87e3342d2dba7d1d113ca1b16c545e344,JournalArticle,"Large language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth). In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. Our method is based on the observation that weights are not equally important: protecting only 1% of salient weights can greatly reduce quantization error. We then propose to search for the optimal per-channel scaling that protects the salient weights by observing the activation, not weights. AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs' generalization ability on different domains and modalities, without overfitting to the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks. Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement an efficient and flexible inference framework tailored for LLMs on the edge, offering more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPU (NVIDIA Jetson Orin 64GB).",2023,147,abs/2306.00978,"[46698300.0, 2214687479.0, 150127950.0, 2202210853.0, 2219266839.0, 2115659426.0]",46698300.0,Prague,3,"[2061706386.0, 144341429.0, 2092826.0]",Y,"['The final optimization problem that is used for training of the propose VAE should be formally defined.', 'The authors rely on two prior works in multi-task learning  that explore parameter sharing (Lee et al, 2016) and subspace learning (Kumar & Daume III 2012) for multi-task learning.', 'The authors evaluate their method based on a mechanical turk survey, where humans are asked to judge the realism of the generated images; additionally, they propose to measure prediction quality by the distance between the manually annotated positions of objects within ground truth and predicted frames.']",,,arXiv.org,"['quantization', 'awq', 'weights', 'llm', 'LLM']"
LLM,8f9e864fab09bbae4a46a2a62bb954db1a88eb3e,Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts,https://www.semanticscholar.org/paper/8f9e864fab09bbae4a46a2a62bb954db1a88eb3e,Conference,"Pre-trained large language models (“LLMs”) like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (“prompting”) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.",2023,177,,"[2138715050.0, 9063054.0, 2200271412.0, 152290618.0]",2138715050.0,Vienna,2,"[144330671.0, 2154742781.0]",Y,"['Perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the E-step output in EM for a GMM.', 'Section 5.3: Amortization error is an important contributor to the slack in the ELBO on MNIST, and the dominant contributor on the more complicated Fashion MNIST dataset.']",International Conference on Human Factors in Computing Systems,23.0,,"['design', 'language', 'interactions', 'llm', 'LLM']"
LLM,003ef1cd670d01af05afa0d3c72d72228f494432,LLM+P: Empowering Large Language Models with Optimal Planning Proficiency,https://www.semanticscholar.org/paper/003ef1cd670d01af05afa0d3c72d72228f494432,JournalArticle,"Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems.\footnote{The code and results are publicly available at https://github.com/Cranial-XIX/llm-pddl.git.",2023,157,abs/2304.11477,"[145306564.0, 48751979.0, 2682457.0, 2155193246.0, 40295359.0, 3045593.0, 2113909888.0]",145306564.0,Bucharest,3,"[2145913600.0, 2108614875.0, 2081346.0]",Y,"['I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations.', '1. There is hardly any baseline compared in the paper.', 'However, on one hand the contribution is rather narrow, and on the other the results presented do not clearly show that the contribution is of significance in practice.']",,,arXiv.org,"['language', 'llms', 'planning', 'llm', 'LLM']"
LLM,1a4c6856292b8c64d19a812a77f0aa6fd47cb96c,AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework,https://www.semanticscholar.org/paper/1a4c6856292b8c64d19a812a77f0aa6fd47cb96c,JournalArticle,"This technical report presents AutoGen , 1 a new framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable , and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. AutoGen ’s design offers multiple advantages: a) it gracefully navigates the strong but imperfect generation and reasoning abilities of these LLMs; b) it leverages human understanding and intelligence, while providing valuable automation through conversations between agents; c) it simplifies and unifies the implementation of complex LLM workflows as automated agent chats. We provide many diverse examples of how developers can easily use AutoGen to effectively solve tasks or build applications, ranging from coding, mathematics, operations research, entertainment, online decision-making, question answering, etc.",2023,131,abs/2308.08155,"[1777934740.0, 33340656.0, 47540245.0, 2115853457.0, 2116579935.0, 3055912.0, 3369602.0, 2231868268.0, 2232024704.0, 2116627318.0]",1777934740.0,Sofia,2,"[1749505.0, 1746466.0]",Y,"['One could also argue that the manuscript would profit from a better theoretical analysis of the IRL problem, say: C. A. Rothkopf, C. Dimitrakakis.', 'Is this on bAbi as well?']",,,arXiv.org,"['autogen', 'llm', 'agents', 'applications', 'LLM']"
LLM,fa75a55760e6ea49b39b83cb85c99a22e1088254,NExT-GPT: Any-to-Any Multimodal LLM,https://www.semanticscholar.org/paper/fa75a55760e6ea49b39b83cb85c99a22e1088254,JournalArticle,"While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/",2023,110,abs/2309.05519,"[1957924118.0, 2142672912.0, 1990265392.0, 144540018.0, 144078686.0]",1957924118.0,Vaduz,2,"[2047998.0, 2151226838.0]",Y,"['Originality The approach is a straightforward extension of the MCMCP approach using generative models.', 'Are the four mixture components over the robot parameters updated independently of each other when the parameter-exploring policy gradients updates are applied?']",,,arXiv.org,"['modalities', 'content', 'nextgpt', 'llm', 'LLM']"
LLM,4be7d1524edb0137599a5cc95f72844b85a52fe1,LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,https://www.semanticscholar.org/paper/4be7d1524edb0137599a5cc95f72844b85a52fe1,JournalArticle,"Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.",2022,337,abs/2208.07339,"[3239480.0, 35084211.0, 2037496520.0, 1982950.0]",3239480.0,Warsaw,3,"[144491689.0, 89387275.0, 144783437.0]",Y,"['Paper of broad interest for control tasks This is a well written paper, very nice work.', 'Standard idea, great results This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives.', ""In Kipf et al.'s GCN paper this is what was done (not over 100 splits as some other commenter claimed. The average over 100 runs  pertained to the ICA method only.)""]",,,arXiv.org,"['performance', 'models', 'inference', 'llm', 'LLM']"
LLM,f406aceba4f29cc7cfbe7edb2f52f01374486589,The Internal State of an LLM Knows When its Lying,https://www.semanticscholar.org/paper/f406aceba4f29cc7cfbe7edb2f52f01374486589,Conference,"While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\% to 83\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.",2023,102,,"[1746466.0, 144135485.0]",1746466.0,Belgrade,2,"[3011964.0, 1388622435.0]",Y,"['* Figure 2 seems like a test made to work for this method and does not add much to the paper.', 'For example, the authors claimed in Section 4.3 that the Stein gradient estimator is faster than other methods, but it is not clear as to why this is the case.']",Conference on Empirical Methods in Natural Language Processing,23.0,,"['llm', 'sentence', 'probability', 'statements', 'LLM']"
LLM,db4cf9f6a653d5c15973e836c800ea47743251ae,Prompt Injection attack against LLM-integrated Applications,https://www.semanticscholar.org/paper/db4cf9f6a653d5c15973e836c800ea47743251ae,JournalArticle,"Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.",2023,84,abs/2306.05499,"[2153627626.0, 73776889.0, 22799258.0, 3088630.0, 2146331573.0, 39584070.0, 51225422.0, 2124949853.0, 2152798056.0]",2153627626.0,Rome,2,"[2152719037.0, 1389546686.0]",Y,"['So, if no fork module is required for a question, the model architecture is effectively same as IEP?', 'These names have not being defined formally in the paper.']",,,arXiv.org,"['injection', 'applications', 'attack', 'llm', 'LLM']"
LLM,ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7,ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,https://www.semanticscholar.org/paper/ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7,JournalArticle,"Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.",2023,102,abs/2308.07201,"[2151547817.0, 2109136284.0, 48576745.0, 2231240312.0, 2195745651.0, 2222862479.0, 2215497308.0]",2151547817.0,Zagreb,2,"[2412941.0, 2142142532.0]",Y,"['2. Even if this was correct, the main point is that this is ""only"" d times worse - see eq (11).', '3. In presenting autoencoders it is crucial to note that they are all built around the idea of compression.']",,,arXiv.org,"['evaluation', 'llms', 'language', 'llm', 'LLM']"
LLM,017010b941d902a467f6d329ae5e74fd67e67912,LLM-Pruner: On the Structural Pruning of Large Language Models,https://www.semanticscholar.org/paper/017010b941d902a467f6d329ae5e74fd67e67912,JournalArticle,"Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner",2023,77,abs/2305.11627,"[15532066.0, 150110431.0, 48631088.0]",15532066.0,Bratislava,3,"[2214585980.0, 2178446.0, 2055623340.0]",Y,"['The paper says Deep Voice 2 (Arik et al., 2017a) is only prior work for multi-speaker TTS.', 'But the aspect I like most about this paper is the experimental analysis.', 'Exactly which gradients are you skipping at random?']",,,Neural Information Processing Systems,"['llms', 'language', 'models', 'llm', 'LLM']"
LLM,0983883619a0ca597d055d0e58da2f514052913d,"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration",https://www.semanticscholar.org/paper/0983883619a0ca597d055d0e58da2f514052913d,JournalArticle,"Although instruction-tuned large language models (LLMs) have exhibited remarkable capabilities across various NLP tasks, their effectiveness on other data modalities beyond text has not been fully studied. In this work, we propose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual, audio, and textual information. Macaw-LLM consists of three main components: a modality module for encoding multi-modal data, a cognitive module for harnessing pretrained LLMs, and an alignment module for harmonizing diverse representations. Our novel alignment module seamlessly bridges multi-modal features to textual features, simplifying the adaptation process from the modality modules to the cognitive module. In addition, we construct a large-scale multi-modal instruction dataset in terms of multi-turn dialogue, including 69K image instances and 50K video instances. We have made our data, code and model publicly available, which we hope can pave the way for future research in multi-modal LLMs and expand the capabilities of LLMs to handle diverse data modalities and address complex real-world scenarios.",2023,62,abs/2306.09093,"[2082426870.0, 2145209409.0, 1800190.0, 14799547.0, 47655790.0, 2112455515.0, 2072684668.0, 2909321.0]",2082426870.0,Paris,3,"[2108355738.0, 50096877.0, 92089600.0]",Y,"['""we use 2 transformation matrixes"" -> if you could please provide more details How is equation 2 related to figure 1?', 'The model is trained on 5 mazes and tested on 2 others.', ""Fig. 3a doesn't seem to show that the output is a clear mixture of Gaussians.""]",,,arXiv.org,"['module', 'llms', 'data', 'llm', 'LLM']"
LLM,bdb68c5e2369633b20e733774ac66eb4600c34d1,LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,https://www.semanticscholar.org/paper/bdb68c5e2369633b20e733774ac66eb4600c34d1,Conference,"The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on both reasoning tasks.",2023,61,,"[1557412457.0, 2150277971.0, 145131956.0, 2143418409.0, 2212836814.0, 38656724.0, 1996394.0, 1746416.0]",1557412457.0,Ljubljana,2,"[2921637.0, 2061202877.0]",Y,"['These structures are extrapolated  by', '(muscle routing parameters, including insertion and attachment points) are optimized along with the control).']",Conference on Empirical Methods in Natural Language Processing,23.0,,"['llms', 'methods', 'peft', 'llm', 'LLM']"
LLM,51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df,SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression,https://www.semanticscholar.org/paper/51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df,JournalArticle,"Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B parameter LLM on a single 24 GB consumer GPU without any performance degradation at 15% speedup thus making powerful LLMs available to consumer without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime. Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.",2023,60,abs/2306.03078,"[3239480.0, 2219555303.0, 52257721.0, 2006108901.0, 1502248377.0, 9543395.0, 2113838061.0, 1713648.0, 3311387.0]",3239480.0,Luxembourg,3,"[2103992921.0, 48647153.0, 2062940513.0]",Y,"['The proposed algorithm is an improvement of Net-Trim (Aghasi et al., 2016), which is to enforce the weights to be sparse.', 'In the experiment, the proposed method shows no or slight degradation of accuracy with big savings in communication cost.', '1. Comparing to previous work (Mordatch & Abbeel, 2018), the task is relatively simple, only requiring the agent to perform binary prediction.']",,,arXiv.org,"['llms', 'quantization', 'parameter', 'llm', 'LLM']"
LLM,f51bc74814a3452009ea5ca262d9768d08149ee6,Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model,https://www.semanticscholar.org/paper/f51bc74814a3452009ea5ca262d9768d08149ee6,JournalArticle,"The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks. Inspired by such successes, we adopt such an instruction-tuned LLM Flan-T5 as the text encoder for text-to-audio (TTA) generation -- a task where the goal is to generate an audio from its textual description. The prior works on TTA either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5. Consequently, our latent diffusion model (LDM)-based approach TANGO outperforms the state-of-the-art AudioLDM on most metrics and stays comparable on the rest on AudioCaps test set, despite training the LDM on a 63 times smaller dataset and keeping the text encoder frozen. This improvement might also be attributed to the adoption of audio pressure level-based sound mixing for training set augmentation, whereas the prior methods take a random mix.",2023,53,abs/2304.13731,"[32528506.0, 35122767.0, 3391951.0, 1746416.0]",32528506.0,London,2,"[6547490.0, 143794144.0]",Y,"['It seems that the methods outperforms existing methods for learning graph representations.', 'Yet, moreover, the negative log likelihood comparison (Table 2) is not an informative comparison, as it speaks only to the power of adding supervision.']",,,arXiv.org,"['encoder', 'language', 'llm', 'text', 'LLM']"
LLM,43e6e8d6663d83f1b74cf5a2be7b040b0928f867,X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages,https://www.semanticscholar.org/paper/43e6e8d6663d83f1b74cf5a2be7b040b0928f867,JournalArticle,"Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces independently. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. Our experiments show that X-LLM demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 84.5\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. And we also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.",2023,52,abs/2305.04160,"[49102717.0, 49835220.0, 2112675689.0, 2120251897.0, 145458655.0, 2111044238.0, 145764891.0]",49102717.0,Prague,2,"[2261454711.0, 66327914.0]",Y,"['Overall, while I find there are some interesting theoretical bits in this paper, it lacks focus, the experiments do not offer any surprises, and there are no comparisons with prior literature.', 'The idea of enforcing information isolation is brilliant.']",,,arXiv.org,"['x2l', 'language', 'llms', 'llm', 'LLM']"
LLM,6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2,LLM-QAT: Data-Free Quantization Aware Training for Large Language Models,https://www.semanticscholar.org/paper/6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2,JournalArticle,"Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings.",2023,50,abs/2305.17888,"[2109370860.0, 9185192.0, 2112729504.0, 48025720.0, 37502184.0, 2121361882.0, 152345059.0, 2065915235.0, 144137037.0]",2109370860.0,Chisinau,2,"[2803317.0, 38421496.0]",Y,"['In this work, the authors suggest the use of control variate schemes for estimating gradient values, within a reinforcement learning  framework.', 'A promising approach on nonparametric modelling of partial differential equations with deep architectures that requires more details.']",,,arXiv.org,"['quantization', 'methods', 'model', 'llm', 'LLM']"
LLM,993df7df129f8d18816877d69923d7df7b347d85,LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion,https://www.semanticscholar.org/paper/993df7df129f8d18816877d69923d7df7b347d85,Conference,"We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",2023,46,,"[2197076899.0, 1384550891.0, 51583409.0]",2197076899.0,Madrid,2,"[1390140002.0, 145843448.0]",Y,"['However, evaluating on simple datasets like Kaggle cat/dog and Oxford Flowers diminishes the value of the paper.', 'Pros: - A new GAIL formulation for saving on interaction data.']",Annual Meeting of the Association for Computational Linguistics,23.0,,"['llms', 'pairranker', 'llmblender', 'llm', 'LLM']"
LLM,ccd94602e3acecf999d0c9ba62b1a8bc02e9f696,PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization,https://www.semanticscholar.org/paper/ccd94602e3acecf999d0c9ba62b1a8bc02e9f696,JournalArticle,"Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our results indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation ability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage. All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM.",2023,67,abs/2306.05087,"[2108024273.0, 2164113313.0, 1557363420.0, 2145500840.0, 35504092.0, 2051536212.0, 1657285750.0, 2143721734.0, 1519290245.0, 1576441343.0, 145235149.0, 2145403564.0, 2211964951.0]",2108024273.0,Bucharest,2,"[5606742.0, 2504776.0]",Y,"['* There are many estimators for f-divergences (like the ones cited above and many others based e.g. on nearest-neighbors) that are sample-based and thus correspond to the ""implicit"" case that the authors discuss.', 'After all, reward seems to play a very important role for the proposed system.']",,,arXiv.org,"['pandalm', 'evaluation', 'models', 'llm', 'LLM']"
LLM,e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0,LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models,https://www.semanticscholar.org/paper/e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0,JournalArticle,"Recent advancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse images. However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning. This work proposes to enhance prompt understanding capabilities in diffusion models. Our method leverages a pretrained large language model (LLM) for grounded generation in a novel two-stage process. In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off-the-shelf diffusion model for layout-grounded image generation. Both stages utilize existing pretrained models without additional model parameter optimization. Our method significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts that require various capabilities, doubling the generation accuracy across four tasks on average. Furthermore, our method enables instruction-based multi-round scene specification and can handle prompts in languages not supported by the underlying diffusion model. We anticipate that our method will unleash users' creativity by accurately following more complex prompts. Our code, demo, and benchmark are available at: https://llm-grounded-diffusion.github.io",2023,45,abs/2305.13655,"[2054733874.0, 2132473300.0, 3383328.0, 1753210.0]",2054733874.0,Stockholm,3,"[150302778.0, 1786389.0, 48152160.0]",Y,"['If we consider e.g., a linear 1-layer autoencoder to be equivalent to PCA (without the rnn layers), in essence this formulation is closely related to applying pca to reduce the initial dimensionality and then t-sne.', 'Clarity: The content of the paper is unclear in certain areas.', 'The node embeddings are then projected into a 2-dimensional space by PCA.']",,,arXiv.org,"['diffusion', 'model', 'models', 'llm', 'LLM']"
LLM,16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277,Time-LLM: Time Series Forecasting by Reprogramming Large Language Models,https://www.semanticscholar.org/paper/16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277,JournalArticle,"Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.",2023,44,abs/2310.01728,"[2254096428.0, 2255363760.0, 2253908414.0, 2237992280.0, 2253786576.0, 2119204984.0, 2254173316.0, 2253824408.0, 2256011160.0, 2254047333.0, 2253561592.0]",2254096428.0,Tirana,2,"[144307989.0, 153693432.0]",Y,"['Overall, i think it makes a good contribution to a field that is gaining importance for mobile and embedded applications of deep convnets.', 'In this work, the authors suggest the use of control variate schemes for estimating gradient values, within a reinforcement learning  framework.']",,,arXiv.org,"['time', 'series', 'models', 'llm', 'LLM']"
LLM,92930ed3560ea6c86d53cf52158bc793b089054d,BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset,https://www.semanticscholar.org/paper/92930ed3560ea6c86d53cf52158bc793b089054d,JournalArticle,"In this paper, we introduce the \textsc{BeaverTails} dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 30,207 question-answer (QA) pairs and 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails. Warning: this paper contains example data that may be offensive or harmful.",2023,63,abs/2307.04657,"[2154630502.0, 2210950163.0, 14548852.0, 2190800297.0, 2221446410.0, 2221566410.0, 2217316509.0, 47796324.0]",2154630502.0,Amsterdam,3,"[2108438707.0, 2141735101.0, 66376493.0]",Y,"['A well written, clear paper presenting a novel representation of graphs   as multi-channel image-like structures from their node embeddings.', '1.  The paper misses some more recent reference, e.g. [a,b].', 'Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice?']",,,Neural Information Processing Systems,"['safety', 'llms', 'harmlessness', 'llm', 'LLM']"
LLM,d1a6b3a5efde3783b53f822dc8dd00aaac934b95,SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification,https://www.semanticscholar.org/paper/d1a6b3a5efde3783b53f822dc8dd00aaac934b95,JournalArticle,"The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree veriﬁcation. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM’s outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is veriﬁed by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree veriﬁer instead of an incremental decoder, which signiﬁcantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.",2023,42,abs/2305.09781,"[1720763480.0, 2638632.0, 2185953295.0, 2217941981.0, 2188983263.0, 2217487123.0, 2111498904.0, 1389546686.0, 2217683047.0, 2072782550.0]",1720763480.0,Skopje,2,"[2108781632.0, 3960565.0]",Y,"['Variance Reduction for Stochastic Gradient Optimization, NIPS.', 'I assume this is an artifact of the way the goal recognizer is trained.']",,,arXiv.org,"['tree', 'llm', 'language', 'models', 'LLM']"
LLM,54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f,ImageBind-LLM: Multi-modality Instruction Tuning,https://www.semanticscholar.org/paper/54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f,JournalArticle,"We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by a proposed visual cache model for further cross-modal embedding enhancement. The training-free cache model retrieves from three million image features extracted by ImageBind, which effectively mitigates the training-inference modality discrepancy. Notably, with our approach, ImageBind-LLM can respond to instructions of diverse modalities and demonstrate significant language generation quality. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.",2023,41,abs/2309.03905,"[150147382.0, 2115713503.0, 1485702259.0, 144740494.0, 2153917002.0, 2238398546.0, 3393556.0, 2238219059.0, 2238210433.0, 2237599228.0, 2238395310.0, 2204576896.0, 2125957.0, 2238219937.0, 27577617.0, 49404547.0, 2059129841.0]",150147382.0,Monaco,3,"[2243646160.0, 1396184193.0, 2128088.0]",Y,"['2. Employ “obverter” technique, showing that it can be an alternative approach comparing to RL', '2. Sharing the RNN for speaking and consuming by picking the token that maximizes the probability might decrease the diversity.', '- The claim of Theorem 2 in appendix B does not follow from its proof: what is proven is that the value of S(w) lies in an interval [1-e..1+e] with a certain probability for all w.']",,,arXiv.org,"['multimodality', 'image', 'imagebindllm', 'llm', 'LLM']"
LLM,34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61,"LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked",https://www.semanticscholar.org/paper/34f9c825ba24889fa5e164ba9f99bfe4fc2f3e61,JournalArticle,"Large language models (LLMs) are popular for high-quality text generation but can produce harmful content, even when aligned with human values through reinforcement learning. Adversarial prompts can bypass their safety measures. We propose LLM Self Defense, a simple approach to defend against these attacks by having an LLM screen the induced responses. Our method does not require any fine-tuning, input preprocessing, or iterative output generation. Instead, we incorporate the generated content into a pre-defined prompt and employ another instance of an LLM to analyze the text and predict whether it is harmful. We test LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent LLMs against various types of attacks, such as forcefully inducing affirmative responses to prompts and prompt engineering attacks. Notably, LLM Self Defense succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5 and Llama 2.",2023,41,abs/2308.07308,"[153223021.0, 2133413904.0, 2070368863.0, 1793506.0]",153223021.0,Monaco,3,"[2072782550.0, 3010457.0, 2059547812.0]",Y,"['In Section 3.1, the attack methods #2 and #3 should be detailed more.', 'While LINE or SDNE, which the authors cite, may not run on some of the larger datasets they can be tested on the smaller datasets.', 'After introducing the idea and related work, the authors in Section 3 give details about how to perform the quantization.']",,,arXiv.org,"['llm', 'self', 'defense', 'attacks', 'LLM']"
LLM,1ab91d6ac7afc1a0121487a9089fa70edc1634d4,Certifying LLM Safety against Adversarial Prompting,https://www.semanticscholar.org/paper/1ab91d6ac7afc1a0121487a9089fa70edc1634d4,JournalArticle,"Large language models (LLMs) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an LLM and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees. Given a prompt, our procedure erases tokens individually and inspects the resulting subsequences using a safety filter. Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size. We implement the safety filter in two ways, using Llama 2 and DistilBERT, and compare the performance of erase-and-check for the two cases. We defend against three attack modes: i) adversarial suffix, where an adversarial sequence is appended at the end of a harmful prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Our experimental results demonstrate that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts. Additionally, we propose three efficient empirical defenses: i) RandEC, a randomized subsampling version of erase-and-check; ii) GreedyEC, which greedily erases tokens that maximize the softmax score of the harmful class; and iii) GradEC, which uses gradient information to optimize tokens to erase. We demonstrate their effectiveness against adversarial prompts generated by the Greedy Coordinate Gradient (GCG) attack algorithm. The code for our experiments is available at https://github.com/aounon/certified-llm-safety.",2023,41,abs/2309.02705,"[31910622.0, 40228633.0, 2822290.0, 34389431.0, 1892673.0]",31910622.0,Oslo,3,"[36877027.0, 2237799101.0, 1725167.0]",Y,"['However, on one hand the contribution is rather narrow, and on the other the results presented do not clearly show that the contribution is of significance in practice.', 'Because of the plethora of VAE models used in video prediction [1] (albeit, used with pre-structured latent spaces), there has to be atleast one VAE baseline.', 'You need to find a better justification for using L2-SVM than ""L2-SVM loss variant is considered to be the best by the author of the paper"", did you try classical SVM and found them performing worse?']",,,arXiv.org,"['safety', 'prompts', 'tokens', 'llm', 'LLM']"
LLM,22ebfc211d184ed615729378a43fde175bf14478,"Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following",https://www.semanticscholar.org/paper/22ebfc211d184ed615729378a43fde175bf14478,JournalArticle,"We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM.",2023,37,abs/2309.00615,"[2237599228.0, 2115713503.0, 2237590429.0, 2193267071.0, 1387903470.0, 150147382.0, 32811782.0, 144740494.0, 2237583550.0, 49404547.0, 1714602.0]",2237599228.0,Minsk,3,"[2143919864.0, 2146174097.0, 2219266839.0]",Y,"['The proposed approach leverages recent work that gives a novel parametrization of control problems in the LDS setting.', 'This is a very interesting conjecture, however unfortunately it is not studied further.', 'Minor comments: In Section 2.2, is Layer 1 the input layer or the next?']",,,arXiv.org,"['3d', 'pointbind', 'multimodality', 'llm', 'LLM']"
LLM,7637ed79d30d0139901175ae4abedd822c217ab4,3D-LLM: Injecting the 3D World into Large Language Models,https://www.semanticscholar.org/paper/7637ed79d30d0139901175ae4abedd822c217ab4,JournalArticle,"Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Speciﬁcally, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efﬁciently train 3D-LLMs, we ﬁrst utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin ( e.g. , the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https:",2023,60,abs/2307.12981,"[151261268.0, 2226286961.0, 2158502526.0, 9696154.0, 15394275.0, 2111329651.0, 2056157586.0]",151261268.0,Riga,3,"[24812041.0, 1891038.0, 2218569240.0]",Y,"['In continuous domains, the state is not unique … so they build a soft next state predictor that gives a probability over next states favoring those demonstrated by the expert.', '3. This paper lack original technical contribution from themselves.', '* Qualitative results on sampling from the model using the CIFAR dataset.']",,,Neural Information Processing Systems,"['3d', '3dllms', 'models', 'llm', 'LLM']"
LLM,ce212cb873a54e5716da53a66b10298ac013008a,BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents,https://www.semanticscholar.org/paper/ce212cb873a54e5716da53a66b10298ac013008a,JournalArticle,"The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, \textit{i.e.} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at \url{https://github.com/salesforce/BOLAA}.",2023,34,abs/2308.05960,"[2223887365.0, 2087699735.0, 2108313930.0, 2147380988.0, 71926704.0, 2223748790.0, 22758695.0, 5478513.0, 9200530.0, 2309967.0, 2115800155.0, 2122258484.0, 46507194.0, 2054594326.0, 1702137.0]",2223887365.0,Luxembourg,2,"[51413028.0, 39888194.0]",Y,"['The main novelty in this paper is that it uses the label as a third view of a multi-view model and make use of cross moments.', 'How does the model combine the heatmap output (which is a sequence of the same length as the time series) and the clustering output (which is a vector of size K) in Figure 1?']",,,arXiv.org,"['laa', 'laas', 'llm', 'llms', 'LLM']"
LLM,e3052ebca5eeae6a8a73e44517903d39746f5f3a,From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning,https://www.semanticscholar.org/paper/e3052ebca5eeae6a8a73e44517903d39746f5f3a,JournalArticle,"In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model's expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both efficiency and resource-conscious advancements. Codes, data, and models are available: https://github.com/tianyi-lab/Cherry_LLM",2023,35,abs/2308.12032,"[2150655891.0, 2144289768.0, 2111336489.0, 1391200710.0, 2108451006.0, 145292435.0, 66063851.0, 2213956781.0, 91353860.0]",2150655891.0,Bern,3,"[2840853.0, 2153282879.0, 143695559.0]",Y,"['Do you have any intuition for why it is sometimes necessary to set beta=0?', 'An image has many attributes: the glint in the corner of a window, the hue of a leaf.', 'Do the authors have any explanation/intuition for this behavior?']",,,arXiv.org,"['models', 'data', 'instruction', 'llm', 'LLM']"
LLM,fb00016c1e048b9373803add001c1ec7e877cb23,Head-to-Tail: How Knowledgeable are Large Language Models (LLM)? A.K.A. Will LLMs Replace Knowledge Graphs?,https://www.semanticscholar.org/paper/fb00016c1e048b9373803add001c1ec7e877cb23,JournalArticle,"Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs? To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.",2023,50,abs/2308.10168,"[49871029.0, 15574937.0, 47291370.0, 2229372676.0, 2152050780.0]",49871029.0,Tallinn,3,"[2122258484.0, 2066641.0, 2273645.0]",Y,"['Also, the detailed specification of the VAE should be detailed.', 'The experimental results are also not explained thoroughly enough.', 'The paper briefly mentions supervising attention models using such boxes, but it isn’t clear how bounding boxes for data points could be used.']",,,arXiv.org,"['llms', 'knowledge', 'llm', 'terms', 'LLM']"
LLM,ead6121fbc787d508dc6a6d7106f72bf0d647d03,Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents,https://www.semanticscholar.org/paper/ead6121fbc787d508dc6a6d7106f72bf0d647d03,JournalArticle,"In this paper, we present a novel framework for enhancing the capabilities of large language models (LLMs) by leveraging the power of multi-agent systems. Our framework introduces a collaborative environment where multiple intelligent agent components, each with distinctive attributes and roles, work together to handle complex tasks more efficiently and effectively. We demonstrate the practicality and versatility of our framework through case studies in artificial general intelligence (AGI), specifically focusing on the Auto-GPT and BabyAGI models. We also examine the""Gorilla""model, which integrates external APIs into the LLM. Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations. By modeling various domains such as courtroom simulations and software development scenarios, we showcase the potential applications and benefits of our proposed multi-agent system. Our framework provides an avenue for advancing the capabilities and performance of LLMs through collaboration and knowledge exchange among intelligent agents.",2023,50,abs/2306.03314,"[2237440339.0, 2163313042.0]",2237440339.0,Bern,3,"[9551276.0, 1633124736.0, 50492525.0]",Y,"['1. The biggest issue with the proposed test is that it conflates mode collapse with non-uniformity.', 'Additionaly, it\'s not clear why learning seems to completely ""fail"" without the pre-trained policy.', 'I could imagine a network learning to ignore features of objects that tend to wander over time.']",,,arXiv.org,"['framework', 'capabilities', 'models', 'llm', 'LLM']"
LLM,6628f9ee35e36cdfdcac8a46cef4dba8d529a83b,Character-LLM: A Trainable Agent for Role-Playing,https://www.semanticscholar.org/paper/6628f9ee35e36cdfdcac8a46cef4dba8d529a83b,Conference,"Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents \textit{memorize} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.",2023,32,,"[95329799.0, 2107897400.0, 2087363104.0, 2258552414.0]",95329799.0,Belgrade,3,"[1684745.0, 2217487123.0, 2061706386.0]",Y,"['Preliminary experimental results are reported which demonstrate that ISRLU can perform similar to ELU.', '3. Some of the architectural choices (the one derived from ""shortcut problem"") are barely explained or looked into.', 'The interpolation results of Figure 11 are also underwhelming, though possibly mostly because the reconstructions are in general not great.']",Conference on Empirical Methods in Natural Language Processing,23.0,,"['llms', 'agents', 'models', 'llm', 'LLM']"
LLM,30cc95639cffca4ffa8c0eafbc502636c0c88fa5,BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions,https://www.semanticscholar.org/paper/30cc95639cffca4ffa8c0eafbc502636c0c88fa5,Conference,"Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process. Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and in undertaking general (not particularly text-rich) VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved 17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME), comparing to our baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence. To demonstrate the broad industry applications enabled by BLIVA, we evaluate the model using a new dataset comprising YouTube thumbnails paired with question-answer sets across 11 diverse categories. For researchers interested in further exploration, our code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.",2023,29,,"[2231783956.0, 48615440.0, 89843190.0, 1956819.0, 8157619.0, 91444480.0]",2231783956.0,Vilnius,2,"[31347453.0, 5478513.0]",Y,"['3. Some of the architectural choices (the one derived from ""shortcut problem"") are barely explained or looked into.', 'The experimental results are very good and give strong support for the proposed normalization.']",AAAI Conference on Artificial Intelligence,23.0,,"['bliva', 'models', 'embeddings', 'llm', 'LLM']"
LLM,8b28792f8405b737229afb92c99c579b86d8aa98,Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations,https://www.semanticscholar.org/paper/8b28792f8405b737229afb92c99c579b86d8aa98,JournalArticle,"We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.",2023,29,abs/2312.06674,"[2065277797.0, 17097160.0, 2273682220.0, 150282885.0, 2273645788.0, 2272672481.0, 2273645419.0, 2273679997.0, 2223748737.0, 2273657478.0, 2072010.0]",2065277797.0,Bucharest,3,"[2336789.0, 1720266.0, 5499803.0]",Y,"['This is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial NLL loss and a reconstruction loss at the last hidden layer.', 'If authors were interested in the tendency of real program translation task, they should arrange the experiment by collecting parallel corpora between some unrelated programming languages using resources in the real world.', 'The evaluation framework is described in enough detail to replicate the results.']",,,arXiv.org,"['llama', 'guard', 'model', 'llm', 'LLM']"
LLM,43e624ddeed82df944a6cae0dedec3372438e243,Accelerating LLM Inference with Staged Speculative Decoding,https://www.semanticscholar.org/paper/43e624ddeed82df944a6cae0dedec3372438e243,JournalArticle,"Recent advances with large language models (LLM) illustrate their diverse capabilities. We propose a novel algorithm, staged speculative decoding, to accelerate LLM inference in small-batch, on-device scenarios. We address the low arithmetic intensity of small-batch inference by improving upon previous work in speculative decoding. First, we restructure the speculative batch as a tree, which reduces generation costs and increases the expected tokens per batch. Second, we add a second stage of speculative decoding. Taken together, we reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L model while perfectly preserving output quality.",2023,28,abs/2308.04623,"[2683016.0, 82614785.0]",2683016.0,Brussels,3,"[1696332.0, 2066229431.0, 2167301.0]",Y,"['The authors evaluated their approach on six sentiment analysis datasets (MR, CR, SUBJ, MPQA, SST, and IMDB), and found that the proposed method is competitive with existing supervised methods.', 'What is the reasonableness of training such kind of data, or are they already avoided from the data?', 'Section 5.2: When trained on a small dataset, training amortization error becomes negligible.']",,,arXiv.org,"['llm', 'inference', 'speculative', 'batch', 'LLM']"
LLM,5e4597eb21a393b23e473cf66cb5ae8b27cab03e,ExpeL: LLM Agents Are Experiential Learners,https://www.semanticscholar.org/paper/5e4597eb21a393b23e473cf66cb5ae8b27cab03e,Conference,"The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.",2023,41,,"[2136104377.0, 46307329.0, 2232847317.0, 2036238525.0, 1679704.0, 2115218570.0]",2136104377.0,San Marino,2,"[145169163.0, 2108485135.0]",Y,"['Second, it is also unclear what the method to generate train/dev/test data is.', '2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing.']",AAAI Conference on Artificial Intelligence,23.0,,"['agent', 'tasks', 'experiences', 'llm', 'LLM']"
LLM,10a0541be17d10d922ffc68a3dae55a13d9c1ab9,LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code Generation,https://www.semanticscholar.org/paper/10a0541be17d10d922ffc68a3dae55a13d9c1ab9,JournalArticle,"There has been a recent explosion of research on Large Language Models (LLMs) for software engineering tasks, in particular code generation. However, results from LLMs can be highly unstable; nondeterministically returning very different codes for the same prompt. Non-determinism is a potential menace to scientific conclusion validity. When non-determinism is high, scientific conclusions simply cannot be relied upon unless researchers change their behaviour to control for it in their empirical analyses. This paper conducts an empirical study to demonstrate that non-determinism is, indeed, high, thereby underlining the need for this behavioural change. We choose to study ChatGPT because it is already highly prevalent in the code generation research literature. We report results from a study of 829 code generation problems from three code generation benchmarks (i.e., CodeContests, APPS, and HumanEval). Our results reveal high degrees of non-determinism: the ratio of coding tasks with zero equal test output across different requests is 72.73%, 60.40%, and 65.85% for CodeContests, APPS, and HumanEval, respectively. In addition, we find that setting the temperature to 0 does not guarantee determinism in code generation, although it indeed brings less non-determinism than the default configuration (temperature=1). These results confirm that there is, currently, a significant threat to scientific conclusion validity. In order to put LLM-based research on firmer scientific foundations, researchers need to take into account non-determinism in drawing their conclusions.",2023,38,abs/2308.02828,"[1492047220.0, 51250527.0, 145836176.0, 2146058962.0]",1492047220.0,Vaduz,2,"[2112573464.0, 2256381600.0]",Y,"['Particularly, I found it interesting to re-evaluate the variance with (virtually) increasing larger batch size.', 'Unsurprisingly (since the EEN noise does not seem to be conditioned on the input), the baseline deterministic model performs quite well.']",,,arXiv.org,"['nondeterminism', 'code', 'generation', 'llm', 'LLM']"
LLM,cd29c25c489562b409a60f83365f93f33ee1a0a1,Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM,https://www.semanticscholar.org/paper/cd29c25c489562b409a60f83365f93f33ee1a0a1,JournalArticle,"Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments on open-source large language models, we demonstrate that RA-LLM can successfully defend against both state-of-the-art adversarial prompts and popular handcrafted jailbreaking prompts by reducing their attack success rates from nearly 100% to around 10% or less.",2023,37,abs/2309.14348,"[143695559.0, 2110924166.0, 7557913.0]",143695559.0,Belgrade,3,"[2673479.0, 1635504630.0, 2109370860.0]",Y,"['- The results on SQUAD seem pretty weak - 52-64%, compared to the SOTA of 81.', 'Interestingly, DQN + heuristic reward approaches expert performance while behavioral cloning never achieves expert performance level even though it has actions.', '* On optimisation paths, the choice of PCA directions is wise compared to random projections, and results are nice as plotted.']",,,arXiv.org,"['llms', 'alignmentbreaking', 'attacks', 'llm', 'LLM']"
LLM,be383c607d4d357c763d2329ab71799c6e1393b4,Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases,https://www.semanticscholar.org/paper/be383c607d4d357c763d2329ab71799c6e1393b4,JournalArticle,"Due to the recent improvements and wide availability of Large Language Models (LLMs), they have posed a serious threat to academic integrity in education. Modern LLM-generated text detectors attempt to combat the problem by offering educators with services to assess whether some text is LLM-generated. In this work, we have collected 124 submissions from computer science students before the creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this data to evaluate eight publicly-available LLM-generated text detectors through the measures of accuracy, false positives, and resilience. The purpose of this work is to inform the community of what LLM-generated text detectors work and which do not, but also to provide insights for educators to better maintain academic integrity in their courses. Our results find that CopyLeaks is the most accurate LLM-generated text detector, GPTKit is the best LLM-generated text detector to reduce false positives, and GLTR is the most resilient LLM-generated text detector. We also express concerns over 52 false positives (of 114 human written submissions) generated by GPTZero. Finally, we note that all LLM-generated text detectors are less accurate with code, other languages (aside from English), and after the use of paraphrasing tools (like QuillBot). Modern detectors are still in need of improvements so that they can offer a full-proof solution to help maintain academic integrity. Further, their usability can be improved by facilitating a smooth API integration, providing clear documentation of their features and the understandability of their model(s), and supporting more commonly used languages.",2023,17,abs/2307.07411,"[2223551699.0, 8722618.0, 153842213.0, 1397294204.0]",2223551699.0,London,2,"[2118890103.0, 90981528.0]",Y,"['The stated Theorem 1 is incorrect.', 'However, no empirical comparison between the two methods is performed.']",,,arXiv.org,"['text', 'detectors', 'integrity', 'llm', 'LLM']"
LLM,04fa3ebc4c0c0b4a2d2d1a3fc612134a05057696,Prompt Injection Attacks and Defenses in LLM-Integrated Applications,https://www.semanticscholar.org/paper/04fa3ebc4c0c0b4a2d2d1a3fc612134a05057696,JournalArticle,"Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we conduct a systematic evaluation on prompt injection attacks and their defenses with 10 LLMs and 7 tasks. We hope our frameworks can inspire future research in this field. Our code is available at https://github.com/liu00222/Open-Prompt-Injection.",2023,15,abs/2310.12815,"[1604963563.0, 2260844526.0, 2260340372.0, 2257508100.0, 144516687.0]",1604963563.0,Paris,2,"[1380530147.0, 2061294792.0]",Y,"['With these in mind I view the comparison results with a bit of uncertainty about the exact amount of gain being achieved, which may beg the question if the algorithmic contributions are buying much for their added complexity?', 'Are the four mixture components over the robot parameters updated independently of each other when the parameter-exploring policy gradients updates are applied?']",,,arXiv.org,"['attacks', 'injection', 'applications', 'llm', 'LLM']"
LLM,b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283,NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails,https://www.semanticscholar.org/paper/b428e9e0e8ac36b3ae29a7ab9b9554c39cedb283,Conference,"NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable. Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.",2023,20,,"[2796756.0, 2137374228.0, 1602996186.0, 2258715782.0, 2258785611.0]",2796756.0,Oslo,2,"[2158995823.0, 2818166.0]",Y,"['Why will D answer negatively (or positively) on this example ?', 'Is the time budget different for each new generated environment?']",Conference on Empirical Methods in Natural Language Processing,23.0,,"['llm', 'rails', 'nemo', 'guardrails', 'LLM']"
LLM,66d41e0f894dda2c37dd5bacbdd7bfd418e3350f,VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View,https://www.semanticscholar.org/paper/66d41e0f894dda2c37dd5bacbdd7bfd418e3350f,Conference,"Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation (VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that VELMA is able to successfully follow navigation instructions in Street View with only two in-context examples. We further finetune the LLM agent on a few thousand examples and achieve around 25% relative improvement in task completion over the previous state-of-the-art for two datasets.",2023,17,,"[47947548.0, 51439692.0, 2168285733.0, 41020222.0, 3289329.0, 1682479.0]",47947548.0,Rome,2,"[2115263944.0, 1720266.0]",Y,"['Paper of broad interest for control tasks This is a well written paper, very nice work.', 'It builds on the previous work of [Finn et al. 2016] that uses a neural network to predict transformation parameters of an affine image transformation for future frame prediction, an idea akin to the Spatial Transformer Network paper of [Jaderberg et al., 2015].']",AAAI Conference on Artificial Intelligence,23.0,,"['navigation', 'agent', 'instructions', 'llm', 'LLM']"
LLM,d84cf745c534c010b8e55e5a4a04878906848dc3,TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series,https://www.semanticscholar.org/paper/d84cf745c534c010b8e55e5a4a04878906848dc3,JournalArticle,"This work summarizes two ways to accomplish Time-Series (TS) tasks in today's Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data; TS-for-LLM (data-centric) converts TS into a model-friendly representation to enable the pre-trained LLM to handle TS data. Given the lack of data, limited resources, semantic context requirements, and so on, this work focuses on TS-for-LLM, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, where the TS embedding space is aligned to LLM embedding layer space, then creates soft prompts to make LLM more open to that embeddings, and finally implements TS tasks using the frozen LLM. We also demonstrate the feasibility of TS-for-LLM through theory and experiments. Experiments are carried out on TS classification, forecasting, and representation tasks using eight frozen LLMs with various structures and sizes. The results show that the pre-trained LLM with TEST strategy can achieve better or comparable performance than today's SOTA TS models and offer benefits for few-shot and generalization. By treating LLM as the pattern machine, TEST can endow LLM's ability to process TS data without compromising language ability. We hope that this study will serve as a foundation for future work to support TS+LLM progress.",2023,22,abs/2308.08241,"[144102247.0, 2110479359.0, 2115263944.0, 2317297.0]",144102247.0,Reykjavik,3,"[2108533.0, 36347083.0, 2110032535.0]",Y,"['The main difference is in the sentence construction strategy.', '“ Would be good to see how this affects results and convergence speed.', 'Interestingly, the assisted method starts off much higher in the “reacher” task.']",,,arXiv.org,"['llm', 'data', 'work', 'tasks', 'LLM']"
LLM,6f75e8b61f13562237851d8119cb2f9d49e073fb,Can LLM-Generated Misinformation Be Detected?,https://www.semanticscholar.org/paper/6f75e8b61f13562237851d8119cb2f9d49e073fb,JournalArticle,"The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.",2023,34,abs/2309.13788,"[2163546329.0, 145800151.0]",2163546329.0,Reykjavik,3,"[2069981213.0, 2866596.0, 2325031.0]",Y,"['An interesting work on the characterization of critical points of neural networks This paper mainly focuses on the square loss function of linear networks.', 'Also, in Section 4.3, it is not clear what the point of this experiment is: whether to show that entropy regularization helps or the Stein gradient estimator outperforms other estimators.', 'It would be nice to have a separate comparison on the time consumption of  different methods.']",,,arXiv.org,"['misinformation', 'llms', 'question', 'llm', 'LLM']"
LLM,c12b80b44d9acfe6cd92fdf965264c4b706c367c,ToolQA: A Dataset for LLM Question Answering with External Tools,https://www.semanticscholar.org/paper/c12b80b44d9acfe6cd92fdf965264c4b706c367c,JournalArticle,"Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning. To overcome these challenges, external tools can be used to enhance LLMs' question-answering abilities. However, current evaluation methods do not distinguish between questions that can be answered using LLMs' internal knowledge and those that require external information through tool use. To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs' ability to use external tools for question answering. Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions. Importantly, we strive to minimize the overlap between our benchmark data and LLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use reasoning abilities. We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements. Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements. Our data and code are freely available to the broader scientific community on GitHub.",2023,48,abs/2306.13304,"[8103389.0, 1633124736.0, 2119044211.0, 2118180896.0, 145657504.0]",8103389.0,Belgrade,2,"[2063938464.0, 2255345252.0]",Y,"['The authors do not propose an original model and they do not describe the used model inside this publication.', 'What is the maximal possible MSE error in these environments?']",,,Neural Information Processing Systems,"['llms', 'tools', 'data', 'llm', 'LLM']"
LLM,4f480bae3196dbbc27ab383bce33478ea963f9b3,LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models,https://www.semanticscholar.org/paper/4f480bae3196dbbc27ab383bce33478ea963f9b3,JournalArticle,"We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.",2023,33,abs/2305.13711,"[1557269413.0, 1725643.0]",1557269413.0,Skopje,2,"[1679216.0, 2066422293.0]",Y,"['There are multiple indications that this bandit-to-supervised baseline is hard to outperform in a number of important applications.', 'Why did the choices that were made in the paper yield this success?']",,,NLP4CONVAI,"['evaluation', 'llmeval', 'method', 'llm', 'LLM']"
LLM,cd2f4aaf98bb1e020cff310000c8049d3460c54e,NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark,https://www.semanticscholar.org/paper/cd2f4aaf98bb1e020cff310000c8049d3460c54e,Conference,"In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.",2023,30,,"[1724648481.0, 1602998334.0, 1453724884.0, 2226458991.0, 2251043402.0, 2064112151.0]",1724648481.0,Belgrade,3,"[48469973.0, 50631038.0, 6562624.0]",Y,"['As such the paper would be a nice contribution to ICLR.', 'I understand that you are trying to improve the existing results with their optimizer, but this paper also introduces new algorithm.', 'Finally, Table 1 seems to have some min/max values the wrong way around.']",Conference on Empirical Methods in Natural Language Processing,23.0,,"['data', 'contamination', 'model', 'llm', 'LLM']"
LLM,8ee45aeb7c97e3346cc62f216f673b91277ac718,LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,https://www.semanticscholar.org/paper/8ee45aeb7c97e3346cc62f216f673b91277ac718,Conference,"This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. 1",2022,137,,"[153409139.0, 2110410087.0, 2158995823.0, 2137028350.0, 2113951006.0, 1758652.0]",153409139.0,Andorra,2,"[4836115.0, 2169159066.0]",Y,"['The generator is a mixture of two Gaussians in one dimension.', 'The encoder network from NEM can be seen as a recurrent network which takes one latent variable theta_k at time t and some input x to produce the next latent variable theta_k at time t+1.']",IEEE International Conference on Computer Vision,22.0,,"['agents', 'tasks', 'language', 'llm', 'LLM']"
LLM,00e18c603e60d861c4e99c541e4d65ef442d5945,LLM in a flash: Efficient Large Language Model Inference with Limited Memory,https://www.semanticscholar.org/paper/00e18c603e60d861c4e99c541e4d65ef442d5945,JournalArticle,"Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First,""windowing""strategically reduces data transfer by reusing previously activated neurons, and second,""row-column bundling"", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.",2023,24,abs/2312.11514,"[1398372702.0, 2256998308.0, 2275238488.0, 1397475144.0, 2237803694.0, 2237799101.0, 32371083.0, 1682124.0]",1398372702.0,Reykjavik,3,"[48559420.0, 1602996186.0, 2146335468.0]",Y,"['Marginal contributions and missing comparison with state of the art In this paper a neural-network based method for multi-frame video prediction is proposed.', 'Moreover, there is no clear theoretical explanation for why this approach ought to work.', 'Additional regularization terms are also added to encourage the model to encode longer term dependencies in its latent distributions.']",,,arXiv.org,"['memory', 'data', 'dram', 'llm', 'LLM']"
LLM,cc78babfacce48e715dac56886d7dd9746cfcab0,RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit,https://www.semanticscholar.org/paper/cc78babfacce48e715dac56886d7dd9746cfcab0,JournalArticle,"Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrieval-augmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a {RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including {request rewriting, document retrieval, passage extraction, answer generation, and fact checking} modules. Our toolkit is publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.",2023,24,abs/2306.05212,"[1830383266.0, 4376097.0, 2243360876.0, 2219726925.0, 1897235.0, 153693432.0]",1830383266.0,Lisbon,3,"[2063047670.0, 145914976.0, 145914256.0]",Y,"['Also, can you experimentally show that attacks on latent variables are more powerful than attacks on inputs?', 'Secondly, in section 4, your analysis depends on the specific type of ResNet you chose.', 'I think this is a nice example of a detailed analysis of the representations acquired by a reinforcement learning agent.']",,,arXiv.org,"['llms', 'systems', 'llm', 'user', 'LLM']"
LLM,84725855d10b531eb8cbe54935dda0440c2fc750,Don't Make Your LLM an Evaluation Benchmark Cheater,https://www.semanticscholar.org/paper/84725855d10b531eb8cbe54935dda0440c2fc750,JournalArticle,"Large language models~(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, \ie \emph{benchmark leakage}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct extensive experiments to study the effect of benchmark leverage, and find that it can dramatically boost the evaluation results, which would finally lead to an unreliable assessment of model performance. To improve the use of existing evaluation benchmarks, we finally present several guidelines for both LLM developers and benchmark maintainers. We hope this work can draw attention to appropriate training and evaluation of LLMs.",2023,37,abs/2311.01964,"[2265383494.0, 1900406.0, 2256558402.0, 2265461972.0, 2257376413.0, 2265519430.0, 2257310922.0, 2186578511.0, 2161986932.0]",2265383494.0,Ljubljana,3,"[1769675.0, 1781242.0, 2188983263.0]",Y,"['I welcome and are grateful for any theory in the area.', '3. This paper lack original technical contribution from themselves.', 'In Line 3 of the paragraph below Equation 5, “classe” should be “class”.']",,,arXiv.org,"['evaluation', 'model', 'benchmarks', 'llm', 'LLM']"
LLM,9fcdbfdf28245010c875ce85502351fe05c04b49,Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View,https://www.semanticscholar.org/paper/9fcdbfdf28245010c875ce85502351fe05c04b49,JournalArticle,"As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets\footnote{\url{https://github.com/zjunlp/MachineSoM}.}, hoping to catalyze further research in this promising avenue.",2023,23,abs/2310.02124,"[2253784578.0, 2152775219.0, 2253472695.0]",2253784578.0,Madrid,3,"[2112558225.0, 7482477.0, 1891038.0]",Y,"['Some reasons below: * There are no specific results on properties of the divergences, or axioms that justify them.', 'The evaluation framework is described in enough detail to replicate the results.', 'The authors mention several philosophical arguments in favor of their approach, but is there a concrete example of an model which is cumbersome to write in an existing framework but easy here?']",,,arXiv.org,"['llm', 'nlp', 'systems', 'collaboration', 'LLM']"
LLM,ce157cea880c9ab64de64f11a531202f5348fa05,"""Kelly is a Warm Person, Joseph is a Role Model"": Gender Biases in LLM-Generated Reference Letters",https://www.semanticscholar.org/paper/ce157cea880c9ab64de64f11a531202f5348fa05,Conference,"Large Language Models (LLMs) have recently emerged as an effective tool to assist individuals in writing various types of content, including professional documents such as recommendation letters. Though bringing convenience, this application also introduces unprecedented fairness concerns. Model-generated reference letters might be directly used by users in professional scenarios. If underlying biases exist in these model-constructed letters, using them without scrutinization could lead to direct societal harms, such as sabotaging application success rates for female applicants. In light of this pressing issue, it is imminent and necessary to comprehensively study fairness issues and associated harms in this real-world use case. In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions: (1) biases in language style and (2) biases in lexical content. We further investigate the extent of bias propagation by analyzing the hallucination bias of models, a term that we define to be bias exacerbation in model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs- ChatGPT and Alpaca, we reveal significant gender biases in LLM-generated recommendation letters. Our findings not only warn against using LLMs for this application without scrutinization, but also illuminate the importance of thoroughly studying hidden biases and harms in LLM-generated professional documents.",2023,23,,"[2165227666.0, 2258548444.0, 2261454711.0, 31099365.0, 2257127887.0, 2256996328.0]",2165227666.0,Paris,2,"[1742135.0, 1695160.0]",Y,"['The proposed GGNN approach outperforms (bi-)LSTMs on both tasks.', 'Otherwise, the perfect latent representation is z=x.']",Conference on Empirical Methods in Natural Language Processing,23.0,,"['biases', 'letters', 'llms', 'llm', 'LLM']"
LLM,98ce7af921e7c52d81df64d632d34eb09522cd75,Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization,https://www.semanticscholar.org/paper/98ce7af921e7c52d81df64d632d34eb09522cd75,JournalArticle,"Large language model (LLM) agents have been shown effective on a wide range of tasks, and by ensembling multiple LLM agents, their performances could be further improved. Existing approaches employ a fixed set of agents to interact with each other in a static architecture, which limits their generalizability to various tasks and requires strong human prior in designing these agents. In this work, we propose to construct a strategic team of agents communicating in a dynamic interaction architecture based on the task query. Specifically, we build a framework named Dynamic LLM-Agent Network ($\textbf{DyLAN}$) for LLM-agent collaboration on complicated tasks like reasoning and code generation. DyLAN enables agents to interact for multiple rounds in a dynamic architecture with inference-time agent selection and an early-stopping mechanism to improve performance and efficiency. We further design an automatic agent team optimization algorithm based on an unsupervised metric termed $\textit{Agent Importance Score}$, enabling the selection of best agents based on the contribution each agent makes. Empirically, we demonstrate that DyLAN performs well in both reasoning and code generation tasks with reasonable computational cost. DyLAN achieves 13.0% and 13.3% improvement on MATH and HumanEval, respectively, compared to a single execution on GPT-35-turbo. On specific subjects of MMLU, agent team optimization in DyLAN increases accuracy by up to 25.0%.",2023,23,abs/2310.02170,"[2117942065.0, 2121290295.0, 2255345252.0, 2254850259.0, 2254124342.0]",2117942065.0,Minsk,2,"[2152290059.0, 2974320.0]",Y,"['I agree with the authors that this line of work, that is not very well known in the current machine learning community, includes a number of ideas that should be able to shed light on some of the currently open theoretical questions.', 'Either human training data showing very effective generalization (if one could somehow make ""novel"" relationships unfamiliar to humans), or a different network architecture that was obviously superior in generalization to CNN+RN.']",,,arXiv.org,"['agents', 'dylan', 'agent', 'llm', 'LLM']"
LLM,a22f3398ea865426c89ee66f4824ec626e56a864,RET-LLM: Towards a General Read-Write Memory for Large Language Models,https://www.semanticscholar.org/paper/a22f3398ea865426c89ee66f4824ec626e56a864,JournalArticle,"Large language models (LLMs) have significantly advanced the field of natural language processing (NLP) through their extensive parameters and comprehensive data utilization. However, existing LLMs lack a dedicated memory unit, limiting their ability to explicitly store and retrieve knowledge for various tasks. In this paper, we propose RET-LLM a novel framework that equips LLMs with a general write-read memory unit, allowing them to extract, store, and recall knowledge from the text as needed for task performance. Inspired by Davidsonian semantics theory, we extract and save knowledge in the form of triplets. The memory unit is designed to be scalable, aggregatable, updatable, and interpretable. Through qualitative evaluations, we demonstrate the superiority of our proposed framework over baseline approaches in question answering tasks. Moreover, our framework exhibits robust performance in handling temporal-based question answering tasks, showcasing its ability to effectively manage time-dependent information.",2023,14,abs/2305.14322,"[2054744.0, 51894641.0, 2133037029.0, 144418438.0]",2054744.0,Budapest,2,"[89843190.0, 1416319999.0]",Y,"['Although, the observations are interesting, especially the one on MNIST where the network performs well even with correct labels slightly above chance, the overall contributions are incremental.', 'On the theoretical side, the linearly constrained weights are only shown to work for a very special case.']",,,arXiv.org,"['llms', 'memory', 'unit', 'llm', 'LLM']"
LLM,7bc9607c5cf3fc817675d46844f529097d579514,Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data,https://www.semanticscholar.org/paper/7bc9607c5cf3fc817675d46844f529097d579514,JournalArticle,"Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.",2023,20,8,"[9551276.0, 1490485182.0, 2115461706.0, 4590999.0, 145135778.0, 144021446.0, 2156149847.0]",9551276.0,Minsk,3,"[3059455.0, 102471415.0, 77790220.0]",Y,"['This network has many tunable parameters, but the main point is that the policy then can condition on this state drawn from the memory.', 'This paper is well-written and easy to follow.', 'Evaluation: Significance: Coresets give significant running time benefits when working with very big datasets.']",,,Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies,"['llms', 'health', 'tasks', 'llm', 'LLM']"
LLM,0f4d00d01d43d3967ee92b58481b5ad530a944d1,A First Look at LLM-Powered Generative News Recommendation,https://www.semanticscholar.org/paper/0f4d00d01d43d3967ee92b58481b5ad530a944d1,JournalArticle,"Personalized news recommendation systems have become essential tools for users to navigate the vast amount of online news content, yet existing news recommenders face significant challenges such as the cold-start problem, user profile modeling, and news content understanding. Previous works have typically followed an inflexible routine to address a particular challenge through model design, but are limited in their ability to understand news content and capture user interests. In this paper, we introduce GENRE, an LLM-powered generative news recommendation framework, which leverages pretrained semantic knowledge from large language models to enrich news data. Our aim is to provide a flexible and unified solution for news recommendation by moving from model design to prompt design. We showcase the use of GENRE for personalized news generation, user profiling, and news summarization. Extensive experiments with various popular recommendation models demonstrate the effectiveness of GENRE. We will publish our code and data 1 for other researchers to reproduce our work.",2023,20,abs/2305.06566,"[150270469.0, 2257286538.0, 2257233277.0, 2187512110.0]",150270469.0,Stockholm,2,"[2257508100.0, 145966834.0]",Y,"['One could even start with a simple Gaussian and linear parameterization of the mean and variance in terms of x.', 'Specifically, an input text is transformed to phoneme encoding and then context vector is created with attention mechanism.']",,,arXiv.org,"['news', 'recommendation', 'content', 'llm', 'LLM']"
LLM,11cf88dce827bd67cbfa60400306318022e736d5,D4: Improving LLM Pretraining via Document De-Duplication and Diversification,https://www.semanticscholar.org/paper/11cf88dce827bd67cbfa60400306318022e736d5,JournalArticle,"Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever data selection can significantly improve LLM pre-training, calls into question the common practice of training for a single epoch on as much data as possible, and demonstrates a path to keep improving our models past the limits of randomly sampling web data.",2023,22,abs/2308.12284,"[2551387.0, 2257241733.0, 2201435.0, 4690624.0]",2551387.0,London,3,"[46175739.0, 2114887261.0, 2113922820.0]",Y,"['I question the value of program synthesis in a language which is not human-readable.', 'Some wall clock time results or FLOPs of train/test time should be provided since you use multiple hops.', 'In light of this, I think it is reasonable to ignore the fact that the proposed initialization does not provide benefits on Penn Treebank and text8.']",,,Neural Information Processing Systems,"['data', 'training', 'selection', 'llm', 'LLM']"
LLM,c0aec04ee86c0724d61c976f19590fbe9c615723,Creating Large Language Model Applications Utilizing LangChain: A Primer on Developing LLM Apps Fast,https://www.semanticscholar.org/paper/c0aec04ee86c0724d61c976f19590fbe9c615723,Conference,"This study focuses on the utilization of Large Language Models (LLMs) for the rapid development of applications, with a spotlight on LangChain, an open-source software library. LLMs have been rapidly adopted due to their capabilities in a range of tasks, including essay composition, code writing, explanation, and debugging, with OpenAI’s ChatGPT popularizing their usage among millions ofusers. The crux of the study centers around LangChain, designed to expedite the development of bespoke AI applications using LLMs. LangChain has been widely recognized in the AI community for its ability to seamlessly interact with various data sources and applications. The paper provides an examination of LangChain's core features, including its components and chains, acting as modular abstractions and customizable, use-case-specific pipelines, respectively. Through a series of practical examples, the study elucidates the potential of this framework in fostering the swift development of LLM-based applications.",2023,21,,"[2113663584.0, 114751633.0]",2113663584.0,Vienna,3,"[151505981.0, 35638374.0, 2059003208.0]",Y,"['Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks Summary: The paper proposes Neumman optimizer, which makes some adjustments to the idealized Neumman algorithm to improve performance and stability in training.', 'Are the four mixture components over the robot parameters updated independently of each other when the parameter-exploring policy gradients updates are applied?', 'An additional heatmap generator component can be further included in the clustering model.']",International Conference on Applied Engineering and Natural Sciences,23.0,,"['applications', 'study', 'development', 'llm', 'LLM']"
LLM,0095acc4f2c3255cf38fdf844003c97858adb418,OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples,https://www.semanticscholar.org/paper/0095acc4f2c3255cf38fdf844003c97858adb418,Conference,"Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors lack robustness against attacks: they degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, a malicious user might attempt to deliberately evade the detectors based on detection results, but this has not been assumed in previous studies. In this paper, we propose OUTFOX, a framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output. In this framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are harder to detect, while the detector uses the adversarially generated essays as examples for in-context learning to learn to detect essays from a strong attacker. Experiments in the domain of student essays show that the proposed detector improves the detection performance on the attacker-generated texts by up to +41.3 points F1-score. Furthermore, the proposed detector shows a state-of-the-art detection performance: up to 96.9 points F1-score, beating existing detectors on non-attacked texts. Finally, the proposed attacker drastically degrades the performance of detectors by up to -57.0 points F1-score, massively outperforming the baseline paraphrasing method for evading detection.",2023,15,,"[2138646471.0, 143655216.0, 1764004.0]",2138646471.0,Kiev,3,"[2165227666.0, 48455738.0, 40075749.0]",Y,"['The WAGE parameters (i.e., the quantized no. of bits used) are 2-8-8-8, respectively.', 'This paper proposes a different approach (with could be combined with these methods) based on a new training procedure.', '3. The paper wants to find a good trade-off on speed and accuracy.']",AAAI Conference on Artificial Intelligence,23.0,,"['detectors', 'texts', 'detection', 'llm', 'LLM']"
LLM,a8b995f0da78a79447dfb18c2337972b044f4239,LLM-FP4: 4-Bit Floating-Point Quantized Transformers,https://www.semanticscholar.org/paper/a8b995f0da78a79447dfb18c2337972b044f4239,Conference,"We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One characteristic of FP quantization is that its performance largely depends on the choice of exponent bits and clipping range. In this regard, we construct a strong FP-PTQ baseline by searching for the optimal quantization parameters. Furthermore, we observe a high inter-channel variance and low intra-channel variance pattern in activation distributions, which adds activation quantization difficulty. We recognize this pattern to be consistent across a spectrum of transformer models designed for diverse tasks, such as LLMs, BERT, and Vision Transformer models. To tackle this, we propose per-channel activation quantization and show that these additional scaling factors can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method, for the first time, can quantize both weights and activations in the LLaMA-13B to only 4-bit and achieves an average score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8 lower than the full-precision model, significantly outperforming the previous state-of-the-art by 12.7 points. Code is available at: https://github.com/nbasyl/LLM-FP4.",2023,15,,"[2220637583.0, 2109370860.0, 2261688809.0, 2261493190.0, 2256381600.0]",2220637583.0,Moscow,2,"[11531589.0, 1390140002.0]",Y,"['Second, it is also unclear what the method to generate train/dev/test data is.', 'Figure 2 which is the graphic representation of the model is hard to read.']",Conference on Empirical Methods in Natural Language Processing,23.0,,"['quantization', 'weights', 'models', 'llm', 'LLM']"
LLM,b85f3a66245d483f3eb3447eaf9950bd55f2b21e,PentestGPT: An LLM-empowered Automatic Penetration Testing Tool,https://www.semanticscholar.org/paper/b85f3a66245d483f3eb3447eaf9950bd55f2b21e,JournalArticle,"Penetration testing, a crucial industrial practice for ensuring system security, has traditionally resisted automation due to the extensive expertise required by human professionals. Large Language Models (LLMs) have shown significant advancements in various domains, and their emergent abilities suggest their potential to revolutionize industries. In this research, we evaluate the performance of LLMs on real-world penetration testing tasks using a robust benchmark created from test machines with platforms. Our findings reveal that while LLMs demonstrate proficiency in specific sub-tasks within the penetration testing process, such as using testing tools, interpreting outputs, and proposing subsequent actions, they also encounter difficulties maintaining an integrated understanding of the overall testing scenario. In response to these insights, we introduce PentestGPT, an LLM-empowered automatic penetration testing tool that leverages the abundant domain knowledge inherent in LLMs. PentestGPT is meticulously designed with three self-interacting modules, each addressing individual sub-tasks of penetration testing, to mitigate the challenges related to context loss. Our evaluation shows that PentestGPT not only outperforms LLMs with a task-completion increase of 228.6\% compared to the \gptthree model among the benchmark targets but also proves effective in tackling real-world penetration testing challenges. Having been open-sourced on GitHub, PentestGPT has garnered over 4,700 stars and fostered active community engagement, attesting to its value and impact in both the academic and industrial spheres.",2023,16,abs/2308.06782,"[73776889.0, 2153627626.0, 1947239202.0, 2217847684.0, 22799258.0, 2110355317.0, 2146331573.0, 2152798056.0, 1775709.0, 1717963.0]",73776889.0,Amsterdam,3,"[2368148.0, 2273645788.0, 22236100.0]",Y,"['For understand more the WAGE, the authors compare on CIFAR10 the test error rate with vanilla CNN and show is small loss in using their network.', 'I feel as though the restriction to the reward function in this case makes the problem uncessarily hard, and does not represent an important use-case.', 'A gain in memory size of 32x is also achieved by using binary weight and activation during the forward pass.']",,,arXiv.org,"['testing', 'penetration', 'llms', 'llm', 'LLM']"
LLM,7a4fe2f003241ad97bf1778e527cb0306fa90da2,CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations,https://www.semanticscholar.org/paper/7a4fe2f003241ad97bf1778e527cb0306fa90da2,Conference,"Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.",2023,15,,"[2149615775.0, 3144356.0, 2260029688.0]",2149615775.0,Stockholm,2,"[145385471.0, 51418452.0]",Y,"['This seems like one of the most novel findings in the paper and is worth highlighting.', 'The model is trained with supervision to output the overhead map of the global map.']",Conference on Empirical Methods in Natural Language Processing,23.0,,"['simulations', 'llm', 'caricature', 'work', 'LLM']"
LLM,da9b8b4073e6ad44b3da66e1e117cb1ddbf8836d,Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels,https://www.semanticscholar.org/paper/da9b8b4073e6ad44b3da66e1e117cb1ddbf8836d,JournalArticle,"Zero-shot text rankers powered by recent LLMs achieve remarkable ranking performance by simply prompting. Existing prompts for pointwise LLM rankers mostly ask the model to choose from binary relevance labels like""Yes""and""No"". However, the lack of intermediate relevance label options may cause the LLM to provide noisy or biased answers for documents that are partially relevant to the query. We propose to incorporate fine-grained relevance labels into the prompt for LLM rankers, enabling them to better differentiate among documents with different levels of relevance to the query and thus derive a more accurate ranking. We study two variants of the prompt template, coupled with different numbers of relevance levels. Our experiments on 8 BEIR data sets show that adding fine-grained relevance labels significantly improves the performance of LLM rankers.",2023,19,abs/2310.14122,"[39371343.0, 2099586642.0, 2261281337.0, 2261361394.0, 2110134250.0, 2261356664.0, 1815447.0]",39371343.0,Belgrade,2,"[19169659.0, 3018223.0]",Y,"['The efficiency of such a search method decreases very fast when the dimensions of the z-space increases.', 'Still, the classification and the reconstructing results are quite impressive as the work is the first empirical evidence that learning invertible representation that preserves information about the input is possible on large-scale classification tasks.']",,,arXiv.org,"['llm', 'relevance', 'rankers', 'labels', 'LLM']"
LLM,b61b260de1599e6e89491cad9160898fcd3b34c2,Can LLM Replace Stack Overflow? A Study on Robustness and Reliability of Large Language Model Code Generation,https://www.semanticscholar.org/paper/b61b260de1599e6e89491cad9160898fcd3b34c2,Conference,"Recently, large language models (LLMs) have shown an extraordinary ability to understand natural language and generate programming code. It has been a common practice for software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability, and robustness of the code generation from LLMs have not yet been thoroughly studied. The executable code is not equivalent to reliable and robust code, especially in the context of real-world software development. For example, the misuse of APIs in the generated code could lead to severe problems, such as resource leaks, program crashes, etc. Existing code evaluation benchmarks and datasets focus on crafting small tasks such as programming questions in coding interviews, which, however, deviates from the problem that developers would ask LLM for real-world coding help. To fill the missing piece, in this work, we propose a dataset RobustAPI for evaluating the reliability and robustness of code generated by LLMs. We collect 1208 coding questions from Stack Overflow on 18 representative Java APIs. We summarize the common misuse patterns of these APIs and evaluate them on current popular LLMs. The evaluation results show that even for GPT-4, 62% of the generated code contains API misuses, which would cause unexpected consequences if the code is introduced into real-world software.",2023,15,,"[2146528797.0, 2108467971.0]",2146528797.0,Athens,3,"[2066499928.0, 145233583.0, 2149798086.0]",Y,"['The paper says Deep Voice 2 (Arik et al., 2017a) is only prior work for multi-speaker TTS.', 'Presumably, there exists some hyperparameters where the model does not achieve 100% test accuracy, in which case, what are the failure modes?', 'The authors also analyze the generalization error bound of DNN after pruning based on the work of (Sokolic et al., 2017).']",AAAI Conference on Artificial Intelligence,23.0,,"['code', 'llms', 'software', 'llm', 'LLM']"
LLM,709af143f78bc62413c50ea1a7ee75b0702c4f59,MART: Improving LLM Safety with Multi-round Automatic Red-Teaming,https://www.semanticscholar.org/paper/709af143f78bc62413c50ea1a7ee75b0702c4f59,JournalArticle,"Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses. While effective, manual red-teaming is costly, and existing automatic red-teaming typically discovers safety risks without addressing them. In this paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which incorporates both automatic adversarial prompt writing and safe response generation, significantly increasing red-teaming scalability and the safety of the target LLM. Specifically, an adversarial LLM and a target LLM interplay with each other in an iterative manner, where the adversarial LLM aims to generate challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned with safety aligned data on these adversarial prompts. In each round, the adversarial LLM crafts better attacks on the updated target LLM, while the target LLM also improves itself through safety fine-tuning. On adversarial prompt benchmarks, the violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs with extensive adversarial prompt writing. Notably, model helpfulness on non-adversarial prompts remains stable throughout iterations, indicating the target LLM maintains strong performance on instruction following.",2023,15,abs/2311.07689,"[2243417222.0, 2266920681.0, 2266467782.0, 2072010.0, 2266748478.0, 2266712798.0, 2253929707.0, 3375249.0]",2243417222.0,San Marino,2,"[2219053320.0, 144669504.0]",Y,"['* A concrete strategy for obtaining good results using the proposed method.', 'he resulting multi-channel image-like structures are then feed into vanilla 2D CNN.']",,,arXiv.org,"['target', 'llm', 'safety', 'llms', 'LLM']"
LLM,9e540662619327a3056d9e40bb58058868f6f805,Prompt Distillation for Efficient LLM-based Recommendation,https://www.semanticscholar.org/paper/9e540662619327a3056d9e40bb58058868f6f805,Conference,"Large language models (LLM) have manifested unparalleled modeling capability on various tasks, e.g., multi-step reasoning, but the input to these models is mostly limited to plain text, which could be very long and contain noisy information. Long text could take long time to process, and thus may not be efficient enough for recommender systems that require immediate response. In LLM-based recommendation models, user and item IDs are usually filled in a template (i.e., discrete prompt) to allow the models to understand a given task, but the models usually need extensive fine-tuning to bridge the user/item IDs and the template words and to unleash the power of LLM for recommendation. To address the problems, we propose to distill the discrete prompt for a specific task to a set of continuous prompt vectors so as to bridge IDs and words and to reduce the inference time. We also design a training strategy with an attempt to improve the efficiency of training these models. Experimental results on three real-world datasets demonstrate the effectiveness of our PrOmpt Distillation (POD) approach on both sequential recommendation and top-N recommendation tasks. Although the training efficiency can be significantly improved, the improvement of inference efficiency is limited. This finding may inspire researchers in the community to further improve the inference efficiency of LLM-based recommendation models.",2023,16,,"[2151529879.0, 2260830380.0, 2146027242.0]",2151529879.0,San Marino,3,"[1403759150.0, 2235966.0, 51026953.0]",Y,"['Results are good, some unclear explanation This paper proposes an end-to-end trainable attention module, which takes as input the 2D feature vector map and outputs a 2D matrix of scores for each map.', '[1] Wang, Weiran, Honglak Lee, and Karen Livescu.', 'However, what is shown in that paper is that the sequence of loss function values converges, which does not imply that the sequence of weight estimates also converges, because of the presence of a non-convex constraint ($b_j^t \\in Q^{n_l}$).']",International Conference on Information and Knowledge Management,32.0,,"['models', 'recommendation', 'efficiency', 'llm', 'LLM']"
LLM,eda6756ab2844c390584686dc5e6385f4a8369cd,LLM-assisted Generation of Hardware Assertions,https://www.semanticscholar.org/paper/eda6756ab2844c390584686dc5e6385f4a8369cd,JournalArticle,"The security of computer systems typically relies on a hardware root of trust. As vulnerabilities in hardware can have severe implications on a system, there is a need for techniques to support security verification activities. Assertion-based verification is a popular verification technique that involves capturing design intent in a set of assertions that can be used in formal verification or testing-based checking. However, writing security-centric assertions is a challenging task. In this work, we investigate the use of emerging large language models (LLMs) for code generation in hardware assertion generation for security, where primarily natural language prompts, such as those one would see as code comments in assertion files, are used to produce SystemVerilog assertions. We focus our attention on a popular LLM and characterize its ability to write assertions out of the box, given varying levels of detail in the prompt. We design an evaluation framework that generates a variety of prompts, and we create a benchmark suite comprising real-world hardware designs and corresponding golden reference assertions that we want to generate with the LLM.",2023,21,abs/2306.14027,"[2113610946.0, 3437933.0, 143645422.0, 1398683279.0, 2057985118.0, 1707355.0, 2220547623.0, 73770687.0, 102544543.0, 51280874.0]",2113610946.0,Skopje,3,"[1732091.0, 16777650.0, 2218326293.0]",Y,"['Nor the model neither the optimized criterion is detailled: the authors present some curve mentioning ""bits per character"" but we do not know what is measured.', '[ds]SSM is integrated with I2A, which generates rollouts of future states, based on the inferred hidden states from the d/sSSM-VAE model.', 'For evaluating whether the data point x is anomalous or not, we search for a latent representation z such that x \\approx g_\\theta(z).']",,,arXiv.org,"['assertions', 'hardware', 'security', 'llm', 'LLM']"
LLM,0894585294c67193ff3190240554677b56fd79a0,From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?,https://www.semanticscholar.org/paper/0894585294c67193ff3190240554677b56fd79a0,JournalArticle,"Large Language Models (LLMs) have found widespread applications in various domains, including web applications, where they facilitate human interaction via chatbots with natural language interfaces. Internally, aided by an LLM-integration middleware such as Langchain, user prompts are translated into SQL queries used by the LLM to provide meaningful responses to users. However, unsanitized user prompts can lead to SQL injection attacks, potentially compromising the security of the database. Despite the growing interest in prompt injection vulnerabilities targeting LLMs, the specific risks of generating SQL injection attacks through prompt injections have not been extensively studied. In this paper, we present a comprehensive examination of prompt-to-SQL (P$_2$SQL) injections targeting web applications based on the Langchain framework. Using Langchain as our case study, we characterize P$_2$SQL injections, exploring their variants and impact on application security through multiple concrete examples. Furthermore, we evaluate 7 state-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacks across language models. Our findings indicate that LLM-integrated applications based on Langchain are highly susceptible to P$_2$SQL injection attacks, warranting the adoption of robust defenses. To counter these attacks, we propose four effective defense techniques that can be integrated as extensions to the Langchain framework. We validate the defenses through an experimental evaluation with a real-world use case application.",2023,16,abs/2308.01990,"[2231175914.0, 2248088902.0, 2231158981.0, 2231663636.0]",2231175914.0,Rome,2,"[2279712392.0, 32336300.0]",Y,"['Some wall clock time results or FLOPs of train/test time should be provided since you use multiple hops.', 'In part 5, the authors compare the distributions (finite Bayesian deep networks and their analogues Gaussian processes) in yet another way: by studying their agreement in terms of inference.']",,,arXiv.org,"['langchain', 'attacks', 'applications', 'llm', 'LLM']"
LLM,1562390dd212516cd857009cbd4f857a902d1f3d,MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents,https://www.semanticscholar.org/paper/1562390dd212516cd857009cbd4f857a902d1f3d,JournalArticle,"Significant advancements have occurred in the application of Large Language Models (LLMs) for various tasks and social simulations. Despite this, their capacities to coordinate within task-oriented social contexts are under-explored. Such capabilities are crucial if LLMs are to effectively mimic human-like social behavior and produce meaningful results. To bridge this gap, we introduce collaborative generative agents, endowing LLM-based Agents with consistent behavior patterns and task-solving abilities. We situate these agents in a simulated job fair environment as a case study to scrutinize their coordination skills. We propose a novel framework that equips collaborative generative agents with human-like reasoning abilities and specialized skills. Our evaluation demonstrates that these agents show promising performance. However, we also uncover limitations that hinder their effectiveness in more complex coordination tasks. Our work provides valuable insights into the role and evolution of LLMs in task-oriented social simulations.",2023,20,abs/2310.06500,"[2257259410.0, 2257107248.0, 2243348413.0]",2257259410.0,Vilnius,2,"[2586969.0, 2051655406.0]",Y,"['Interesting experiments but lack of model description The authors propose to use a byte level RNN to classify reviews.', 'Again, pooling operations (average, max, etc.) on the learned node2vec embeddings are examples of simpler alternatives.']",,,arXiv.org,"['agents', 'tasks', 'simulations', 'llm', 'LLM']"
LLM,32524aa3ae8522542753ed7e6f4cca3970e4acab,Can an Embodied Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation,https://www.semanticscholar.org/paper/32524aa3ae8522542753ed7e6f4cca3970e4acab,JournalArticle,"We present language-guided exploration (LGX), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to an uniquely described target object in a previously unseen environment. Our approach makes use of large language models (LLMs) for this task by leveraging the LLM's commonsense-reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via real-world experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects.",2023,21,9,"[1395945114.0, 2057888241.0, 1699159.0]",1395945114.0,London,3,"[1380075136.0, 3086996.0, 38656724.0]",Y,"['This is demonstrated in comparison to weight normalization in Figure 4.', 'I myself am very curious about what would happen and would love to see this exchange catalyzed.', 'Only two scene types (bedroom, kitchen) and four object classes (bed, window, appliance, counter) are used for evaluation.']",,,IEEE Robotics and Automation Letters,"['object', 'navigation', 'lgx', 'llm', 'LLM']"
LLM,681253389d2cc27103753749f4c7556699d55471,Temporal Data Meets LLM - Explainable Financial Time Series Forecasting,https://www.semanticscholar.org/paper/681253389d2cc27103753749f4c7556699d55471,JournalArticle,"This paper presents a novel study on harnessing Large Language Models' (LLMs) outstanding knowledge and reasoning abilities for explainable financial time series forecasting. The application of machine learning models to financial time series comes with several challenges, including the difficulty in cross-sequence reasoning and inference, the hurdle of incorporating multi-modal signals from historical news, financial knowledge graphs, etc., and the issue of interpreting and explaining the model results. In this paper, we focus on NASDAQ-100 stocks, making use of publicly accessible historical stock price data, company metadata, and historical economic/financial news. We conduct experiments to illustrate the potential of LLMs in offering a unified solution to the aforementioned challenges. Our experiments include trying zero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with a public LLM model Open LLaMA. We demonstrate our approach outperforms a few baselines, including the widely applied classic ARMA-GARCH model and a gradient-boosting tree model. Through the performance comparison results and a few examples, we find LLMs can make a well-thought decision by reasoning over information from both textual news and price time series and extracting insights, leveraging cross-sequence information, and utilizing the inherent knowledge embedded within the LLM. Additionally, we show that a publicly available LLM such as Open-LLaMA, after fine-tuning, can comprehend the instruction to generate explainable forecasts and achieve reasonable performance, albeit relatively inferior in comparison to GPT-4.",2023,18,abs/2306.11025,"[2118210915.0, 2141144864.0, 48907594.0, 2190030596.0, 47781311.0, 2220668748.0]",2118210915.0,Rome,2,"[1505708061.0, 2185953295.0]",Y,"['* A concrete strategy for obtaining good results using the proposed method.', '4. Is trade-off between 1 to 2 bits really important?']",,,arXiv.org,"['model', 'llm', 'llms', 'knowledge', 'LLM']"
LLM,9db0247728950788a2b42097d81dc0e24eed6bb2,LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent,https://www.semanticscholar.org/paper/9db0247728950788a2b42097d81dc0e24eed6bb2,JournalArticle,"3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs significantly improve the grounding capability, especially for complex language queries, making LLM-Grounder an effective approach for 3D vision-language tasks in robotics. Videos and interactive demos can be found on the project website https://chat-with-nerf.github.io/ .",2023,17,abs/2309.12311,"[46477844.0, 2243416833.0, 1390481263.0, 2243383240.0, 2243403473.0, 1786435.0, 2243382885.0]",46477844.0,Bern,3,"[145773524.0, 2125031571.0, 2118207557.0]",Y,"['The effect of using the faster approximation on performance also remains to be investigated.', 'The current paper presentation is a bit too dense to clearly understand the LL machine model and the two-phase algorithm.', 'Could you explain how classes are predicted given a test problem?']",,,arXiv.org,"['3d', 'language', 'llmgrounder', 'llm', 'LLM']"
LLM,0e9a44ce661c3535d5ce747912540080324489f5,Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena,https://www.semanticscholar.org/paper/0e9a44ce661c3535d5ce747912540080324489f5,JournalArticle,"Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM performance and occasional outperformance by simpler methods indicate opportunities for further advancements in LLM design and the value of our simulation environment for ongoing testing and refinement.",2023,20,abs/2310.05746,"[5040052.0, 2145968425.0, 2062940513.0, 3165738.0, 46666605.0]",5040052.0,Vilnius,2,"[2185503028.0, 9229182.0]",Y,"['Classifier is trained to not only maximize classification accuracy on the real training data but also to output a uniform distribution for the generated samples.', 'A gain in memory size of 32x is also achieved by using binary weight and activation during the forward pass.']",,,arXiv.org,"['llms', 'skills', 'llm', 'advancements', 'LLM']"
LLM,263a58f4fd32caca1dad2351af4d711aec451fe6,Evil Geniuses: Delving into the Safety of LLM-based Agents,https://www.semanticscholar.org/paper/263a58f4fd32caca1dad2351af4d711aec451fe6,JournalArticle,"Rapid advancements in large language models (LLMs) have revitalized in LLM-based agents, exhibiting impressive human-like behaviors and cooperative capabilities in various scenarios. However, these agents also bring some exclusive risks, stemming from the complexity of interaction environments and the usability of tools. This paper delves into the safety of LLM-based agents from three perspectives: agent quantity, role definition, and attack level. Specifically, we initially propose to employ a template-based attack strategy on LLM-based agents to find the influence of agent quantity. In addition, to address interaction environment and role specificity issues, we introduce Evil Geniuses (EG), an effective attack method that autonomously generates prompts related to the original role to examine the impact across various role definitions and attack levels. EG leverages Red-Blue exercises, significantly improving the generated prompt aggressiveness and similarity to original roles. Our evaluations on CAMEL, Metagpt and ChatDev based on GPT-3.5 and GPT-4, demonstrate high success rates. Extensive evaluation and discussion reveal that these agents are less robust, prone to more harmful behaviors, and capable of generating stealthier content than LLMs, highlighting significant safety challenges and guiding future research. Our code is available at https://github.com/T1aNS1R/Evil-Geniuses.",2023,17,abs/2311.11855,"[2243374631.0, 2109457982.0, 2267384625.0, 3431029.0, 2267649469.0]",2243374631.0,Kiev,3,"[2114076472.0, 2118180896.0, 2403851.0]",Y,"['It is not the same task.', 'Turning to the experimental section, I think the authors did a good job of evaluating their approach with the ablation study and comparisons with PPO and TRPO.', 'The results are that this procedure improves upon the trivial baseline  but still significantly underperforms supervised training.']",,,arXiv.org,"['agents', 'role', 'attack', 'llm', 'LLM']"
LLM,16753e0317730e8c1b297338300a8c6163dd06f2,VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning,https://www.semanticscholar.org/paper/16753e0317730e8c1b297338300a8c6163dd06f2,JournalArticle,"Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models. This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency groupings of the entities and backgrounds. Next, guided by this output from the video planner, our video generator, Layout2Vid, has explicit control over spatial layouts and can maintain temporal consistency of entities/backgrounds across scenes, while only trained with image-level annotations. Our experiments demonstrate that VideoDirectorGPT framework substantially improves layout and movement control in both single- and multi-scene video generation and can generate multi-scene videos with visual consistency across scenes, while achieving competitive performance with SOTAs in open-domain single-scene T2V generation. We also demonstrate that our framework can dynamically control the strength for layout guidance and can also generate videos with user-provided images. We hope our framework can inspire future work on better integrating the planning ability of LLMs into consistent long video generation.",2023,16,abs/2309.15091,"[39729530.0, 2008198436.0, 2706729.0, 143977268.0]",39729530.0,Zagreb,2,"[2620186.0, 1903751380.0]",Y,"['3) Can you write the exact dynamics used for Theorem 4.1 ?', 'They present initial results on the performance of their framework called Multitask Neural Model Search (MNMS) controller.']",,,arXiv.org,"['video', 'generation', 'llms', 'llm', 'LLM']"
LLM,be2b0396de9431bae931642516a1d3e4906329f5,Low-code LLM: Visual Programming over LLMs,https://www.semanticscholar.org/paper/be2b0396de9431bae931642516a1d3e4906329f5,JournalArticle,"Effectively utilizing LLMs for complex tasks is challenging, often involving a time-consuming and uncontrollable prompt engineering process. This paper intro-duces a novel human-LLM interaction framework, Low-code LLM . It incorporates six types of simple low-code visual programming interactions, all supported by clicking, dragging, or text editing, to achieve more controllable and stable responses. Through visual interaction with a graphical user interface, users can incorporate their ideas into the workﬂow without writing trivial prompts. The proposed Low-code LLM framework consists of a Planning LLM that designs a structured planning workﬂow for complex tasks, which can be correspondingly edited and conﬁrmed by users through low-code visual programming operations, and an Executing LLM that generates responses following the user-conﬁrmed workﬂow. We highlight three advantages of the low-code LLM: controllable generation results, user-friendly human-LLM interaction, and broadly applicable scenarios. We demonstrate its beneﬁts using four typical applications. By introducing this approach, we aim to bridge the gap between humans and LLMs, enabling more effective and efﬁcient utilization of LLMs for complex tasks. Our system will be soon publicly available at LowCodeLLM .",2023,22,abs/2304.08103,"[116275985.0, 35374367.0, 51198241.0, 2108725194.0, 3887469.0, 48741177.0, 2151101534.0, 2214585980.0, 2212903837.0, 2111130689.0, 2069738994.0, 2072609829.0]",116275985.0,Budapest,3,"[35195456.0, 6117577.0, 2176030144.0]",Y,"['The paper presents an interesting idea and is fairly well written.', 'This paper proposes a different approach (with could be combined with these methods) based on a new training procedure.', 'In practice, I believe this is not an unreasonable requirement.']",,,arXiv.org,"['llm', 'tasks', 'interaction', 'lowcode', 'LLM']"
LLM,529ff7d6441d244212cf2becafd12a7e67ac56d9,FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning,https://www.semanticscholar.org/paper/529ff7d6441d244212cf2becafd12a7e67ac56d9,JournalArticle,"LLMs have demonstrated great capabilities in various NLP tasks. Different entities can further improve the performance of those LLMs on their specific downstream tasks by fine-tuning LLMs. When several entities have similar interested tasks, but their data cannot be shared because of privacy concerns regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. However, fine-tuning LLMs in federated learning settings still lacks adequate support from existing FL frameworks because it has to deal with optimizing the consumption of significant communication and computational resources, data preparation for different tasks, and distinct information protection demands. This paper first discusses these challenges of federated fine-tuning LLMs, and introduces our package FS-LLM as a main contribution, which consists of the following components: (1) we build an end-to-end benchmarking pipeline, automizing the processes of dataset preprocessing, federated fine-tuning execution, and performance evaluation on federated LLM fine-tuning; (2) we provide comprehensive federated parameter-efficient fine-tuning algorithm implementations and versatile programming interfaces for future extension in FL scenarios with low communication and computation costs, even without accessing the full model; (3) we adopt several accelerating and resource-efficient operators for fine-tuning LLMs with limited resources and the flexible pluggable sub-routines for interdisciplinary study. We conduct extensive experiments to validate the effectiveness of FS-LLM and benchmark advanced LLMs with state-of-the-art parameter-efficient fine-tuning algorithms in FL settings, which also yields valuable insights into federated fine-tuning LLMs for the research community. To facilitate further research and adoption, we release FS-LLM at https://github.com/alibaba/FederatedScope/tree/llm.",2023,21,abs/2309.00363,"[2162042348.0, 2237427279.0, 2007553615.0, 49025612.0, 2162036220.0, 2211993531.0, 52133762.0, 2237607166.0, 1696332.0, 2237499232.0]",2162042348.0,Tirana,2,"[2169159066.0, 2149615775.0]",Y,"['This second part was confusing - although the maze can be viewed as a graph, many other works apply ConvNets to maze environments [1, 2, 3], and their work has little relation to other work on graph CNNs.', 'For completeness, it would be great to include one more algorithm in the evaluation: an ablation of DnC which does not involve a central policy at all.']",,,arXiv.org,"['llms', 'tasks', 'entities', 'llm', 'LLM']"
LLM,cb3968152f7d93f53d24b00279a90d5071ddc85a,Understanding the Effects of RLHF on LLM Generalisation and Diversity,https://www.semanticscholar.org/paper/cb3968152f7d93f53d24b00279a90d5071ddc85a,JournalArticle,"Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the tradeoff between generalisation and diversity.",2023,15,abs/2310.06452,"[2066422293.0, 2008790203.0, 31434304.0, 2256999781.0, 2072738644.0, 1864353.0, 48647153.0]",2066422293.0,Bratislava,3,"[2125957.0, 3050846.0, 51163989.0]",Y,"['Interesting problem and approach, but not ready for ICLR In this paper, the authors define a simulated, multi-agent “taxi pickup” task in a GridWorld environment.', 'It is perhaps interesting that one can make deep learning learn to cooperate with imperfect information, but one could have illustrated the game theory equally well with other techniques.', 'The authors later describes how to improve this technique by further equating certain observations and exploring only one in each equivalence class.']",,,arXiv.org,"['models', 'rlhf', 'diversity', 'llm', 'LLM']"
LLM,70ca38ad480c0be0eca89ccc4972d6cc9a5824da,LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins,https://www.semanticscholar.org/paper/70ca38ad480c0be0eca89ccc4972d6cc9a5824da,JournalArticle,"Large language model (LLM) platforms, such as ChatGPT, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. While these plugins extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the potential for the types of issues that we outline in our attack taxonomy. We conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future LLM-based computing platforms.",2023,16,abs/2309.10254,"[2066229431.0, 1769675.0, 3268360.0]",2066229431.0,Vaduz,2,"[2326758.0, 2490652.0]",Y,"['You could use an NER for W/O-NE-Table and update the NE embeddings, and it should be as good as With-NE-Table model (and fairer to compare with too).', '2. In figure 2 and the associated analyses, why were 20 shape terms used rather than 8 to parallel the other cases?']",,,arXiv.org,"['llm', 'platforms', 'plugins', 'framework', 'LLM']"
LLM,139a0c7a60667979dcb57eae677f75ff3f0b0196,LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?,https://www.semanticscholar.org/paper/139a0c7a60667979dcb57eae677f75ff3f0b0196,JournalArticle,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks.",2023,17,abs/2307.10719,"[2224017005.0, 47473421.0, 2681954.0, 1967156.0, 2734935.0]",2224017005.0,Athens,2,"[2890261.0, 2072801764.0]",Y,"['The stated Theorem 1 is incorrect.', '1. The authors tested out this new activation function on RNNs.']",,,arXiv.org,"['censorship', 'llms', 'problem', 'llm', 'LLM']"
LLM,dd4d82299b4209db539d639f836fcee663cf72b3,"LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples",https://www.semanticscholar.org/paper/dd4d82299b4209db539d639f836fcee663cf72b3,JournalArticle,"Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception. And the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that non-sense prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. This phenomenon forces us to revisit that hallucination may be another view of adversarial examples, and it shares similar features with conventional adversarial examples as the basic feature of LLMs. Therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. Finally, we explore basic feature of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub.",2023,31,abs/2310.01469,"[2256816230.0, 2253467098.0, 2253826413.0, 2253434320.0, 2265968596.0]",2256816230.0,Vilnius,2,"[92954142.0, 2149970173.0]",Y,"['Clarity: The paper is clearly written and easy to follow and understand.', 'A good paper, but it could be better for writing and baseline comparisons This paper studies the problem of text-to-speech synthesis (TTS) ""in the wild"" and proposes to use the shifting buffer memory.']",,,arXiv.org,"['llms', 'hallucination', 'prompts', 'llm', 'LLM']"
LLM,91b2b47cabd800ef658b65bfe1f52b7293a740c3,LLM-powered Data Augmentation for Enhanced Crosslingual Performance,https://www.semanticscholar.org/paper/91b2b47cabd800ef658b65bfe1f52b7293a740c3,Conference,"This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency.",2023,27,,"[2161240241.0, 143990839.0, 8129718.0]",2161240241.0,Oslo,3,"[2185503028.0, 1740261.0, 2061202877.0]",Y,"[""This manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum, which actually has been shown by existing works like Kawaguchi (2016) with some assumptions."", ""There's clear value in having good inductive biases (e.g. expressed in the form of the discriminator architecture) when defining divergences for practical applications."", 'Algorithm 1 does not make clear the relationship between the model learned in step 2 and the algorithms in steps 4 to 6.']",Conference on Empirical Methods in Natural Language Processing,23.0,,"['data', 'languages', 'llms', 'llm', 'LLM']"
LLM,7c217cc7524251f42887438834912e06129c3299,To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis,https://www.semanticscholar.org/paper/7c217cc7524251f42887438834912e06129c3299,JournalArticle,"Recent research has highlighted the importance of dataset size in scaling language models. However, large language models (LLMs) are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs. To further enhance LLMs, a straightforward approach is to repeat the pre-training data for additional epochs. In this study, we empirically investigate three key aspects under this approach. First, we explore the consequences of repeating pre-training data, revealing that the model is susceptible to overfitting, leading to multi-epoch degradation. Second, we examine the key factors contributing to multi-epoch degradation, finding that significant factors include dataset size, model parameters, and training objectives, while less influential factors consist of dataset quality and model FLOPs. Finally, we explore whether widely used regularization can alleviate multi-epoch degradation. Most regularization techniques do not yield significant improvements, except for dropout, which demonstrates remarkable effectiveness but requires careful tuning when scaling up the model size. Additionally, we discover that leveraging mixture-of-experts (MoE) enables cost-effective and efficient hyper-parameter tuning for computationally intensive dense LLMs with comparable trainable parameters, potentially impacting efficient LLM development on a broader scale.",2023,24,abs/2305.13230,"[2144332771.0, 2117786875.0, 150341221.0, 2109654065.0, 2054451943.0]",2144332771.0,Prague,2,"[144154709.0, 2276851.0]",Y,"['In part 7, it is concluded that the result that has been proven for size of layers going to infinity (Theorem 1) seems to empirically be verified on finite networks similar to those used in the literature.', 'Actually, the proof never makes any connection to optimization.']",,,Neural Information Processing Systems,"['model', 'size', 'data', 'llm', 'LLM']"
LLM,5645502d73c6907f1671923638773152e55bfb00,TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks,https://www.semanticscholar.org/paper/5645502d73c6907f1671923638773152e55bfb00,Conference,"While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw more accurate conclusions about LLMs' performance on a specific complex task.",2023,17,,"[2692077.0, 2185503028.0]",2692077.0,San Marino,3,"[144906624.0, 143666627.0, 3001926.0]",Y,"['It would be interesting to compare to a baseline where the control systems are allowed to adapt to the individual design parameters.', 'Could you explain how classes are predicted given a test problem?', 'I could then identify the influential dimension(s) by taking the largest absolute values in the gradient vector.']",Conference on Empirical Methods in Natural Language Processing,23.0,,"['llms', 'studies', 'prompts', 'llm', 'LLM']"
Deep Learning,3c8a456509e6c0805354bd40a35e3f2dbf8069b1,"PyTorch: An Imperative Style, High-Performance Deep Learning Library",https://www.semanticscholar.org/paper/3c8a456509e6c0805354bd40a35e3f2dbf8069b1,JournalArticle,"Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.",2019,31617,abs/1912.01703,"[3407277.0, 39793298.0, 1403239967.0, 1977806.0, 2065251344.0, 114250963.0, 2059271276.0, 3370429.0, 3365851.0, 3029482.0, 3050846.0, 1473151134.0, 2052812305.0, 2375710.0, 10707709.0, 41203992.0, 22236100.0, 32163737.0, 152599430.0, 2113829116.0, 2127604.0]",3407277.0,Sofia,2,"[2220637583.0, 1775709.0]",Y,"['The evaluation framework is described in enough detail to replicate the results.', 'It reformulates the non-convex optimization problem in (Aghasi et al., 2016) as a difference of convex (DC) problem, which can be solved quite efficiently using the DCA algorithm (Tao and An, 1997).']",,,Neural Information Processing Systems,"['pytorch', 'speed', 'principles', 'deep learning', 'Deep Learning']"
Deep Learning,7aa38b85fa8cba64d6a4010543f6695dbf5f1386,Towards Deep Learning Models Resistant to Adversarial Attacks,https://www.semanticscholar.org/paper/7aa38b85fa8cba64d6a4010543f6695dbf5f1386,JournalArticle,"Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",2017,9383,abs/1706.06083,"[143826246.0, 17775913.0, 152772922.0, 2754804.0, 2869958.0]",143826246.0,Belgrade,3,"[2108965351.0, 2231868268.0, 2187454523.0]",Y,"['1. This manuscript provides the sufficient and necessary characterization of critical points for deep networks.', ""Pros and Cons ============ + good results + interesting idea of using the algorithm for RLfD - weak experiments for an application paper - not clear what's new"", 'The trained residual function can be used to predict a residual z_i for x_i.']",,,International Conference on Learning Representations,"['networks', 'models', 'security', 'deep learning', 'Deep Learning']"
Deep Learning,f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6,Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,https://www.semanticscholar.org/paper/f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6,Conference,"Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",2015,7451,,"[2681954.0, 1744700.0]",2681954.0,Sofia,2,"[2243374631.0, 2171114745.0]",Y,"['Intriguing two phase RL approach for learning neural controllers for discrete programs This paper presents a reinforcement learning based approach to learn context-free parsers from pairs of input programs and their corresponding parse trees.', 'They then evaluate several existing network architectures on these tasks, and show that results are not as impressive as others might have assumed they would be.']",International Conference on Machine Learning,15.0,,"['uncertainty', 'learning', 'tools', 'deep learning', 'Deep Learning']"
Deep Learning,e9a986c8ff6c2f381d026fe014f6aaa865f34da7,Deep Learning with Differential Privacy,https://www.semanticscholar.org/paper/e9a986c8ff6c2f381d026fe014f6aaa865f34da7,Conference,"Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",2016,4616,,"[2057642721.0, 1396184193.0, 153440022.0, 145057514.0, 145591745.0, 35210462.0, 2152832173.0]",2057642721.0,Madrid,2,"[1710223.0, 2138715050.0]",Y,"['Secondly, the DDT’s leaves are parametrized with the encoder distribution q(z|x), and thus gradient information flows back through the DDT into the posterior approximations in order to make them more discriminative.', 'To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER.']",Conference on Computer and Communications Security,16.0,,"['privacy', 'techniques', 'networks', 'deep learning', 'Deep Learning']"
Deep Learning,5b6ec746d309b165f9f9def873a2375b6fb40f3d,Xception: Deep Learning with Depthwise Separable Convolutions,https://www.semanticscholar.org/paper/5b6ec746d309b165f9f9def873a2375b6fb40f3d,Conference,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",2016,11616,,[1565641737.0],1565641737.0,Vaduz,2,"[1746807.0, 1904203865.0]",Y,"['Are the test environments sufficiently different from the training ones?', 'In the first paragraph of Section 4.5, I disagree with the sentence, ""Similar observations can be made for the other language pairs we considered.""']",Computer Vision and Pattern Recognition,16.0,,"['inception', 'convolution', 'depthwise', 'deep learning', 'Deep Learning']"
Deep Learning,d997beefc0922d97202789d2ac307c55c2c52fba,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,https://www.semanticscholar.org/paper/d997beefc0922d97202789d2ac307c55c2c52fba,Conference,"Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",2016,11051,,"[144329939.0, 144914140.0, 2216377.0, 51352814.0]",144329939.0,Copenhagen,2,"[1384224631.0, 32244429.0]",Y,"[""And the results of figure 9, showing poor reconstructions when changing the class representation essentially demonstrates that the method isn't able to factorize class and style successfully."", ""The rollouts are then fed into the agent's policy / value function.""]",Computer Vision and Pattern Recognition,16.0,,"['network', 'data', 'point', 'deep learning', 'Deep Learning']"
Deep Learning,54ddb00fa691728944fd8becea90a373d21597cf,Understanding deep learning requires rethinking generalization,https://www.semanticscholar.org/paper/54ddb00fa691728944fd8becea90a373d21597cf,JournalArticle,"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. 
Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. 
We interpret our experimental findings by comparison with traditional models.",2016,4263,abs/1611.03530,"[151505981.0, 1751569.0, 1775622.0, 9229182.0, 1689108.0]",151505981.0,Sarajevo,2,"[103073143.0, 50631038.0]",Y,"['Different RL models like Hillclimb-MLE, PPO, GAN, A2C are investigated and discussed.', 'Then the addition of the VAE had no measurable effect on the PixelCNN++’s performance, i.e., it seems like a bad idea due to the added complexity and loss of tractability.']",,,International Conference on Learning Representations,"['networks', 'training', 'regularization', 'deep learning', 'Deep Learning']"
Deep Learning,ff7bcaa4556cb13fc7bf03e477172493546172cd,What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?,https://www.semanticscholar.org/paper/ff7bcaa4556cb13fc7bf03e477172493546172cd,Conference,"There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.",2017,3815,,"[47645184.0, 2681954.0]",47645184.0,Lisbon,3,"[2327080.0, 10707709.0, 2548384.0]",Y,"['The idea of enforcing information isolation is brilliant.', 'But, it would be good if this can be supported with real life examples.', 'I do not know whether using a centralized network where each agent has a window of observations is a novel algorithm.']",Neural Information Processing Systems,17.0,,"['uncertainty', 'model', 'tasks', 'deep learning', 'Deep Learning']"
Deep Learning,8674494bd7a076286b905912d26d47f7501c4046,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,https://www.semanticscholar.org/paper/8674494bd7a076286b905912d26d47f7501c4046,Conference,"Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",2017,8165,,"[144329939.0, 47782132.0, 144914140.0, 51352814.0]",144329939.0,Bucharest,2,"[144418438.0, 1390195074.0]",Y,"['Experiments conducted on image classification and weakly supervised segmentation show the effectiveness of the proposed method.', 'The problem set-up of unpaired summarization is not particularly compelling, since summaries are typically found paired with their original documents.']",Neural Information Processing Systems,17.0,,"['point', 'pointnet', 'network', 'deep learning', 'Deep Learning']"
Deep Learning,2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c,Understanding deep learning (still) requires rethinking generalization,https://www.semanticscholar.org/paper/2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c,JournalArticle,"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models. We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.",2021,1364,64,"[151505981.0, 1751569.0, 1775622.0, 9229182.0, 1689108.0]",151505981.0,Sarajevo,2,"[2112558225.0, 2283098327.0]",Y,"['Additional regularization terms are also added to encourage the model to encode longer term dependencies in its latent distributions.', 'Comments: I really appreciate the author(s) by providing experiments using real models on the ImageNet dataset.']",,,Communications of the ACM,"['networks', 'training', 'regularization', 'deep learning', 'Deep Learning']"
Deep Learning,811df72e210e20de99719539505da54762a11c6d,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d,Conference,"Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",2018,5850,,"[2587648.0, 35499972.0, 1689992.0, 1736651.0]",2587648.0,Lisbon,3,"[48907594.0, 2133037029.0, 35367497.0]",Y,"['As noted above, it is not clear what is the significance of this combination or how does it improve performance.', 'Also, that the ""style"" representation contain less (and I\'d say slightly less, in Figure 7 b and d, we still see a lot of same class nearest neighbors) is not exactly a surprising result.', 'The proposed GGNN approach outperforms (bi-)LSTMs on both tasks.']",International Conference on Machine Learning,18.0,,"['methods', 'rl', 'framework', 'deep learning', 'Deep Learning']"
Deep Learning,b79e5e4622a95417deec313cd543617b19611bea,Deep Learning using Rectified Linear Units (ReLU),https://www.semanticscholar.org/paper/b79e5e4622a95417deec313cd543617b19611bea,JournalArticle,"We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer $h_{n - 1}$ in a neural network, then multiply it by weight parameters $\theta$ to get the raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$, i.e. $f(o) = \max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide class predictions $\hat{y}$ through argmax function, i.e. argmax $f(x)$.",2018,2349,abs/1803.08375,[26412983.0],26412983.0,Belgrade,2,"[144370168.0, 145142172.0]",Y,"['Are the test environments sufficiently different from the training ones?', 'Significance The approach improves on previous category estimation approaches by embracing the expressiveness of recent generative models.']",,,arXiv.org,"['function', 'relu', 'classification', 'deep learning', 'Deep Learning']"
Deep Learning,c889d6f98e6d79b89c3a6adf8a921f88fa6ba518,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518,Conference,"We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",2017,9447,,"[46881670.0, 1689992.0, 1736651.0]",46881670.0,Moscow,2,"[4373076.0, 52257721.0]",Y,"['To see why these models are different, if it is the model of (2), and we look at only distinct words, the diagonal of the matrix P[v_i,v_i] does not really make sense and certainly will not follow Equation (7).', '— Can the authors provide training time comparison of their model and other/baseline models?']",International Conference on Machine Learning,17.0,,"['model', 'learning', 'metalearning', 'deep learning', 'Deep Learning']"
Deep Learning,14014c024674991149f3ecf9314c93f7e029ef1a,"Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges",https://www.semanticscholar.org/paper/14014c024674991149f3ecf9314c93f7e029ef1a,JournalArticle,"The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.",2021,793,abs/2104.13478,"[1732570.0, 143627859.0, 2056266.0, 1742197495.0]",1732570.0,Tirana,3,"[1736279.0, 20825661.0, 2235966.0]",Y,"['""we use 2 transformation matrixes"" -> if you could please provide more details How is equation 2 related to figure 1?', '3. The section on curriculum learning does not mention relevant work on “starting small”  and the “less is more"" hypothesis in language development by Jeff Elman and Elissa Newport: https://pdfs.semanticscholar.org/371b/240bebcaa68921aa87db4cd3a5d4e2a3a36b.pdf http://www.sciencedirect.com/science/article/pii/0388000188900101', 'The main difference from previous approaches is that the model is that the embeddings are trained end-to-end for a specific task, rather than trying to produce generically useful embeddings.']",,,arXiv.org,"['learning', 'methods', 'principles', 'deep learning', 'Deep Learning']"
Deep Learning,69e76e16740ed69f4dc55361a3d319ac2f1293dd,Asynchronous Methods for Deep Reinforcement Learning,https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd,Conference,"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",2016,7633,,"[3255983.0, 36045539.0, 153583218.0, 1753223.0, 2542999.0, 3367786.0, 145824029.0, 2645384.0]",3255983.0,Luxembourg,2,"[2267664.0, 1718134.0]",Y,"['Interesting paper, would like to see more experiments The paper proposed a new activation function that tries to alleviate the use of  other form of normalization methods for RNNs.', '“ Would be good to see how this affects results and convergence speed.']",International Conference on Machine Learning,16.0,,"['reinforcement', 'learning', 'network', 'deep learning', 'Deep Learning']"
Deep Learning,024006d4c2a89f7acacc6e4438d156525b60a98f,Continuous control with deep reinforcement learning,https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f,JournalArticle,"We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",2015,10952,abs/1509.02971,"[2542999.0, 2323922.0, 1863250.0, 2801204.0, 1968210.0, 2109481.0, 145824029.0, 1688276.0]",2542999.0,Belgrade,2,"[144330671.0, 2046999993.0]",Y,"['The method is tested on a number of datasets (each used as source and target) and shows good transfer learning performance on each one.', 'The trained residual function can be used to predict a residual z_i for x_i.']",,,International Conference on Learning Representations,"['algorithm', 'action', 'domain', 'deep learning', 'Deep Learning']"
Deep Learning,8388f1be26329fa45e5807e968a641ce170ea078,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,https://www.semanticscholar.org/paper/8388f1be26329fa45e5807e968a641ce170ea078,JournalArticle,"In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",2015,12658,abs/1511.06434,"[38909097.0, 2096458.0, 2127604.0]",38909097.0,Monaco,3,"[2927870.0, 38929912.0, 134898163.0]",Y,"['A state screening function (SSF) method is proposed to drive the learner to remain in areas of the state space that have been covered by the teacher.', '* Baseline in which the labels are not mixed, in order to ensure that the gains are not coming from the data augmentation only.', 'To generate these samples, they first employ a model-free algorithm.']",,,International Conference on Learning Representations,"['cnns', 'learning', 'networks', 'deep learning', 'Deep Learning']"
Deep Learning,2c03df8b48bf3fa39054345bafabfeff15bfd11d,Deep Residual Learning for Image Recognition,https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d,Conference,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",2015,157545,,"[39353098.0, 1771551.0, 3080683.0]",39353098.0,Amsterdam,2,"[3589089.0, 2323922.0]",Y,"['Figure #s are missing off several figures.', 'Simulation results with MuJoCo physics simulator show that this simple trick reduces the amount of needed data by an order of magnitude.']",Computer Vision and Pattern Recognition,15.0,,"['nets', 'networks', 'layers', 'deep learning', 'Deep Learning']"
Deep Learning,0c00a328fa7cd56ee60338c54e89bd48310db80b,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,https://www.semanticscholar.org/paper/0c00a328fa7cd56ee60338c54e89bd48310db80b,JournalArticle,"The discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks, such as Gaussian denoising, single image super-resolution, and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing.",2016,5760,26,"[144110274.0, 1724520.0, 39773686.0, 1803714.0, 36685537.0]",144110274.0,London,3,"[2223887365.0, 2235966.0, 3149531.0]",Y,"['This paper addresses complex dynamical systems modelling through nonparametric Partial Differential Equations using neural architectures.', '(3): missing “-“ sign Page 3, in eq. (6): missing “transpose” on \\nabla \\hat{f} Page 4, first equation: O(|| \\eta*mu_t ||^2) Page 5, in eq.', 'The fact that the proposed technique is simple yet yields such speedups is encouraging.']",,,IEEE Transactions on Image Processing,"['image', 'model', 'gaussian', 'deep learning', 'Deep Learning']"
Deep Learning,8ec5896b4490c6e127d1718ffc36a3439d84cb81,On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,https://www.semanticscholar.org/paper/8ec5896b4490c6e127d1718ffc36a3439d84cb81,JournalArticle,"The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say $32$-$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.",2016,2508,abs/1609.04836,"[2844898.0, 2205699.0, 2784955.0, 1711231.0, 144669504.0]",2844898.0,Tirana,2,"[40046694.0, 47947548.0]",Y,"['The authors do provide a novel formulation and demonstrate the gains on a variety of concrete problems taken form the literature.', 'My main concern, however, is in the current sampling-based search algorithm in the latent z-space, which the authors have already admitted in the paper.']",,,International Conference on Learning Representations,"['methods', 'generalization', 'largebatch', 'deep learning', 'Deep Learning']"
Deep Learning,c468bbde6a22d961829e1970e6ad5795e05418d1,The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,https://www.semanticscholar.org/paper/c468bbde6a22d961829e1970e6ad5795e05418d1,Conference,"While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called ""perceptual losses""? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",2018,7179,,"[2844849.0, 2094770.0, 1763086.0, 2177801.0, 39231399.0]",2844849.0,Warsaw,2,"[145683384.0, 2231240312.0]",Y,"['The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models.', 'For the denoising comparison, how do the results compare to those obtained if you simulate a Markov Chain (sample latent state conditioned on noisy image, sample latent state, sample denoised observation, repeat using denoised observation) using a VAE?']",2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,18.0,,"['features', 'similarity', 'metrics', 'deep learning', 'Deep Learning']"
Deep Learning,3b9732bb07dc99bde5e1f9f75251c6ea5039373e,Deep Reinforcement Learning with Double Q-Learning,https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e,Conference,"
 
 The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.
 
",2015,6104,,"[7634925.0, 35099444.0, 145824029.0]",7634925.0,Sofia,3,"[1764004.0, 3437933.0, 34905515.0]",Y,"['Unsurprisingly (since the EEN noise does not seem to be conditioned on the input), the baseline deterministic model performs quite well.', '3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling.', 'The “obverter” technique is quite interesting since it incorporates the concept from the theory of mind which is similar to human alignment or AGI approach.']",AAAI Conference on Artificial Intelligence,15.0,,"['algorithm', 'overestimations', 'qlearning', 'deep learning', 'Deep Learning']"
Deep Learning,31f9eb39d840821979e5df9f34a6e92dd9c879f2,Learning Deep Features for Discriminative Localization,https://www.semanticscholar.org/paper/31f9eb39d840821979e5df9f34a6e92dd9c879f2,Conference,"In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.",2015,7937,,"[145291669.0, 2556428.0, 2677488.0, 143868587.0, 143805211.0]",145291669.0,Bern,3,"[145590185.0, 34842481.0, 2153917002.0]",Y,"['A more systematic set of experiments could compare learning the proposed weightings on the first K layers of the network (for K={0, 1, …, N}) and learning independent weights for the latter N-K layers, but I understand this would be a rather large experimental burden.', 'It is observed that words case and average case empirical error estimates diverge when the input is high dimensional.', 'Second: They then train a model based agent using the collected transitions ( St, At, St+1 ).']",Computer Vision and Pattern Recognition,15.0,,"['pooling', 'network', 'localization', 'deep learning', 'Deep Learning']"
Deep Learning,e30d9b8ce108d982169621b88a5e3fb69fec70e1,Using Deep Learning for Image-Based Plant Disease Detection,https://www.semanticscholar.org/paper/e30d9b8ce108d982169621b88a5e3fb69fec70e1,JournalArticle,"Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.",2016,2313,7,"[24178944.0, 2068146094.0, 3046313.0]",24178944.0,Sofia,2,"[100841270.0, 2921637.0]",Y,"['The proposed method demonstrated noticeable performance improvement on various discriminative tasks over existing approaches.', 'Secondly, the DDT’s leaves are parametrized with the encoder distribution q(z|x), and thus gradient information flows back through the DDT into the posterior approximations in order to make them more discriminative.']",,,Frontiers in Plant Science,"['crop', 'learning', 'disease', 'deep learning', 'Deep Learning']"
Deep Learning,150f95f9c73820e0a0fa1546140e9f2bdfd25954,Temporal Graph Networks for Deep Learning on Dynamic Graphs,https://www.semanticscholar.org/paper/150f95f9c73820e0a0fa1546140e9f2bdfd25954,JournalArticle,"Graph Neural Networks (GNNs) have recently become increasingly popular due to their ability to learn complex systems of relations or interactions arising in a broad spectrum of problems ranging from biology and particle physics to social networks and recommendation systems. Despite the plethora of different models for deep learning on graphs, few approaches have been proposed thus far for dealing with graphs that present some sort of dynamic nature (e.g. evolving features or connectivity over time). In this paper, we present Temporal Graph Networks (TGNs), a generic, efficient framework for deep learning on dynamic graphs represented as sequences of timed events. Thanks to a novel combination of memory modules and graph-based operators, TGNs are able to significantly outperform previous approaches being at the same time more computationally efficient. We furthermore show that several previous models for learning on dynamic graphs can be cast as specific instances of our framework. We perform a detailed ablation study of different components of our framework and devise the best configuration that achieves state-of-the-art performance on several transductive and inductive prediction tasks for dynamic graphs.",2020,386,abs/2006.10637,"[2056294358.0, 2029302.0, 51484149.0, 1775620.0, 2500309.0, 1732570.0]",2056294358.0,Oslo,2,"[37574242.0, 2110420880.0]",Y,"['The authors propose a new gradient compression method for efficient distributed training of neural networks.', '3.\tThe idea of using different compositions of color and figure during train and test is interesting.']",,,arXiv.org,"['graphs', 'networks', 'framework', 'deep learning', 'Deep Learning']"
Deep Learning,7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7,Learning Transferable Features with Deep Adaptation Networks,https://www.semanticscholar.org/paper/7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7,Conference,"Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multikernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.",2015,4409,,"[35776445.0, 2146174097.0, 2144499343.0, 1694621.0]",35776445.0,Copenhagen,2,"[1800564.0, 1786155.0]",Y,"['The authors show through several experiments that the divide and conquer (DnC) technique can solve more complex tasks than can be solved with conventional policy gradient methods (TRPO is used as the baseline).', 'Some comments: - Perhaps, it is better to move Section 3.3 before Section 3.2 to emphasize the main contribution of this work, i.e., using Stein’s identity to derive an estimate of the gradient of the score function.']",International Conference on Machine Learning,15.0,,"['domain', 'network', 'adaptation', 'deep learning', 'Deep Learning']"
Deep Learning,31f10a6f602bef0306ac37322f84f6163c8a8ecb,CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning,https://www.semanticscholar.org/paper/31f10a6f602bef0306ac37322f84f6163c8a8ecb,JournalArticle,"We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.",2017,2257,abs/1711.05225,"[2706258.0, 46559852.0, 29972904.0, 145951921.0, 3776937.0, 15069782.0, 51235411.0, 30043065.0, 2356307.0, 3474704.0, 4204731.0, 34699434.0]",2706258.0,Stockholm,3,"[150270469.0, 2219490144.0, 2691974.0]",Y,"['2. The proposed attention mechanism seems to be fairly domain (or task ) specific, and may not be beneficial for strong generalization (generalization over unseen category).', 'On the other hand, I agree that sample-independent perturbation is much more practical than sample-dependent perturbation.', '5. It seems the construction of Bloomier filter is costly and the proposed method has to construct Bloomier filters for all layers.']",,,arXiv.org,"['chexnet', 'radiologists', 'diseases', 'deep learning', 'Deep Learning']"
Deep Learning,2eda2921a8da4b325f9d05f556594a5884c398a7,Overfitting in adversarially robust deep learning,https://www.semanticscholar.org/paper/2eda2921a8da4b325f9d05f556594a5884c398a7,Conference,"It is common practice in deep learning to use overparameterized networks and train for as long as possible; there are numerous studies that show, both theoretically and empirically, that such practices surprisingly do not unduly harm the generalization performance of the classifier. In this paper, we empirically study this phenomenon in the setting of adversarially trained deep networks, which are trained to minimize the loss under worst-case adversarial perturbations. We find that overfitting to the training set does in fact harm robust performance to a very large degree in adversarially robust training across multiple datasets (SVHN, CIFAR-10, CIFAR-100, and ImageNet) and perturbation models ($\ell_\infty$ and $\ell_2$). Based upon this observed effect, we show that the performance gains of virtually all recent algorithmic improvements upon adversarial training can be matched by simply using early stopping. We also show that effects such as the double descent curve do still occur in adversarially trained models, yet fail to explain the observed overfitting. Finally, we study several classical and modern deep learning remedies for overfitting, including regularization and data augmentation, and find that no approach in isolation improves significantly upon the gains achieved by early stopping. All code for reproducing the experiments as well as pretrained model weights and training logs can be found at this https URL.",2020,620,,"[51026953.0, 47260842.0, 117539586.0]",51026953.0,San Marino,3,"[6322777.0, 2279712392.0, 2116502347.0]",Y,"['Should these parameters be take out of the n-step advantage function A?', 'I encourage the authors to have the work proofread by a native speaker; clearer writing will ultimately increase the reach and impact of the paper.', 'But, it would be good if this can be supported with real life examples.']",International Conference on Machine Learning,20.0,,"['training', 'performance', 'networks', 'deep learning', 'Deep Learning']"
Deep Learning,f9314fd99be5f2b1b3efcfab87197d578160d553,Deep Learning Enabled Semantic Communication Systems,https://www.semanticscholar.org/paper/f9314fd99be5f2b1b3efcfab87197d578160d553,JournalArticle,"Recently, deep learned enabled end-to-end communication systems have been developed to merge all physical layer blocks in the traditional communication systems, which make joint transceiver optimization possible. Powered by deep learning, natural language processing has achieved great success in analyzing and understanding a large amount of language texts. Inspired by research results in both areas, we aim to provide a new view on communication systems from the semantic level. Particularly, we propose a deep learning based semantic communication system, named DeepSC, for text transmission. Based on the Transformer, the DeepSC aims at maximizing the system capacity and minimizing the semantic errors by recovering the meaning of sentences, rather than bit- or symbol-errors in traditional communications. Moreover, transfer learning is used to ensure the DeepSC applicable to different communication environments and to accelerate the model training process. To justify the performance of semantic communications accurately, we also initialize a new metric, named sentence similarity. Compared with the traditional communication system without considering semantic information exchange, the proposed DeepSC is more robust to channel variation and is able to achieve better performance, especially in the low signal-to-noise (SNR) regime, as demonstrated by the extensive simulation results.",2020,493,69,"[66320873.0, 67022972.0, 1410112765.0, 143604406.0]",66320873.0,Warsaw,2,"[2211993531.0, 150302778.0]",Y,"[""As such it's not even clear if this is proper for a conference."", 'For example, the authors claimed in Section 4.3 that the Stein gradient estimator is faster than other methods, but it is not clear as to why this is the case.']",,,IEEE Transactions on Signal Processing,"['communication', 'deepsc', 'systems', 'deep learning', 'Deep Learning']"
Deep Learning,91e611c3e8705002438fb4439733e47ddec85b5d,fastai: A Layered API for Deep Learning,https://www.semanticscholar.org/paper/91e611c3e8705002438fb4439733e47ddec85b5d,JournalArticle,"fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4–5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching.",2020,695,11,"[2093348519.0, 2286145455.0]",2093348519.0,Prague,2,"[2061294792.0, 2241838.0]",Y,"['* Figure 2 seems like a test made to work for this method and does not add much to the paper.', 'Figure 2 which is the graphic representation of the model is hard to read.']",,,Inf.,"['library', 'learning', 'data', 'deep learning', 'Deep Learning']"
Deep Learning,68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a,ADMM-CSNet: A Deep Learning Approach for Image Compressive Sensing,https://www.semanticscholar.org/paper/68ff94fd4c6c93b2da78adcda53ff49ded9f8b2a,JournalArticle,"Compressive sensing (CS) is an effective technique for reconstructing image from a small amount of sampled data. It has been widely applied in medical imaging, remote sensing, image compression, etc. In this paper, we propose two versions of a novel deep learning architecture, dubbed as ADMM-CSNet, by combining the traditional model-based CS method and data-driven deep learning method for image reconstruction from sparsely sampled measurements. We first consider a generalized CS model for image reconstruction with undetermined regularizations in undetermined transform domains, and then two efficient solvers using Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing the model are proposed. We further unroll and generalize the ADMM algorithm to be two deep architectures, in which all parameters of the CS model and the ADMM algorithm are discriminatively learned by end-to-end training. For both applications of fast CS complex-valued MR imaging and CS imaging of real-valued natural images, the proposed ADMM-CSNet achieved favorable reconstruction accuracy in fast computational speed compared with the traditional and the other deep learning methods.",2020,456,42,"[2108850237.0, 2152146009.0, 1680740.0, 98220533.0]",2108850237.0,Luxembourg,2,"[48152160.0, 1775620.0]",Y,"[""This manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum, which actually has been shown by existing works like Kawaguchi (2016) with some assumptions."", 'If we consider e.g., a linear 1-layer autoencoder to be equivalent to PCA (without the rnn layers), in essence this formulation is closely related to applying pca to reduce the initial dimensionality and then t-sne.']",,,IEEE Transactions on Pattern Analysis and Machine Intelligence,"['image', 'cs', 'imaging', 'deep learning', 'Deep Learning']"
Deep Learning,af9280741ef627f0d6c8437605d002d3bfc2d1b1,Bayesian Deep Learning and a Probabilistic Perspective of Generalization,https://www.semanticscholar.org/paper/af9280741ef627f0d6c8437605d002d3bfc2d1b1,JournalArticle,"The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.",2020,501,abs/2002.08791,"[145771261.0, 7991830.0]",145771261.0,Madrid,2,"[145531067.0, 16042895.0]",Y,"['In summary, the proposed method may be promising, but far more experiments are needed.', ""However, I think there's some chance that if the same tasks were in the hands of people who *wanted* CNNs or CNN+RN to work well, the results might have been different.""]",,,Neural Information Processing Systems,"['marginalization', 'perspective', 'approach', 'deep learning', 'Deep Learning']"
Deep Learning,0e779fd59353a7f1f5b559b9d65fa4bfe367890c,Geometric Deep Learning: Going beyond Euclidean data,https://www.semanticscholar.org/paper/0e779fd59353a7f1f5b559b9d65fa4bfe367890c,JournalArticle,"Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.",2016,2825,34,"[1732570.0, 143627859.0, 1688882.0, 3149531.0, 1697397.0]",1732570.0,Bern,2,"[1786155.0, 2108485135.0]",Y,"['The model is further extended for conditional generation and demonstrated on a range of image benchmark data sets.', 'It is a data structure that maps from sparse indices to their corresponding values with chances that returns incorrect values for non-existing indices.']",,,IEEE Signal Processing Magazine,"['networks', 'data', 'structure', 'deep learning', 'Deep Learning']"
Deep Learning,07cca761749bfe21c2d096ff60f32b574d5c84c4,Normalized Loss Functions for Deep Learning with Noisy Labels,https://www.semanticscholar.org/paper/07cca761749bfe21c2d096ff60f32b574d5c84c4,Conference,"Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: any loss can be made robust to noisy labels. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of underfitting. To address this, we propose a framework to build robust loss functions called Active Passive Loss (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60% or 80% incorrect labels.",2020,317,,"[9576855.0, 1753845931.0, 1919541.0, 9035741.0, 144757691.0, 145148600.0]",9576855.0,Moscow,3,"[144985567.0, 2211964951.0, 1390481263.0]",Y,"['To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER.', 'I found the formulation of the \\alpha to be non-intuitive and confusing at times.', 'The paper is generally well written, easy to read and understand, and the results are compelling.']",International Conference on Machine Learning,20.0,,"['loss', 'functions', 'labels', 'deep learning', 'Deep Learning']"
Deep Learning,94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,Traffic Flow Prediction With Big Data: A Deep Learning Approach,https://www.semanticscholar.org/paper/94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,JournalArticle,"Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.",2015,2430,16,"[34859953.0, 2241838.0, 1903243.0, 2146248481.0, 143769760.0]",34859953.0,Minsk,2,"[37502184.0, 1389546686.0]",Y,"['This is far below the norm for semantic segmentation work in computer vision.', 'I encourage the authors to develop the problem and method further, as well as the analysis and evaluation.']",,,IEEE transactions on intelligent transportation systems (Print),"['traffic', 'flow', 'prediction', 'deep learning', 'Deep Learning']"
Deep Learning,94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,Traffic Flow Prediction With Big Data: A Deep Learning Approach,https://www.semanticscholar.org/paper/94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,JournalArticle,"Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.",2015,2430,16,"[34859953.0, 2241838.0, 1903243.0, 2146248481.0, 143769760.0]",34859953.0,Minsk,2,"[37502184.0, 1389546686.0]",Y,"['This is far below the norm for semantic segmentation work in computer vision.', 'I encourage the authors to develop the problem and method further, as well as the analysis and evaluation.']",,,IEEE transactions on intelligent transportation systems (Print),"['traffic', 'flow', 'prediction', 'big data', 'Deep Learning']"
Big Data,94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,Traffic Flow Prediction With Big Data: A Deep Learning Approach,https://www.semanticscholar.org/paper/94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,JournalArticle,"Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.",2015,2430,16,"[34859953.0, 2241838.0, 1903243.0, 2146248481.0, 143769760.0]",34859953.0,Prague,3,"[1390195074.0, 2660652.0, 47319783.0]",Y,"['However, on one hand the contribution is rather narrow, and on the other the results presented do not clearly show that the contribution is of significance in practice.', '1) Most previous work are all implemented as post-hoc additions to fully trained networks while this work is end-to-end trainable.', 'I think this is a nice example of a detailed analysis of the representations acquired by a reinforcement learning agent.']",,,IEEE transactions on intelligent transportation systems (Print),"['traffic', 'flow', 'prediction', 'deep learning', 'Big Data']"
Big Data,94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,Traffic Flow Prediction With Big Data: A Deep Learning Approach,https://www.semanticscholar.org/paper/94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,JournalArticle,"Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.",2015,2430,16,"[34859953.0, 2241838.0, 1903243.0, 2146248481.0, 143769760.0]",34859953.0,Prague,3,"[1390195074.0, 2660652.0, 47319783.0]",Y,"['However, on one hand the contribution is rather narrow, and on the other the results presented do not clearly show that the contribution is of significance in practice.', '1) Most previous work are all implemented as post-hoc additions to fully trained networks while this work is end-to-end trainable.', 'I think this is a nice example of a detailed analysis of the representations acquired by a reinforcement learning agent.']",,,IEEE transactions on intelligent transportation systems (Print),"['traffic', 'flow', 'prediction', 'big data', 'Big Data']"
Deep Learning,8760bc7631c0cb04e7138254e9fd6451b7def8ca,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,https://www.semanticscholar.org/paper/8760bc7631c0cb04e7138254e9fd6451b7def8ca,Conference,"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10 × or 100 × ? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between ‘enormous data’ and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",2017,2030,,"[1491624845.0, 1781242.0, 2108498897.0, 1726095131.0]",1491624845.0,London,3,"[32246932.0, 48740398.0, 2113618679.0]",Y,"['2. Even if this was correct, the main point is that this is ""only"" d times worse - see eq (11).', ""In any case, I also don't think that was the argument you were trying to make."", 'In a bit more detail, the key idea of the paper, at least to the extent that I understood it, was that the authors are able to introduce a model-based variance-reduction baseline into the policy gradient term.']",IEEE International Conference on Computer Vision,17.0,,"['vision', 'data', 'tasks', 'deep learning', 'Deep Learning']"
Deep Learning,d81fc968196e06ccafd7ea4c008b13e1cad1be64,An End-to-End Deep Learning Architecture for Graph Classification,https://www.semanticscholar.org/paper/d81fc968196e06ccafd7ea4c008b13e1cad1be64,Conference,"
 
 Neural networks are typically designed to deal with data in tensor forms. In this paper, we propose a novel neural network architecture accepting graphs of arbitrary structure. Given a dataset containing graphs in the form of (G,y) where G is a graph and y is its class, we aim to develop neural networks that read the graphs directly and learn a classification function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classification purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the first challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs. Experiments on benchmark graph classification datasets demonstrate that the proposed architecture achieves highly competitive performance with state-of-the-art graph kernels and other graph neural network methods. Moreover, the architecture allows end-to-end gradient-based training with original graphs, without the need to first transform graphs into vectors.
 
",2018,1277,,"[3098251.0, 7217944.0, 40059761.0, 9527255.0]",3098251.0,Moscow,2,"[1764236.0, 145033446.0]",Y,"['since you mention at the end of page 4 that you will only rely on linear SVMs for computational reasons.', '5. It would be nice to see the gradient flow with the new activation function compared to the ones without.']",AAAI Conference on Artificial Intelligence,18.0,,"['graph', 'graphs', 'networks', 'deep learning', 'Deep Learning']"
Deep Learning,a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f,"Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning",https://www.semanticscholar.org/paper/a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f,JournalArticle,"Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.",2016,4153,35,"[1797022.0, 144531567.0, 39703662.0, 50706692.0, 1742135.0, 37754137.0, 1722252.0, 1691549.0, 144838131.0]",1797022.0,Monaco,2,"[1903751380.0, 2729273.0]",Y,"['Cons Does not provide new theory but combines existing ideas in a new manner.', 'However, the presentation is somewhat confusing and the resulting architecture does not seem justified by the theory The paper considers the challenges of disentangling factors of variation in images: for example disentangling viewpoint from vehicle type in an image of a car.']",,,IEEE Transactions on Medical Imaging,"['image', 'cnn', 'detection', 'deep learning', 'Deep Learning']"
Quantum Computing,53103ae318a19569ac82cee5062de2cf73bf386c,Splitting phonons: Building a platform for linear mechanical quantum computing,https://www.semanticscholar.org/paper/53103ae318a19569ac82cee5062de2cf73bf386c,JournalArticle,"Linear optical quantum computing provides a desirable approach to quantum computing, with only a short list of required computational elements. The similarity between photons and phonons points to the interesting potential for linear mechanical quantum computing using phonons in place of photons. Although single-phonon sources and detectors have been demonstrated, a phononic beam splitter element remains an outstanding requirement. Here we demonstrate such an element, using two superconducting qubits to fully characterize a beam splitter with single phonons. We further use the beam splitter to demonstrate two-phonon interference, a requirement for two-qubit gates in linear computing. This advances a new solid-state system for implementing linear quantum computing, further providing straightforward conversion between itinerant phonons and superconducting qubits. Description Editor’s summary Phonons are the fundamental quantum vibrations within materials, with individual phonons representing the collective motion of many trillions of atoms. Efforts are underway to determine whether these mechanical vibrations can be developed into a quantum-computing architecture just like their optical cousin, photons. Qiao et al. demonstrate a beam splitter for single phonons and controlled two-phonon interference. Adding to the ability to launch and detect single phonons, a beam splitter now provides the final piece in the toolbox to develop a mechanically based platform for quantum computing. —Ian S. Osborne A beam splitter for phonons completes the toolbox required to develop a mechanically based quantum computing system.",2023,20,380,"[2249759901.0, 15642782.0, 2249761102.0, 2282113562.0, 143717184.0, 92647195.0, 36460588.0, 1499289559.0, 2206684539.0, 102445836.0, 2175142898.0, 2249760489.0]",2249759901.0,Valletta,2,"[2110675251.0, 2144447082.0]",Y,"['Probably we can suppress the range of each attention by introducing some prior knowledge about syntax trees (e.g., only paying attention to the descendants in a specific subtree).', 'Even the ADE20K dataset, from which this paper samples, is substantially larger and has an established benchmarking methodology (see http://placeschallenge.csail.mit.edu/).']",,,Science,"['phonons', 'quantum', 'computing', 'quantum computing', 'Quantum Computing']"
Quantum Computing,6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5,MQT Bench: Benchmarking Software and Design Automation Tools for Quantum Computing,https://www.semanticscholar.org/paper/6c260fb515bd0a75b4c49bd40ab7a7cf9c9262f5,JournalArticle,"Quantum software tools for a wide variety of design tasks on and across different levels of abstraction are crucial in order to eventually realize useful quantum applications. This requires practical and relevant benchmarks for new software tools to be empirically evaluated and compared to the current state of the art. Although benchmarks for specific design tasks are commonly available, the demand for an overarching cross-level benchmark suite has not yet been fully met and there is no mutual consolidation in how quantum software tools are evaluated thus far. In this work, we propose the MQT Bench benchmark suite (as part of the Munich Quantum Toolkit, MQT) based on four core traits: (1) cross-level support for different abstraction levels, (2) accessibility via an easy-to-use web interface (https://www.cda.cit.tum.de/mqtbench/) and a Python package, (3) provision of a broad selection of benchmarks to facilitate generalizability, as well as (4) extendability to future algorithms, gate-sets, and hardware architectures. By comprising more than 70,000 benchmark circuits ranging from 2 to 130 qubits on four abstraction levels, MQT Bench presents a first step towards benchmarking different abstraction levels with a single benchmark suite to increase comparability, reproducibility, and transparency.",2022,43,7,"[2103454094.0, 121839134.0, 144385184.0]",2103454094.0,Warsaw,3,"[2551387.0, 122064392.0, 5521724.0]",Y,"['Across the two NLP tasks authors show that MNMS models trained simultaneously give better performance than hand tuned architectures.', 'What is LSS in figure 4?', 'Unclear statements/notations: * end of page 3, notations are not entirely consist with previous notations * I do not understand which distribution is assumed on epsilon and gamma when taking the expectancy in equation (9).']",,,Quantum,"['quantum', 'levels', 'benchmark', 'quantum computing', 'Quantum Computing']"
Quantum Computing,ef35da3a3c471a6a15c7a3b09586483eb50cbef0,Practical application-specific advantage through hybrid quantum computing,https://www.semanticscholar.org/paper/ef35da3a3c471a6a15c7a3b09586483eb50cbef0,JournalArticle,"Quantum computing promises to tackle technological and industrial problems insurmountable for classical computers. However, today's quantum computers still have limited demonstrable functionality, and it is expected that scaling up to millions of qubits is required for them to live up to this touted promise. The feasible route in achieving practical quantum advantage goals is to implement a hybrid operational mode that realizes the cohesion of quantum and classical computers. Here we present a hybrid quantum cloud based on a memory-centric and heterogeneous multiprocessing architecture, integrated into a high-performance computing data center grade environment. We demonstrate that utilizing the quantum cloud, our hybrid quantum algorithms including Quantum Encoding (QuEnc), Hybrid Quantum Neural Networks and Tensor Networks enable advantages in optimization, machine learning, and simulation fields. We show the advantage of hybrid algorithms compared to standard classical algorithms in both the computational speed and quality of the solution. The achieved advance in hybrid quantum hardware and software makes quantum computing useful in practice today.",2022,29,abs/2205.04858,"[31347453.0, 2087046300.0, 2164706014.0, 2557591.0, 1388013676.0, 2050228932.0, 2065938.0, 2140212060.0, 47610941.0, 143700248.0]",31347453.0,Podgorica,3,"[3383328.0, 2054733874.0, 48576745.0]",Y,"['The paper uses some interesting properties of the CPD model to derive an efficient optimization solver for the BCD subproblems.', 'They present initial results on the performance of their framework called Multitask Neural Model Search (MNMS) controller.', 'It is also simple and easy to understand.']",,,arXiv.org,"['quantum', 'computers', 'algorithms', 'quantum computing', 'Quantum Computing']"
Quantum Computing,b000a4b30f6206f6cfb033a79aad1ba810c972a4,Perceval: A Software Platform for Discrete Variable Photonic Quantum Computing,https://www.semanticscholar.org/paper/b000a4b30f6206f6cfb033a79aad1ba810c972a4,JournalArticle,"We introduce Perceval, an open-source software platform for simulating and interfacing with discrete-variable photonic quantum computers, and describe its main features and components. Its Python front-end allows photonic circuits to be composed from basic photonic building blocks like photon sources, beam splitters, phase-shifters and detectors. A variety of computational back-ends are available and optimised for different use-cases. These use state-of-the-art simulation techniques covering both weak simulation, or sampling, and strong simulation. We give examples of Perceval in action by reproducing a variety of photonic experiments and simulating photonic implementations of a range of quantum algorithms, from Grover's and Shor's to examples of quantum machine learning. Perceval is intended to be a useful toolkit for experimentalists wishing to easily model, design, simulate, or optimise a discrete-variable photonic experiment, for theoreticians wishing to design algorithms and applications for discrete-variable photonic quantum computing platforms, and for application designers wishing to evaluate algorithms on available state-of-the-art photonic quantum computers.",2022,27,7,"[2161241855.0, 2078675007.0, 2161242301.0, 2161242145.0, 2161242134.0, 2161241528.0, 5344810.0, 102509601.0, 88251701.0, 103146051.0, 50738985.0, 48700430.0, 2591230.0, 47627363.0, 33091515.0, 48530805.0]",2161241855.0,Prague,3,"[2120801160.0, 1786389.0, 2544049.0]",Y,"['For example, what is the size of each layer and the dimension of the encoded space?', 'The interpolation results of Figure 11 are also underwhelming, though possibly mostly because the reconstructions are in general not great.', 'The real value of the paper is in providing an alternative way of thinking about LSTMs that is theoretically sound and intuitive.']",,,Quantum,"['quantum', 'simulation', 'computers', 'quantum computing', 'Quantum Computing']"
Quantum Computing,b977e8de38dc0d13817bca1ed20036badfe2a58c,Pulse based Variational Quantum Optimal Control for hybrid quantum computing,https://www.semanticscholar.org/paper/b977e8de38dc0d13817bca1ed20036badfe2a58c,JournalArticle,"This work studies pulse based variational quantum algorithms (VQAs), which are designed to determine the ground state of a quantum mechanical system by combining classical and quantum hardware. In contrast to more standard gate based methods, pulse based methods aim to directly optimize the laser pulses interacting with the qubits, instead of using some parametrized gate based circuit. Using the mathematical formalism of optimal control, these laser pulses are optimized. This method has been used in quantum computing to optimize pulses for quantum gate implementations, but has only recently been proposed for full optimization in VQAs. Pulse based methods have several advantages over gate based methods such as faster state preparation, simpler implementation and more freedom in moving through the state space. Based on these ideas, we present the development of a novel adjoint based variational method. This method can be tailored towards and applied in neutral atom quantum computers. This method of pulse based variational quantum optimal control is able to approximate molecular ground states of simple molecules up to chemical accuracy and is able to compete with the gate based variational quantum eigensolver in terms of total number of quantum evaluations. The total evolution time T and the form of the control Hamiltonian Hc are important factors in the convergence behavior to the ground state energy, both having influence on the quantum speed limit and the controllability of the system.",2022,16,7,"[32519404.0, 144910138.0, 153538835.0]",32519404.0,Rome,2,"[2143685866.0, 2080690.0]",Y,"['This is a solid paper about model evaluation in the chemical domain.', '1. Comparing to previous work (Mordatch & Abbeel, 2018), the task is relatively simple, only requiring the agent to perform binary prediction.']",,,Quantum,"['quantum', 'pulse', 'state', 'quantum computing', 'Quantum Computing']"
Quantum Computing,329d31f881a17861eedeef6a9d8fd509cddd2b7c,QUARK: A Framework for Quantum Computing Application Benchmarking,https://www.semanticscholar.org/paper/329d31f881a17861eedeef6a9d8fd509cddd2b7c,Conference,"Quantum computing (QC) is anticipated to provide a speedup over classical approaches for specific problems in optimization, simulation, and machine learning. With the advances in quantum computing toward practical applications, the need to analyze and compare different quantum solutions is increasing. While different low-level benchmarks exist, they often do not provide sufficient insights into real-world application-level performance. We propose an application-centric benchmark method and the QUantum computing Application benchmaRK (QUARK) framework to foster the investigation and creation of application benchmarks for QC. This paper establishes three significant contributions: (1) it makes a case for application-level benchmarks and provides an in-depth ""pen and paper"" benchmark formulation of two reference problems: robot path and vehicle option optimization from the industrial domain; (2) it proposes the open-source QUARK framework for designing, implementing, executing, and analyzing benchmarks; (3) it provides multiple reference implementations for these two reference problems based on different known, and where needed, extended, classical and quantum algorithmic approaches and analyzes their performance on different types of infrastructures.",2022,20,,"[2147184066.0, 2105502136.0, 9653883.0, 50631038.0]",2147184066.0,Moscow,2,"[2217847684.0, 2158502526.0]",Y,"['The current paper presentation is a bit too dense to clearly understand the LL machine model and the two-phase algorithm.', 'So, I wish to see a section on testing with Resnet and GoogleNet.']",International Conference on Quantum Computing and Engineering,22.0,,"['quantum', 'problems', 'benchmarks', 'quantum computing', 'Quantum Computing']"
Quantum Computing,6fed828456964d29517f6caf31b700d8aec82153,Enabling multi-programming mechanism for quantum computing in the NISQ era,https://www.semanticscholar.org/paper/6fed828456964d29517f6caf31b700d8aec82153,JournalArticle,"NISQ devices have several physical limitations and unavoidable noisy quantum operations, and only small circuits can be executed on a quantum machine to get reliable results. This leads to the quantum hardware under-utilization issue. Here, we address this problem and improve the quantum hardware throughput by proposing a Quantum Multi-programming Compiler (QuMC) to execute multiple quantum circuits on quantum hardware simultaneously. This approach can also reduce the total runtime of circuits. We first introduce a parallelism manager to select an appropriate number of circuits to be executed at the same time. Second, we present two different qubit partitioning algorithms to allocate reliable partitions to multiple circuits – a greedy and a heuristic. Third, we use the Simultaneous Randomized Benchmarking protocol to characterize the crosstalk properties and consider them in the qubit partition process to avoid the crosstalk effect during simultaneous executions. Finally, we enhance the mapping transition algorithm to make circuits executable on hardware using a decreased number of inserted gates. We demonstrate the performance of our QuMC approach by executing circuits of different sizes on IBM quantum hardware simultaneously. We also investigate this method on VQE algorithm to reduce its overhead.",2021,143,abs/2102.05321,"[2055318525.0, 3182535.0]",2055318525.0,Ljubljana,3,"[40080808.0, 97233415.0, 2947357.0]",Y,"['A well written, clear paper presenting a novel representation of graphs   as multi-channel image-like structures from their node embeddings.', 'There is a p(z_1:T) term that should appear in the integrand.', 'The paper is well-written, clearly illustrating the goal of this work and the corresponding approach.']",,,Quantum,"['quantum', 'circuits', 'hardware', 'quantum computing', 'Quantum Computing']"
Quantum Computing,047286f5b9315a8e8bf56c4fc936e62f21495892,Resource Allocation in Quantum Networks for Distributed Quantum Computing,https://www.semanticscholar.org/paper/047286f5b9315a8e8bf56c4fc936e62f21495892,Conference,"The evolution of quantum computing technologies has been advancing at a steady pace in the recent years, and the current trend suggests that it will become available at scale for commercial purposes in the near future. The acceleration can be boosted by pooling compute infrastructures to either parallelize algorithm execution or solve bigger instances that are not feasible on a single quantum computer, which requires an underlying Quantum Internet: the interconnection of quantum computers by quantum links and repeaters to exchange entangled quantum bits. However, Quantum Internet research so far has been focused on provisioning point-to-point flows only, which is suitable for (e.g.) quantum sensing and metrology, but not for distributed quantum computing. In this paper, after a primer on quantum computing and networking, we investigate the requirements and objectives of smart computing on distributed nodes from the perspective of quantum network provisioning. We then design a resource allocation strategy that is evaluated through a comprehensive simulation campaign, whose results highlight the key features and performance issues, and lead the way to further investigation in this direction.",2022,14,,"[1741486.0, 2288804757.0, 1739490.0]",1741486.0,Sofia,3,"[2762838.0, 3428490.0, 1806271.0]",Y,"['Is the implicit assumption that for real, high-dimensional data the generator and data manifolds will *never* overlap?', ""* There's abundant literature on f-divergences which show that there's a 1-1 relationship between divergences and optimal (Bayes) risks of classification problems (e.g. Reid at al. Information, Divergence and Risk for Binary Experiments in JMLR and Garcia-Garcia et al. Divergences and Risks for Multiclass Experiments in COLT)."", 'For instance, averaging the node embeddings is something that has shown promising results in previous work.']",International Conference on Smart Computing,22.0,,"['quantum', 'computing', 'internet', 'quantum computing', 'Quantum Computing']"
Quantum Computing,c2aab470b8cf92f090e0a3bac1794b21500585e6,Evolution of Quantum Computing: A Systematic Survey on the Use of Quantum Computing Tools,https://www.semanticscholar.org/paper/c2aab470b8cf92f090e0a3bac1794b21500585e6,Conference,"Quantum Computing (QC) refers to an emerging paradigm that inherits and builds with the concepts and phenomena of Quantum Mechanic (QM) with the significant potential to unlock a remarkable opportunity to solve complex and computationally intractable problems that scientists could not tackle previously. In recent years, tremendous efforts and progress in QC mark a significant milestone in solving real-world problems much more efficiently than classical computing technology. While considerable progress is being made to move quantum computing in recent years, significant research efforts need to be devoted to move this domain from an idea to a working paradigm. In this paper, we conduct a systematic survey and categorize papers, tools, frameworks, platforms that facilitate quantum computing and analyze them from an application and Quantum Computing perspective. We present quantum Computing Layers, Characteristics of Quantum Computer platforms, Circuit Simulator, Open-source Tools- Cirq, TensorFlow Quantum, ProjectQ etc. that allow implementing quantum programs in Python using a powerful and intuitive syntax. Following that, we discuss the current essence, identify open challenges, and provide future research direction. We conclude that scores of frameworks, tools and platforms are emerged in the past few years, improvement of currently available facilities would exploit the research activities in the quantum research community.",2022,12,,"[9368179.0, 2127923241.0, 50768264.0, 22706311.0, 1883858.0, 28334651.0, 9878116.0, 4625670.0, 39864830.0]",9368179.0,Tirana,3,"[144231976.0, 2120251897.0, 2065048323.0]",Y,"['remark on theorem 1: This result generalizes a result proven in 2015 stating that the normality of a layer propagates to the next as the size of the first layer goes to infinity.', 'Clarity ===== The paper reads well, but it is not really clear what the claimed contribution is.', 'With this metric, the comparison would be easier and more intuitive.']",Annual International Computer Software and Applications Conference,46.0,,"['quantum', 'computing', 'research', 'quantum computing', 'Quantum Computing']"
Quantum Computing,208ac1f2ec9bf367a9981fedb6d9ea6aa9889099,Low-overhead fault-tolerant quantum computing using long-range connectivity,https://www.semanticscholar.org/paper/208ac1f2ec9bf367a9981fedb6d9ea6aa9889099,JournalArticle,"Vast numbers of qubits will be needed for large-scale quantum computing because of the overheads associated with error correction. We present a scheme for low-overhead fault-tolerant quantum computation based on quantum low-density parity-check (LDPC) codes, where long-range interactions enable many logical qubits to be encoded with a modest number of physical qubits. In our approach, logic gates operate via logical Pauli measurements that preserve both the protection of the LDPC codes and the low overheads in terms of the required number of additional qubits. Compared with surface codes with the same code distance, we estimate order-of-magnitude improvements in the overheads for processing around 100 logical qubits using this approach. Given the high thresholds demonstrated by LDPC codes, our estimates suggest that fault-tolerant quantum computation at this scale may be achievable with a few thousand physical qubits at comparable error rates to what is needed for current approaches.",2021,41,8,"[2142766693.0, 40026378.0, 3029700.0, 46610676.0]",2142766693.0,Sarajevo,3,"[66320873.0, 2278834796.0, 2006205621.0]",Y,"['Then the addition of the VAE had no measurable effect on the PixelCNN++’s performance, i.e., it seems like a bad idea due to the added complexity and loss of tractability.', 'The paragraph just above section 4 says that the authors sample a batch of training data for this, but assume that the test point x_star has to be included in this batch.', 'For instance, since two runs of node2vec will give you highly varying embeddings (depending on the initialization), you will have to run node2vec several times to reduce the variance of your resulting discretized density maps.']",,,Science Advances,"['qubits', 'quantum', 'overheads', 'quantum computing', 'Quantum Computing']"
Quantum Computing,3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b,Quantum PUF for Security and Trust in Quantum Computing,https://www.semanticscholar.org/paper/3a9f7c5bdd7f9560bf150332e97d6b1facdfce9b,JournalArticle,"Quantum computing is a promising paradigm to solve computationally intractable problems. Various companies such as, IBM, Rigetti and D-Wave offer quantum computers using a cloud-based platform that possess several interesting features namely, (i) quantum hardware with various number of qubits and coupling maps exist at the cloud end that offer different computing capabilities; (ii) multiple hardware with identical coupling maps exist in the suite; (iii) coupling map of larger hardware with more number of qubits can fit the coupling map of many smaller hardware; (iv) the quality of each of the hardware is distinct; (v) user cannot validate the origination of the result obtained from a quantum hardware. In other words, the user relies on the scheduler of the cloud provider to allocate the requested hardware; (vi) the queue of quantum programs at the cloud end is typically long and maximizing the throughput, which is the key to reducing costs and helping the scientific community in their explorations. The above factors motivate a new threat model with following possibilities: (a) in future, less-trustworthy quantum computers from 3rd parties can allocate poor quality hardware to save on cost or towards satisfying their falsely-advertised qubit or quantum hardware specifications; (b) the workload scheduling algorithm could have a bug or malicious code segment which will try to maximize throughput at the cost of allocation to poor fidelity hardware. Such bugs are possible for trustworthy providers; (c) a rogue employee in trusted cloud vendor could try to sabotage the vendor’s reputation by degrading the user compute fidelity just by tampering with the scheduling algorithm or rerouting the program; (d) a rogue employee can steal information by redirecting the programs to a 3rd party quantum hardware where they have full control. If the allocated hardware is inferior in quality, the user will suffer from poor quality result or longer convergence time. We propose two flavors of a Quantum Physically Unclonable Function (QuPUF) to address this issue- one based on superposition and another based on decoherence. Our experiments on real quantum hardware reveal that temporal variations in qubit quality can degrade the quality of the proposed QuPUF. We add a parametric rotation to the QuPUF for stability. Experiments on real IBM quantum hardware show that the proposed QuPUF can achieve inter-die Hamming Distance (HD) of 55% and intra-HD as low as 4%, as compared to ideal cases of 50% and 0% respectively. The proposed QuPUFs can also be used as a standalone solution for any other application.",2021,35,11,"[2074099273.0, 153329056.0, 71667767.0, 144695135.0, 144505057.0]",2074099273.0,Madrid,2,"[2145209409.0, 2742475.0]",Y,"['Across the two NLP tasks authors show that MNMS models trained simultaneously give better performance than hand tuned architectures.', 'The paper notices through simulations that in a grid search over the initial parameters of generator optimal discriminator training always succeeds in recovering the true generator parameters, whereas the other two methods fail and exhibit mode collapse.']",,,IEEE Journal on Emerging and Selected Topics in Circuits and Systems,"['hardware', 'quantum', 'quality', 'quantum computing', 'Quantum Computing']"
Quantum Computing,71ba8a8c75e0826f94eb53ee7a4566d4fc5b5256,Distributed Quantum Computing and Network Control for Accelerated VQE,https://www.semanticscholar.org/paper/71ba8a8c75e0826f94eb53ee7a4566d4fc5b5256,JournalArticle,"Interconnecting small quantum computers will be essential in the future for creating large-scale, robust quantum computers. Methods for distributing monolithic quantum algorithms efficiently are, thus, needed. In this article, we consider an approach for distributing the accelerated variational quantum eigensolver algorithm over arbitrary sized—in terms of number of qubits—distributed quantum computers. We consider approaches for distributing qubit assignments of the Ansatz states required to estimate the expectation value of Hamiltonian operators in quantum chemistry in a parallelized computation and provide a systematic approach to generate distributed quantum circuits for distributed quantum computing. Moreover, we propose an architecture for a distributed quantum control system in the context of centralized and decentralized network control.",2021,31,2,"[52221692.0, 2697097.0, 32610684.0]",52221692.0,Budapest,2,"[2444919.0, 144666776.0]",Y,"['Also, that the ""style"" representation contain less (and I\'d say slightly less, in Figure 7 b and d, we still see a lot of same class nearest neighbors) is not exactly a surprising result.', '3. The section on curriculum learning does not mention relevant work on “starting small”  and the “less is more"" hypothesis in language development by Jeff Elman and Elissa Newport: https://pdfs.semanticscholar.org/371b/240bebcaa68921aa87db4cd3a5d4e2a3a36b.pdf http://www.sciencedirect.com/science/article/pii/0388000188900101']",,,IEEE Transactions on Quantum Engineering,"['quantum', 'computers', 'approach', 'quantum computing', 'Quantum Computing']"
Quantum Computing,3ea34401909978d3d3d0c25c8746e02c7d2a7c77,Optimal Layout Synthesis for Quantum Computing,https://www.semanticscholar.org/paper/3ea34401909978d3d3d0c25c8746e02c7d2a7c77,Conference,"Recent years have witnessed the fast development of quantum computing. Researchers around the world are eager to run larger and larger quantum algorithms that promise speedups impossible to any classical algorithm. However, the available quantum computers are still volatile and error-prone. Thus, layout synthesis, which transforms quantum programs to meet these hardware limitations, is a crucial step in the realization of quantum computing. In this paper, we present two synthesizers, one optimal and one approximate but nearly optimal. Although a few optimal approaches to this problem have been published, our optimal synthesizer explores a larger solution space, thus is optimal in a stronger sense. In addition, it reduces time and space complexity exponentially compared to some leading optimal approaches. The key to this success is a more efficient spacetime-based variable encoding of the layout synthesis problem as a mathematical programming problem. By slightly changing our formulation, we arrive at an approximate synthesizer that is even more efficient and outperforms some leading heuristic approaches, in terms of additional gate cost, by up to 100%, and also fidelity by up to 10x on a comprehensive set of benchmark programs and architectures. For a specific family of quantum programs named QAOA, which is deemed to be a promising application for near-term quantum computers, we further adjust the approximate synthesizer by taking commutation into consideration, achieving up to 75% reduction in depth and up to 65% reduction in additional cost compared to the tool used in a leading QAOA study.",2020,98,,"[2218979994.0, 2259796.0]",2218979994.0,Chisinau,2,"[2156120640.0, 2115626527.0]",Y,"['Across the two NLP tasks authors show that MNMS models trained simultaneously give better performance than hand tuned architectures.', 'Except from a few typos here and there, the paper is overall well-written.']",2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD),20.0,,"['quantum', 'programs', 'approaches', 'quantum computing', 'Quantum Computing']"
Quantum Computing,bcc82ce554942880814243fc8c08a88b9d2aad09,Reading the road: challenges and opportunities on the path to responsible innovation in quantum computing,https://www.semanticscholar.org/paper/bcc82ce554942880814243fc8c08a88b9d2aad09,JournalArticle,"ABSTRACT
 Novel technologies such as quantum computing present new opportunities to support societal needs, but societal engagement is vital to secure public trust. Quantum computing technologies are at a pivotal point in their journey from foundational research to deployment, creating a moment for society to investigate, reflect, and consult on their implications. Responsible Innovation (RI) is one method for considering impacts, engaging with societal needs, reflecting on any concerns, and influencing the trajectory of the innovation in response. This paper draws on the empirical work of the RI team embedded in the Networked Quantum Information Technologies Hub. The team investigated researchers’ perceptions of RI and their understanding of societal impacts of quantum technologies, and sought to gauge the challenges of embedding RI across a multi-disciplinary, large-scale enterprise such as the UK quantum programme. The work demonstrated some of the difficulties involved in embedding RI approaches, and in creating a dialogue between innovators and societies. Finally, the authors offer recommendations to policymakers, researchers, and industrial organisations, for better practice in responsible quantum computing, and to ensure that societal considerations are discussed alongside commercial motivations. Applying RI to quantum computing at this pivotal point has implications for RI in other emerging technologies.",2021,26,35,"[100782743.0, 2092826.0, 1771117.0]",100782743.0,Luxembourg,3,"[2536434.0, 2183294.0, 153533172.0]",Y,"['A promising approach on nonparametric modelling of partial differential equations with deep architectures that requires more details.', 'It is intuitively expected that the proposed approach has some merit in some domains, but it is unclear exactly when and where it is.', 'In general, the proposed work is very interesting and the idea is neat.']",,,Technology Analysis & Strategic Management,"['quantum', 'technologies', 'ri', 'quantum computing', 'Quantum Computing']"
Quantum Computing,e576a2d97950b1f6831f88575dd3f370053f6af7,Distributed Quantum Computing with QMPI,https://www.semanticscholar.org/paper/e576a2d97950b1f6831f88575dd3f370053f6af7,Conference,"Practical applications of quantum computers require millions of physical qubits and it will be challenging for individual quantum processors to reach such qubit numbers. It is therefore timely to investigate the resource requirements of quantum algorithms in a distributed setting, where multiple quantum processors are inter-connected by a coherent network. We introduce an extension of the Message Passing Interface (MPI) to enable high-performance implementations of distributed quantum algorithms. In turn, these implementations can be used for testing, debugging, and resource estimation. In addition to a prototype implementation of quantum MPI, we present a performance model for distributed quantum computing, SENDQ. The model is inspired by the classical LogP model, making it useful to inform algorithmic decisions when program-ming distributed quantum computers. Specifically, we consider several optimizations of two quantum algorithms for problems in physics and chemistry, and we detail their effects on performance in the SENDQ model.",2021,25,,"[3393711.0, 3393324.0, 1713648.0, 1752096.0]",3393711.0,Vienna,3,"[6322777.0, 2253929707.0, 3087426.0]",Y,"['They are both trained on the same training data, only test data is of different length and ideally both models should achieve similar accuracy for the first 10 subproblems (same trend as DDRstack).', 'Figure #s are missing off several figures.', 'There should be experiments that compare the the Q+P model with incresing number of atoms against a full CNN, to see whether the Q+P can converge to maximal performance.']","International Conference for High Performance Computing, Networking, Storage and Analysis",21.0,,"['quantum', 'model', 'algorithms', 'quantum computing', 'Quantum Computing']"
Quantum Computing,6be56f559a74c0124526242e70cbdfd16cbc60a7,Quantum Computing - from NISQ to PISQ,https://www.semanticscholar.org/paper/6be56f559a74c0124526242e70cbdfd16cbc60a7,JournalArticle,"Given the impeding timeline of developing good quality quantum processing units, it is the moment to rethink the approach to advance quantum computing research. Rather than waiting for quantum hardware technologies to mature, we need to start assessing in tandem the impact of the occurrence of quantum computing in various scientific fields.However, to this purpose, we need to use a complementary but quite different approach than proposed by the NISQ vision, which is heavily focused on and burdened by the engineering challenges.That is why we propose and advocate the PISQ-approach: Perfect Intermediate-Scale Quantum computing based on the already known concept of perfect qubits.This will allow researchers to focus much more on the development of new applications by defining the algorithms in terms of perfect qubits and evaluate them on quantum computing simulators that are executed on supercomputers.It is not the long-term solution but will currently allow universities to research on quantum logic and algorithms and companies can already start developing their internal know-how on quantum solutions.",2021,21,41,"[1737836.0, 144330671.0, 145594823.0]",1737836.0,Brussels,2,"[2248088902.0, 2605688.0]",Y,"['Clarity: The paper is clearly written and easy to follow and understand.', '2) In equation (2), l_i and g should have different dimensionality, how does addition work?']",,,IEEE Micro,"['quantum', 'computing', 'approach', 'quantum computing', 'Quantum Computing']"
Quantum Computing,390bcf15a1b13cb0d5966859c35c69a31238e838,Optimized Compiler for Distributed Quantum Computing,https://www.semanticscholar.org/paper/390bcf15a1b13cb0d5966859c35c69a31238e838,JournalArticle,"Practical distributed quantum computing requires the development of efficient compilers, able to make quantum circuits compatible with some given hardware constraints. This problem is known to be tough, even for local computing. Here, we address it on distributed architectures. As generally assumed in this scenario, telegates represent the fundamental remote (inter-processor) operations. Each telegate consists of several tasks: (i) entanglement generation and distribution, (ii) local operations, and (iii) classical communications. Entanglement generations and distribution is an expensive resource, as it is time-consuming. To mitigate its impact, we model an optimization problem that combines running-time minimization with the usage of distributed entangled states. Specifically, we formulated the distributed compilation problem as a dynamic network flow. To enhance the solution space, we extend the formulation, by introducing a predicate that manipulates the circuit given in input and parallelizes telegate tasks. To evaluate our framework, we split the problem into three sub-problems, and solve it by means of an approximation routine. Experiments demonstrate that the run-time is resistant to the problem size scaling. Moreover, we apply the proposed algorithm to compile circuits under different topologies, showing that topologies with a higher ratio between edges and nodes give rise to shallower circuits.",2021,23,4,"[1509419973.0, 34702644.0, 98912691.0, 10719003.0, 2148614280.0, 48994822.0, 3062937.0]",1509419973.0,Nicosia,3,"[35638374.0, 2113771309.0, 144482217.0]",Y,"['Pros: Well written, thorough treatment of the approaches Improvements on top of Dual-AC with ablation study show improvement Cons: Empirical gains might not be very large', 'May you make this statement more clearly by adding an equation for example?', 'For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers.']",,,ACM Transactions on Quantum Computing,"['problem', 'circuits', 'quantum', 'quantum computing', 'Quantum Computing']"
Quantum Computing,f0eb17c6def9aaf95ef3c8ce12b9c3ceb9030274,Asleep at the wheel? Responsible Innovation in quantum computing,https://www.semanticscholar.org/paper/f0eb17c6def9aaf95ef3c8ce12b9c3ceb9030274,JournalArticle,"ABSTRACT Quantum computing is an emerging set of technologies which promise to transform aspects of computing in ways that, though increasingly defined, are still largely theoretical. Responsible Innovation (RI) asserts that technologies with potentially transformative capacity on society should be approached with care and forethought; this paper is based on applying RI in one of the UK’s National Quantum Technology Hubs. Quantum computing is at a key juncture as it emerges from the laboratory to be of interest commercially. This provides an opportunity to observe and influence the trajectory of this technology. Quantum computing is widely envisioned to have major impacts on computing and society; there are, however, great uncertainties about development timescales and the scope and impact of applications. From experiences with a major quantum computing project in the UK, we discuss the challenges in applying RI to quantum computing. Existing RI practices struggle to address the societal implications of such a complex and innovative technology. We argue that uncovering the visions and sociotechnical imaginaries that inform the development this technology enables RI to make valuable insights into future societal implications of quantum computing. This provides lessons for RI in emerging technologies more widely.",2021,15,33,"[2092826.0, 100782743.0, 1771117.0, 2149472612.0]",2092826.0,Vaduz,2,"[81408931.0, 143811079.0]",Y,"['The motivation is difficult to grasp and the contributions do not seem compelling.', 'In the paper, the authors proposed using GAN for anomaly detection.']",,,Technology Analysis & Strategic Management,"['quantum', 'ri', 'computing', 'quantum computing', 'Quantum Computing']"
Quantum Computing,43eea2a73997294193228d50f9ff25fc5345664b,Quantum Algorithms and Simulation for Parallel and Distributed Quantum Computing,https://www.semanticscholar.org/paper/43eea2a73997294193228d50f9ff25fc5345664b,Workshop,"A viable approach for building large-scale quantum computers is to interlink small-scale quantum computers with a quantum network to create a larger distributed quantum computer. When designing quantum algorithms for such a distributed quantum computer, one can make use of the added parallelization and distribution abilities inherent in the system. An added difficulty to then overcome for distributed quantum computing is that a complex control system to orchestrate the various components is required. In this work, we aim to address these issues. We explicitly define what it means for a quantum algorithm to be distributed and then present various quantum algorithms that fit the definition. We discuss potential benefits and propose a high-level scheme for controlling the system. With this, we present our software framework called Interlin-q, a simulation platform that aims to simplify designing and verifying parallel and distributed quantum algorithms. We demonstrate Interlin-q by implementing some of the discussed algorithms using Interlin-q and layout future steps for developing Interlin-q into a control system for distributed quantum computers.",2021,17,,"[115799034.0, 2061202877.0, 2042703517.0, 52221692.0]",115799034.0,Oslo,2,"[1709023.0, 2403712.0]",Y,"['Deterministic latent models seem to work better than stochastic ones.', 'There are no program synthesis examples demonstrating types of functions which perform complex tasks involving e.g. recursion, such as sorting operations.']",2021 IEEE/ACM Second International Workshop on Quantum Computing Software (QCS),21.0,,"['quantum', 'algorithms', 'system', 'quantum computing', 'Quantum Computing']"
Quantum Computing,82973c5f56681190a0dbb4c4449ed60d5f805135,EQC: ensembled quantum computing for variational quantum algorithms,https://www.semanticscholar.org/paper/82973c5f56681190a0dbb4c4449ed60d5f805135,Conference,"Variational quantum algorithm (VQA), which is comprised of a classical optimizer and a parameterized quantum circuit, emerges as one of the most promising approaches for harvesting the power of quantum computers in the noisy intermediate scale quantum (NISQ) era. However, the deployment of VQAs on contemporary NISQ devices often faces considerable system and time-dependant noise and prohibitively slow training speeds. On the other hand, the expensive supporting resources and infrastructure make quantum computers extremely keen on high utilization. In this paper, we propose a virtualized way of building up a quantum backend for variational quantum algorithms: rather than relying on a single physical device which tends to introduce ever-changing device-specific noise with less reliable performance as time-since-calibration grows, we propose to constitute a quantum ensemble, which dynamically distributes quantum tasks asynchronously across a set of physical devices, and adjusts the ensemble configuration with respect to machine status. In addition to reduced machine-dependant noise, the ensemble can provide significant speedups for VQA training. With this idea, we build a novel VQA training framework called EQC - a distributed gradient-based processor-performance-aware optimization system - that comprises: (i) a system architecture for asynchronous parallel VQA cooperative training; (ii) an analytical model for assessing the quality of a circuit output concerning its architecture, transpilation, and runtime conditions; (iii) a weighting mechanism to adjust the quantum ensemble's computational contribution according to the systems' current performance. Evaluations comprising 500K times' circuit evaluations across 10 IBMQ NISQ devices using a VQE and a QAOA applications demonstrate that EQC can attain error rates very close to the most performant device of the ensemble, while boosting the training speed by 10.5X on average (up to 86X and at least 5.2x). EQC is available at https://github.com/pnnl/eqc.",2021,29,,"[1949572253.0, 1766680.0, 3253856.0, 145560079.0, 143924199.0, 1743474.0, 2107856766.0, 2112839155.0]",1949572253.0,San Marino,3,"[2047998.0, 2161986932.0, 144303419.0]",Y,"['Like for example a penalty in how many examples a expert has catched.', 'Firstly, the model has ample free parameters to overfit when such a tiny test set is used.', 'However, the sensitivity calculations in the SVM context is new as per my knowledge.']",International Symposium on Computer Architecture,49.0,,"['quantum', 'training', 'vqa', 'quantum computing', 'Quantum Computing']"
Quantum Computing,ade9a900acc3c138021070537840488526796d35,A comparative study of universal quantum computing models: towards a physical unification,https://www.semanticscholar.org/paper/ade9a900acc3c138021070537840488526796d35,JournalArticle,"Quantum computing has been a fascinating research field in quantum physics. Recent progresses motivate us to study in depth the universal quantum computing models (UQCM), which lie at the foundation of quantum computing and have tight connections with fundamental physics. Although being developed decades ago, a physically concise principle or picture to formalize and understand UQCM is still lacking. This is challenging given the diversity of still-emerging models, but important to understand the difference between classical and quantum computing. In this work, we carried out a primary attempt to unify UQCM by classifying a few of them as two categories, hence making a table of models. With such a table, some known models or schemes appear as hybridization or combination of models, and more importantly, it leads to new schemes that have not been explored yet. Our study of UQCM also leads to some insights into quantum algorithms. This work reveals the importance and feasibility of systematic study of computing models.",2021,12,3,[2124015766.0],2124015766.0,Vaduz,2,"[143769760.0, 2772470.0]",Y,"['I feel that the authors should give a more prominent disclaimer to potential users of the test.', 'A temporal clustering model and a DCNN decoder are applied on the encoded representations and jointly trained.']",,,Quantum Engineering,"['quantum', 'models', 'study', 'quantum computing', 'Quantum Computing']"
Quantum Computing,be082d70534db088315f2cc5b42c2fdcd58c1b8c,Optimality Study of Existing Quantum Computing Layout Synthesis Tools,https://www.semanticscholar.org/paper/be082d70534db088315f2cc5b42c2fdcd58c1b8c,JournalArticle,"Layout synthesis, an important step in quantum computing, processes quantum circuits to satisfy device layout constraints. In this paper, we construct QUEKO benchmarks for this problem, which have known optimal depths and gate counts. We use QUEKO to evaluate the optimality of current layout synthesis tools, including Cirq from Google, Qiskit from IBM, <inline-formula><tex-math notation=""LaTeX"">$\mathsf {t}|\mathsf {ket}\rangle$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant=""sans-serif"">t</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant=""sans-serif"">ket</mml:mi><mml:mo>〉</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""tan-ieq1-3009140.gif""/></alternatives></inline-formula> from Cambridge Quantum Computing, and a recent academic work. To our surprise, despite over a decade of research and development by academia and industry on compilation and synthesis for quantum circuits, we are still able to demonstrate large optimality gaps: 1.5-12x on average on a smaller device and 5-45x on average on a larger device. This suggests substantial room for improvement of the efficiency of quantum computer by better layout synthesis tools. Finally, we also prove the NP-completeness of the layout synthesis problem for quantum computing. We have made the QUEKO benchmarks open-source.",2020,59,70,"[2218979994.0, 2259796.0]",2218979994.0,Dublin,3,"[2275238488.0, 120174856.0, 144365875.0]",Y,"['It would be interesting (in future work) to explore how the partitioning could perhaps be discovered automatically.', 'Does each component is related to a certain topic?', '[1] Semi-Supervised Learning with Generative Adversarial Networks (https://arxiv.org/abs/1606.01583) [2] Good Semi-supervised Learning that Requires a Bad GAN (https://arxiv.org/abs/1705.09783)']",,,IEEE transactions on computers,"['quantum', 'synthesis', 'layout', 'quantum computing', 'Quantum Computing']"
Quantum Computing,48c73c389c3f36d70407eb8309a0b41578c15fc8,QProv: A provenance system for quantum computing,https://www.semanticscholar.org/paper/48c73c389c3f36d70407eb8309a0b41578c15fc8,JournalArticle,"Quantum computing promises breakthroughs in various application areas, such as machine learning, chemistry, or simulations. However, today’s quantum computers are error ‐ prone and have limited capabilities. This leads to various challenges when developing and executing quantum algorithms, for example, the mitigation of occurring errors or the selection of a suitable quantum computer to execute a certain quantum circuit. To address these challenges, detailed information about the quantum circuit to be executed as well as past executions, and the up ‐ to ‐ date information about the available quantum computers are required. Thus, this data must be continuously collected and stored in the long ‐ term, which is currently not supported. To overcome this problem, a provenance approach is introduced for quantum computing. Therefore, relevant provenance attributes that should be gathered in the area of quantum computing are identified. Furthermore, QProv, a provenance system that automatically collects the identified provenance attributes and provides them in a uniform manner to the user is introduced. Finally, a case study with the collected provenance data and corresponding use cases that can benefit from this provenance data are presented here.",2021,10,2,"[1474569575.0, 2890261.0, 1688415.0, 51418452.0, 1565015599.0]",1474569575.0,Chisinau,3,"[134771108.0, 1725976794.0, 2276851.0]",Y,"['L. Martino, J. Vicent, G. Camps-Valls, ""Automatic Emulator and Optimized Look-up Table Generation for Radiative Transfer Models"", IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2017.', 'Simple yet efficient new algorithm for gradient compression with good performance.', 'This is different from the standard bag-of-words model where words are sampled independently and word counts do matter.']",,,IET Quantum Communication,"['quantum', 'provenance', 'computing', 'quantum computing', 'Quantum Computing']"
Graph Database,6c755fc901d0b41a5d73c265f64a5aacf62e83b8,GDsmith: Detecting Bugs in Cypher Graph Database Engines,https://www.semanticscholar.org/paper/6c755fc901d0b41a5d73c265f64a5aacf62e83b8,Conference,"Graph database engines stand out in the era of big data for their efficiency of modeling and processing linked data. To assure high quality of graph database engines, it is highly critical to conduct automatic test generation for graph database engines, e.g., random test generation, the most commonly adopted approach in practice. However, random test generation faces the challenge of generating complex inputs (i.e., property graphs and queries) for producing non-empty query results; generating such type of inputs is important especially for detecting wrong-result bugs. To address this challenge, in this paper, we propose GDsmith, the first approach for testing Cypher graph database engines. GDsmith ensures that each randomly generated query satisfies the semantic requirements. To increase the probability of producing complex queries that return non-empty results, GDsmith includes two new techniques: graph-guided generation of complex pattern combinations and data-guided generation of complex conditions. Our evaluation results demonstrate that GDsmith is effective and efficient for producing complex queries that return non-empty results for bug detection, and substantially outperforms the baselines. GDsmith successfully detects 28 bugs on the released versions of three highly popular open-source graph database engines and receives positive feedback from their developers.",2023,8,,"[2171106811.0, 49661434.0, 31131132.0, 2118207557.0, 144281339.0, 5779643.0, 2057038192.0]",2171106811.0,Budapest,3,"[48594758.0, 1765169.0, 2504776.0]",Y,"['The key idea is to use a critic which randomly drops the activations in the logit and maximizes the sensitivity between two versions of discriminators.', 'The submitted paper shows that this principle is not a necessary condition large-scale classification.', 'Well structured analysis paper on shortcut connections but contributions/results are not compelling This paper performs an analysis of shortcut connections in ResNet-like architectures.']",International Symposium on Software Testing and Analysis,32.0,,"['graph', 'database', 'engines', 'graph database', 'Graph Database']"
Graph Database,a2a514ed839dafdd0fb76d6c2615f25f35bf8087,Testing Graph Database Engines via Query Partitioning,https://www.semanticscholar.org/paper/a2a514ed839dafdd0fb76d6c2615f25f35bf8087,Conference,"Graph Database Management Systems (GDBMSs) store data as graphs and allow the efficient querying of nodes and their relationships. Logic bugs are bugs that cause a GDBMS to return an incorrect result for a given query (e.g., by returning incorrect nodes or relationships). The impact of such bugs can be severe, as they often go unnoticed. The core insight of this paper is that Query Partitioning, a test oracle that has been proposed to test Relational Database Systems, is applicable to testing GDBMSs as well. The core idea of Query Partitioning is that, given a query, multiple queries are derived whose results can be combined to reconstruct the given query’s result. Any discrepancy in the result indicates a logic bug. We have implemented this approach as a practical tool named GDBMeter and evaluated GDBMeter on three popular GDBMSs and found a total of 40 unique, previously unknown bugs. We consider 14 of them to be logic bugs, the others being error or crash bugs. Overall, 27 of the bugs have been fixed, and 35 confirmed. We compared our approach to the state-of-the-art approach to testing GDBMS, which relies on differential testing; we found that it results in a high number of false alarms, while Query Partitioning reported actual logic bugs without any false alarms. Furthermore, despite the previous efforts in testing Neo4j and JanusGraph, we found 18 additional bugs. The developers appreciate our work and plan to integrate GDBMeter into their testing process. We expect that this simple, yet effective approach and the practical tool will be used to test other GDBMSs.",2023,10,,"[1663632797.0, 2868147.0, 1625425838.0, 38319925.0]",1663632797.0,Copenhagen,3,"[3419650.0, 143884284.0, 1783781.0]",Y,"['There are a few notational issues in the paper that should be addressed.', ""The authors' technique may let us do this data-generation easily."", ""Creative and interesting The paper introduces an application of Graph Neural Networks (Li's Gated Graph Neural Nets, GGNNs, specifically) for reasoning about programs and programming.""]",International Symposium on Software Testing and Analysis,32.0,,"['bugs', 'query', 'gdbmss', 'graph database', 'Graph Database']"
Graph Database,2c3eef2f17369912e330281d54b535675077e4ca,Representing Paths in Graph Database Pattern Matching,https://www.semanticscholar.org/paper/2c3eef2f17369912e330281d54b535675077e4ca,JournalArticle,"
 Modern graph database query languages such as GQL, SQL/PGQ, and their academic predecessor G-Core promote paths to first-class citizens in the sense that their pattern matching facility can return
 paths
 , as opposed to only nodes and edges. This is challenging for database engines, since graphs can have a large number of paths between a given node pair, which can cause huge intermediate results in query evaluation.
 
 
 We introduce the concept of
 path multiset representations (PMRs)
 , which can represent multisets of paths exponentially succinctly and therefore bring significant advantages for representing intermediate results. We give a detailed theoretical analysis that shows that they are especially well-suited for representing results of regular path queries and extensions thereof involving counting, random sampling, and unions. Our experiments show that they drastically improve scalability for regular path query evaluation, with speedups of several orders of magnitude.
",2023,7,16,"[144352362.0, 2879007.0, 2164445100.0, 2261302914.0, 1709642.0, 2434366.0]",144352362.0,Belgrade,2,"[2192200.0, 1631386300.0]",Y,"['The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy?', 'This paper is not well-adapted for an ICLR audience, many of which are not experts in compilers or LLVM.']",,,Proceedings of the VLDB Endowment,"['paths', 'query', 'results', 'graph database', 'Graph Database']"
Graph Database,e67a2817089312746d69b38ce9abfdc4b1bc69c3,Finding bugs in Gremlin-based graph database systems via Randomized differential testing,https://www.semanticscholar.org/paper/e67a2817089312746d69b38ce9abfdc4b1bc69c3,Conference,"Graph database systems (GDBs) allow efficiently storing and retrieving graph data, and have become the critical component in many applications, e.g., knowledge graphs, social networks, and fraud detection. It is important to ensure that GDBs operate correctly. Logic bugs can occur and make GDBs return an incorrect result for a given query. These bugs are critical and can easily go unnoticed by developers when the graph and queries become complicated. Despite the importance of GDBs, logic bugs in GDBs have received less attention than those in relational database systems. In this paper, we present Grand, an approach for automatically finding logic bugs in GDBs that adopt Gremlin as their query language. The core idea of Grand is to construct semantically equivalent databases for multiple GDBs, and then compare the results of a Gremlin query on these databases. If the return results of a query on multiple GDBs are different, the likely cause is a logic bug in these GDBs. To effectively test GDBs, we propose a model-based query generation approach to generate valid Gremlin queries that can potentially return non-empty results, and a data mapping approach to unify the format of query results for different GDBs. We evaluate Grand on six widely-used GDBs, e.g., Neo4j and HugeGraph. In total, we have found 21 previously-unknown logic bugs in these GDBs. Among them, developers have confirmed 18 bugs, and fixed 7 bugs.",2022,15,,"[2158585032.0, 2964640.0, 134898163.0, 2093481779.0, 2131285720.0, 2118120527.0, 2152692124.0, 40231586.0, 144525882.0]",2158585032.0,Sarajevo,2,"[103131985.0, 2108097584.0]",Y,"['Deep Temporal Clustering This paper proposes an algorithm for jointly performing dimensionality reduction and temporal clustering in a deep learning context.', 'Doing so will help better understand what is gained from using retaining a probabilistic form of memory versus a determinstic memory indexed with attention as in [Li et. al].']",International Symposium on Software Testing and Analysis,31.0,,"['gdbs', 'bugs', 'query', 'graph database', 'Graph Database']"
Graph Database,d1ae4ab5047489c2b010c7ce72262982ad66ad60,ByteGraph: A High-Performance Distributed Graph Database in ByteDance,https://www.semanticscholar.org/paper/d1ae4ab5047489c2b010c7ce72262982ad66ad60,JournalArticle,"Most products at ByteDance, e.g., TikTok, Douyin, and Toutiao, naturally generate massive amounts of graph data. To efficiently store, query and update massive graph data is challenging for the broad range of products at ByteDance with various performance requirements. We categorize graph workloads at ByteDance into three types: online analytical, transaction, and serving processing, where each workload has its own characteristics. Existing graph databases have different performance bottlenecks in handling these workloads and none can efficiently handle the scale of graphs at ByteDance. We developed ByteGraph to process these graph workloads with high throughput, low latency and high scalability. There are several key designs in ByteGraph that make it efficient for processing our workloads, including edge-trees to store adjacency lists for high parallelism and low memory usage, adaptive optimizations on thread pools and indexes, and geographic replications to achieve fault tolerance and availability. ByteGraph has been in production use for several years and its performance has shown to be robust for processing a wide range of graph workloads at ByteDance.",2022,6,15,"[2145413970.0, 2108844303.0, 2167037428.0, 1490915504.0, 2145763285.0, 2109512262.0, 123816348.0, 2088215217.0, 2193954145.0, 32058742.0, 2116124684.0, 2144671306.0, 2424392.0, 2182246691.0, 2112662828.0, 2184079980.0, 113398129.0, 2695617.0, 2174235132.0, 2112409349.0, 46255707.0, 2157513188.0, 2153604994.0, 2113918898.0, 50841357.0, 2116502347.0]",2145413970.0,Podgorica,2,"[1403239967.0, 144906624.0]",Y,"['“ Would be good to see how this affects results and convergence speed.', 'Overall, this works seems somewhat too preliminary at this stage.']",,,Proceedings of the VLDB Endowment,"['graph', 'workloads', 'bytedance', 'graph database', 'Graph Database']"
Graph Database,1cff064f815111a71a98afda7aee1867ad617901,Digital Contact Tracing Based on a Graph Database Algorithm for Emergency Management During the COVID-19 Epidemic: Case Study,https://www.semanticscholar.org/paper/1cff064f815111a71a98afda7aee1867ad617901,JournalArticle,"Background The COVID-19 epidemic is still spreading globally. Contact tracing is a vital strategy in epidemic emergency management; however, traditional contact tracing faces many limitations in practice. The application of digital technology provides an opportunity for local governments to trace the contacts of individuals with COVID-19 more comprehensively, efficiently, and precisely. Objective Our research aimed to provide new solutions to overcome the limitations of traditional contact tracing by introducing the organizational process, technical process, and main achievements of digital contact tracing in Hainan Province. Methods A graph database algorithm, which can efficiently process complex relational networks, was applied in Hainan Province; this algorithm relies on a governmental big data platform to analyze multisource COVID-19 epidemic data and build networks of relationships among high-risk infected individuals, the general population, vehicles, and public places to identify and trace contacts. We summarized the organizational and technical process of digital contact tracing in Hainan Province based on interviews and data analyses. Results An integrated emergency management command system and a multi-agency coordination mechanism were formed during the emergency management of the COVID-19 epidemic in Hainan Province. The collection, storage, analysis, and application of multisource epidemic data were realized based on the government’s big data platform using a centralized model. The graph database algorithm is compatible with this platform and can analyze multisource and heterogeneous big data related to the epidemic. These practices were used to quickly and accurately identify and trace 10,871 contacts among hundreds of thousands of epidemic data records; 378 closest contacts and a number of public places with high risk of infection were identified. A confirmed patient was found after quarantine measures were implemented by all contacts. Conclusions During the emergency management of the COVID-19 epidemic, Hainan Province used a graph database algorithm to trace contacts in a centralized model, which can identify infected individuals and high-risk public places more quickly and accurately. This practice can provide support to government agencies to implement precise, agile, and evidence-based emergency management measures and improve the responsiveness of the public health emergency response system. Strengthening data security, improving tracing accuracy, enabling intelligent data collection, and improving data-sharing mechanisms and technologies are directions for optimizing digital contact tracing.",2021,15,9,"[2054289955.0, 101108644.0, 2054838160.0, 1492120545.0, 2115905112.0]",2054289955.0,Monaco,3,"[2115218570.0, 52222067.0, 2111572794.0]",Y,"['Firstly, the model has ample free parameters to overfit when such a tiny test set is used.', 'There were a few things that jumped out to me that I was surprised by.', 'I could then identify the influential dimension(s) by taking the largest absolute values in the gradient vector.']",,,JMIR mHealth and uHealth,"['data', 'epidemic', 'contact', 'graph database', 'Graph Database']"
Graph Database,a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854,"MillenniumDB: A Persistent, Open-Source, Graph Database",https://www.semanticscholar.org/paper/a6b7b5dbd1eb6ac765cdb9c9f70b90aa11e45854,JournalArticle,"In this systems paper, we present MillenniumDB: a novel graph database engine that is modular, persistent, and open source. MillenniumDB is based on a graph data model, which we call domain graphs, that provides a simple abstraction upon which a variety of popular graph models can be supported. The engine itself is founded on a combination of tried and tested techniques from relational data management, state-of-the-art algorithms for worst-case-optimal joins, as well as graph-specific algorithms for evaluating path queries. In this paper, we present the main design principles underlying MillenniumDB, describing the abstract graph model and query semantics supported, the concrete data model and query syntax implemented, as well as the storage, indexing, query planning and query evaluation techniques used. We evaluate MillenniumDB over real-world data and queries from the Wikidata knowledge graph, where we find that it outperforms other popular persistent graph database engines (including both enterprise and open source alternatives) that support similar query features.",2021,13,abs/2111.01540,"[2434366.0, 2059572334.0, 2772109.0, 144658846.0, 1754091.0, 34652974.0, 144007515.0, 143678972.0, 1706922.0, 134212079.0]",2434366.0,Reykjavik,3,"[1724648481.0, 47781311.0, 2093474992.0]",Y,"['The authors propose the use of a gamma prior as the distribution over the latent representation space in GANs.', 'In the meantime, they learn to generate reviews.', 'Interestingly, the assisted method starts off much higher in the “reacher” task.']",,,arXiv.org,"['graph', 'query', 'millenniumdb', 'graph database', 'Graph Database']"
Graph Database,27beaa5db6c37c611f082855c6385b264874b8f5,SynLethDB 2.0: a web-based knowledge graph database on synthetic lethality for novel anticancer drug discovery,https://www.semanticscholar.org/paper/27beaa5db6c37c611f082855c6385b264874b8f5,JournalArticle,"Two genes are synthetic lethal if mutations in both genes result in impaired cell viability, while mutation of either gene does not affect the cell survival. The potential usage of synthetic lethality (SL) in anticancer therapeutics has attracted many researchers to identify synthetic lethal gene pairs. To include newly identified SLs and more related knowledge, we present a new version of the SynLethDB database to facilitate the discovery of clinically relevant SLs. We extended the first version of SynLethDB database significantly by including new SLs identified through CRISPR screening, a knowledge graph about human SLs, and new web interface, etc. Over 16,000 new SLs and 26 types of other relationships have been added, encompassing relationships among 14,100 genes, 53 cancers, and 1,898 drugs, etc. Moreover, a brand-new web interface has been developed to include modules such as SL query by disease or compound, SL partner gene set enrichment analysis and knowledge graph browsing through a dynamic graph viewer. The data can be downloaded directly from the website or through the RESTful APIs. The database is accessible online at http://synlethdb.sist.shanghaitech.edu.cn/v2.",2021,17,2022,"[2146041856.0, 150238039.0, 2155815886.0, 2152719037.0, 2108965351.0, 2146671969.0, 2115692258.0]",2146041856.0,Belgrade,3,"[49192881.0, 144502489.0, 2211993531.0]",Y,"['There were a few things that jumped out to me that I was surprised by.', 'Nevertheless, I suspect that the drawback of this method compared to existing ones is computational cost.', 'This is a solid paper about model evaluation in the chemical domain.']",,,bioRxiv,"['sls', 'genes', 'gene', 'graph database', 'Graph Database']"
Graph Database,f4cfc7cbad257f1688772d59f694c16189dba811,Columnar Storage and List-based Processing for Graph Database Management Systems,https://www.semanticscholar.org/paper/f4cfc7cbad257f1688772d59f694c16189dba811,JournalArticle,"We revisit column-oriented storage and query processing techniques in the context of contemporary graph database management systems (GDBMSs). Similar to column-oriented RDBMSs, GDBMSs support read-heavy analytical workloads that however have fundamentally different data access patterns than traditional analytical workloads. We first derive a set of desiderata for optimizing storage and query processors of GDBMS based on their access patterns. We then present the design of columnar storage, compression, and query processing techniques based on these desiderata. In addition to showing direct integration of existing techniques from columnar RDBMSs, we also propose novel ones that are optimized for GDBMSs. These include a novel list-based query processor, which avoids expensive data copies of traditional block-based processors under many-to-many joins, a new data structure we call single-indexed edge property pages and an accompanying edge ID scheme, and a new application of Jacobson's bit vector index for compressing NULL values and empty lists. We integrated our techniques into the GraphflowDB in-memory GDBMS. Through extensive experiments, we demonstrate the scalability and query performance benefits of our techniques.",2021,12,14,"[1505708061.0, 26391899.0, 1783781.0]",1505708061.0,Oslo,2,"[2108025636.0, 51413028.0]",Y,"['It would not be fair to claim superiority over RN since you only evaluate on bABi while RN also demonstrated results on other tasks.', 'The authors do mention this issue, but do not put much effort into evaluating its implications in practice, or parsing Theorems 1 and']",,,Proceedings of the VLDB Endowment,"['query', 'techniques', 'storage', 'graph database', 'Graph Database']"
Graph Database,6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb,UniKG: A Unified Interoperable Knowledge Graph Database System,https://www.semanticscholar.org/paper/6a85b59bb66e9f70c6e201a265d58a7f3aeb3aeb,Conference,"Knowledge graph currently has two main data models: RDF graph and property graph. The query language on RDF graph is SPARQL, while the query language on property graph is mainly Cypher. Different data models and query languages hinder the wider application of knowledge graphs. In this demonstration, we propose a unified interoperable knowledge graph database system, UniKG. (1) Based on the relational model, a unified storage scheme is utilized to efficiently store RDF graphs and property graphs, and support the query requirements of knowledge graphs. (2) Using the characteristicset-based method, the storage problem of untyped entities is addressed in UniKG. (3) UniKG realizes the interoperability of SPARQL and Cypher, and enables them to interchangeably operate on the same knowledge graph. (4) With a unified Web interface, users are allowed to query with two different languages over the same knowledge graph and visualize query results and explanations.",2021,8,,"[2116441064.0, 122024145.0, 152814510.0, 2118153844.0, 2113771309.0, 2332105.0]",2116441064.0,Valletta,3,"[2109971162.0, 48586730.0, 3081813.0]",Y,"['However, how widespread is this problem across other models or are you simply addressing a point problem for RN?', 'It looks like the authors extract position information from flappy bird frames, so the algorithm is only using images for obstacle reasoning?', 'A new method for weight quantization.']",IEEE International Conference on Data Engineering,37.0,,"['graph', 'knowledge', 'query', 'graph database', 'Graph Database']"
Graph Database,6b3756d32ab5b0a5715a5cfc3672290d2d643017,A Graph Database Representation of Portuguese Criminal-Related Documents,https://www.semanticscholar.org/paper/6b3756d32ab5b0a5715a5cfc3672290d2d643017,JournalArticle,"Organizations have been challenged by the need to process an increasing amount of data, both structured and unstructured, retrieved from heterogeneous sources. Criminal investigation police are among these organizations, as they have to manually process a vast number of criminal reports, news articles related to crimes, occurrence and evidence reports, and other unstructured documents. Automatic extraction and representation of data and knowledge in such documents is an essential task to reduce the manual analysis burden and to automate the discovering of names and entities relationships that may exist in a case. This paper presents SEMCrime, a framework used to extract and classify named-entities and relations in Portuguese criminal reports and documents, and represent the data retrieved into a graph database. A 5WH1 (Who, What, Why, Where, When, and How) information extraction method was applied, and a graph database representation was used to store and visualize the relations extracted from the documents. Promising results were obtained with a prototype developed to evaluate the framework, namely a name-entity recognition with an F-Measure of 0.73, and a 5W1H information extraction performance with an F-Measure of 0.65.",2021,7,8,"[92089600.0, 2560362.0, 2103992921.0]",92089600.0,Valletta,3,"[2744698.0, 103131985.0, 5344810.0]",Y,"['Unless I am misunderstanding the experimental setup, this is not supported by the result, correct?', 'The biggest weakness of the paper (and the reason for my final decision) is that the paper completely goes easy on baseline models.', 'It is perhaps interesting that one can make deep learning learn to cooperate with imperfect information, but one could have illustrated the game theory equally well with other techniques.']",,,Informatics,"['documents', 'data', 'extraction', 'graph database', 'Graph Database']"
Graph Database,0a71dd8bec060195e14eb9d0a7abbc08d960d4d5,Research on Data Asset Management System of Graph Database Based on Internet of Things,https://www.semanticscholar.org/paper/0a71dd8bec060195e14eb9d0a7abbc08d960d4d5,Conference,"With the development of the times and the progress of society, the popularization rate of computer network technology and information technology in China is accelerating, and the Internet of things technology also appears in people’s vision, and is gradually known by people. In the context of this era of big data, it has brought great challenges to all walks of life. The development of everything must conform to the development theme of the times. In order to meet the challenge of the research and development of the data asset management system of the graph database in the new era, this paper puts forward the method of applying the Internet of things technology to the research and development of the data asset management system of the graph database. Combining with the foreign research and development plans of the data asset management system of the graph database, the data resources of the graph database are carried out from the platform system, the management structure and the organization arrangement Based on the research and analysis of production management system, a research scheme of data asset management system of graph database which can meet the development requirements of the new era is worked out. Through long-term research and analysis, we can find that the Internet of things technology analysis method proposed in this paper can effectively provide new development ideas for the research and development of data asset management system based on graph database under the Internet of things technology.",2021,4,,"[46694091.0, 2047926221.0, 2064360636.0, 2154976675.0, 48586730.0]",46694091.0,Nicosia,3,"[2065277797.0, 144782078.0, 2115646383.0]",Y,"['However, batch normalization only sees the variation in the activations given to it by a SPECIFIC set of weights.', 'Many relevant papers could be referenced in the introduction as examples of successes in computer vision tasks,  identity mapping initialization, recent interpretations of ResNets/DensetNets, etc.', 'This paper proposes a different approach (with could be combined with these methods) based on a new training procedure.']",Journal of Physics: Conference Series,21.0,,"['development', 'data', 'research', 'graph database', 'Graph Database']"
Graph Database,5ea7bf772fecf95cbf53b2c7f719c9440322a115,GRANEF: Utilization of a Graph Database for Network Forensics,https://www.semanticscholar.org/paper/5ea7bf772fecf95cbf53b2c7f719c9440322a115,Conference,"Understanding the information in captured network traffic, extracting the necessary data, and performing incident investigations are principal tasks of network forensics. The analysis of such data is typically performed by tools allowing manual browsing, filtering, and aggregation or tools based on statistical analyses and visualizations facilitating data comprehension. However, the human brain is used to perceiving the data in associations, which these tools can provide only in a limited form. We introduce a GRANEF toolkit that demonstrates a new approach to exploratory network data analysis based on associations stored in a graph database. In this article, we describe data transformation principles, utilization of a scalable graph database, and data analysis techniques. We then discuss and evaluate our proposed approach using a realistic dataset. Although we are at the beginning of our research, the current results show the great potential of association-based analysis.",2021,4,,"[2697611.0, 2082303711.0]",2697611.0,Zagreb,3,"[49399380.0, 2144447082.0, 144782078.0]",Y,"['These five objectives together are sufficient to achieve impressive English <-> German and Engish <-> French results in Multi30k, a bilingual image caption scenario with short simple sentences, and to achieve a strong start for a standard WMT scenario.', 'Actually, the proof never makes any connection to optimization.', 'This paper introduces a method for regularizing the REINFORCE algorithm by keeping around a small set of known high-quality samples as part of the sample set when performing stochastic gradient estimation.']",International Conference on Security and Cryptography,21.0,,"['data', 'analysis', 'network', 'graph database', 'Graph Database']"
Graph Database,09f54c64b39f5f7e7570f9f4ce3e3af544401e14,A Quantitative Analysis of Student Solutions to Graph Database Problems,https://www.semanticscholar.org/paper/09f54c64b39f5f7e7570f9f4ce3e3af544401e14,Conference,"As data grow both in size and in connectivity, the interest to use graph databases in the industry has been proliferating. However, there has been little research on graph database education. In response to the need to introduce college students to graph databases, this paper is the first to analyze students' errors in homework submissions of queries written in Cypher, the query language for Neo4j---the most prominent graph database. Based on 40,093 student submissions from homework assignments in an upper-level computer science database course at one university, this paper provides a quantitative analysis of students' learning when solving graph database problems. The data shows that students struggle the most to correctly use Cypher's WITH clause to define variable names before referencing in the WHERE clause and these errors persist over multiple homework problems requiring the same techniques, and we suggest a further improvement on the classification of syntactic errors.",2021,4,,"[153314895.0, 66327914.0, 2051972259.0, 2517099.0]",153314895.0,Copenhagen,2,"[1779967.0, 2109139810.0]",Y,"['Actually, the proof never makes any connection to optimization.', 'The use of the proposed gamma distribution, as a simple alternative, overcomes this problem.']",Annual Conference on Innovation and Technology in Computer Science Education,26.0,,"['graph', 'database', 'students', 'graph database', 'Graph Database']"
Graph Database,db6084fdb3baceddacdc726474722debe1ef7e65,TigerGraph: A Native MPP Graph Database,https://www.semanticscholar.org/paper/db6084fdb3baceddacdc726474722debe1ef7e65,JournalArticle,"We present TigerGraph, a graph database system built from the ground up to support massively parallel computation of queries and analytics. 
TigerGraph's high-level query language, GSQL, is designed for compatibility with SQL, while simultaneously allowing NoSQL programmers to continue thinking in Bulk-Synchronous Processing (BSP) terms and reap the benefits of high-level specification. 
GSQL is sufficiently high-level to allow declarative SQL-style programming, yet sufficiently expressive to concisely specify the sophisticated iterative algorithms required by modern graph analytics and traditionally coded in general-purpose programming languages like C++ and Java. 
We report very strong scale-up and scale-out performance over a benchmark we published on GitHub for full reproducibility.",2019,53,abs/1901.08248,"[50136367.0, 47103320.0, 1738190.0, 2058056841.0]",50136367.0,Berlin,2,"[1384221174.0, 1783281.0]",Y,"['What is new compared to [Finn et al. 2016] is that the authors managed to train the network in combination with an adversarial loss, which allows for the generation of more realistic images.', 'As the authors note, this kind of character level modelling has been used in many previous works.']",,,arXiv.org,"['highlevel', 'graph', 'gsql', 'graph database', 'Graph Database']"
Graph Database,b0b770fb8c7760749c88e3c83ae173cdb07f7bd5,An Attack Path Generation Methods Based on Graph Database,https://www.semanticscholar.org/paper/b0b770fb8c7760749c88e3c83ae173cdb07f7bd5,Conference,"With the popularity of network technology and the expansion of network scale, the network security risks are increasingly serious. Network vulnerability assessment methods, a technology of active network security defense, have attracted many researchers. Most existing network vulnerability assessment methods store different types of data in different ways, which makes querying and analyzing inefficient, especially in the complex large-scale network environment. In order to solve this problem, this paper proposes a method of network vulnerability assessment based on graph database. The network host information, association relationship between hosts and vulnerability information of the target network are stored in the graph database, the query and analysis are carried out by using the graph database query language. Graph database stores the information of the network hosts, association relationship among hosts and vulnerabilities of the target network. The graph database query language supports querying and analysis. Visualizing the network topology, vulnerability information and all possible attack paths provides a reference to develop the network security protection strategy. Experiments' results illustrate that the method runs efficiently and helps with querying and analysis, which is applicable to large-scale complex network environment.",2020,13,,"[79470079.0, 80752053.0, 1596817678.0, 1672530059.0]",79470079.0,Budapest,3,"[3474704.0, 2108706355.0, 2157681212.0]",Y,"['* There are many estimators for f-divergences (like the ones cited above and many others based e.g. on nearest-neighbors) that are sample-based and thus correspond to the ""implicit"" case that the authors discuss.', 'For completeness, it would be great to include one more algorithm in the evaluation: an ablation of DnC which does not involve a central policy at all.', 'The evaluation with the sequence of checkpoints was created by using every fifth image.']","2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)",4.0,,"['network', 'vulnerability', 'graph', 'graph database', 'Graph Database']"
Graph Database,91d6e8ba5dd90b02fe3bd870b19da13a6167af53,The Property Graph Database Model,https://www.semanticscholar.org/paper/91d6e8ba5dd90b02fe3bd870b19da13a6167af53,Workshop,"Most of the current graph database systems have been designed to support property graphs. Surprisingly, there is no standard specification of the database model behind such systems. This paper presents a formal definition of the property graph database model. Specifically, we define the property graph data structure, basic notions of integrity constraints (e.g. graph schema), and a graph query language.",2018,127,,[2772109.0],2772109.0,Athens,3,"[2115725948.0, 22758695.0, 47490276.0]",Y,"['The comparison with general approaches such as seq2seq and stack LSTM might not be that fair as they are not restricted to only those operators and this possibly also explains the low generalization accuracies.', 'Test accuracy is not improved, however.', 'See e.g. the discussion in ""Variational Lossy Autoencoder"" (Chen et al. 2016) - Experiments are not complete (e.g. for AR, as noted in the paper).']",Alberto Mendelzon Workshop on Foundations of Data Management,18.0,,"['graph', 'database', 'property', 'graph database', 'Graph Database']"
Graph Database,b613887337a5d2e8fc8773037116be81e6346835,A1: A Distributed In-Memory Graph Database,https://www.semanticscholar.org/paper/b613887337a5d2e8fc8773037116be81e6346835,Conference,"A1 is an in-memory distributed database used by the Bing search engine to support complex queries over structured data. The key enablers for A1 are availability of cheap DRAM and high speed RDMA (Remote Direct Memory Access) networking in commodity hardware. A1 uses FaRM [11,12] as its underlying storage layer and builds the graph abstraction and query engine on top. The combination of in-memory storage and RDMA access requires rethinking how data is allocated, organized and queried in a large distributed system. A single A1 cluster can store tens of billions of vertices and edges and support a throughput of 350+ million of vertex reads per second with end to end query latency in single digit milliseconds. In this paper we describe the A1 data model, RDMA optimized data structures and query execution.",2020,22,,"[1790681.0, 1904916.0, 2089990776.0, 73527512.0, 2070951368.0, 1630331317.0, 1630330861.0, 40444389.0, 1630293705.0, 1630330434.0, 2238313.0, 40443723.0, 2069452048.0, 2111073292.0]",1790681.0,Bern,2,"[2108467971.0, 3382568.0]",Y,"['Moreover, the discrimator D  (which is trained to discriminate between real or fake examples) seems to be directly used to tell if an example is throw from the targeted distribution.', 'Motivation In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle.']",SIGMOD Conference,20.0,,"['a1', 'data', 'rdma', 'graph database', 'Graph Database']"
Graph Database,6fb020754f6de564c3a0a07bb656c0a90be1f87d,Incorruptible Auditing: Blockchain-Powered Graph Database Management,https://www.semanticscholar.org/paper/6fb020754f6de564c3a0a07bb656c0a90be1f87d,Conference,"In modern and interconnected world, information is accumulatively stored digitally, making the process of exchanging, gathering and querying the information much easier. Continuously, it has introduced new challenges about how to ensure its consistency and reliability due to the sheer volume of data. A blockchain-based information system can provide an incorruptible record of history, enabling better auditing and data management practices. The paper describes how to combine an Exonum blockchain and a Neo4j graph database into a system that can provide a verifiable audit trail of data integrity and its modifications for information stored in a graph database.",2020,10,,"[2069559207.0, 1903751380.0, 40915325.0, 1904203865.0, 1904199594.0, 1500655986.0]",2069559207.0,Bern,2,"[2054451943.0, 1772311.0]",Y,"[""MODEL & ARCHITECTURE The PATH function given a current state s and a goal state s', returns a distribution over the best first action to take to get to the goal P(A)."", 'It might be good to emphasize that you don’t train on the IWAE bound in any experiments.']",International Conference on Blockchain,20.0,,"['information', 'data', 'system', 'graph database', 'Graph Database']"
Graph Database,33e332837e91c1048c3ed165cd16bf7607c3bf06,Issues and Concepts of Graph Database and a Comparative Analysis on list of Graph Database tools,https://www.semanticscholar.org/paper/33e332837e91c1048c3ed165cd16bf7607c3bf06,Conference,"The work is review in nature and focuses on basic concepts and example on Graph Database with a special focus on list of standard computerized tools available for handling the queries using graph database structure. The implementation benefits of each tool and a comparative analysis on various functionalities has been presented in this work. This work also elaborates on popular Graph Databases tool that includes Allegro Graph, ArangoDB, OrientDB, Infinite Graph ,Neo4j, Titan, FlockDB, Bitsy, StarDog, MongoDB and investigate their acceptance for solving day to day problems.",2020,9,,"[152781515.0, 46501582.0, 105003008.0, 152125305.0]",152781515.0,Bratislava,2,"[41020222.0, 145929920.0]",Y,"[""1. I'm concerned that the contribution of this manuscript is a little incremental."", 'Why choose F = 10 and K = 3?']",International Conference on Computational Collective Intelligence,20.0,,"['graph', 'work', 'database', 'graph database', 'Graph Database']"
Graph Database,72afe82af4c2ca100c36eb35292e85d806527f0a,Construction of typhoon disaster knowledge graph based on graph database Neo4j,https://www.semanticscholar.org/paper/72afe82af4c2ca100c36eb35292e85d806527f0a,Conference,"The typhoon knowledge graph can correlate various kinds of information in the typhoon data, conduct overall and related analysis, and finally provide effective assistance for typhoon prevention and post-disaster protection. The data of typhoon landing in China from 2000 to 2015 were selected to build a typhoon knowledge graph based on Neo4j graph database platform. The typhoon knowledge graph can be used to understand the occurrence of historical typhoons and obtain the distribution of typhoon data in time and space.",2020,8,,"[2158490269.0, 2118798587.0, 2152209915.0, 1877327478.0, 145104321.0, 145843448.0, 2157681212.0, 2157843743.0, 47149500.0, 2108691840.0]",2158490269.0,Amsterdam,3,"[145560079.0, 35210462.0, 1904916.0]",Y,"['The most important idea of the papier (PDE-net) is to learn both differential operators and the function that governs the PDE.', 'Considering that this is a big, complicated system, it is crucial that the authors included both an ablation experiment to see which pieces were most important, and an experiment that indicates the amount of labeled data that would be required to achieve the same results with a supervised system.', 'It might be worthwhile to briefly describe the encoding/construction algorithm used in the paper.']",Chinese Control and Decision Conference,20.0,,"['typhoon', 'graph', 'knowledge', 'graph database', 'Graph Database']"
Graph Database,cde39ce861e4c7514ee07fd91b6b8aac50cbf01b,RedisGraph GraphBLAS Enabled Graph Database,https://www.semanticscholar.org/paper/cde39ce861e4c7514ee07fd91b6b8aac50cbf01b,Workshop,"RedisGraph is a Redis module developed by Redis Labs to add graph database functionality to the Redis database. RedisGraph represents connected data as adjacency matrices. By representing the data as sparse matrices and employing the power of GraphBLAS (a highly optimized library for sparse matrix operations), RedisGraph delivers a fast and efficient way to store, manage and process graphs. Initial benchmarks indicate that RedisGraph is significantly faster than comparable graph databases.",2019,22,,"[115694632.0, 2069668457.0, 74882299.0, 3257323.0, 113272803.0, 114367405.0, 2310211.0]",115694632.0,Dublin,3,"[3482535.0, 2113663584.0, 2706258.0]",Y,"['The models that are reviewed in the appendix, i.e. the continuous and Ising perceptron and the committee machine are more relevant.', '3. The authors provided various experiments to showcase their approach Cons:', 'Comments: The paper is badly structured and is sometimes hard to read because it does not present in a linear way the classic ingredients of Machine Learning, expression of the full function to be estimated, equations of each layer, description of the set of parameters to be learned and the loss function.']","IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum",19.0,,"['redisgraph', 'redis', 'graph', 'graph database', 'Graph Database']"
Graph Database,ce54e3b89a2570035b70885e6901ad4c92ae41c9,Construction of power projects knowledge graph based on graph database Neo4j,https://www.semanticscholar.org/paper/ce54e3b89a2570035b70885e6901ad4c92ae41c9,Conference,"In ""The Belt and Road"", China’s overseas power projects grow more and more, which is associated with a large number of dispersing project information. Construction of power project knowledge graph based on graph database Neo4j can facilitate the management of overseas power projects and to have an intuitive understanding of the relationships between projects for further overall planning. Meanwhile, enterprises can describe the spatial distribution characteristics of countries along the ""The Belt and Road"" according to the knowledge graph of overseas power projects, so as to understand the space and potential of future power investment development in different countries.",2020,6,,"[2129795635.0, 13324446.0, 2000860679.0, 2108097584.0, 2000992985.0, 2162074006.0]",2129795635.0,Berlin,2,"[2746913.0, 144310754.0]",Y,"['During research, we have multiple executable oracles and need to produce good training data from them.', 'Concerning the lack of mathematical rigour in the statistical physics literature on which the authors comment, they might want to relate to a very recent work https://arxiv.org/pdf/1708.03395.pdf work that sets all the past statistical physics results on optimal generalization in single layer neural networks on fully rigorous basis by proving that the corresponding formulas stemming from the replica method are indeed correct.']","International Conference on Computer, Information and Telecommunication Systems",20.0,,"['power', 'projects', 'graph', 'graph database', 'Graph Database']"
Graph Database,b09139c153bac8893e8faea2b3a59159234caadc,A Graph Database Approach to Wireless IIoT Workcell Performance Evaluation,https://www.semanticscholar.org/paper/b09139c153bac8893e8faea2b3a59159234caadc,Conference,"The workcell is considered a main building block of various industrial settings. Hence, it is examined as a primary testing environment for studying wireless communication techniques in factory automation processes. A new testbed was recently designed and developed to facilitate such studies in workcells by replicating various data flows in an emulated production environment. In this paper, an approach to storing and analyzing network performance data from a manufacturing factory workcell is introduced. A robotic testbed was constructed using two collaborative grade robot arms, machine emulators, and wireless communication devices. A graph database approach was implemented to capture network and operational event data among the components within the testbed. A schema is proposed, developed, and elaborated; a database is then populated with events from the testbed, and the resulting graph is presented. Query commands are then presented as a means to examine and analyze network performance and relationships within the components of the network. Additionally, we demonstrate how to extract correlations between receive signal power and network delay within the testbed using the graph database query language. Finally, using the inherently interconnected nature of the graph database, we discuss applying the graph database approach toward examining more complex relationships between the wireless communications network and the operational system.",2020,5,,"[3420212.0, 2049649.0, 1979284.0, 151500725.0, 1784025.0]",3420212.0,Athens,3,"[143924672.0, 2677700.0, 2146245.0]",Y,"['A number of heuristics are used to augment this reward function so as to provide shaping rewards along the way and speed up learning.', 'This would contradict some previously established convergence results for this type of problems: Reddi et al. (2016) Stochastic Variance Reduction for Nonconvex Optimization, ICML and Wang et al. 2013.', 'After introducing the idea and related work, the authors in Section 3 give details about how to perform the quantization.']",International Conference on Industrial Technology,20.0,,"['network', 'graph', 'database', 'graph database', 'Graph Database']"
Graph Database,a281d563261c738f13b9e58a525e7e265a619c93,Suitability of Graph Database Technology for the Analysis of Spatio-Temporal Data,https://www.semanticscholar.org/paper/a281d563261c738f13b9e58a525e7e265a619c93,JournalArticle,"Every day large quantities of spatio-temporal data are captured, whether by Web-based companies for social data mining or by other industries for a variety of applications ranging from disaster relief to marine data analysis. Making sense of all this data dramatically increases the need for intelligent backend systems to provide realtime query response times while scaling well (in terms of storage and performance) with increasing quantities of structured or semi-structured, multi-dimensional data. Currently, relational database solutions with spatial extensions such as PostGIS, seem to come to their limits. However, the use of graph database technology has been rising in popularity and has been found to handle graph-like spatio-temporal data much more effectively. Motivated by the need to effectively store multi-dimensional, interconnected data, this paper investigates whether or not graph database technology is better suited when compared to the extended relational approach. Three database technologies will be investigated using real world datasets namely: PostgreSQL, JanusGraph, and TigerGraph. The datasets used are the Yelp challenge dataset and an ambulance response simulation dataset, thus combining real world spatial data with realistic simulations offering more control over the dataset. Our extensive evaluation is based on how each database performs under practical data analysis scenarios similar to those found on enterprise level.",2020,5,12,"[1727622943.0, 2403851.0, 1720266.0]",1727622943.0,Copenhagen,3,"[2056158839.0, 144637915.0, 153311051.0]",Y,"['The idea is a simple but useful extension of these previous works.', '[ds]SSM is integrated with I2A, which generates rollouts of future states, based on the inferred hidden states from the d/sSSM-VAE model.', 'The approach is presented very thoroughly.']",,,Future Internet,"['data', 'database', 'quantities', 'graph database', 'Graph Database']"
Graph Database,957f5b1e7ca48891c2e279aefbfa0f04d989c21e,In-Depth Benchmarking of Graph Database Systems with the Linked Data Benchmark Council (LDBC) Social Network Benchmark (SNB),https://www.semanticscholar.org/paper/957f5b1e7ca48891c2e279aefbfa0f04d989c21e,JournalArticle,"In this study, we present the first results of a complete implementation of the LDBC SNB benchmark -- interactive short, interactive complex, and business intelligence -- in two native graph database systems---Neo4j and TigerGraph. In addition to thoroughly evaluating the performance of all of the 46 queries in the benchmark on four scale factors -- SF-1, SF-10, SF-100, and SF-1000 -- and three computing architectures -- on premise and in the cloud -- we also measure the bulk loading time and storage size. Our results show that TigerGraph is consistently outperforming Neo4j on the majority of the queries---by two or more orders of magnitude (100X factor) on certain interactive complex and business intelligence queries. The gap increases with the size of the data since only TigerGraph is able to scale to SF-1000---Neo4j finishes only 12 of the 25 business intelligence queries in reasonable time. Nonetheless, Neo4j is generally faster at bulk loading graph data up to SF-100. A key to our study is the active involvement of the vendors in the tuning of their platforms. In order to encourage reproducibility, we make all the code, scripts, and configuration parameters publicly available online.",2019,13,abs/1907.07405,[2289824.0],2289824.0,Vienna,3,"[46458150.0, 152599430.0, 2309967.0]",Y,"['1. The framework uses the class information, i.e., “only data samples from the normal class are used for training”, but it is still considered unsupervised.', 'As such the paper would be a nice contribution to ICLR.', 'It might be worthwhile to briefly describe the encoding/construction algorithm used in the paper.']",,,arXiv.org,"['business', 'intelligence', 'study', 'graph database', 'Graph Database']"
Graph Database,8ef0c1c2030aa265a4e7c836d080c2e2088efde6,(Graph Database: A Survey),https://www.semanticscholar.org/paper/8ef0c1c2030aa265a4e7c836d080c2e2088efde6,Conference,"The advantages of Relational Database Management System (RDBMS) model and design methodology are being utilized by industry/institutions for any software design and implementation. The future of RDBMS certainly will be the Graph Databases with NoSql methodologies, which is emerging as beyond of relational model. In this paper, there is a need to highlight all the databases evolved after RDBMS. They couldn’t stay in market for so long period and survey has been made to highlight those databases after RDBMS. Relational Database Management System has certain advantages like (i) Storing in Tables, Column and Rows (ii) Data Storing in Normal Form (iii) Easy to use via SQL to retrieve information via complex join operators (iv) Maintainability via Reverse Engineering (v) Indexing and quick search. Due to these inherent features of RDBMS and SQL, it is necessary to explore and compare RDBMS with NoSQL methods to avoid complex join operation. Recently, numerous software industries and research institutions are trying their old RDBMS system to be re-engineered into some other architecture via nodes, edges and relationships where different type of information can be stored easily. So, it is a big challenges for any industry and institutions how quickly they can re-engineer their old RDBMS into Graph Databases which is also called now-a-days the future of databases. In this project, it is highlighted that the importance of the re-engineering work lies in three different directions such as (i) Comparison of RDBMS with GDBMS (Graph Database Management System) where face book, twitter, Amazon, Google are adopting (ii) Survey work of Graph Databases and (iii) Graph Database Models have increasingly become a topic of interest. The representation of data in the form of a graph lends itself well to structure a data with a schema. No standard system of query languages yet had been found to have been unique and stable for graph databases. Research and industry adoptions will determine the future direction of graph databases.(iv) Beyond RDBMS artifacts were established by industry and academics. It feeds a series of recycling collectives trying to eke out an existence of positive incentives and principles.",2020,7,,"[2052840328.0, 1996173264.0]",2052840328.0,Chisinau,3,"[32559865.0, 2163313042.0, 9319875.0]",Y,"['The presentation of the paper could be significantly improved.', 'The authors show mixup provides improvement over baselines in the following settings: * Image Classification on Imagenet.', 'Minors: There are some mixed-up notations: tilde{A_i} => A_i , and rank(A_2) => rank(A)_2 in Proposition 3.']","2020 International Conference on Computer, Electrical & Communication Engineering (ICCECE)",20.0,,"['graph', 'rdbms', 'system', 'graph database', 'Graph Database']"
Graph Database,537f5e8e4139392cd2d108f32495e5b2b80151ac,Implementation of a HL7-CQL Engine Using the Graph Database Neo4J,https://www.semanticscholar.org/paper/537f5e8e4139392cd2d108f32495e5b2b80151ac,JournalArticle,"The Clinical Quality Language (CQL) is a useful tool for defining search requests for data stores containing FHIR data. Unfortunately, there are only few execution engines that are able to evaluate CQL queries. As FHIR data represents a graph structure, the authors pursue the approach of storing all data contained in a FHIR server in the graph database Neo4J and to translate CQL queries into Neo4J's query language Cypher. The query results returned by the graph database are retranslated into their FHIR representation and returned to the querying user. The approach has been positively tested on publicly available FHIR servers with a handcrafted set of example CQL queries.",2019,5,267,"[1785288.0, 98034859.0, 40992258.0, 38827385.0, 145222303.0, 1755795.0, 1707592.0]",1785288.0,Ljubljana,3,"[152290618.0, 1733143.0, 91444480.0]",Y,"['Finally, they use the challenging Youtube data to train the model and show promising results.', 'Also, can you experimentally show that attacks on latent variables are more powerful than attacks on inputs?', 'This section could be improved by demonstrating the approach on more datasets.']",,,"Jahrestagung der Deutschen Gesellschaft für Medizinische Informatik, Biometrie und Epidemiologie","['data', 'cql', 'graph', 'graph database', 'Graph Database']"
Graph Database,099043827df60225cf33c820052716cce64d49e9,A Review on Graph Database and its representation,https://www.semanticscholar.org/paper/099043827df60225cf33c820052716cce64d49e9,Conference,"Extensively, facts are represented characteristically as a table for the purpose of making it indexed with increased readability. Currently, the tendencies are altering as Graph databases are rapidly attaining popularity. Actually, it is appropiately termed as ""the outlook of DBMS"". The demonstration of facts within the procedure of a graph advances within the circumstances sound for the prearranged facts through a dynamic schema. This paper discusses the backbone of graph database as to why they are gaining much popularity in present situations illustrating the dissimilar types available and their distinction. Owing to the extensive usage of graph algorithms with models, neither a standardised system nor query language has been dispossessed with graph databases. Research and industry acceptance will regulate the upcoming course of graph databases. The authors have tried in representing the graph database with a real life scenario.",2019,7,,"[152125305.0, 46501582.0, 2744320.0]",152125305.0,Rome,2,"[143666627.0, 143834867.0]",Y,"['What are the class you are interested in?', 'For example, does this observation that the local curvature of the loss around minima is different for ResNets and VGG allows to interpret the difference in their performances ?']",2019 International Conference on Recent Advances in Energy-efficient Computing and Communication (ICRAECC),19.0,,"['graph', 'facts', 'popularity', 'graph database', 'Graph Database']"
Graph Database,45674df7143e43bc589cfabd26dd194c2a7f090d,Computational Modelling for Bankruptcy Prediction: Semantic Data Analysis Integrating Graph Database and Financial Ontology,https://www.semanticscholar.org/paper/45674df7143e43bc589cfabd26dd194c2a7f090d,Conference,"In this paper, we propose a novel intelligent methodology to construct a Bankruptcy Prediction Computation Model, which is aimed to execute a company's financial status analysis accurately. Based on the semantic data analysis and management, our methodology considers Semantic Database System as the core of the system. It comprises three layers: an Ontology of Bankruptcy Prediction, Semantic Search Engine, and a Semantic Analysis Graph Database system. The Ontological layer defines the basic concepts of the financial risk management as well as the objects that serve as sources of knowledge for predicting a company's bankruptcy. The Graph Database layer utilises a powerful semantic data technology, which serves as a semantic data repository for our model. The article provides a detailed description of the construction of the Ontology and its informal conceptual representation. We also present a working prototype of the Graph Database system, constructed using the Neo4j application, and show the connection between well-known financial ratios. We argue that this methodology which utilises state of the art semantic data management mechanisms enables data processing and relevant computations in a more efficient way than approaches using the traditional relational database. These give us solid grounds to build a system that is capable of tackling the data of any complexity level.",2019,8,,"[1394550182.0, 144031464.0]",1394550182.0,Berlin,2,"[2119407396.0, 1380082069.0]",Y,"['1. The authors tested out this new activation function on RNNs.', 'ATARI 2600 games: I am not sure what state restoration is.']",Conference on Business Informatics,21.0,,"['data', 'database', 'system', 'graph database', 'Graph Database']"
Graph Database,a0367346bc355c36badec2d2c47ce55a320cd75e,The study on data migration from relational database to graph database,https://www.semanticscholar.org/paper/a0367346bc355c36badec2d2c47ce55a320cd75e,Conference,"Under the background of big data, using relational databases to manage massive data may have some problems just like storage capacity and query efficiency. So, there is a new kind of databases called NoSQL to store data. However, the data models of NoSQL databases are different from relation databases. In order to finish migrating historical data from relational databases to NoSQL databases, in terms of graph database in the NoSQL databases, this paper takes ER diagram as the original model, graph model as target, and makes some transformational rules by using the relationships of entities. And this paper proposes an algorithm which can finish data migration by traversing ER diagram and using the transformational rules. This method can reduce the impact of model differences between relational databases and graph databases, ensure the integrity constraint of data, and automatically complete data migration. The experimental results show the validity and correctness of the data migration.",2019,7,,"[71778404.0, 2114140713.0]",71778404.0,Rome,2,"[48727916.0, 2109512262.0]",Y,"['For instance, since two runs of node2vec will give you highly varying embeddings (depending on the initialization), you will have to run node2vec several times to reduce the variance of your resulting discretized density maps.', 'Moreover, the discrimator D  (which is trained to discriminate between real or fake examples) seems to be directly used to tell if an example is throw from the targeted distribution.']",Journal of Physics: Conference Series,19.0,,"['data', 'databases', 'nosql', 'graph database', 'Graph Database']"
Graph Database,223187cf10a24b62b9b0cf5b146cc83526df2ea5,From DIKW pyramid to graph database: a tool for machine processing of nutritional epidemiologic research data,https://www.semanticscholar.org/paper/223187cf10a24b62b9b0cf5b146cc83526df2ea5,Conference,"There is an increased interest in the application of information technology to advance nutritional research. In nutrition science, a graph database enables the creation of multilateral logic relationships throughout the database, which can be used to electronically store, visualize, and scale the outputs of nutritional research. It provides a knowledge structure to standardize nutritional research outputs, which is both human- and machine-readable in a Resource Description Framework format. However, the development of various specific graph databases may cause difficulties for data integration and decrease human-readability. In this article, we propose an approach to develop a graph database according to the Data, Information, Knowledge, and Wisdom or “DIKW” pyramid for nutritional epidemiologic data. Then, authoritative ontologies are suggested to construct the nodes and edges of the graph database to facilitate data integration. Finally, the findability and re-usability of the knowledge in the graph database are showcased using the SPARQL and SQWRL query languages.",2019,7,,"[2604647.0, 1737629.0, 2016236.0]",2604647.0,Podgorica,3,"[3144356.0, 48741177.0, 2093582149.0]",Y,"['This is demonstrated in comparison to weight normalization in Figure 4.', 'Interestingly, DQN + heuristic reward approaches expert performance while behavioral cloning never achieves expert performance level even though it has actions.', 'The last part contains a discussion concerning the extent to which it is actually a desired or a undesired result in classical deep learning use-cases, and the authors provide intuitive conditions under which the convergence would not hold.']",2019 IEEE International Conference on Big Data (Big Data),19.0,,"['graph', 'database', 'data', 'graph database', 'Graph Database']"
Graph Database,c2528e88d5554e9df9f9d482ad46cb5331c4d794,Benchmarking Graph Database Backends - What Works Well with Wikidata?,https://www.semanticscholar.org/paper/c2528e88d5554e9df9f9d482ad46cb5331c4d794,JournalArticle,"Knowledge bases often utilize graphs as logical model. RDF-based knowledge bases (KB) are prime examples, as RDF (Resource Description Framework) does use graph as logical model. Graph databases are an emerging breed of NoSQL-type databases, offering graph as the logical model. Although there are specialized databases, the so-called triple stores, for storing RDF data, graph databases can also be promising candidates for storing knowledge. In this paper, we benchmark different graph database implementations loaded with Wikidata, a real-life, large-scale knowledge base. Graph databases come in all shapes and sizes, offer different APIs and graph models. Hence we used a measurement system, that can abstract away the API differences. For the modeling aspect, we made measurements with different graph encodings previously suggested in the literature, in order to observe the impact of the encoding aspect on the overall performance. 
 ",2019,8,24,"[2058456649.0, 2057440722.0, 144521155.0]",2058456649.0,Tallinn,2,"[3422872.0, 3356500.0]",Y,"['Preference elicitation and inverse reinforcement learning.', 'Exactly which gradients are you skipping at random?']",,,Acta Cybernetica,"['graph', 'knowledge', 'model', 'graph database', 'Graph Database']"
Graph Database,10b4b926904ad153f791ec680218e1610747a0c8,SQL Database with physical database tuning technique and NoSQL graph database comparisons,https://www.semanticscholar.org/paper/10b4b926904ad153f791ec680218e1610747a0c8,Conference,"Relational databases are used in many organizations of various natures from last three decades such as Education, health, businesses and in many other applications. SQL databases are designed to manage structured data and show tremendous performance. Atomicity, Consistency Isolation, Durability (ACID) property of Relational databases is used to manage data integrity and consistency. Physical database techniques are used to increase the performance of relational databases. Tablespaces also called subfolder is one of the physical database technique used by Oracle SQL database. Tablespaces are used to store the data logically in separate data files. Now-a-days huge amount and varied nature (unstructured and semi structured) of data is generated by the various organizations i.e., videos, images, blogs etc. This large amount of data is not handled by the SQL databases efficiently. NoSQL databases are used to process and analyze the large amount of data efficiently. Four different types of NoSQL databases are used in the industry according to the organization requirement. In this article, first, we do the physical database tuning of the Oracle Relational database and then compared with NoSQL Graph database. Relational database performance is increased up to 50% due to physical database tuning technique (Tablespaces). Besides, physical database tuning approach of relational database NoSQL graph database performed better in all our proposed scenarios.",2019,21,,"[51488437.0, 2056158839.0, 2151264132.0, 2061173271.0]",51488437.0,Luxembourg,3,"[2079275650.0, 101370046.0, 150270469.0]",Y,"['Why choose F = 10 and K = 3?', 'Other comments: - your notation is quite sloppy and may have lead to errors.', 'Reproducibility in continuous control is particularly problematic.']","2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)",3.0,,"['database', 'data', 'databases', 'graph database', 'Graph Database']"
Graph Database,f295157f37cfb43cd8d8d2690ea124edc5ea59c2,Beyond Macrobenchmarks: Microbenchmark-based Graph Database Evaluation,https://www.semanticscholar.org/paper/f295157f37cfb43cd8d8d2690ea124edc5ea59c2,JournalArticle,"
 Despite the increasing interest in graph databases their requirements and specifications are not yet fully understood by everyone, leading to a great deal of variation in the supported functionalities and the achieved performances. In this work, we provide a comprehensive study of the existing graph database systems. We introduce a novel microbenchmarking framework that provides insights on their performance that go beyond what macro-benchmarks can offer. The framework includes the largest set of queries and operators so far considered. The graph database systems are evaluated on synthetic and real data, from different domains, and at scales much larger than any previous work. The framework is materialized as an open-source suite and is easily extended to new datasets, systems, and queries
 1
 .
",2018,47,12,"[2574504.0, 3415440.0, 2163752.0]",2574504.0,Ljubljana,2,"[2156641189.0, 48667025.0]",Y,"['The authors just pick three layers from VGG to add attention, why picking those 3 layers?', 'The provided math does not support any of these steps.']",,,Proceedings of the VLDB Endowment,"['graph', 'systems', 'framework', 'graph database', 'Graph Database']"
Graph Database,dcbaf58b16ac7ef947879ea37c021466357b291a,Use of Graph Database for the Integration of Heterogeneous Biological Data,https://www.semanticscholar.org/paper/dcbaf58b16ac7ef947879ea37c021466357b291a,JournalArticle,"Understanding complex relationships among heterogeneous biological data is one of the fundamental goals in biology. In most cases, diverse biological data are stored in relational databases, such as MySQL and Oracle, which store data in multiple tables and then infer relationships by multiple-join statements. Recently, a new type of database, called the graph-based database, was developed to natively represent various kinds of complex relationships, and it is widely used among computer science communities and IT industries. Here, we demonstrate the feasibility of using a graph-based database for complex biological relationships by comparing the performance between MySQL and Neo4j, one of the most widely used graph databases. We collected various biological data (protein-protein interaction, drug-target, gene-disease, etc.) from several existing sources, removed duplicate and redundant data, and finally constructed a graph database containing 114,550 nodes and 82,674,321 relationships. When we tested the query execution performance of MySQL versus Neo4j, we found that Neo4j outperformed MySQL in all cases. While Neo4j exhibited a very fast response for various queries, MySQL exhibited latent or unfinished responses for complex queries with multiple-join statements. These results show that using graph-based databases, such as Neo4j, is an efficient way to store complex biological relationships. Moreover, querying a graph database in diverse ways has the potential to reveal novel relationships among heterogeneous biological data.",2017,76,15,"[3515169.0, 2248549493.0, 2249671765.0]",3515169.0,Prague,3,"[2078772072.0, 2039003.0, 2047926221.0]",Y,"['This paper paper combines the two methods to create Relational Neural Expectation Maximization (R-NEM), allowing direct interaction at inference time between the latent variables that encode a scene.', 'The authors also introduce a specific control variate technique based on the so-called Stein’s identity.', 'The algorithm replaces the membership probabilities found in the E-step of EM for a GMM with the outputs of a subnetwork in the end-to-end architecture.']",,,Genomics & Informatics,"['relationships', 'data', 'mysql', 'graph database', 'Graph Database']"
Graph Database,32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b,Graphflow: An Active Graph Database,https://www.semanticscholar.org/paper/32c1e0f9ef6f1e337fe4aa2a4907314e5a3f4a5b,Conference,"Many applications detect the emergence or deletion of certain subgraphs in their input graphs continuously. In order to evaluate such continuous subgraph queries, these applications resort to inefficient or highly specialized solutions because existing graph databases are passive systems that only support one-time subgraph queries. We demonstrate Graphflow, a prototype active graph data-base that evaluates general one-time and continuous subgraph queries. Graphflow supports the property graph data model and the Cypher++ query language, which extends Neo4j's declarative Cypher language with subgraph-condition-action triggers. At the core of Graphflow's query processor are two worst-case optimal join algorithms called Generic Join and our new Delta Generic Join algorithm for one-time and continuous subgraph queries, respectively.",2017,108,,"[98182097.0, 32455748.0, 10754597.0, 2115896116.0, 1783781.0]",98182097.0,Riga,3,"[3011964.0, 113398129.0, 1719124.0]",Y,"['I recommend producing another new figure of doing such comparison.', ""At test time, (if I understand correctly, please correct me if I haven't), the model is evaluated by having multiple copies of the same test point within an episode."", 'I was not able to imagine a reasonable setting where we would have access to a reward function of this form without input/output examples.']",SIGMOD Conference,17.0,,"['subgraph', 'queries', 'graph', 'graph database', 'Graph Database']"
Graph Database,4cd033a56b19f87f6adfefeef5fcc990306ecf40,Neo4j graph database realizes efficient storage performance of oilfield ontology,https://www.semanticscholar.org/paper/4cd033a56b19f87f6adfefeef5fcc990306ecf40,JournalArticle,"The integration of oilfield multidisciplinary ontology is increasingly important for the growth of the Semantic Web. However, current methods encounter performance bottlenecks either in storing data and searching for information when processing large amounts of data. To overcome these challenges, we propose a domain-ontology process based on the Neo4j graph database. In this paper, we focus on data storage and information retrieval of oilfield ontology. We have designed mapping rules from ontology files to regulate the Neo4j database, which can greatly reduce the required storage space. A two-tier index architecture, including object and triad indexing, is used to keep loading times low and match with different patterns for accurate retrieval. Therefore, we propose a retrieval method based on this architecture. Based on our evaluation, the retrieval method can save 13.04% of the storage space and improve retrieval efficiency by more than 30 times compared with the methods of relational databases.",2018,34,13,"[51417783.0, 2115736466.0, 2510306.0, 2108756985.0, 145510499.0, 51917504.0]",51417783.0,Paris,2,"[1753285996.0, 2220547623.0]",Y,"['The model defined here is also very strange, especially Equation (2) is not really consistent with Equation (7).', 'The basic setup assumes a setting where we do not have access to input/output samples, but instead only have access to a separate reward function for each desired program that indicates how close a predicted program is to the correct one.']",,,PLoS ONE,"['retrieval', 'ontology', 'data', 'graph database', 'Graph Database']"
Graph Database,9a0965beef113cc37491004b1848149e00300561,A Graph Database Model for Knowledge Extracted from Place Descriptions,https://www.semanticscholar.org/paper/9a0965beef113cc37491004b1848149e00300561,JournalArticle,"Everyday place descriptions provide a rich source of knowledge about places and their relative locations. This research proposes a place graph model for modelling this spatial, non-spatial, and contextual knowledge from place descriptions. The model extends a prior place graph, and overcomes a number of limitations. The model is implemented using a graph database, and a management system has also been developed that allows operations including querying, mapping, and visualizing the stored knowledge in an extended place graph. Then three experimental tasks, namely georeferencing, reasoning, and querying, are selected to demonstrate the superiority of the extended model.",2018,22,7,"[2149052018.0, 2074432.0, 145285033.0, 1686534.0]",2149052018.0,Stockholm,2,"[2604647.0, 9185192.0]",Y,"['The authors propose to train a generator network in combination with the classifier and an adversarial discriminator.', ""My main concern with this work is that I don't see any mechanism in the framework that prevents an expert  (or few of them) to win all examples except its own learning capacities.""]",,,ISPRS Int. J. Geo Inf.,"['place', 'graph', 'model', 'graph database', 'Graph Database']"
Graph Database,5f7f10f913ecc478ff7ba304c265fd3c700b47d7,Modeling Graph Database Schema,https://www.semanticscholar.org/paper/5f7f10f913ecc478ff7ba304c265fd3c700b47d7,JournalArticle,"The authors present a new method for creating a graph database schema (GDBS) based on an entity-relationship diagram (ERD) of the application domain, which is mapped to a GDBS in a two-step process. First, the original ERD is adjusted to a semantically equivalent ERD (enabling it to be mapped in step two). In the second step, the adjusted ERD is mapped to a GDBS according to specific rules. The resulting GDBS includes integrity constraints that enrich existing graph databases.",2017,35,19,"[1414019666.0, 1732091.0, 1762969.0, 1753089.0]",1414019666.0,Stockholm,2,"[50487261.0, 1482544386.0]",Y,"['On the negative side, the paper is only qualitative.', 'Not that there is anything wrong with short papers, but in this case both the clarity of presentation and details are lacking.']",,,IT Professional,"['gdbs', 'erd', 'graph', 'graph database', 'Graph Database']"
Graph Database,ce3285bf1853f00c00535325851df5c33a0fc5d6,The Fragment Network: A Chemistry Recommendation Engine Built Using a Graph Database.,https://www.semanticscholar.org/paper/ce3285bf1853f00c00535325851df5c33a0fc5d6,JournalArticle,"The hit validation stage of a fragment-based drug discovery campaign involves probing the SAR around one or more fragment hits. This often requires a search for similar compounds in a corporate collection or from commercial suppliers. The Fragment Network is a graph database that allows a user to efficiently search chemical space around a compound of interest. The result set is chemically intuitive, naturally grouped by substitution pattern and meaningfully sorted according to the number of observations of each transformation in medicinal chemistry databases. This paper describes the algorithms used to construct and search the Fragment Network and provides examples of how it may be used in a drug discovery context.",2017,29,60 14,"[145366422.0, 32682297.0, 2462060.0]",145366422.0,Copenhagen,3,"[39231399.0, 1630293705.0, 46537606.0]",Y,"['This is then used to develop a simple bias initialization scheme for the gates when the range of temporal dependencies relevant for a problem can be estimated or are known.', 'The partitioning of each task must currently be designed by hand.', 'It may also be interesting to provide suitable theoretical analysis, e.g., relationships with the SVD of the embedding matrix.']",,,Journal of Medicinal Chemistry,"['fragment', 'drug', 'discovery', 'graph database', 'Graph Database']"
Graph Database,448959eff044f02040ded5afd483b7c4e811b0ac,Engineering Knowledge Graph from Patent Database,https://www.semanticscholar.org/paper/448959eff044f02040ded5afd483b7c4e811b0ac,JournalArticle,"
 We propose a large, scalable engineering knowledge graph, comprising sets of real-world engineering “facts” as < entity, relationship, entity > triples that are found in the patent database. We apply a set of rules based on the syntactic and lexical properties of claims in a patent document to extract facts. We aggregate these facts within each patent document and integrate the aggregated sets of facts across the patent database to obtain an engineering knowledge graph. Such a knowledge graph is expected to support inference, reasoning, and recalling in various engineering tasks. The knowledge graph has a greater size and coverage in comparison with the previously used knowledge graphs and semantic networks in the engineering literature.",2021,41,22,"[51471694.0, 1906960.0, 2053320598.0, 145990580.0]",51471694.0,Monaco,2,"[2146335468.0, 2184079980.0]",Y,"['They then evaluate several existing network architectures on these tasks, and show that results are not as impressive as others might have assumed they would be.', 'Exploiting the generative capacity of the network, they play with the ""sentiment neuron"" to deform a review.']",,,Journal of Computing and Information Science in Engineering,"['engineering', 'knowledge', 'graph', 'graph database', 'Graph Database']"
Graph Database,31d7d7b9c7b776c639316027e6ae5f2ff2673da2,Fast Dual Simulation Processing of Graph Database Queries,https://www.semanticscholar.org/paper/31d7d7b9c7b776c639316027e6ae5f2ff2673da2,Conference,"Graph database query languages feature expressive yet computationally expensive pattern matching capabilities. Answering optional query clauses in SPARQL for instance renders the query evaluation problem immediately PSPACE-complete. Light-weight graph pattern matching relations, such as simulation, have recently been investigated as promising alternatives to more expensive query mechanisms like, e.g., computing subgraph isomorphism. Still, pattern matching alone lacks expressive query capabilities: graph patterns may be combined by usual inner joins. However, including more sophisticated operators is inevitable to make solutions more useful for emerging applications. In this paper we bridge this gap by introducing a new dual simulation process operating on SPARQL queries. In addition to supporting the full syntactic structure of SPARQL queries, it features polynomial-time pattern matching to compute an overapproximation of the query results. Moreover, to achieve running times competing with state-of-the-art database systems, we develop a novel algorithmic solution to dual simulation graph pattern matching, based on a system of inequalities that allows for several optimization heuristics. Finally, we achieve soundness of our process for SPARQL queries including UNION, AND and OPTIONAL operators not restricted to well-designed patterns. Our experiments on synthetic and real-world graph data promise a clear gain for graph database systems when incorporating the new dual simulation techniques.",2018,8,,"[3304707.0, 3245041.0, 79691050.0, 77790220.0, 1720266.0]",3304707.0,Rome,2,"[2145734797.0, 2086632521.0]",Y,"['- I believe that different Monte Carlo (or Quasi-Monte Carlo) strategies can be applied in order to estimate the integral (expected value) in Eq.', 'Pros: - A new GAIL formulation for saving on interaction data.']",IEEE International Conference on Data Engineering,35.0,,"['graph', 'query', 'pattern', 'graph database', 'Graph Database']"
Graph Database,746a81aa26d3ebfb81acfd6af958d6a21603cd21,Design and Implementation of Movie Recommender System Based on Graph Database,https://www.semanticscholar.org/paper/746a81aa26d3ebfb81acfd6af958d6a21603cd21,Conference,"with the continuous development of Internet technology, information overload is becoming more and more serious. It's getting harder to get useful information from the network. Although the search engine can help users find information they need from the vast amounts of information in a certain extent, but cannot completely solve the problem of information overload, when users cannot accurately describe the information they need, you need to recommend system to help users find valuable information for users. So recommender systems are becoming more and more important. The movie recommender system implemented in this paper is based on the traditional user-based collaborative filtering algorithm, and the user project scoring matrix is pre filled. At the same time, database technology of this system uses graph database which is good at dealing with complex relations. In data visualization, the degree of recommendation of a movie is expressed by the size of the node and the thickness of the edge, so as to improve the user experience.",2017,17,,"[46255467.0, 49673164.0, 2112638989.0, 34701398.0]",46255467.0,San Marino,3,"[30671790.0, 2162042348.0, 1729109.0]",Y,"['The paper makes some bold claims.', 'Without this baseline, it is hard to tell whether GAN training is even useful.', '3. There is a lack comparison to other methods such as Shaham et al. (2017).']",Web Information System and Application Conference,14.0,,"['information', 'users', 'system', 'graph database', 'Graph Database']"
Graph Database,c6879e43828b293567f5e2da039d23845189d6a7,"Managing Cyber Threat Intelligence in a Graph Database: Methods of Analyzing Intrusion Sets, Threat Actors, and Campaigns",https://www.semanticscholar.org/paper/c6879e43828b293567f5e2da039d23845189d6a7,Conference,"Efforts to cope jointly with the ever-increasing number of breach incidents have resulted in the establishment of the standard format and protocol and given birth to many consultative groups. In addition, various channels that distribute Cyber Threat Intelligence information free of charge have emerged, and studies on utilizing such channels have spread. As the market for sharing information professionally is expanding, the need to manage the shared information in various ways in order to achieve better result has arisen. This paper proposes a standardized management structure and method based on the standardized format and a meaning and standard of Cyber Threat Intelligence that can be shared outside when loading OSINT information collected from various channels into the graph database. This paper also proposes a method of supporting the detection provided by existing security equipment with the information saved in the graph database and an effective method of analysis. Lastly, the paper discusses the advantages that can be expected from saving cyber threat information in the graph database developed using information collected from the outside.",2018,10,,"[2108129412.0, 9460711.0, 153378387.0, 2151900144.0, 2109139810.0]",2108129412.0,Valletta,3,"[2059129841.0, 1713648.0, 152125305.0]",Y,"['The experimental results seem promising, although not earthshattering.', 'Perhaps one could use a different (less striped) animal, e.g. raccoon.', 'Without experiments over other datasets and wall clock time results, it is hard to appreciate the significance of this improvement.']",International Conference on Platform Technology and Service,18.0,,"['information', 'channels', 'cyber', 'graph database', 'Graph Database']"
Graph Database,e89c37b7c2ff465db43c4b9f674867ec4b98aa8b,EpiGeNet: A Graph Database of Interdependencies Between Genetic and Epigenetic Events in Colorectal Cancer,https://www.semanticscholar.org/paper/e89c37b7c2ff465db43c4b9f674867ec4b98aa8b,JournalArticle,"The development of colorectal cancer (CRC)-the third most common cancer type-has been associated with deregulations of cellular mechanisms stimulated by both genetic and epigenetic events. StatEpigen is a manually curated and annotated database, containing information on interdependencies between genetic and epigenetic signals, and specialized currently for CRC research. Although StatEpigen provides a well-developed graphical user interface for information retrieval, advanced queries involving associations between multiple concepts can benefit from more detailed graph representation of the integrated data. This can be achieved by using a graph database (NoSQL) approach. Data were extracted from StatEpigen and imported to our newly developed EpiGeNet, a graph database for storage and querying of conditional relationships between molecular (genetic and epigenetic) events observed at different stages of colorectal oncogenesis. We illustrate the enhanced capability of EpiGeNet for exploration of different queries related to colorectal tumor progression; specifically, we demonstrate the query process for (i) stage-specific molecular events, (ii) most frequently observed genetic and epigenetic interdependencies in colon adenoma, and (iii) paths connecting key genes reported in CRC and associated events. The EpiGeNet framework offers improved capability for management and visualization of data on molecular events specific to CRC initiation and progression.",2017,19,24 10,"[8613913.0, 50701671.0, 47631235.0, 36994619.0, 48107154.0, 31590259.0, 1679216.0, 48662628.0]",8613913.0,Bratislava,3,"[2113959133.0, 50841357.0, 4007395.0]",Y,"['(As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.)', 'Thus, as such, the novelty or the contributions of this paper are minor.', 'The paper and included experiments are a valuable contribution to the community interested in solving harder and harder tasks using reinforcement learning.']",,,J. Comput. Biol.,"['events', 'statepigen', 'database', 'graph database', 'Graph Database']"
Graph Database,9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a,Graph Database for Recipe Recommendations,https://www.semanticscholar.org/paper/9ce948246c918e2c1846c3fc76d3e1c9ab1b0c2a,Conference,"Graph databases represent a paradigm shift from relational databases with a strong support for “ relationships”. As compared to relational databases which compute relationships at runtime, graph databases persist relationships for fast querying and data retrieval. This work presents a recipe recommender as a graph database, Neo4j application. Given any set of ingredients, this application recommends a variety of recipes with the help of a data set containing thousands of ingredients. Further based on availability of ingredients with a user, this application helps discover the list of possible dishes with these ingredients. In order to implement this application, ingredients and recipes have been crawled from cookery based websites using Python scripts. The crawled data has been inserted into the Neo4j database and subsequently inter-relationships between ingredients and recipes nodes have been analyzed. Execution of self designed queries has verified the time-efficiency of the proposed approach.",2018,4,,"[82008243.0, 150281558.0, 3357166.0, 2081215.0]",82008243.0,Podgorica,3,"[1753210.0, 40071013.0, 49627183.0]",Y,"['A temporal clustering model and a DCNN decoder are applied on the encoded representations and jointly trained.', 'This paper shows an observation of “super-convergence” when training resnet with cyclical learning rates but does not provide conclusive analysis or experiment results.', 'PSNR is evaluated on predicted frames -- PSNR does not appear to be explicitly defined but I am taking it to be the metric defined in the 2nd paragraph from the bottom on page 7.  The new model ""EEN"" is compared to a deterministic model and conditional GAN.  The GAN never seems to perform well -- the authors claim mode collapse, but I wonder if the GAN was simply hard to train in the first place and this is the key reason?']","2018 7th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",7.0,,"['ingredients', 'application', 'graph', 'graph database', 'Graph Database']"
Graph Database,ce51eff5b529ee572dab1c1f38f20adc8e89bab2,System G Distributed Graph Database,https://www.semanticscholar.org/paper/ce51eff5b529ee572dab1c1f38f20adc8e89bab2,JournalArticle,"Motivated by the need to extract knowledge and value frominterconnected data, graph analytics on big data is a veryactive area of research in both industry and academia. Tosupport graph analytics efficiently a large number of in mem-ory graph libraries, graph processing systems and graphdatabases have emerged. Projects in each of these cate-gories focus on particular aspects such as static versus dy-namic graphs, off line versus on line processing, small versuslarge graphs, etc.While there has been much advance in graph processingin the past decades, there is still a need for a fast graph pro-cessing, using a cluster of machines with distributed storage.In this paper, we discuss a novel distributed graph databasecalled System G designed for efficient graph data storage andprocessing on modern computing architectures. In particu-lar we describe a single node graph database and a runtimeand communication layer that allows us to compose a dis-tributed graph database from multiple single node instances.From various industry requirements, we find that fast inser-tions and large volume concurrent queries are critical partsof the graph databases and we optimize our database forsuch features. We experimentally show the efficiency ofSystem G for storing data and processing graph queries onstate-of-the-art platforms.",2018,4,abs/1802.03057,"[49573525.0, 2231831.0, 2108602524.0, 48239920.0, 2059069849.0, 3166546.0, 2108965391.0, 35638374.0]",49573525.0,San Marino,2,"[48919600.0, 1402912902.0]",Y,"['Finally, it might be interesting to initialize the convolutions in the shortcut connections with the identity, and check what they have leant at the end of the training.', 'This gives an ellipsoid decision boundary around the origin.']",,,arXiv.org,"['graph', 'data', 'database', 'graph database', 'Graph Database']"
Graph Database,bbb52447f2f38aad9613ba026f88b57637ffcbea,Towards FAIRer Biological Knowledge Networks Using a Hybrid Linked Data and Graph Database Approach,https://www.semanticscholar.org/paper/bbb52447f2f38aad9613ba026f88b57637ffcbea,JournalArticle,"Abstract The speed and accuracy of new scientific discoveries – be it by humans or artificial intelligence – depends on the quality of the underlying data and on the technology to connect, search and share the data efficiently. In recent years, we have seen the rise of graph databases and semi-formal data models such as knowledge graphs to facilitate software approaches to scientific discovery. These approaches extend work based on formalised models, such as the Semantic Web. In this paper, we present our developments to connect, search and share data about genome-scale knowledge networks (GSKN). We have developed a simple application ontology based on OWL/RDF with mappings to standard schemas. We are employing the ontology to power data access services like resolvable URIs, SPARQL endpoints, JSON-LD web APIs and Neo4j-based knowledge graphs. We demonstrate how the proposed ontology and graph databases considerably improve search and access to interoperable and reusable biological knowledge (i.e. the FAIRness data principles).",2018,20,15,"[2507242.0, 2117776059.0, 31590259.0, 1398327640.0]",2507242.0,Kiev,3,"[1750856.0, 50320297.0, 51235411.0]",Y,"['4. The localization performance of the proposed attention mechanism is evaluated by weakly-supervised semantic segmentation tasks.', 'It is nice to know the bounds given in the paper and to understand the theoretical conditions under which we can obtain running time benefits using corsets.', 'Following the design of the tandem blocks proposed in the paper, I wonder why the tandem block B3x3(2,w) was not included.']",,,Journal of Integrative Bioinformatics,"['data', 'knowledge', 'search', 'graph database', 'Graph Database']"
Graph Database,cf523942d56e90db182c5788845f6502da9a307d,3D Mapping Database Aided GNSS Based Collaborative Positioning Using Factor Graph Optimization,https://www.semanticscholar.org/paper/cf523942d56e90db182c5788845f6502da9a307d,JournalArticle,"The recent development in vehicle-to-everything (V2X) communication opens a new opportunity to improve the positioning performance of the road users. We explore the benefit of connecting the raw data of the global navigation satellite system (GNSS) from the agents. In urban areas, GNSS positioning is highly degraded due to signal blockage and reflection. 3D building model can play a major role in mitigating the GNSS multipath and non-line-of-sight (NLOS) effects. To combine the benefits of 3D models and V2X, we propose a novel 3D mapping aided (3DMA) GNSS-based collaborative positioning method that makes use of the available surrounding GNSS receivers’ measurements. By complementarily integrating the ray-tracing based 3DMA GNSS and the double difference technique, the random errors (such as multipath and NLOS) are mitigated while eliminating the systematic errors (such as atmospheric delay and satellite clock/orbit biases) between road user. To improve the accuracy and robustness of the collaborative algorithm, factor graph optimization (FGO) is employed to optimize the positioning solutions among agents. Multiple low-cost GNSS receivers are used to collect both static and dynamic data in Hong Kong and to evaluate the proposed algorithm by post-processing. We reduce the GNSS positioning error from over 30 meters to less than 10 meters for road users in a deep urban canyon.",2021,28,22,"[46266394.0, 1410897553.0, 41022905.0, 2631044.0]",46266394.0,Nicosia,2,"[2054289955.0, 2845020.0]",Y,"['Although this does not rule out angle bias in general, it does so for the very special case where the expected value of the random vector is a vector consisting of a common value.', 'An encoder is later trained to map real images into the space of latent codes for the generator, allowing the system to be applied to real image segmentation tasks.']",,,IEEE transactions on intelligent transportation systems (Print),"['gnss', 'road', '3d', 'graph database', 'Graph Database']"
Graph Database,2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0,Topology Modeling and Analysis of a Power Grid Network Using a Graph Database,https://www.semanticscholar.org/paper/2396dbcfe1f7f9dafc6696c3c06b16fd3029f7a0,JournalArticle,"We introduce a new method for storing, modeling, and analyzing power grid data. First, we present an architecture for building the network model for a power grid using the open source graph database Neo4j. Second, we design singleand multi-threading systems for initial energization analysis of the power grid network. We design the shortest path search function and conditional search function based on Neo4j. Finally, we compare the functionality and efficiency of our graph database with a traditional relational database in system initial energization analysis and the shortest path function problems on small to large data sets. The results demonstrate the efficiency and effectiveness of topology modeling and analysis using graph database for a power grid network.",2017,14,10,"[2204964.0, 3256994.0, 121865955.0, 2145308743.0, 2064971594.0, 143627522.0]",2204964.0,Helsinki,2,"[35208858.0, 1775620.0]",Y,"['Cooperative multi-agent problem solving is an important problem in machine learning, artificial intelligence, and cognitive science.', 'Flexible muscle-based locomotion for bipedal creatures.']",,,International Journal of Computational Intelligence Systems,"['power', 'grid', 'database', 'graph database', 'Graph Database']"
Graph Database,fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53,GeaBase: A High-Performance Distributed Graph Database for Industry-Scale Applications,https://www.semanticscholar.org/paper/fb8e253b8b0358a7b95ddc9ac4c74f93513a9c53,Conference,"Graph analytics have been gaining tractions rapidly in the past few years. It has a wide array of application areas in the industry, ranging from e-commerce, social network and recommendation systems to fraud detection and virtually any problem that requires insights into data connections, not just data itself. In this paper, we present GeaBase, a new distributed graph database that provides the capability to store and analyze graph-structured data in real-time at massive scale. We describe the details of the system and the implementation, including a novel update architecture, called Update Center (UC), and a new language that is suitable for both graph traversal and analytics. We also compare the performance of GeaBase to a widely used open-source graph database Titan. Experiments show that GeaBase is up to 182x faster than Titan in our testing scenarios. We also achieves 22x higher throughput on social network workloads in the comparison.",2017,12,,"[2677700.0, 7806657.0, 3358986.0, 1943322867.0, 2144151933.0, 2108966388.0, 2114147314.0, 24812041.0, 2218569240.0]",2677700.0,San Marino,2,"[46175739.0, 2065041692.0]",Y,"['4. The localization performance of the proposed attention mechanism is evaluated by weakly-supervised semantic segmentation tasks.', 'The authors do provide a novel formulation and demonstrate the gains on a variety of concrete problems taken form the literature.']",International Conference on Advanced Cloud and Big Data,17.0,,"['graph', 'data', 'geabase', 'graph database', 'Graph Database']"
Graph Database,7676c02ea839ff1ceb6e5e1427c42bc45e169bde,The Spatio-Temporal Data Modeling and Application Based on Graph Database,https://www.semanticscholar.org/paper/7676c02ea839ff1ceb6e5e1427c42bc45e169bde,Conference,"Traditional spatio-temporal data model (STDM) is based on relational database, it is hard to convert problem domain model to relational model, which result in complicated query and low expansibility. In this regard, we propose the spatio-temporal data model based on graph database. The data model integrates TGIS's three key elements: space, time and attributes, and expressed spatio-temporal characteristics of TGIS explicitly. Finally, this paper gives a particular description of logistics distribution route optimization. Experimental results show that the model is proved to be appropriate for expressing the spatio-temporal process of world.",2017,9,,"[8507683.0, 2145295170.0, 2145734797.0, 2053831005.0, 48152160.0]",8507683.0,Sarajevo,2,"[1410127739.0, 2120664.0]",Y,"['Originality and Significance:  The area of relation extraction seems to me to be very important and probably a bit less intensively worked on that it should be.', 'Firstly, the paper has a fatal mathematical flaw.']",International Conference on Information Science and Control Engineering,4.0,,"['model', 'data', 'database', 'graph database', 'Graph Database']"
Graph Database,704011527f183b561ea6a75b21e4cefe5aa77fca,Book recommendation using Neo4j graph database in BibTeX book metadata,https://www.semanticscholar.org/paper/704011527f183b561ea6a75b21e4cefe5aa77fca,Conference,"In digital era, book has an important role in life. There are not only a lot of books for different purpose. But also, there are many book metadata which can use for another reason, such as book recommendation. By processing the book metadata, an information can be given to user that needs book recommendation. By combining BibTeX book metadata and Graph Database from Neo4j, data from metadata can be processed. Then, with cypher query by inputting author's parameter or book type's parameter, user can get book recommendation based on their input's criteria. The result is exactly the same with process the metadata manually in relational database. Neo4j, from this paper, takes 180 milliseconds to execute cypher query with author's criteria and takes 184 milliseconds to execute cypher query with book type's criteria.",2017,11,,"[121066428.0, 2803317.0]",121066428.0,Nicosia,3,"[2138053020.0, 1628391446.0, 150341221.0]",Y,"['In section 4, authors claim that their results are competitive with the best published results for a similar number of parameters.', 'Their abstract also claims to utilize a convex programming formulation.', 'The details of their proposed method are covered in Algorithm 1 on Page 12, where an additional GAN (generative adversarial network) I_{\\gamma}, which can be regarded as the inverse function of the original GAN G_{\\theta}, is trained to learn a map from the original input data space to the latent z-space.']",International Conference on Science in Information Technology,3.0,,"['book', 'metadata', 'recommendation', 'graph database', 'Graph Database']"
Graph Database,78b2d392ebb100a220ceab6529d26909b27eaa32,"From Relational Database to Big Data: Converting Relational to Graph Database, MOOC Database as Example",https://www.semanticscholar.org/paper/78b2d392ebb100a220ceab6529d26909b27eaa32,JournalArticle,"Existing graph database management systems provide an efficient solution to data storage where graph models are widely used, the conversion of an existing application from a relational to graph data can be convenient but it is usually a hard task for database administrators. In this work, we propose a conversion tool from a relational to graph database. The approach supports the conversion of the schema and the data.",2017,5,8,"[2981793.0, 1596123047.0, 1596120258.0, 1732949.0]",2981793.0,Belgrade,3,"[144385184.0, 36460588.0, 1802817.0]",Y,"['Preference elicitation and inverse reinforcement learning.', 'Is the implicit assumption that for real, high-dimensional data the generator and data manifolds will *never* overlap?', 'In my first reading of the paper, I felt that the baselines in the experiments are too primitive.']",,,J. Ubiquitous Syst. Pervasive Networks,"['graph', 'database', 'data', 'graph database', 'Graph Database']"
Graph Database,b889b1d6944213bc2ca29e3ad07ee65ede20892d,An X10-Based Distributed Streaming Graph Database Engine,https://www.semanticscholar.org/paper/b889b1d6944213bc2ca29e3ad07ee65ede20892d,Conference,"Streaming graph data mining has become a significant issue in high performance graph mining due to the increasing appearance of graph data sets as streams. In this paper we propose Acacia-Stream which is a scalable distributed streaming graph database engine developed with X10 programming language. Graph streams are partitioned using a streaming graph partitioner algorithm in Acacia-Stream and streaming graph processing queries are run on the graph streams. The partitioned data sets are persisted on secondary storage across X10 places. We investigate on the use of three different streaming graph partitioner algorithms called hash, Linear Deterministic Greedy, and Fennel algorithms and report their performance. Furthermore, to demonstrate Acacia-Stream's streaming graph processing capabilities we implement streaming triangle counting with Acacia-Stream. We present performance results gathered from Acacia-Stream with different large scale streaming data sets in both horizontal and vertical scalability experiments. Furthermore, we compare streaming graph loading performance of Acacia-Stream with Neo4j and Oracle's PGX graph database servers. From these experiments we observed that Acacia-Stream's Fennel partitioner based graph uploader can upload a 948MB rmat22 graph in 1283.42 seconds which is 38% faster than PGX graph database server and 12.8 times faster than Neo4j database server. Acacia-Stream's Streaming Partitioner's batch size adjustments based optimizations reduced the time used by the network communications almost by half.",2017,4,,"[2741023.0, 35709316.0, 35433878.0, 49627183.0, 35367497.0, 1971912.0, 2231831.0]",2741023.0,Stockholm,3,"[2125957.0, 153693432.0, 1500655986.0]",Y,"['Since any CoffeeScript programs can be compiled into the corresponding Javascript programs, we should assume that CoffeeScript is the only subset of Javascript (without physical difference of syntax), and this translation task may never capture the whole tendency of Javascript.', 'The fact that the proposed technique is simple yet yields such speedups is encouraging.', '1. The authors tested out this new activation function on RNNs.']",International Conference on High Performance Computing,24.0,,"['graph', 'acaciastream', 'data', 'graph database', 'Graph Database']"
Graph Database,aa6c2afadd660fe4efbac699f7854e8f6f240c38,GenCoNet – A Graph Database for the Analysis of Comorbidities by Gene Networks,https://www.semanticscholar.org/paper/aa6c2afadd660fe4efbac699f7854e8f6f240c38,JournalArticle,"Abstract The prevalence of comorbid diseases poses a major health issue for millions of people worldwide and an enormous socio-economic burden for society. The molecular mechanisms for the development of comorbidities need to be investigated. For this purpose, a workflow system was developed to aggregate data on biomedical entities from heterogeneous data sources. The process of integrating and merging all data sources of the workflow system was implemented as a semi-automatic pipeline that provides the import, fusion, and analysis of the highly connected biomedical data in a Neo4j database GenCoNet. As a starting point, data on the common comorbid diseases essential hypertension and bronchial asthma was integrated. GenCoNet (https://genconet.kalis-amts.de) is a curated database that provides a better understanding of hereditary bases of comorbidities.",2018,9,15,"[38421496.0, 2040118.0, 4373076.0, 144294520.0, 2080135160.0, 2513276.0, 144649232.0, 144098801.0]",38421496.0,Lisbon,2,"[51172373.0, 41211459.0]",Y,"['The value for a bin is the (normalized) number of nodes falling into the corresponding region.', ""This paper presents a nice idea, and I'm sure that with some polish it will become a very nice conference submission.""]",,,Journal of Integrative Bioinformatics,"['data', 'comorbid', 'diseases', 'graph database', 'Graph Database']"
Data Management,8dd0c1e955c66092ff951941a151336211e6e171,PhyloSuite: An integrated and scalable desktop platform for streamlined molecular sequence data management and evolutionary phylogenetics studies,https://www.semanticscholar.org/paper/8dd0c1e955c66092ff951941a151336211e6e171,JournalArticle,"Multigene and genomic data sets have become commonplace in the field of phylogenetics, but many existing tools are not designed for such data sets, which often makes the analysis time‐consuming and tedious. Here, we present PhyloSuite, a (cross‐platform, open‐source, stand‐alone Python graphical user interface) user‐friendly workflow desktop platform dedicated to streamlining molecular sequence data management and evolutionary phylogenetics studies. It uses a plugin‐based system that integrates several phylogenetic and bioinformatic tools, thereby streamlining the entire procedure, from data acquisition to phylogenetic tree annotation (in combination with iTOL). It has the following features: (a) point‐and‐click and drag‐and‐drop graphical user interface; (b) a workplace to manage and organize molecular sequence data and results of analyses; (c) GenBank entry extraction and comparative statistics; and (d) a phylogenetic workflow with batch processing capability, comprising sequence alignment (mafft and macse), alignment optimization (trimAl, HmmCleaner and Gblocks), data set concatenation, best partitioning scheme and best evolutionary model selection (PartitionFinder and modelfinder), and phylogenetic inference (MrBayes and iq‐tree). PhyloSuite is designed for both beginners and experienced researchers, allowing the former to quick‐start their way into phylogenetic analysis, and the latter to conduct, store and manage their work in a streamlined way, and spend more time investigating scientific questions instead of wasting it on transferring files from one software program to another.",2019,1581,20,"[113087456.0, 3817732.0, 5989466.0, 2286418480.0, 2108930648.0, 2109050852.0, 150116413.0]",113087456.0,Vilnius,3,"[1633124736.0, 153329056.0, 1698925.0]",Y,"['Also, I would prefer a longer, descriptive and informative label to make the figure as self-explained as possible.', 'For inference only, other works has more to offer but this is a promising technique for learning.', 'The main novelty in this paper is that it uses the label as a third view of a multi-view model and make use of cross moments.']",,,Molecular Ecology Resources,"['data', 'sequence', 'sets', 'data management', 'Data Management']"
Data Management,28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8,Milvus: A Purpose-Built Vector Data Management System,https://www.semanticscholar.org/paper/28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8,Conference,"Recently, there has been a pressing need to manage high-dimensional vector data in data science and AI applications. This trend is fueled by the proliferation of unstructured data and machine learning (ML), where ML models usually transform unstructured data into feature vectors for data analytics, e.g., product recommendation. Existing systems and algorithms for managing vector data have two limitations: (1) They incur serious performance issue when handling large-scale and dynamic vector data; and (2) They provide limited functionalities that cannot meet the requirements of versatile applications. This paper presents Milvus, a purpose-built data management system to efficiently manage large-scale vector data. Milvus supports easy-to-use application interfaces (including SDKs and RESTful APIs); optimizes for the heterogeneous computing platform with modern CPUs and GPUs; enables advanced query processing beyond simple vector similarity search; handles dynamic data for fast updates while ensuring efficient query processing; and distributes data across multiple nodes to achieve scalability and availability. We first describe the design and implementation of Milvus. Then we demonstrate the real-world use cases supported by Milvus. In particular, we build a series of 10 applications (e.g., image/video search, chemical structure analysis, COVID-19 dataset search, personalized recommendation, biological multi-factor authentication, intelligent question answering) on top of Milvus. Finally, we experimentally evaluate Milvus with a wide range of systems including two open source systems (Vearch and Microsoft SPTAG) and three commercial systems. Experiments show that Milvus is up to two orders of magnitude faster than the competitors while providing more functionalities. Now Milvus is deployed by hundreds of organizations worldwide and it is also recognized as an incubation-stage project of the LF AI & Data Foundation. Milvus is open-sourced at https://github.com/milvus-io/milvus.",2021,90,,"[2141735101.0, 1666260553.0, 1764236.0, 145914256.0, 2153916637.0, 2153701193.0, 2144797977.0, 46909714.0, 2136332740.0, 2115215055.0, 2114076472.0, 2116525062.0, 2113959133.0, 2117315688.0, 2111258930.0, 2116566794.0, 2129460589.0, 2113623161.0, 2144097347.0, 2113618679.0, 2156252582.0, 2113619066.0]",2141735101.0,Sofia,2,"[2206684539.0, 49997612.0]",Y,"['Also, do the cluster centroids appear to be roughly stable over many runs of the algorithm?', 'Intriguing two phase RL approach for learning neural controllers for discrete programs This paper presents a reinforcement learning based approach to learn context-free parsers from pairs of input programs and their corresponding parse trees.']",SIGMOD Conference,21.0,,"['data', 'milvus', 'vector', 'data management', 'Data Management']"
Data Management,91bda0785eaf642515eefc9ff2ecd7ddbacaccae,FDM: Fuzzy-Optimized Data Management Technique for Improving Big Data Analytics,https://www.semanticscholar.org/paper/91bda0785eaf642515eefc9ff2ecd7ddbacaccae,JournalArticle,"Big data analytics and processing require complex architectures and sophisticated techniques for extracting useful information from the accumulated information. Visualizing the extracted data for real-time solutions is demanding in accordance with the semantics and the classification employed by the processing models. This article introduces fuzzy-optimized data management (FDM) technique for classifying and improving coalition of accumulated information based semantics and constraints. The dependency of the information is classified on the basis of the relationships modeled between the data based on the attributes. This technique segregates the considered attributes based on similarity index boundaries to process complex data in a controlled time. The performance of the proposed FDM is analyzed using a real-time weather forecast dataset consisting of sensor data (observed) and image data (captured). With this dataset, the functions of FDM such as input semantics analytics and classification based on similarity are performed. The metrics classification and processing time and similarity index are analyzed for the varying data sizes, classification instances, and dataset records. The proposed FDM is found to achieve 36.28% less processing time for varying classification instances, and 12.57% high similarity index.",2021,56,29,"[10148761.0, 2006530617.0, 46916453.0, 145377105.0, 152571594.0, 18180865.0, 27648952.0, 90943712.0]",10148761.0,Amsterdam,2,"[144105277.0, 35206168.0]",Y,"['Some reasons below: * There are no specific results on properties of the divergences, or axioms that justify them.', 'On the theoretical side, the linearly constrained weights are only shown to work for a very special case.']",,,IEEE transactions on fuzzy systems,"['data', 'classification', 'processing', 'data management', 'Data Management']"
Data Management,02fa2389b1b64b661192e224bed8af6df0ce80f6,Deep Reinforcement Learning Assisted Federated Learning Algorithm for Data Management of IIoT,https://www.semanticscholar.org/paper/02fa2389b1b64b661192e224bed8af6df0ce80f6,JournalArticle,"The continuous expanded scale of the industrial Internet of Things (IIoT) leads to IIoT equipments generating massive amounts of user data every moment. According to the different requirement of end users, these data usually have high heterogeneity and privacy, while most of users are reluctant to expose them to the public view. How to manage these time series data in an efficient and safe way in the field of IIoT is still an open issue, such that it has attracted extensive attention from academia and industry. As a new machine learning paradigm, federated learning (FL) has great advantages in training heterogeneous and private data. This article studies the FL technology applications to manage IIoT equipment data in wireless network environments. In order to increase the model aggregation rate and reduce communication costs, we apply deep reinforcement learning (DRL) to IIoT equipment selection process, specifically to select those IIoT equipment nodes with accurate models. Therefore, we propose a FL algorithm assisted by DRL, which can take into account the privacy and efficiency of data training of IIoT equipment. By analyzing the data characteristics of IIoT equipments, we use MNIST, fashion MNIST, and CIFAR-10 datasets to represent the data generated by IIoT. During the experiment, we employ the deep neural network model to train the data, and experimental results show that the accuracy can reach more than 97%, which corroborates the effectiveness of the proposed algorithm.",2021,93,17,"[40075749.0, 2144447082.0, 1750017.0, 145169163.0]",40075749.0,Helsinki,3,"[2260340372.0, 2176030144.0, 116180161.0]",Y,"['There is no easy way to fix this and for the purpose of sampling the paper simply treats the weights as immutable.', 'Similarly, equation 4 penalizes differences between predicted and observed state transitions.', 'Unsurprisingly (since the EEN noise does not seem to be conditioned on the input), the baseline deterministic model performs quite well.']",,,IEEE Transactions on Industrial Informatics,"['iiot', 'data', 'equipment', 'data management', 'Data Management']"
Data Management,9f9afed22cdc73be627932270a9bcae341df99d4,Towards Observability Data Management at Scale,https://www.semanticscholar.org/paper/9f9afed22cdc73be627932270a9bcae341df99d4,JournalArticle,"Observability has been gaining importance as a key capability in today's large-scale software systems and services. Motivated by current experience in industry exemplified by Slack and as a call to arms for database research, this paper outlines the challenges and opportunities involved in designing and building Observability Data Management Systems (ODMSs) to handle this emerging workload at scale.",2021,24,49,"[2678944.0, 8347899.0, 2031287.0, 1773620.0]",2678944.0,Bucharest,3,"[2111044238.0, 152814510.0, 2093582149.0]",Y,"['Yet, this is not discussed in the paper.', 'The efficiency of such a search method decreases very fast when the dimensions of the z-space increases.', 'Much of the authors’ analysis is based on a qualitative evaluation of samples.']",,,SIGMOD record,"['observability', 'systems', 'importance', 'data management', 'Data Management']"
Data Management,cd5b4b113d5fa90b5dc5aa372111b89d18df88fb,Automating Research Data Management Using Machine-Actionable Data Management Plans,https://www.semanticscholar.org/paper/cd5b4b113d5fa90b5dc5aa372111b89d18df88fb,JournalArticle,"Many research funders mandate researchers to create and maintain data management plans (DMPs) for research projects that describe how research data is managed to ensure its reusability. A DMP, being a static textual document, is difficult to act upon and can quickly become obsolete and impractical to maintain. A new generation of machine-actionable DMPs (maDMPs) was therefore proposed by the Research Data Alliance to enable automated integration of information and updates. maDMPs open up a variety of use cases enabling interoperability of research systems and automation of data management tasks. In this article, we describe a system for machine-actionable data management planning in an institutional context. We identify common use cases within research that can be automated to benefit from machine-actionability of DMPs. We propose a reference architecture of an maDMP support system that can be embedded into an institutional research data management infrastructure. The system semi-automates creation and maintenance of DMPs, and thus eases the burden for the stakeholders responsible for various DMP elements. We evaluate the proposed system in a case study conducted at the largest technical university in Austria and quantify to what extent the DMP templates provided by the European Commission and a national funding body can be pre-filled. The proof-of-concept implementation shows that maDMP workflows can be semi-automated, thus workload on involved parties can be reduced and quality of information increased. The results are especially relevant to decision makers and infrastructure operators who want to design information systems in a systematic way that can utilize the full potential of maDMPs.",2021,11,13,"[1752597.0, 2124473614.0, 145494588.0]",1752597.0,Ljubljana,3,"[2152290059.0, 3190187.0, 16443937.0]",Y,"['Also, the detailed specification of the VAE should be detailed.', 'The proposed algorithm in 3 out of 5 datasets, two of theme with statistical significant.', 'The authors show through several experiments that the divide and conquer (DnC) technique can solve more complex tasks than can be solved with conventional policy gradient methods (TRPO is used as the baseline).']",,,ACM Transactions on Management Information Systems,"['research', 'data', 'management', 'data management', 'Data Management']"
Data Management,1ff76ab0fcf22110df62337d462e15d79a2a2593,A Blockchain-Based Trusted Data Management Scheme in Edge Computing,https://www.semanticscholar.org/paper/1ff76ab0fcf22110df62337d462e15d79a2a2593,JournalArticle,"With rapid development of computing technologies, large amount of data are gathered from edge terminals or Internet of Things (IoT) devices, however data trust and security in edge computing environment are very important issues to be considered, especially when the gathered data are fraud or dishonest, or the data are misused or spread without any authorization, which may lead to serious problems. In this article, a blockchain-based trusted data management scheme (called BlockTDM) in edge computing is proposed to solve the above problems, in which we proposed a flexible and configurable blockchain architecture that includes mutual authentication protocol, flexible consensus, smart contract, block and transaction data management, blockchain nodes management, and deployment. The BlockTDM scheme can support matrix-based multichannel data segment and isolation for sensitive or privacy data protection, and moreover, we have designed user-defined sensitive data encryption before the transaction payload stores in blockchain system, and have implemented conditional access and decryption query of the protected blockchain data and transactions through smart contract. Finally, we have evaluated the proposed BlockTDM scheme security, availability, and efficiency with large amount of experiments. Analysis and evaluations manifest that the proposed BlockTDM scheme provides a general, flexible, and configurable blockchain-based paradigm for trusted data management with tamper-resistance, which is suitable for edge computing with high-level security and creditability.",2020,130,16,"[31303748.0, 12717353.0, 32069527.0, 47224454.0, 9195315.0, 2061881331.0]",31303748.0,San Marino,3,"[2144797977.0, 144098801.0, 1508456598.0]",Y,"['One aspect that seemed under-addressed and which often a crucial aspect of a good framework, is how general purpose code e.g. for loading data or logging interacts with the accelerated tensor code.', 'That feels like confirmation bias and also does not really say anything about the parametric adversarial GANs, which are the focus of the paper.', 'If Neumann optimizer can surpass Adam on ImageNet, I think your algorithm will be widely used after being published.']",,,IEEE Transactions on Industrial Informatics,"['data', 'edge', 'management', 'data management', 'Data Management']"
Data Management,b795c74a0150ec091003ffbaa5bd7d74487c137b,Responsible data management,https://www.semanticscholar.org/paper/b795c74a0150ec091003ffbaa5bd7d74487c137b,JournalArticle,"The need for responsible data management intensifies with the growing impact of data on society. One central locus of the societal impact of data are Automated Decision Systems (ADS), socio-legal-technical systems that are used broadly in industry, non-profits, and government. ADS process data about people, help make decisions that are consequential to people's lives, are designed with the stated goals of improving efficiency and promoting equitable access to opportunity, involve a combination of human and automated decision making, and are subject to auditing for legal compliance and to public disclosure. They may or may not use AI, and may or may not operate with a high degree of autonomy, but they rely heavily on data. In this article, we argue that the data management community is uniquely positioned to lead the responsible design, development, use, and oversight of ADS. We outline a technical research agenda that requires that we step outside our comfort zone of engineering for efficiency and accuracy, to also incorporate reasoning about values and beliefs. This seems high-risk, but one of the upsides is being able to explain to our children what we do and why it matters.",2020,84,13,"[1682824.0, 1686294.0, 145531067.0]",1682824.0,Sofia,3,"[2128664093.0, 134757625.0, 4590999.0]",Y,"['Experiments evaluate the ability to port the model learned in an unsupervised manner to semantic segmentation tasks, using a limited amount of supervision for the end task.', 'I do not think the method is theoretically well-motivated as presented, but the empirical results seem solid.', 'It is not the same task.']",,,Proceedings of the VLDB Endowment,"['data', 'ads', 'management', 'data management', 'Data Management']"
Data Management,799d5a8271887adede035644d878c7bd555576df,Geospatial Data Management Research: Progress and Future Directions,https://www.semanticscholar.org/paper/799d5a8271887adede035644d878c7bd555576df,JournalArticle,"Without geospatial data management, today’s challenges in big data applications such as earth observation, geographic information system/building information modeling (GIS/BIM) integration, and 3D/4D city planning cannot be solved. Furthermore, geospatial data management plays a connecting role between data acquisition, data modelling, data visualization, and data analysis. It enables the continuous availability of geospatial data and the replicability of geospatial data analysis. In the first part of this article, five milestones of geospatial data management research are presented that were achieved during the last decade. The first one reflects advancements in BIM/GIS integration at data, process, and application levels. The second milestone presents theoretical progress by introducing topology as a key concept of geospatial data management. In the third milestone, 3D/4D geospatial data management is described as a key concept for city modelling, including subsurface models. Progress in modelling and visualization of massive geospatial features on web platforms is the fourth milestone which includes discrete global grid systems as an alternative geospatial reference framework. The intensive use of geosensor data sources is the fifth milestone which opens the way to parallel data storage platforms supporting data analysis on geosensors. In the second part of this article, five future directions of geospatial data management research are presented that have the potential to become key research fields of geospatial data management in the next decade. Geo-data science will have the task to extract knowledge from unstructured and structured geospatial data and to bridge the gap between modern information technology concepts and the geo-related sciences. Topology is presented as a powerful and general concept to analyze GIS and BIM data structures and spatial relations that will be of great importance in emerging applications such as smart cities and digital twins. Data-streaming libraries and “in-situ” geo-computing on objects executed directly on the sensors will revolutionize geo-information science and bridge geo-computing with geospatial data management. Advanced geospatial data visualization on web platforms will enable the representation of dynamically changing geospatial features or moving objects’ trajectories. Finally, geospatial data management will support big geospatial data analysis, and graph databases are expected to experience a revival on top of parallel and distributed data stores supporting big geospatial data analysis.",2020,71,9,"[3248270.0, 145663597.0, 46581566.0, 145216339.0, 1576996670.0, 2071346808.0, 1403759150.0, 2418044.0, 1483744000.0]",3248270.0,Oslo,3,"[40833997.0, 119869488.0, 117539586.0]",Y,"['It is conjectured that the observed behaviour has to do with high dimensional geometrie.', ""I don't understand what is offered beyond the original papers."", '3.\tIt would have been good if the paper had the experiments on reconstructing quantitave data from plots and using bounding boxes for providing attention supervision, in order to concretize the usage of these annotations?']",,,ISPRS Int. J. Geo Inf.,"['data', 'management', 'analysis', 'data management', 'Data Management']"
Data Management,98b9086750f08a21c8778ab986339321e9caf790,The Best of Both Worlds: A General Architecture for Data Management in Blockchain-enabled Internet-of-Things,https://www.semanticscholar.org/paper/98b9086750f08a21c8778ab986339321e9caf790,JournalArticle,"The rapid proliferation of Internet-of-Things (IoT) devices has brought great challenges of data management, i.e., storing, retrieving and manipulating a large volume of IoT data. Conventional IoT systems rely on centralized architectures to manage IoT data, hence suffering from limited scalability, lack of transparency, and single point of failure issues. As such, we employ blockchain as a distributed ledger to support the decentralized approach of data management in IoT systems, where IoT data are stored in the deployed blockchain for further utilization, e.g., retrieve and audit. A general architecture combining blockchain and IoT systems is presented. Nevertheless, as the resource constraints of IoT devices may still exist during the process of data transmissions from IoT devices to the blockchain network, we propose a case study of a learning-assisted resource allocation method to support intelligent data management. The numerical results show that the proposed scheme achieves superior performance compared with baseline solutions.",2020,67,34,"[2943819.0, 2145954062.0, 3046954.0, 1713586.0, 144442055.0, 2554658.0]",2943819.0,Andorra,2,"[2074099273.0, 38448016.0]",Y,"['Table 2: It would be good to see standard errors on these numbers; they may be quite high given that they’re only evaluated on 100 examples.', 'In general, the proposed work is very interesting and the idea is neat.']",,,IEEE Network,"['iot', 'data', 'management', 'data management', 'Data Management']"
Data Management,2019cf49b51021a376f9833a53565513f0d8107b,InFeMo: Flexible Big Data Management Through a Federated Cloud System,https://www.semanticscholar.org/paper/2019cf49b51021a376f9833a53565513f0d8107b,JournalArticle,"This paper introduces and describes a novel architecture scenario based on Cloud Computing and counts on the innovative model of Federated Learning. The proposed model is named Integrated Federated Model, with the acronym InFeMo. InFeMo incorporates all the existing Cloud models with a federated learning scenario, as well as other related technologies that may have integrated use with each other, offering a novel integrated scenario. In addition to this, the proposed model is motivated to deliver a more energy efficient system architecture and environment for the users, which aims to the scope of data management. Also, by applying the InFeMo the user would have less waiting time in every procedure queue. The proposed system was built on the resources made available by Cloud Service Providers (CSPs) and by using the PaaS (Platform as a Service) model, in order to be able to handle user requests better and faster. This research tries to fill a scientific gap in the field of federated Cloud systems. Thus, taking advantage of the existing scenarios of FedAvg and CO-OP, we were keen to end up with a new federated scenario that merges these two algorithms, and aiming for a more efficient model that is able to select, depending on the occasion, if it “trains” the model locally in client or globally in server.",2020,56,22,"[144717855.0, 41037178.0, 144901889.0]",144717855.0,Vilnius,2,"[1394243478.0, 2229703159.0]",Y,"['This paper is well-written and easy to follow.', 'The paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optimized.']",,,ACM Trans. Internet Techn.,"['model', 'scenario', 'cloud', 'data management', 'Data Management']"
Data Management,447884e7da189102189a156966623335c72199b0,ACTION-EHR: Patient-Centric Blockchain-Based Electronic Health Record Data Management for Cancer Care,https://www.semanticscholar.org/paper/447884e7da189102189a156966623335c72199b0,JournalArticle,"Background With increased specialization of health care services and high levels of patient mobility, accessing health care services across multiple hospitals or clinics has become very common for diagnosis and treatment, particularly for patients with chronic diseases such as cancer. With informed knowledge of a patient’s history, physicians can make prompt clinical decisions for smarter, safer, and more efficient care. However, due to the privacy and high sensitivity of electronic health records (EHR), most EHR data sharing still happens through fax or mail due to the lack of systematic infrastructure support for secure, trustable health data sharing, which can also cause major delays in patient care. Objective Our goal was to develop a system that will facilitate secure, trustable management, sharing, and aggregation of EHR data. Our patient-centric system allows patients to manage their own health records across multiple hospitals. The system will ensure patient privacy protection and guarantee security with respect to the requirements for health care data management, including the access control policy specified by the patient. Methods We propose a permissioned blockchain-based system for EHR data sharing and integration. Each hospital will provide a blockchain node integrated with its own EHR system to form the blockchain network. A web-based interface will be used for patients and doctors to initiate EHR sharing transactions. We take a hybrid data management approach, where only management metadata will be stored on the chain. Actual EHR data, on the other hand, will be encrypted and stored off-chain in Health Insurance Portability and Accountability Act–compliant cloud-based storage. The system uses public key infrastructure–based asymmetric encryption and digital signatures to secure shared EHR data. Results In collaboration with Stony Brook University Hospital, we developed ACTION-EHR, a system for patient-centric, blockchain-based EHR data sharing and management for patient care, in particular radiation treatment for cancer. The prototype was built on Hyperledger Fabric, an open-source, permissioned blockchain framework. Data sharing transactions were implemented using chaincode and exposed as representational state transfer application programming interfaces used for the web portal for patients and users. The HL7 Fast Healthcare Interoperability Resources standard was adopted to represent shared EHR data, making it easy to interface with hospital EHR systems and integrate a patient’s EHR data. We tested the system in a distributed environment at Stony Brook University using deidentified patient data. Conclusions We studied and developed the critical technology components to enable patient-centric, blockchain-based EHR sharing to support cancer care. The prototype demonstrated the feasibility of our approach as well as some of the major challenges. The next step will be a pilot study with health care providers in both the United States and Switzerland. Our work provides an exemplar testbed to build next-generation EHR sharing infrastructures.",2020,94,22,"[2142586.0, 3419650.0, 2145164466.0, 2059003208.0, 1753286463.0, 2104195713.0, 1920698256.0, 2097767549.0, 1753285996.0, 1753093575.0, 46779638.0, 1751802.0, 2173602.0, 2113586661.0, 1409895301.0]",2142586.0,Podgorica,3,"[49247239.0, 145193818.0, 2241838.0]",Y,"['The generator is a mixture of two Gaussians in one dimension.', 'Another concern is that the authors have not provided sufficient number of examples to show the advantages of their proposed method over the other method (such as FGSM) in generating the adversaries.', ""Regardless - an important note to the authors is that it's a particularly long and verbose paper, coming in at 16 pages of the main paper(!) with nearly 50 (!) pages of supplementary material where the heart and meat of the proofs and experiments reside.""]",,,Journal of Medical Internet Research,"['ehr', 'data', 'health', 'data management', 'Data Management']"
Data Management,28cc044d5ba938472bc53d87240583982ad21663,Data Management for Data Science - Towards Embedded Analytics,https://www.semanticscholar.org/paper/28cc044d5ba938472bc53d87240583982ad21663,Conference,"textabstractThe rise of Data Science has caused an influx of new usersin need of data management solutions. However, insteadof utilizing existing RDBMS solutions they are opting touse a stack of independent solutions for data storage andprocessing glued together by scripting languages. This is notbecause they do not need the functionality that an integratedRDBMS provides, but rather because existing RDBMS im-plementations do not cater to their use case. To solve theseissues, we propose a new class of data management systems:embedded analytical systems. These systems are tightlyintegrated with analytical tools, and provide fast and effi-cient access to the data stored within them. In this work,we describe the unique challenges and opportunities w.r.tworkloads, resilience and cooperation that are faced by thisnew class of systems and the steps we have taken towardsaddressing them in the DuckDB system.",2020,27,,"[3428490.0, 3011964.0]",3428490.0,Tirana,3,"[2193954145.0, 1720381.0, 1939292.0]",Y,"['The papers is well written and clear, and proposes an interesting idea of representing graphs as multi-channel image-like structures.', 'Shouldn’t they be adaptive values with respect to the number of candidate traces found so far?', 'Also, I think it’s a bit of an exaggeration to call a gap of 2.71 nats “much tighter” than a gap of 3.01 nats.']",Conference on Innovative Data Systems Research,20.0,,"['data', 'solutions', 'systems', 'data management', 'Data Management']"
Data Management,e359e8960b0b09e8685a32927b7818f4b06ef881,Intelligent Data Management and Security in Cloud Computing,https://www.semanticscholar.org/paper/e359e8960b0b09e8685a32927b7818f4b06ef881,JournalArticle,"This paper will present the authors’ own techniques of secret data management and protection, with particular attention paid to techniques securing data services. Among the solutions discussed, there will be information-sharing protocols dedicated to the tasks of secret (confidential) data sharing. Such solutions will be presented in an algorithmic form, aimed at solving the tasks of protecting and securing data against unauthorized acquisition. Data-sharing protocols will execute the tasks of securing a special type of information, i.e., data services. The area of data protection will be defined for various levels, within which will be executed the tasks of data management and protection. The authors’ solution concerning securing data with the use of cryptographic threshold techniques used to split the secret among a specified group of secret trustees, simultaneously enhanced by the application of linguistic methods of description of the shared secret, forms a new class of protocols, i.e., intelligent linguistic threshold schemes. The solutions presented in this paper referring to the service management and securing will be dedicated to various levels of data management. These levels could be differentiated both in the structure of a given entity and in its environment. There is a special example thereof, i.e., the cloud management processes. These will also be subject to the assessment of feasibility of application of the discussed protocols in these areas. Presented solutions will be based on the application of an innovative approach, in which we can use a special formal graph for the creation of a secret representation, which can then be divided and transmitted over a distributed network.",2020,34,20,"[2388293.0, 1790345.0, 145655733.0]",2388293.0,London,3,"[49030422.0, 2204964.0, 32371083.0]",Y,"['The authors later describes how to improve this technique by further equating certain observations and exploring only one in each equivalence class.', 'The main difference is in the sentence construction strategy.', 'An encoder is later trained to map real images into the space of latent codes for the generator, allowing the system to be applied to real image segmentation tasks.']",,,Italian National Conference on Sensors,"['data', 'management', 'solutions', 'data management', 'Data Management']"
Data Management,ff74bfbd9ebf4c54809873aecb04be27e9402cb8,Data management for developing digital twin ontology model,https://www.semanticscholar.org/paper/ff74bfbd9ebf4c54809873aecb04be27e9402cb8,JournalArticle,"Digital Twin (DT) is the imitation of the real world product, process or system. Digital Twin is the ideal solution for data-driven optimisations in different phases of the product lifecycle. With the rapid growth in DT research, data management for digital twin is a challenging field for both industries and academia. The challenges for DT data management are analysed in this article are data variety, big data & data mining and DT dynamics. The current research proposes a novel concept of DT ontology model and methodology to address these data management challenges. The DT ontology model captures and models the conceptual knowledge of the DT domain. Using the proposed methodology, such domain knowledge is transformed into a minimum data model structure to map, query and manage databases for DT applications. The proposed research is further validated using a case study based on Condition-Based Monitoring (CBM) DT application. The query formulation around minimum data model structure further shows the effectiveness of the current approach by returning accurate results, along with maintaining semantics and conceptual relationships along DT lifecycle. The method not only provides flexibility to retain knowledge along DT lifecycle but also helps users and developers to design, maintain and query databases effectively for DT applications and systems of different scale and complexities.",2020,31,235,"[2109039649.0, 1891038.0, 1891038.0, 123445664.0, 144782078.0, 2064304090.0, 1911359.0, 98725327.0]",2109039649.0,Belgrade,3,"[1390051934.0, 1749505.0, 51169038.0]",Y,"['Motivation In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle.', 'In the NAS overview section, readers would benefit more if authors spend more time in outlining the RL detail used in the original NAS framework instead of Figure 1 which looks like a space filler.', 'This Lipschitz property has already been proposed by recent methods and has showed some success.']",,,"Proceedings of the Institution of mechanical engineers. Part B, journal of engineering manufacture","['dt', 'data', 'model', 'data management', 'Data Management']"
Data Management,2737a61f6557fe7bf53a608c668de2eff1f582f0,GPU-accelerated data management under the test of time,https://www.semanticscholar.org/paper/2737a61f6557fe7bf53a608c668de2eff1f582f0,Conference,"GPUs are becoming increasingly popular in large scale data center installations due to their strong, embarrassingly parallel, processing capabilities. Data management systems are riding the wave by using GPUs to accelerate query execution, mainly for analytical workloads. However, this acceleration comes at the price of a slow interconnect which imposes strong restrictions in bandwidth and latency when bringing data from the main memory to the GPU for processing. The related research in data management systems mostly relies on late materialization and data sharing to mitigate the overheads introduced by slow interconnects even in the standard CPU processing case. Finally, workload trends move beyond analytical to fresh data processing, typically referred to as Hybrid Transactional and Analytical Processing (HTAP). Therefore, we experience an evolution in three different axes: interconnect technology, GPU architecture, and workload characteristics. In this paper, we break the evolution of the technological landscape into steps and we study the applicability and performance of late materialization and data sharing in each one of them. We demonstrate that the standard PCIe interconnect substantially limits the performance of state-of-the-art GPUs and we propose a hybrid materialization approach which combines eager with lazy data transfers. Further, we show that the wide gap between GPU and PCIe throughput can be bridged through efﬁcient data sharing techniques. Finally, we provide an H 2 TAP system design which removes software-level interference and we show that the interference in the memory bus is minimal, allowing data transfer optimizations as in OLAP workloads",2020,22,,"[10195630.0, 66243223.0, 3416158.0, 88694945.0, 2649908.0, 1728318.0]",10195630.0,Reykjavik,3,"[4478199.0, 1693689.0, 2152798056.0]",Y,"['3. The paper wants to find a good trade-off on speed and accuracy.', 'Cons/Questions/Suggestions The distinction between the convolutional and fully-connected layers (called “classifiers”) in the approach description (sec', 'However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator.']",Conference on Innovative Data Systems Research,20.0,,"['data', 'processing', 'gpus', 'data management', 'Data Management']"
Data Management,d7b820af40a9e2660ef700d39f7b2e27b43435c5,Analysis of Data Management in Blockchain-Based Systems: From Architecture to Governance,https://www.semanticscholar.org/paper/d7b820af40a9e2660ef700d39f7b2e27b43435c5,JournalArticle,"In a blockchain-based system, data and the consensus-based process of recording and updating them over distributed nodes are central to enabling the trustless multi-party transactions. Thus, properly understanding what and how the data are stored and manipulated ultimately determines the degree of utility, performance, and cost of a blockchain-based application. While blockchains enhance the quality of the data by providing a transparent, immutable, and consistent data store, the technology also brings new challenges from a data management perspective. In this paper, we analyse blockchains from the viewpoint of a developer to highlight important concepts and considerations when incorporating a blockchain into a larger software system as a data store. The work aims to increase the level of understanding of blockchain technology as a data store and to promote a methodical approach in applying it to large software systems. First, we identify the common architectural layers of a typical software system with data stores and conceptualise each layer in blockchain terms. Second, we examine the placement and flow of data in blockchain-based applications. Third, we explore data administration aspects for blockchains, especially as a distributed data store. Fourth, we discuss the analytics of blockchain data and trustable data analytics enabled by blockchain. Lastly, we examine the data governance issues in blockchains in terms of privacy and quality assurance.",2019,93,7,"[33548540.0, 3087664.0, 134047389.0, 2248812173.0, 35529345.0]",33548540.0,Nicosia,3,"[3182535.0, 2070951368.0, 8491577.0]",Y,"['The paper is interesting and well-written.', '4. The localization performance of the proposed attention mechanism is evaluated by weakly-supervised semantic segmentation tasks.', 'The paper presents a novel representation of graphs as multi-channel image-like structures.']",,,IEEE Access,"['data', 'store', 'system', 'data management', 'Data Management']"
Data Management,9e66ae24a541255c2d931184498ee116ce81478a,A Cross-Chain Solution to Integrating Multiple Blockchains for IoT Data Management,https://www.semanticscholar.org/paper/9e66ae24a541255c2d931184498ee116ce81478a,JournalArticle,"With the rapid development of the internet of things (IoT), traditional industries are setting off a massive wave of digitization. In the era of the Internet of Everything, millions of devices and links in IoT pose more significant challenges to data management. Most existing solutions employ centralized systems to control IoT devices, which brings about the privacy and security issues in IoT data management. Recently, blockchain has attracted much attention in the field of IoT due to its decentralization, traceability, and non-tamperability. However, it is non-trivial to apply the current blockchain techniques to IoT due to the lack of scalability and high resource costs. Different blockchain platforms have their particular advantages in the scenario of IoT data management. In this paper, we propose a cross-chain framework to integrate multiple blockchains for efficient and secure IoT data management. Our solution builds an interactive decentralized access model which employs a consortium blockchain as the control station. Other blockchain platforms customized for specific IoT scenarios run as the backbone of all IoT devices. It is equivalent to opening the off-chain channels on the consortium blockchain. Our model merges transactions in these channels for confirmation based on the notary mechanism. Finally, we implement a prototype of the proposed model based on hyperledge Fabric and IOTA Tangle. We evaluate the performance of our method through extensive experiments. The results demonstrate the effectiveness and efficiency of our framework.",2019,86,19,"[2146420685.0, 143813668.0, 2115626527.0, 2112264042.0]",2146420685.0,Skopje,3,"[1753223.0, 1957273475.0, 38697713.0]",Y,"['The justification given is that it is ""to address the difficulty of training due to the complex nature of the problem"" but this is not really satisfying as the problems are not that hard.', 'For evaluating whether the data point x is anomalous or not, we search for a latent representation z such that x \\approx g_\\theta(z).', '- It is not clear what Table 2 is showing.']",,,Italian National Conference on Sensors,"['iot', 'data', 'management', 'data management', 'Data Management']"
Data Management,3be6a57d6db95bba2962a1f3476414a0a9b230b5,EdgeCare: Leveraging Edge Computing for Collaborative Data Management in Mobile Healthcare Systems,https://www.semanticscholar.org/paper/3be6a57d6db95bba2962a1f3476414a0a9b230b5,JournalArticle,"With the wide application of mobile healthcare systems, the total amount of healthcare data is ever increasing rapidly as users interact with healthcare service providers frequently. This leads to a challenging task to manage healthcare data. Existing work mainly pay attention to centralized and blockchain-based mechanisms. But they cannot adapt to the increasing amount of global healthcare data and suffer from complex application challenges, respectively. Decentralized and collaborative data management assisted by edge computing exhibits major advantages in improving overall system performance. We present a secure and efficient data management system named as EdgeCare for mobile healthcare systems. Local authorities are established to schedule edge servers for processing healthcare data and facilitating data trading. A hierarchical architecture with collaboration is designed for feasible implementation of EdgeCare. After that, we investigate secure data uploading and sharing in the system. We use an electronic medical record to show how healthcare data is processed with security considerations. We also conduct the Stackelberg game-based optimization algorithm to approach the optimal incentive mechanism for a data collector and users in the fair decentralized data trading. The numerical results with security analysis are provided to demonstrate that EdgeCare offers effective solutions to protect healthcare data, and support efficient data trading.",2019,64,7,"[2118890103.0, 3116552.0, 2109490422.0, 143791274.0, 1731449.0]",2118890103.0,Lisbon,3,"[1707592.0, 1384221131.0, 2206684539.0]",Y,"['The technique systematically interacts with the oracle using observations, which are abstractions of environment states, and it is guaranteed to produce a data set that completely specifies the oracle.', 'Originality: The paper uses previously developed theory of importance sampling.', 'The definition of value function at the bottom of page 4 uses the definition for continual tasks but in the current setting the tasks are naturally episodic.']",,,IEEE Access,"['data', 'healthcare', 'system', 'data management', 'Data Management']"
Data Management,5687c9e8da574453fd873662b95caec70dac9d1e,Framework of an IoT-based Industrial Data Management for Smart Manufacturing,https://www.semanticscholar.org/paper/5687c9e8da574453fd873662b95caec70dac9d1e,JournalArticle,"The Internet of Things (IoT) is the global network of interrelated physical devices such as sensors, actuators, smart applications, objects, computing devices, mechanical machines, and people that are becoming an essential part of the internet. In an industrial environment, these devices are the source of data which provide abundant information in manufacturing processes. Nevertheless, the massive, heterogeneous, and time-sensitive nature of the data brings substantial challenges to the real-time collection, processing, and decision making. Therefore, this paper presents a framework of an IoT-based Industrial Data Management System (IDMS) which can manage the huge industrial data, support online monitoring, and control smart manufacturing. The framework contains five basic layers such as physical, network, middleware, database, and application layers to provide a service-oriented architecture for the end users. Experimental results from a smart factory case study demonstrate that the framework can manage the regular data and urgent events generated from various factory devices in the distributed industrial environment through state-of-the-art communication protocols. The collected data is converted into useful information which improves productivity and the prognosis of production lines.",2019,67,8,"[145429147.0, 1784862.0, 33018846.0, 2108614875.0]",145429147.0,Moscow,3,"[2231158981.0, 2992833.0, 31910622.0]",Y,"['This reward function is used to train a separate RNN for each desired program.', 'It would also be great to integrate a direct regularization mechanism in the cost  in order to do so.', 'I would like to see the same spectra included.']",,,J. Sens. Actuator Networks,"['data', 'devices', 'framework', 'data management', 'Data Management']"
Data Management,b05306f0b142e5afb3974b1b79996e5b82653662,Rethinking Data Management Systems for Disaggregated Data Centers,https://www.semanticscholar.org/paper/b05306f0b142e5afb3974b1b79996e5b82653662,Conference,"One recent trend of cloud data center design is resource disaggregation . Instead of having server units with “converged” compute, memory, and storage resources, a disaggregated data center (DDC) has pools of resources of each type connected via a network. While the systems community has been investigating the research challenges of DDC by designing new OS and network stacks, the implications of DDC for next-generation database systems remain unclear. In this paper, we take a ﬁrst step towards understanding how DDCs might affect the design of relational databases, discuss the potential advantages and drawbacks in the context of data processing, and outline research challenges in addressing them.",2020,24,,"[2112197162.0, 2111027355.0, 143857631.0, 144237796.0, 30894196.0, 35206168.0]",2112197162.0,Ljubljana,3,"[2093481779.0, 2144151933.0, 123445664.0]",Y,"['The idea of enforcing information isolation is brilliant.', 'A thorough exploration of techniques for unsupervised translation, a very strong start for this problem This paper describes an approach to train a neural machine translation system without parallel data.', 'While LINE or SDNE, which the authors cite, may not run on some of the larger datasets they can be tested on the smaller datasets.']",Conference on Innovative Data Systems Research,20.0,,"['data', 'ddc', 'center', 'data management', 'Data Management']"
Data Management,676664ee7471738577f641e6159e7596625b7fdb,Ten principles for machine-actionable data management plans,https://www.semanticscholar.org/paper/676664ee7471738577f641e6159e7596625b7fdb,JournalArticle,"Data management plans (DMPs) are documents accompanying research proposals and project outputs. DMPs are created as free-form text and describe the data and tools employed in scientific investigations. They are often seen as an administrative exercise and not as an integral part of research practice. There is now widespread recognition that the DMP can have more thematic, machine-actionable richness with added value for all stakeholders: researchers, funders, repository managers, research administrators, data librarians, and others. The research community is moving toward a shared goal of making DMPs machine-actionable to improve the experience for all involved by exchanging information across research tools and systems and embedding DMPs in existing workflows. This will enable parts of the DMP to be automatically generated and shared, thus reducing administrative burdens and improving the quality of information within a DMP. This paper presents 10 principles to put machine-actionable DMPs (maDMPs) into practice and realize their benefits. The principles contain specific actions that various stakeholders are already undertaking or should undertake in order to work together across research communities to achieve the larger aims of the principles themselves. We describe existing initiatives to highlight how much progress has already been made toward achieving the goals of maDMPs as well as a call to action for those who wish to get involved.",2019,49,15,"[1752597.0, 145730688.0, 1772144.0, 144783437.0]",1752597.0,Minsk,2,"[2218973562.0, 2634786.0]",Y,"['The proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero.', 'This analysis is conducted by drawing on results from the field of critical percolation in physics.']",,,PLoS Comput. Biol.,"['research', 'data', 'dmp', 'data management', 'Data Management']"
Data Management,eebf1a2705bf4ac256d141b0067f1b0ea1dc7632,Data Management Challenges for Deep Learning,https://www.semanticscholar.org/paper/eebf1a2705bf4ac256d141b0067f1b0ea1dc7632,Conference,"Deep learning is one of the most exciting and fast-growing techniques in Artificial Intelligence. The unique capacity of deep learning models to automatically learn patterns from the data differentiates it from other machine learning techniques. Deep learning is responsible for a significant number of recent breakthroughs in AI. However, deep learning models are highly dependent on the underlying data. So, consistency, accuracy, and completeness of data is essential for a deep learning model. Thus, data management principles and practices need to be adopted throughout the development process of deep learning models. The objective of this study is to identify and categorise data management challenges faced by practitioners in different stages of end-to-end development. In this paper, a case study approach is employed to explore the data management issues faced by practitioners across various domains when they use real-world data for training and deploying deep learning models. Our case study is intended to provide valuable insights to the deep learning community as well as for data scientists to guide discussion and future research in applied deep learning with real-world data.",2019,48,,"[1423751237.0, 144913779.0, 40225855.0, 1826416.0, 1939487.0]",1423751237.0,Monaco,2,"[1402912902.0, 22654490.0]",Y,"['Interesting experiments but lack of model description The authors propose to use a byte level RNN to classify reviews.', 'If the contribution of the paper is the ""output stochastic"" noise model, I think it is worth experimenting with the design options one has with such a model.']",EUROMICRO Conference on Software Engineering and Advanced Applications,45.0,,"['data', 'learning', 'models', 'data management', 'Data Management']"
Data Management,6ba00c2386f2edc0b43eec442cd1923b5d964633,"""Data Stewardship Wizard"": A Tool Bringing Together Researchers, Data Stewards, and Data Experts around Data Management Planning",https://www.semanticscholar.org/paper/6ba00c2386f2edc0b43eec442cd1923b5d964633,JournalArticle,"The Data Stewardship Wizard is a tool for data management planning that is focused on getting the most value out of data management planning for the project itself rather than on fulfilling obligations. It is based on FAIR Data Stewardship, in which each data-related decision in a project acts to optimize the Findability, Accessibility, Interoperability and/or Reusability of the data. The background to this philosophy is that the first reuser of the data is the researcher themselves. The tool encourages the consulting of expertise and experts, can help researchers avoid risks they did not know they would encounter by confronting them with practical experience from others, and can help them discover helpful technologies they did not know existed. In this paper, we discuss the context and motivation for the tool, we explain its architecture and we present key functions, such as the knowledge model evolvability and migrations, assembling data management plans, metrics and evaluation of data management plans.",2019,36,18,"[2810600.0, 50371588.0, 69513411.0, 1394243406.0, 1394243478.0]",2810600.0,Lisbon,2,"[2158994272.0, 144547315.0]",Y,"['Good paper, pushing the limits of RL to harder tasks.', 'I do not think the method is theoretically well-motivated as presented, but the empirical results seem solid.']",,,Data Science Journal,"['data', 'management', 'tool', 'data management', 'Data Management']"
Data Management,7af07490da518c8ef3cf2ae106071df2c2d0101e,Patient-generated health data management and quality challenges in remote patient monitoring,https://www.semanticscholar.org/paper/7af07490da518c8ef3cf2ae106071df2c2d0101e,JournalArticle,"Abstract Background Patient-Generated Health Data (PGHD) in remote monitoring programs is a promising source of precise, personalized data, encouraged by expanding growth in the health technologies market. However, PGHD utilization in clinical settings is low. One of the critical challenges that impedes confident clinical use of PGHD is that these data are not managed according to any recognized approach for data quality assurance. Objective This article aims to identify the PGHD management and quality challenges that such an approach must address, as these are expressed by key PGHD stakeholder groups. Materials and Methods In-depth interviews were conducted with 20 experts who have experience in the use of PGHD in remote patient monitoring, including: healthcare providers, health information professionals within clinical settings, and commercial providers of remote monitoring solutions. Participants were asked to describe PGHD management processes in the remote monitoring programs in which they are involved, and to express their perspectives on PGHD quality challenges during the data management stages. Results The remote monitoring programs in the study did not follow clear PGHD management or quality assurance approach. Participants were not fully aware of all the considerations of PGHD quality. Digital health literacy, wearable accuracy, difficulty in data interpretation, and lack of PGHD integration with electronic medical record systems were among the key challenges identified that impact PGHD quality. Conclusion Co-development of PGHD quality guidelines with relevant stakeholders, including patients, is needed to ensure that quality remote monitoring data from wearables is available for use in more precise and personalized patient care.",2019,75,2,"[5840536.0, 144370168.0, 2189206.0, 144132699.0]",5840536.0,Bratislava,2,"[2921637.0, 1806271.0]",Y,"['One direction to strengthen this paper is to examine if RMN can do better than pair-wise interactions (and other baselines) for more complex reasoning tasks.', 'I believe that this paper is severely flawed.']",,,JAMIA Open,"['quality', 'data', 'pghd', 'data management', 'Data Management']"
Data Management,cf41991d89301c3c12420d150792cb1163999962,Data Management in Supply Chain Using Blockchain: Challenges and a Case Study,https://www.semanticscholar.org/paper/cf41991d89301c3c12420d150792cb1163999962,Conference,"Supply chain management (SCM) is fundamental for gaining financial, environmental and social benefits in the supply chain industry. However, traditional SCM mechanisms usually suffer from a wide scope of issues such as lack of information sharing, long delays for data retrieval, and unreliability in product tracing. Recent advances in blockchain technology show great potential to tackle these issues due to its salient features including immutability, transparency, and decentralization. Although there are some proof-of-concept studies and surveys on blockchain-based SCM from the perspective of logistics, the underlying technical challenges are not clearly identified. In this paper, we provide a comprehensive analysis of potential opportunities, new requirements, and principles of designing blockchain-based SCM systems. We summarize and discuss four crucial technical challenges in terms of scalability, throughput, access control, data retrieval and review the promising solutions. Finally, a case study of designing blockchain-based food traceability system is reported to provide more insights on how to tackle these technical challenges in practice.",2019,71,,"[46476972.0, 144115026.0, 2108824565.0, 1380530147.0, 1388034901.0, 2065096893.0, 2152801260.0, 1524736374.0, 48362791.0]",46476972.0,Zagreb,3,"[1688882.0, 145720154.0, 1737629.0]",Y,"['These five objectives together are sufficient to achieve impressive English <-> German and Engish <-> French results in Multi30k, a bilingual image caption scenario with short simple sentences, and to achieve a strong start for a standard WMT scenario.', 'The authors do provide a novel formulation and demonstrate the gains on a variety of concrete problems taken form the literature.', '[ds]SSM is integrated with I2A, which generates rollouts of future states, based on the inferred hidden states from the d/sSSM-VAE model.']",International Conference on Computer Communications and Networks,28.0,,"['scm', 'challenges', 'supply', 'data management', 'Data Management']"
Data Management,3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8,Big Data Management Canvas: A Reference Model for Value Creation from Data,https://www.semanticscholar.org/paper/3b19ca7f051b7bd7ef0f2b1834c4823aca8a01c8,JournalArticle,"Many big data projects are technology-driven and thus, expensive and inefficient. It is often unclear how to exploit existing data resources and map data, systems and analytics results to actual use cases. Existing big data reference models are mostly either technological or business-oriented in nature, but do not consequently align both aspects. To address this issue, a reference model for big data management is proposed that operationalizes value creation from big data by linking business targets with technical implementation. The purpose of this model is to provide a goal- and value-oriented framework to effectively map and plan purposeful big data systems aligned with a clear value proposition. Based on an epistemic model that conceptualizes big data management as a cognitive system, the solution space of data value creation is divided into five layers: preparation, analysis, interaction, effectuation, and intelligence. To operationalize the model, each of these layers is subdivided into corresponding business and IT aspects to create a link from use cases to technological implementation. The resulting reference model, the big data management canvas, can be applied to classify and extend existing big data applications and to derive and plan new big data solutions, visions, and strategies for future projects. To validate the model in the context of existing information systems, the paper describes three cases of big data management in existing companies.",2019,27,3,[143918410.0],143918410.0,Kiev,3,"[5548191.0, 144294520.0, 2151529879.0]",Y,"[""The protagonist is attempting to achieve the given task while the antagonist's goal is for the task to fail."", 'My main concerns are related to the contribution of the paper and experimental pipeline followed to perform the comparison.', 'There is no easy way to fix this and for the purpose of sampling the paper simply treats the weights as immutable.']",,,Big Data and Cognitive Computing,"['data', 'model', 'management', 'data management', 'Data Management']"
Data Management,6dd3e404aac097f63d42dc19fd08c7ec281f90b4,Qualitative Data Management and Analysis within a Data Repository,https://www.semanticscholar.org/paper/6dd3e404aac097f63d42dc19fd08c7ec281f90b4,JournalArticle,"Data repositories can support secure data management for multi-institutional and geographically dispersed research teams. Primarily designed to provide secure access, storage, and sharing of quantitative data, limited focus has been given to the unique considerations of data repositories for qualitative research. We share our experiences of using a data repository in a large qualitative nursing research study. Over a 27-month period, data collected by this 15-member team from 83 participants included photos, audio recordings and transcripts of interviews, and field notes. The data repository supported the secure collection, storage, and management of over 1,800 files with data. However, challenges were introduced during analysis that required negotiations about the structure and processes of the data repository. We discuss strengths and limitations of data repositories, and introduce practical strategies for developing a data management plan for qualitative research, which is supported through a data repository.",2019,20,42,"[48232006.0, 1390140002.0, 5973699.0, 3971602.0, 14746788.0, 6429295.0]",48232006.0,Tirana,3,"[2181284180.0, 2152663837.0, 2053320598.0]",Y,"['This would require a phi(s) function that is trained in a way that doesn’t depend on the action a.', 'In light of this, I think it is reasonable to ignore the fact that the proposed initialization does not provide benefits on Penn Treebank and text8.', 'Also, I think it’s a bit of an exaggeration to call a gap of 2.71 nats “much tighter” than a gap of 3.01 nats.']",,,Western Journal of Nursing Research,"['data', 'research', 'repositories', 'data management', 'Data Management']"
Data Management,d7fddafbbc372da4fa884f67bdc32db71b888806,IMG/VR v.2.0: an integrated data management and analysis system for cultivated and environmental viral genomes,https://www.semanticscholar.org/paper/d7fddafbbc372da4fa884f67bdc32db71b888806,JournalArticle,"Abstract The Integrated Microbial Genome/Virus (IMG/VR) system v.2.0 (https://img.jgi.doe.gov/vr/) is the largest publicly available data management and analysis platform dedicated to viral genomics. Since the last report published in the 2016, NAR Database Issue, the data has tripled in size and currently contains genomes of 8389 cultivated reference viruses, 12 498 previously published curated prophages derived from cultivated microbial isolates, and 735 112 viral genomic fragments computationally predicted from assembled shotgun metagenomes. Nearly 60% of the viral genomes and genome fragments are clustered into 110 384 viral Operational Taxonomic Units (vOTUs) with two or more members. To improve data quality and predictions of host specificity, IMG/VR v.2.0 now separates prokaryotic and eukaryotic viruses, utilizes known prophage sequences to improve taxonomic assignments, and provides viral genome quality scores based on the estimated genome completeness. New features also include enhanced BLAST search capabilities for external queries. Finally, geographic map visualization to locate user-selected viral genomes or genome fragments has been implemented and download options have been extended. All of these features make IMG/VR v.2.0 a key resource for the study of viruses.",2018,142,47,"[1396822935.0, 52161096.0, 2956683.0, 2412941.0, 39517703.0, 2285301474.0, 2078394.0, 144896762.0, 2254214902.0, 1702316.0, 1396822962.0, 2241382793.0, 2805081.0]",1396822935.0,Minsk,2,"[153311051.0, 3050846.0]",Y,"['One thing I hope the author could provide more clarification is the use of NER.', '1. IPS for losses<0 and risk minimization: raise the probability of every sample in the log irrespective of the loss itself']",,,Nucleic Acids Res.,"['data', 'viruses', 'fragments', 'data management', 'Data Management']"
Data Management,f6dab84c2c00ab92d8ee9d9359d7e530512114f9,"Finance Big Data: Management, Analysis, and Applications",https://www.semanticscholar.org/paper/f6dab84c2c00ab92d8ee9d9359d7e530512114f9,JournalArticle,"Big Data is an emerging paradigm in almost all industries. Finance big data (FBD) is becoming one of the most promising areas of management and governance in the financial sector. It is significantly changing business models in financial companies. Many researchers argue that Big Data is fueling the transformation of finance and business at-large in the ways that we cannot as yet assess. A new research area is evolving to study quantitative models and econometric approaches for financial studies that can bridge the gap between empirical finance research and data science. In this fascinating area, experts and scientists can propose novel finance business models by using the Big Data methods, present sophisticated methods for risk control with machine learning tools, provide visualization tools for financial markets analysis, create new finance sentiment indexes by mining public feelings from the massive textual data from social networks, and deploy the information-based tools in other creative ways. Due to the 4V characteristics of Big Data—volume (large data scale), velocity (real-time data streaming), variety (different data formats), and veracity (data uncertainty)—a long list of challenges for FBD management, analytics, and applications exists. These challenges include (1) to organize and manage FBD in effective and efficient ways; (2) to find novel business models from FBD analytics; (3) to handle traditional finance issues like high-frequency trading, sentiments, credit risk, financial analysis, risk management and regulation, and others, in creative Big Data–driven ways; (4) to integrate the variety of heterogeneous data from different sources; and (5) to ensure the security and safety of finance systems and to protect the individual privacy in view of the availability of Big Data. To meet these challenges, we need fundamental research on both data analytics technology and finance business. This special issue, “Finance Big Data: Management, Analysis, and Applications,” of International Journal of Electronic Commerce, is motivated by the need to meet the challenges of the fast development of finance big data. The papers brought together in this special issue highlight research efforts focused on the development of methods, tools, and techniques for the handling of various aspects of FBD from academia and industries. Viktor Manahov and Hanxiong Zhang, in “Forecasting Financial Markets Using High-Frequency Trading Data: Examination with Strongly Typed Genetic Programming,” develop an artificial futures market populated with high-frequency (HF) traders and institutional traders using Strongly Typed Genetic Programming trading algorithm. The authors simulate real-life futures trading at the millisecond time frame by applying Strongly Typed",2019,30,23,"[152354805.0, 48081155.0, 2148904286.0]",152354805.0,Valletta,3,"[1720266.0, 145990580.0, 9264826.0]",Y,"['The authors propose to train a generator network in combination with the classifier and an adversarial discriminator.', 'In the videos, it seems that the people and chairs are always in the same place.', 'Minor comments: In Section 2.2, is Layer 1 the input layer or the next?']",,,International Journal of Electronic Commerce,"['data', 'finance', 'fbd', 'data management', 'Data Management']"
Data Management,9727206903eb40d4fa42606711bad3402f2ba9aa,Decentralized IoT Data Management Using BlockChain and Trusted Execution Environment,https://www.semanticscholar.org/paper/9727206903eb40d4fa42606711bad3402f2ba9aa,Conference,"Due to the centralization of authority in the management of data generated by IoT devices, there is a lack of transparency in how user data is being shared among third party entities. With the popularity of adoption of blockchain technology, which provide decentralized management of assets such as currency as seen in Bitcoin, we propose a decentralized system of data management for IoT devices where all data access permission is en-forced using smart contracts and the audit trail of data access is stored in the blockchain. With smart contracts applications, multiple parties can specify rules to govern their interactions which is independently enforced in the blockchain without the need for a centralized system. We provide a framework that store the hash of the data in the blockchain and store the raw data in a secure storage platform using trusted execution environment (TEE). In particular, we consider Intel SGX as a part of TEE that ensure data security and privacy for sensitive part of the application (code and data).",2018,108,,"[39712836.0, 38804742.0, 145155297.0, 3071249.0]",39712836.0,Vienna,3,"[1630331317.0, 152355659.0, 2855934.0]",Y,"['2) compressing the embedding space using pca', 'The basic idea is to use a multi-step dynamics model as a ""baseline"" (more properly a control variate, as the terminology in the paper uses, but I think baselines are more familiar to the RL community) to reduce the variance of a policy gradient estimator, while remaining unbiased.', ""Creative and interesting The paper introduces an application of Graph Neural Networks (Li's Gated Graph Neural Nets, GGNNs, specifically) for reasoning about programs and programming.""]",IEEE International Conference on Information Reuse and Integration,18.0,,"['data', 'management', 'iot', 'data management', 'Data Management']"
Data Modeling,b7f5973dbf2ef76fcc3aea616a7f72ca79f09020,Towards high-precision data modeling of SHM measurements using an improved sparse Bayesian learning scheme with strong generalization ability,https://www.semanticscholar.org/paper/b7f5973dbf2ef76fcc3aea616a7f72ca79f09020,JournalArticle,"Central to structural health monitoring (SHM) is data modeling, manipulation, and interpretation on the basis of a sophisticated SHM system. Despite continuous evolution of SHM technology, the precise modeling and forecasting of SHM measurements under various uncertainties to extract structural condition-relevant knowledge remains a challenge. Aiming to resolve this problem, a novel application of a fully probabilistic and high-precision data modeling method was proposed in the context of an improved Sparse Bayesian Learning (iSBL) scheme. The proposed iSBL data modeling framework features the following merits. It can remove the need to specify the number of terms in the data-fitting function, and automatize sparsity of the Bayesian model based on the features of SHM monitoring data, which will enhance the generalization ability and then improve the data prediction accuracy. Embedded in a Bayesian framework which exhibits built-in protection against over-fitting problems, the proposed iSBL scheme has high robustness to data noise, especially for data forecasting. The model is verified to be effective on SHM vibration field monitoring data collected from a real-world large-scale cable-stayed bridge. The recorded acceleration data with two different vibration patterns, that is, stationary ambient vibration data and non-stationary decay vibration data, are investigated, returning accurate probabilistic predictions in both the time and frequency domains.",2023,12,23,"[2153282879.0, 2181284180.0, 2160872910.0, 2171114745.0, 100841270.0, 50040606.0, 3124770.0, 2204699413.0, 2218326293.0, 2217936172.0]",2153282879.0,Berlin,3,"[1724282.0, 39231399.0, 2145403564.0]",Y,"['Not that there is anything wrong with short papers, but in this case both the clarity of presentation and details are lacking.', 'For the denoising comparison, how do the results compare to those obtained if you simulate a Markov Chain (sample latent state conditioned on noisy image, sample latent state, sample denoised observation, repeat using denoised observation) using a VAE?', 'Since the authors combine their method with a GAN, it is not surprising that the generated images look more realistic.']",,,Structural Health Monitoring,"['data', 'shm', 'modeling', 'data modeling', 'Data Modeling']"
Data Modeling,aea731e7cf33aa3d482b13f42cedbc1adb3271c6,"The “Problem” of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation",https://www.semanticscholar.org/paper/aea731e7cf33aa3d482b13f42cedbc1adb3271c6,Conference,"Human variation in labeling is often considered noise. Annotation projects for machine learning (ML) aim at minimizing human label variation, with the assumption to maximize data quality and in turn optimize and maximize machine learning metrics. However, thisconventional practice assumes that there exists a *ground truth*, and neglects that there exists genuine human variation in labeling due to disagreement, subjectivity in annotation or multiple plausible answers.In this position paper, we argue that this big open problem of human label variation persists and critically needs more attention to move our field forward. This is because human label variation impacts all stages of the ML pipeline: *data, modeling and evaluation*. However, few works consider all of these dimensions jointly; and existing research is fragmented. We reconcile different previously proposed notions of human label variation, provide a repository of publicly-available datasets with un-aggregated labels, depict approaches proposed so far, identify gaps and suggest ways forward. As datasets are becoming increasingly available, we hope that this synthesized view on the “problem” will lead to an open discussion on possible strategies to devise fundamentally new directions.",2022,58,,[2022124.0],2022124.0,Dublin,3,"[151500725.0, 144259957.0, 2223551699.0]",Y,"['Specifically, for KDE and OC-SVM, a naive PCA is used to reduce the data dimension.', 'The proposed method achieves perfect accuracy in every condition.', 'Review This paper describes a method for computing representations for out-of-vocabulary words, e.g. based on their spelling or dictionary definitions.']",Conference on Empirical Methods in Natural Language Processing,22.0,,"['variation', 'label', 'labeling', 'data modeling', 'Data Modeling']"
Data Modeling,a2ec47b9bcc95d2456a8a42199233e5d9129ef18,TabTransformer: Tabular Data Modeling Using Contextual Embeddings,https://www.semanticscholar.org/paper/a2ec47b9bcc95d2456a8a42199233e5d9129ef18,JournalArticle,"We propose TabTransformer, a novel deep tabular data modeling architecture for supervised and semi-supervised learning. The TabTransformer is built upon self-attention based Transformers. The Transformer layers transform the embeddings of categorical features into robust contextual embeddings to achieve higher prediction accuracy. Through extensive experiments on fifteen publicly available datasets, we show that the TabTransformer outperforms the state-of-the-art deep learning methods for tabular data by at least 1.0% on mean AUC, and matches the performance of tree-based ensemble models. Furthermore, we demonstrate that the contextual embeddings learned from TabTransformer are highly robust against both missing and noisy data features, and provide better interpretability. Lastly, for the semi-supervised setting we develop an unsupervised pre-training procedure to learn data-driven contextual embeddings, resulting in an average 2.1% AUC lift over the state-of-the-art methods.",2020,227,abs/2012.06678,"[2152663837.0, 3083159.0, 2008176653.0, 3386660.0]",2152663837.0,Nicosia,3,"[102544543.0, 2117786875.0, 27316199.0]",Y,"['Presents an interesting idea fairly clearly, but overall very similar to expectation-linear dropout.', 'Currently, I don’t understand from the manuscript, how DQN is actually trained.', 'Interesting work that could be grounded more strongly in cognitive science This paper presents an analysis of the properties of agents who learn grounded language through reinforcement learning in a simple environment that combines verbal instruction with visual information.']",,,arXiv.org,"['embeddings', 'data', 'learning', 'data modeling', 'Data Modeling']"
Data Modeling,a85c45ce7c893388e8eafa8a653b042e1497db48,Cross-Node Federated Graph Neural Network for Spatio-Temporal Data Modeling,https://www.semanticscholar.org/paper/a85c45ce7c893388e8eafa8a653b042e1497db48,Conference,"Vast amount of data generated from networks of sensors, wearables, and the Internet of Things (IoT) devices underscores the need for advanced modeling techniques that leverage the spatio-temporal structure of decentralized data due to the need for edge computation and licensing (data access) issues. While federated learning (FL) has emerged as a framework for model training without requiring direct data sharing and exchange, effectively modeling the complex spatio-temporal dependencies to improve forecasting capabilities still remains an open problem. On the other hand, state-of-the-art spatio-temporal forecasting models assume unfettered access to the data, neglecting constraints on data sharing. To bridge this gap, we propose a federated spatio-temporal model -- Cross-Node Federated Graph Neural Network (CNFGNN) -- which explicitly encodes the underlying graph structure using graph neural network (GNN)-based architecture under the constraint of cross-node federated learning, which requires that data in a network of nodes is generated locally on each node and remains decentralized. CNFGNN operates by disentangling the temporal dynamics modeling on devices and spatial dynamics on the server, utilizing alternating optimization to reduce the communication cost, facilitating computations on the edge devices. Experiments on the traffic flow forecasting task show that CNFGNN achieves the best forecasting performance in both transductive and inductive learning settings with no extra computation cost on edge devices, while incurring modest communication cost.",2021,70,,"[27737939.0, 2267664.0, 47909531.0]",27737939.0,Riga,3,"[2837279.0, 1792647.0, 1720266.0]",Y,"['2. Indeed, AlexNet is a good seedbed to test binary methods.', 'The proposed method is new and technically sound.', '- I suggest to divide Section 3.1 in two subsections.']",Knowledge Discovery and Data Mining,27.0,,"['data', 'edge', 'forecasting', 'data modeling', 'Data Modeling']"
Data Modeling,73cbaf5f2441ef3478266b5438c0e90d1ae71652,Unsupervised Grouped Axial Data Modeling via Hierarchical Bayesian Nonparametric Models With Watson Distributions,https://www.semanticscholar.org/paper/73cbaf5f2441ef3478266b5438c0e90d1ae71652,JournalArticle,"This paper aims at proposing an unsupervised hierarchical nonparametric Bayesian framework for modeling axial data (i.e., observations are axes of direction) that can be partitioned into multiple groups, where each observation within a group is sampled from a mixture of Watson distributions with an infinite number of components that are allowed to be shared across different groups. First, we propose a hierarchical nonparametric Bayesian model for modeling grouped axial data based on the hierarchical Pitman-Yor process mixture model of Watson distributions. Then, we demonstrate that by setting the discount parameters of the proposed model to 0, another hierarchical nonparametric Bayesian model based on hierarchical Dirichlet process can be derived for modeling axial data. To learn the proposed models, we systematically develop a closed-form optimization algorithm based on the collapsed variational Bayes (CVB) inference. Furthermore, to ensure the convergence of the proposed learning algorithm, an annealing mechanism is introduced to the framework of CVB inference, leading to an averaged collapsed variational Bayes inference strategy. The merits of the proposed models for modeling grouped axial data are demonstrated through experiments on both synthetic data and real-world applications involving gene expression data clustering and depth image analysis.",2021,36,44,"[2038786.0, 2155558174.0, 1729109.0]",2038786.0,Rome,3,"[2119204984.0, 144695135.0, 3776937.0]",Y,"['Classifier is trained to not only maximize classification accuracy on the real training data but also to output a uniform distribution for the generated samples.', 'Variance Reduction for Stochastic Gradient Optimization, NIPS.', 'Are the test environments sufficiently different from the training ones?']",,,IEEE Transactions on Pattern Analysis and Machine Intelligence,"['data', 'model', 'inference', 'data modeling', 'Data Modeling']"
Data Modeling,d916776e0c6a04b0def4c22257c188776c2edab2,Industrial Big Data Modeling and Monitoring Framework for Plant-Wide Processes,https://www.semanticscholar.org/paper/d916776e0c6a04b0def4c22257c188776c2edab2,JournalArticle,"This article proposes a distributed parallel modeling and monitoring framework for plant-wide processes with big data. The “distributed” contains two layers of meaning. One is the spatially distributed modeling and hierarchical monitoring for the plant-wide process with multiple operating units. The other represents the distributed parallel modeling for big process data with various features. Under the framework, the distributed parallel mixture probabilistic latent variable model is proposed based on the stochastic variational inference algorithm and the parameter server architecture to cope with the big process data. Then, the model is utilized to develop the plant-wide hierarchical and distributed process monitoring algorithms, where the multilevel monitoring indexes and fault contribution indexes are established based on the Bayesian fusion algorithm for process fault detection and diagnosis. The performance comparison and visualization for the industrial plant-wide process case has demonstrated the reliability and superiority of the proposed algorithm and framework.",2021,24,17,"[48028582.0, 145619185.0]",48028582.0,Berlin,2,"[2119543279.0, 2055642882.0]",Y,"['Some wall clock time results or FLOPs of train/test time should be provided since you use multiple hops.', 'It seems the Blizzard results in Figure 2 are missing no reconstruction loss + full backprop.']",,,IEEE Transactions on Industrial Informatics,"['process', 'monitoring', 'plantwide', 'data modeling', 'Data Modeling']"
Data Modeling,8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78,Multidimensional Data Modeling and Model Validation for Digital Twin Workshop,https://www.semanticscholar.org/paper/8cb6b81d333f907a3d9d0aa4fb4859bf5984ef78,JournalArticle,"
 Digital twin workshop (DTW) is an important embodiment of intelligent manufacturing in the workshop level, which enables the smart production control and management of the workshop. However, there still exist problems including data modeling and verification of digital model in the process of DTW construction. To solve these problem, multidimensional data modeling and model validation methods of DTW are proposed in this article. First, five-order tensor models for representing manufacturing elements are established to unify the data from physical workshop (PW) and virtual workshop (VW). Then, the mathematical method for verifying DTW twin model is proposed from the recessive and explicit perspective. Finally, a case study of an aerospace machining workshop is carried out to verify the operability and effectiveness of the proposed method. The case analysis shows that the proposed methods can effectively evaluate whether the twin model accurately provides the description of the actual behavior process of physical workshop, and the proposed methods have good performance.",2021,12,21,"[2105608191.0, 2143379026.0, 2051655406.0, 2052219438.0, 9319875.0, 1768360511.0]",2105608191.0,Nicosia,2,"[1919541.0, 2118902636.0]",Y,"['The method leads to better performance than using no external resources, but not as high performance as using Glove embeddings.', 'Note that after I skimmed through the submissions to this conference, there seem to be interesting papers on the topic of gradients.']",,,Journal of Computing and Information Science in Engineering,"['workshop', 'dtw', 'model', 'data modeling', 'Data Modeling']"
Data Modeling,b266510f5f9b40d42b51884ad13a1867fb3284fd,Conceptual Data Modeling: Entity-Relationship Models as Thinging Machines,https://www.semanticscholar.org/paper/b266510f5f9b40d42b51884ad13a1867fb3284fd,JournalArticle,"Data modeling is a process of developing a model to design and develop a data system that supports an organization s various business processes. A conceptual data model represents a technology-independent specification of structure of data to be stored within a database. The model aims at providing richer expressiveness and incorporating a set of semantics to (a) support the design, control, and integrity parts of the data stored in data management structures and (b) coordinate viewing of connections and ideas on a database. The described structure of the data is often represented in an entity-relationship (ER) model, which was one of the first data-modeling techniques and is likely to continue to be a popular way of characterizing entity classes, attributes and relationships. This paper is an attempt to examine the basic ER modeling notions to analyze the concepts to which they refer as well as ways to represent them. In such a mission, we apply a new modeling methodology (thinging machine; TM) to ER in terms of its fundamental building constructs, representation entities, relationships and attributes. The goal of this venture is to further the understanding of data models and enrich their semantics. Three specific contributions to modeling in this context are incorporated: (a) using the TM model s five generic actions to inject processing in the ER structure; (b) relating the single ontological element of TM modeling (i.e., a thing/machine or thimac) to ER entities and relationships; and (c) proposing a high-level integrated, extended ER model that includes structural and time-oriented notions (e.g., events or behavior).",2021,8,abs/2109.14717,[1405319717.0],1405319717.0,Tallinn,2,"[144171096.0, 2144495208.0]",Y,"['How do we interpret the generated heatmap?', 'This is different from the standard bag-of-words model where words are sampled independently and word counts do matter.']",,,arXiv.org,"['data', 'model', 'modeling', 'data modeling', 'Data Modeling']"
Data Modeling,244e08e109f6f4fa0f846977e1aef1e7b6a9e816,Robust Online Sequential RVFLNs for Data Modeling of Dynamic Time-Varying Systems With Application of an Ironmaking Blast Furnace,https://www.semanticscholar.org/paper/244e08e109f6f4fa0f846977e1aef1e7b6a9e816,JournalArticle,"By dealing with robust modeling and online learning together in a unified random vector functional-link networks (RVFLNs) framework, this paper presents a novel robust online sequential RVFLNs for data modeling of dynamic time-varying systems together with its application for a blast furnace (BF) ironmaking process. First, to overcome the difficulties caused by the nonlinear time-varying dynamics of process and to enable the RVFLNs to learn online and to avoid data saturation, an improved online sequential version of RVFLNs (OS-RVFLNs) is presented by sequential learning with forgetting factor. It has been shown that the improved OS-RVFLNs with forgetting factor is not only suitable for the large-scale and real-time data transfer situation but also can adjust the sensitivity of the algorithm to different samples. Second, in order to solve the issue of modeling robustness when the dataset is contaminated with various outliers, a Cauchy distribution function weighted M-estimator is introduced to strengthen the robustness of the improved OS-RVFLNs. The non-Gaussian Cauchy distribution function is used to estimate the weights of different data and thus the corresponding contribution on modeling can be properly distinguished. Experiments using actual industrial data of a large BF ironmaking process have demonstrated that the proposed algorithm produces a much stronger robustness and better estimation accuracy than other algorithms.",2020,28,50,"[144255239.0, 2108700974.0, 39483422.0, 50651835.0, 145011556.0]",144255239.0,Copenhagen,2,"[144985567.0, 2512621.0]",Y,"['It would be good if they could expand and analyze how well does their framework generalizes across other non-binary tasks, tasks in other domains and different NNs.', 'Considering that this is a big, complicated system, it is crucial that the authors included both an ablation experiment to see which pieces were most important, and an experiment that indicates the amount of labeled data that would be required to achieve the same results with a supervised system.']",,,IEEE Transactions on Cybernetics,"['data', 'rvflns', 'process', 'data modeling', 'Data Modeling']"
Data Modeling,417326e51d78ba8bd2621f23e539b41bbdd336d6,Dynamic Probabilistic Latent Variable Model for Process Data Modeling and Regression Application,https://www.semanticscholar.org/paper/417326e51d78ba8bd2621f23e539b41bbdd336d6,JournalArticle,"Dynamic and uncertainty are two main features of the industrial process data which should be paid attention when carrying out process data modeling and analytics. In this paper, the dynamical and uncertain data characteristics are both taken into consideration for the regression modeling purpose. Based on the probabilistic latent variable modeling framework, the linear dynamic system is introduced for incorporation of the dynamical data feature. The expectation–maximization Algorithm is introduced for parameter learning of the dynamical probabilistic latent variable model, based on which a new soft sensing scheme is then formulated for online prediction of key/quality variables in the process. An industrial case study illustrates the necessity and effectiveness of introducing the dynamical data information into the probabilistic latent variable model.",2019,53,27,"[145619185.0, 2109110276.0]",145619185.0,Minsk,3,"[1414760032.0, 145683384.0, 9185192.0]",Y,"['There should be experiments that compare the the Q+P model with incresing number of atoms against a full CNN, to see whether the Q+P can converge to maximal performance.', 'The conditional variant is less obvious, requiring two kinds of negative images, and again the proposed approach seems technically sound.', 'Do you have any intuition for why it is sometimes necessary to set beta=0?']",,,IEEE Transactions on Control Systems Technology,"['data', 'process', 'modeling', 'data modeling', 'Data Modeling']"
Data Modeling,627d7d631fd4e0e2179f82199f014deb7ff0ea0b,Bayesian synthesis of probabilistic programs for automatic data modeling,https://www.semanticscholar.org/paper/627d7d631fd4e0e2179f82199f014deb7ff0ea0b,JournalArticle,"We present new techniques for automatically constructing probabilistic programs for data analysis, interpretation, and prediction. These techniques work with probabilistic domain-specific data modeling languages that capture key properties of a broad class of data generating processes, using Bayesian inference to synthesize probabilistic programs in these modeling languages given observed data. We provide a precise formulation of Bayesian synthesis for automatic data modeling that identifies sufficient conditions for the resulting synthesis procedure to be sound. We also derive a general class of synthesis algorithms for domain-specific languages specified by probabilistic context-free grammars and establish the soundness of our approach for these languages. We apply the techniques to automatically synthesize probabilistic programs for time series data and multivariate tabular data. We show how to analyze the structure of the synthesized programs to compute, for key qualitative properties of interest, the probability that the underlying data generating process exhibits each of these properties. Second, we translate probabilistic programs in the domain-specific language into probabilistic programs in Venture, a general-purpose probabilistic programming system. The translated Venture programs are then executed to obtain predictions of new time series data and new multivariate data records. Experimental results show that our techniques can accurately infer qualitative structure in multiple real-world data sets and outperform standard data analysis methods in forecasting and predicting new data.",2019,46,3,"[145044852.0, 1397895859.0, 1922848.0, 1720971.0, 1735083.0]",145044852.0,Helsinki,3,"[1410115257.0, 145836176.0, 2695617.0]",Y,"['After equation 5, the authors suggest categorical loss for discrete problems, but cross-entropy loss might work better.', 'Some reasons below: * There are no specific results on properties of the divergences, or axioms that justify them.', '[a] https://arxiv.org/pdf/1603.05027.pdf Moreover, the literature review is very limited.']",,,Proc. ACM Program. Lang.,"['data', 'programs', 'techniques', 'data modeling', 'Data Modeling']"
Data Modeling,72a6a5d5864eb915231c7128b90977f1d2acf6e9,Deep spatio-temporal residual neural networks for road-network-based data modeling,https://www.semanticscholar.org/paper/72a6a5d5864eb915231c7128b90977f1d2acf6e9,JournalArticle,"ABSTRACT Recently, researchers have introduced deep learning methods such as convolutional neural networks (CNN) to model spatio-temporal data and achieved better results than those with conventional methods. However, these CNN-based models employ a grid map to represent spatial data, which is unsuitable for road-network-based data. To address this problem, we propose a deep spatio-temporal residual neural network for road-network-based data modeling (DSTR-RNet). The proposed model constructs locally-connected neural network layers (LCNR) to model road network topology and integrates residual learning to model the spatio-temporal dependency. We test the DSTR-RNet by predicting the traffic flow of Didi cab service, in an 8-km2 region with 2,616 road segments in Chengdu, China. The results demonstrate that the DSTR-RNet maintains the spatial precision and topology of the road network as well as improves the prediction accuracy. We discuss the prediction errors and compare the prediction results to those of grid-based CNN models. We also explore the sensitivity of the model to its parameters; this will aid the application of this model to network-based data modeling.",2019,33,33,"[2115242952.0, 49829302.0, 46866973.0]",2115242952.0,Dublin,2,"[143609903.0, 1746959.0]",Y,"['Good paper, pushing the limits of RL to harder tasks.', 'The problem has been well motivated and all the relevant issues point out for the reader.']",,,International Journal of Geographical Information Science,"['model', 'data', 'network', 'data modeling', 'Data Modeling']"
Data Modeling,6dd481d5eb8d76d4b61a1829c7687d008e0937ab,A Unified Framework for Plasma Data Modeling in Dynamic Positron Emission Tomography Studies,https://www.semanticscholar.org/paper/6dd481d5eb8d76d4b61a1829c7687d008e0937ab,JournalArticle,"Objective: Full quantification of dynamic positron emission tomography (PET) data requires the knowledge of tracer concentration in the arterial plasma. However, its accurate measurement is challenging due to the presence of radiolabeled metabolites and measurement noise. Mathematical models are fitted to the plasma data for both radiometabolite correction and data denoising. However, the models used are generally not physiologically informed and not consistently applied across studies even when quantifying the kinetics of the same radiotracer, introducing methodological variability affecting the results interpretation. The aim of this study was to develop and validate a unified framework for the arterial data modeling to achieve an accurate and fully automated description of the plasma tracer kinetics. Methods: The proposed pipeline employs basis pursuit techniques for estimating both radiometabolites and parent concentration models from the raw plasma measurements, allowing the resulting algorithm to be both robust and flexible to the different quality of data available. The pipeline was tested on four PET tracers ([11C]PBR28, [11C]MePPEP, [11C]WAY-100635, and [11C]PIB) with continuous and discrete blood sampling. Results: Compared to the standard procedure, the pipeline provided similar fit of the parent fraction but yielded a better description of the total plasma radioactivity, which in turn allowed a more accurate fit of the tissue PET data. Conclusion: The new method showed superior fits compared to the standard pipeline, for both continuous and discrete arterial sampling protocol, yielding to better description of PET data. Significance: The proposed pipeline has the potential to standardize the blood data modeling in dynamic PET studies given its robustness, flexibility and easiness of use.",2019,16,66,"[2744698.0, 38121795.0, 144666389.0, 3589089.0, 6315093.0, 46174961.0, 2991080.0]",2744698.0,Warsaw,3,"[2110410087.0, 143826246.0, 152742012.0]",Y,"['In conclusion, this paper does not have a major flaw.', 'Thus, I applaud the authors for combining the two in a way that admits efficient training.', 'However, the sensitivity calculations in the SVM context is new as per my knowledge.']",,,IEEE Transactions on Biomedical Engineering,"['data', 'plasma', 'pipeline', 'data modeling', 'Data Modeling']"
Data Modeling,d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5,An Effective and Scalable Data Modeling for Enterprise Big Data Platform,https://www.semanticscholar.org/paper/d4ae73ac17c6bc8a537df2cebf50c9b4a6b420d5,Conference,"The enormous growth of the internet, enterprise applications, social media, and IoT devices in the current time caused a huge spike in enterprise data growth. Big data platform provided scalable storage to manage enterprise data growth and served easier data access to decision-makers, stakeholders and business users. It is a well-known challenge to classify, organize and store all this data and process it to provide business insights. Due to nature, variety, velocity, volume and value of data make it difficult to effectively process big data. Enterprises face challenges to apply complex business rules, to generate insights and to support data-driven decisions in a timely fashion. As big data lake integrates streams of data from a bunch of business units, stakeholders usually analyze enterprise-wide data from various data models. Data models are a vital component of Big data platform. Users may do complex processing, run queries and perform big table joins to generate required metrics depending on the available data models. It is usually a time consuming and resource-intensive process to find the value from data. It is a no-brainer that big data platform in the enterprise needs high-quality data modeling methods to reach an optimal mix of cost, performance, and quality. This paper addresses these challenges by proposing an effective and scalable way to organize and store data in Big Data Lake. It presents some of the basic principles and methodology to build scalable data models in a distributed environment. It also describes how it overcomes common challenges and presents findings.",2019,21,,[31933741.0],31933741.0,Monaco,2,"[2113909888.0, 93841942.0]",Y,"['Follow up experiments extend the basic setup significantly.', 'In its current state and format the major issue with this work is the lack of more in-depth performance analysis which would help the reader draw more solid conclusions about the generalization of the approach.']",2019 IEEE International Conference on Big Data (Big Data),19.0,,"['data', 'enterprise', 'business', 'data modeling', 'Data Modeling']"
Data Modeling,eeac4411ae119c6c7ac33a11f762f2495b4dd960,Predicting the Risk of Heart Failure With EHR Sequential Data Modeling,https://www.semanticscholar.org/paper/eeac4411ae119c6c7ac33a11f762f2495b4dd960,JournalArticle,"Electronic health records (EHRs) contain patient diagnostic records, physician records, and records of hospital departments. For heart failure, we can obtain mass unstructured data from EHR time series. By analyzing and mining these time-based EHRs, we can identify the links between diagnostic events and ultimately predict when a patient will be diagnosed. However, it is difficult to use the existing EHR data directly, because they are sparse and non-standardized. Thus, this paper proposes an effective and robust architecture for heart failure prediction. The main contribution of this paper is to predict heart failure using a neural network (i.e., to predict the possibility of cardiac illness based on patient’s electronic medical data). Specifically, we employed one-hot encoding and word vectors to model the diagnosis events and predicted heart failure events using the basic principles of a long short-term memory network model. Evaluations based on a real-world data set demonstrate the promising utility and efficacy of the proposed architecture in the prediction of the risk of heart failure.",2018,99,6,"[144732658.0, 2106075012.0, 2109343347.0, 5818674.0, 5028442.0, 2115493235.0]",144732658.0,Copenhagen,2,"[32131713.0, 1394243406.0]",Y,"['For the denoising comparison, how do the results compare to those obtained if you simulate a Markov Chain (sample latent state conditioned on noisy image, sample latent state, sample denoised observation, repeat using denoised observation) using a VAE?', 'However, no empirical comparison between the two methods is performed.']",,,IEEE Access,"['heart', 'failure', 'records', 'data modeling', 'Data Modeling']"
Data Modeling,b6becea767675ea6ee43c78ce747077a5050019c,Fuzzy Spatiotemporal Data Modeling Based on UML,https://www.semanticscholar.org/paper/b6becea767675ea6ee43c78ce747077a5050019c,JournalArticle,"With the wide application of spatiotemporal data, more and more data need to be modeled. Fuzziness is one of the important characteristics of spatiotemporal data, but most of the existing spatiotemporal data models are regarded as accurate data and most of the spatiotemporal data models are static. The purpose of this paper is to build a fuzzy spatiotemporal data model based on UML by expanding the standard modeling language UML. On this basis, the historical topological state, effective time, and transaction time are added to make the model dynamic, that is, the states of multiple times are combined together to form a timeline to represent the development process of a fuzzy spatiotemporal entity. Considering the fuzziness of data and the dynamic development of objects, the model can describe the fuzzy spatiotemporal objects better, and make the model dynamic. At the same time, the operation of the UML class diagram is added to make the model data exercisable and compared according to their own needs, which increases the practicability of the model. Finally, an example of land desertification in Alashan is given, to prove the practicability of the model constructed in this paper.",2019,11,7,"[2155516106.0, 1779967.0]",2155516106.0,Prague,3,"[1732570.0, 2064855598.0, 3422038.0]",Y,"['Finally, it might be interesting to initialize the convolutions in the shortcut connections with the identity, and check what they have leant at the end of the training.', 'Only two scene types (bedroom, kitchen) and four object classes (bed, window, appliance, counter) are used for evaluation.', 'The paper considers single-agent problems and tests on Ms Pacman etc.']",,,IEEE Access,"['data', 'model', 'time', 'data modeling', 'Data Modeling']"
Data Modeling,ddc6e677715c03fe574319d3f80a3e1577bdbdd3,Dynamic Multivariate Functional Data Modeling via Sparse Subspace Learning,https://www.semanticscholar.org/paper/ddc6e677715c03fe574319d3f80a3e1577bdbdd3,JournalArticle,"ABSTRACT Multivariate functional data from a complex system are naturally high-dimensional and have a complex cross-correlation structure. The complexity of data structure can be observed as that (1) some functions are strongly correlated with similar features, while some others may have almost no cross-correlations with quite diverse features; and (2) the cross-correlation structure may also change over time due to the system evolution. With this regard, this article presents a dynamic subspace learning method for multivariate functional data modeling. In particular, we consider that different functions come from different subspaces, and only functions of the same subspace have cross-correlations with each other. The subspaces can be automatically formulated and learned by reformatting the problem as a sparse regression. By allowing but regularizing the regression change over time, we can describe the cross-correlation dynamics. The model can be efficiently estimated by the fast iterative shrinkage-thresholding algorithm, and the features of each subspace can be extracted using the smooth multi-channel functional principal component analysis. Some theoretical properties of the model are presented. Numerical studies, together with case studies, demonstrate the efficiency and applicability of the proposed methodology.",2018,27,63,"[2111572794.0, 144303419.0, 2117177710.0, 153533172.0]",2111572794.0,Warsaw,2,"[2142586.0, 52198091.0]",Y,"['The technique systematically interacts with the oracle using observations, which are abstractions of environment states, and it is guaranteed to produce a data set that completely specifies the oracle.', 'Moreover, a basic system called delta t block implements one level of full approximation and is stoked several times.']",,,Technometrics,"['data', 'crosscorrelation', 'structure', 'data modeling', 'Data Modeling']"
Data Modeling,63adc1e5086481e36b19b62707a96b799da51e59,Data modeling versus simulation modeling in the big data era: case study of a greenhouse control system,https://www.semanticscholar.org/paper/63adc1e5086481e36b19b62707a96b799da51e59,JournalArticle,"Recently, big data has received greater attention in diverse research fields, including medicine, science, engineering, management, defense, politics, and others. Such research uses big data to predict target systems, thereby constructing a model of the system in two ways: data modeling and simulation modeling. Data modeling is a method in which a model represents correlation relationships between one set of data and the other set of data. On the other hand, physics-based simulation modeling (or simply simulation modeling) is a more classical, but more powerful, method in which a model represents causal relationships between a set of controlled inputs and corresponding outputs. This paper (i) clarifies the difference between the two modeling approaches, (ii) explains their advantages and limitations and compares each characteristic, and (iii) presents a complementary cooperation modeling approach. Then, we apply the proposed modeling to develop a greenhouse control system in the real world. Finally, we expect that this modeling approach will be an alternative modeling approach in the big data era.",2017,61,93,"[2608638.0, 48418805.0, 3160707.0, 1761763.0]",2608638.0,Stockholm,3,"[9551276.0, 2254124342.0, 49462969.0]",Y,"['Each extension to the Dual Actor-Critic is well motivated and clear in context.', 'Both datasets should be compared to LASSO as well.', 'Indeed, the authors have succeed in showing that this is not necessarily the case.']",,,International Conference on Advances in System Simulation,"['data', 'modeling', 'model', 'data modeling', 'Data Modeling']"
Data Modeling,fdc57c18f3b636c3273542327ae540217972558f,"DART: Recent Advances in Remote Sensing Data Modeling With Atmosphere, Polarization, and Chlorophyll Fluorescence",https://www.semanticscholar.org/paper/fdc57c18f3b636c3273542327ae540217972558f,JournalArticle,"To better understand the life-essential cycles and processes of our planet and to further develop remote sensing (RS) technology, there is an increasing need for models that simulate the radiative budget (RB) and RS acquisitions of urban and natural landscapes using physical approaches and considering the three-dimensional (3-D) architecture of Earth surfaces. Discrete anisotropic radiative transfer (DART) is one of the most comprehensive physically based 3-D models of Earth-atmosphere radiative transfer, covering the spectral domain from ultraviolet to thermal infrared wavelengths. It simulates the optical 3-D RB and optical signals of proximal, aerial, and satellite imaging spectrometers and laser scanners, for any urban and/or natural landscapes and for any experimental and instrumental configurations. It is freely available for research and teaching activities. In this paper, we briefly introduce DART theory and present recent advances in simulated sensors (LiDAR and cameras with finite field of view) and modeling mechanisms (atmosphere, specular reflectance with polarization and chlorophyll fluorescence). A case study demonstrating a novel application of DART to investigate urban landscapes is also presented.",2017,148,10,"[1412902270.0, 2327930.0, 144868747.0, 7471201.0, 144579083.0, 48538041.0, 2990847.0, 32719983.0, 31140405.0, 3394506.0, 114824980.0, 46771083.0, 8599371.0, 33617898.0, 3420619.0, 2660652.0, 143717263.0]",1412902270.0,Berlin,2,"[2574504.0, 2149869431.0]",Y,"['I am impressed with the structure and presentation of the paper.', 'The evaluation framework is described in enough detail to replicate the results.']",,,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"['radiative', 'landscapes', '3d', 'data modeling', 'Data Modeling']"
Data Modeling,e3b94a5f28522e6825aff16ff07d56bd70d26c96,The YANG 1.1 Data Modeling Language,https://www.semanticscholar.org/paper/e3b94a5f28522e6825aff16ff07d56bd70d26c96,JournalArticle,"YANG is a data modeling language used to model configuration data,
state data, Remote Procedure Calls, and notifications for network
management protocols. This document describes the syntax and semantics
of version 1.1 of the YANG language. YANG version 1.1 is a maintenance
release of the YANG language, addressing ambiguities and defects in
the original specification. There are a small number of backward
incompatibilities from YANG version 1. This document also specifies
the YANG mappings to the Network Configuration Protocol (NETCONF).",2016,127,7950,[103682810.0],103682810.0,Bratislava,3,"[2106075012.0, 2481509.0, 1391200710.0]",Y,"['What is new compared to [Finn et al. 2016] is that the authors managed to train the network in combination with an adversarial loss, which allows for the generation of more realistic images.', 'Despite the thoroughness of the task and model descriptions, the proposed method is not well motivated.', 'Some selections, which are presented in the papers are not explained, for example, the gradient penalty, the choice of \\lambda and the choice of points for gradient computation.']",,,Request for Comments,"['yang', 'data', 'language', 'data modeling', 'Data Modeling']"
Data Modeling,56ae2837b6cd4d143bbfa4a4b06811d70126106f,Unevenly Sampled Dynamic Data Modeling and Monitoring With an Industrial Application,https://www.semanticscholar.org/paper/56ae2837b6cd4d143bbfa4a4b06811d70126106f,JournalArticle,"In this paper, a dynamic modeling method for unevenly sampled data is proposed for the monitoring of bi-layer (i.e., a process layer and a quality layer) dynamic processes. First, a novel uneven data dynamic canonical correlation analysis method with an integrated dynamic time window is proposed for interlayer latent structure modeling, which captures the dynamic relations between regularly sampled process data and quality data with slow and irregular sampling. The new model is a step toward big data modeling to deal with data irregularity and diversity. Second, after extracting covariations using an interlayer model, intralayer variations are extracted using subsequent principal component analysis on the residual subspaces of the original process data and quality data, respectively. Third, a concurrent monitoring method for unevenly sampled bi-layer data is proposed. Finally, the proposed method is demonstrated using an illustrative simulation example and applied successfully to a real blast furnace iron-making process.",2017,33,13,"[46581019.0, 143609842.0, 145011556.0]",46581019.0,Vienna,2,"[41052836.0, 39517703.0]",Y,"['I was not able to imagine a reasonable setting where we would have access to a reward function of this form without input/output examples.', 'The effect of using the faster approximation on performance also remains to be investigated.']",,,IEEE Transactions on Industrial Informatics,"['data', 'method', 'process', 'data modeling', 'Data Modeling']"
Data Modeling,818de553ecd306735971aba04bbfc29d17457084,Robust Spectral Clustering for Noisy Data: Modeling Sparse Corruptions Improves Latent Embeddings,https://www.semanticscholar.org/paper/818de553ecd306735971aba04bbfc29d17457084,Conference,"Spectral clustering is one of the most prominent clustering approaches. However, it is highly sensitive to noisy input data. In this work, we propose a robust spectral clustering technique able to handle such scenarios. To achieve this goal, we propose a sparse and latent decomposition of the similarity graph used in spectral clustering. In our model, we jointly learn the spectral embedding as well as the corrupted data - thus, enhancing the clustering performance overall. We propose algorithmic solutions to all three established variants of spectral clustering, each showing linear complexity in the number of edges. Our experimental analysis confirms the significant potential of our approach for robust spectral clustering. Supplementary material is available at www.kdd.in.tum.de/RSC.",2017,63,,"[11754930.0, 22654490.0, 3075189.0]",11754930.0,Brussels,2,"[1996394.0, 2081346.0]",Y,"['The experiments are very clearly presented and solidly designed.', '1. This manuscript provides the sufficient and necessary characterization of critical points for deep networks.']",Knowledge Discovery and Data Mining,23.0,,"['clustering', 'data', 'approaches', 'data modeling', 'Data Modeling']"
Data Modeling,b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0,Bayesian Random Vector Functional-Link Networks for Robust Data Modeling,https://www.semanticscholar.org/paper/b4c929c703868b7f6bf7778c1e6f1a2baaac3ae0,JournalArticle,"Random vector functional-link (RVFL) networks are randomized multilayer perceptrons with a single hidden layer and a linear output layer, which can be trained by solving a linear modeling problem. In particular, they are generally trained using a closed-form solution of the (regularized) least-squares approach. This paper introduces several alternative strategies for performing full Bayesian inference (BI) of RVFL networks. Distinct from standard or classical approaches, our proposed Bayesian training algorithms allow to derive an entire probability distribution over the optimal output weights of the network, instead of a single pointwise estimate according to some given criterion (e.g., least-squares). This provides several known advantages, including the possibility of introducing additional prior knowledge in the training process, the availability of an uncertainty measure during the test phase, and the capability of automatically inferring hyper-parameters from given data. In this paper, two BI algorithms for regression are first proposed that, under some practical assumptions, can be implemented by a simple iterative process with closed-form computations. Simulation results show that one of the proposed algorithms, Bayesian RVFL, is able to outperform standard training algorithms for RVFL networks with a proper regularization factor selected carefully via a line search procedure. A general strategy based on variational inference is also presented, with an application to data modeling problems with noisy outputs or outliers. As we discuss in this paper, using recent advances in automatic differentiation this strategy can be applied to a wide range of additional situations in an immediate fashion.",2018,26,48,"[1752983.0, 70129091.0, 1737292.0]",1752983.0,Nicosia,2,"[2039003.0, 2200379863.0]",Y,"['Similarly, equation 4 penalizes differences between predicted and observed state transitions.', 'Unless I am misunderstanding the experimental setup, this is not supported by the result, correct?']",,,IEEE Transactions on Cybernetics,"['rvfl', 'algorithms', 'networks', 'data modeling', 'Data Modeling']"
Data Modeling,2141334fad7248fc707607bc9453d44686ae07a7,How to Make Sense of Team Sport Data: From Acquisition to Data Modeling and Research Aspects,https://www.semanticscholar.org/paper/2141334fad7248fc707607bc9453d44686ae07a7,JournalArticle,"Automatic and interactive data analysis is instrumental in making use of increasing amounts of complex data. Owing to novel sensor modalities, analysis of data generated in professional team sport leagues such as soccer, baseball, and basketball has recently become of concern, with potentially high commercial and research interest. The analysis of team ball games can serve many goals, e.g., in coaching to understand effects of strategies and tactics, or to derive insights improving performance. Also, it is often decisive to trainers and analysts to understand why a certain movement of a player or groups of players happened, and what the respective influencing factors are. We consider team sport as group movement including collaboration and competition of individuals following specific rule sets. Analyzing team sports is a challenging problem as it involves joint understanding of heterogeneous data perspectives, including high-dimensional, video, and movement data, as well as considering team behavior and rules (constraints) given in the particular team sport. We identify important components of team sport data, exemplified by the soccer case, and explain how to analyze team sport data in general. We identify challenges arising when facing these data sets and we propose a multi-facet view and analysis including pattern detection, context-aware analysis, and visual explanation. We also present applicable methods and technologies covering the heterogeneous aspects in team sport data.",2017,66,2,"[40617842.0, 2024783.0, 3471557.0, 51169038.0, 40106679.0, 2064855598.0, 2942725.0, 1706471.0, 9106757.0, 1786155.0]",40617842.0,Nicosia,2,"[40071013.0, 2117315688.0]",Y,"['The authors should consider exploring and discussing the effects of adding/moving/removing objects on the performance.', '4) Novelty The main contribution of this paper is basically a set of experiments looking into architectural choices.']",,,International Conference on Data Technologies and Applications,"['data', 'team', 'sport', 'data modeling', 'Data Modeling']"
Data Modeling,8ba8a0d18a06752f5a39996ccf1e914da0941443,Combining first-principles and data modeling for the accurate prediction of the refractive index of organic polymers.,https://www.semanticscholar.org/paper/8ba8a0d18a06752f5a39996ccf1e914da0941443,JournalArticle,"Organic materials with a high index of refraction (RI) are attracting considerable interest due to their potential application in optic and optoelectronic devices. However, most of these applications require an RI value of 1.7 or larger, while typical carbon-based polymers only exhibit values in the range of 1.3-1.5. This paper introduces an efficient computational protocol for the accurate prediction of RI values in polymers to facilitate in silico studies that can guide the discovery and design of next-generation high-RI materials. Our protocol is based on the Lorentz-Lorenz equation and is parametrized by the polarizability and number density values of a given candidate compound. In the proposed scheme, we compute the former using first-principles electronic structure theory and the latter using an approximation based on van der Waals volumes. The critical parameter in the number density approximation is the packing fraction of the bulk polymer, for which we have devised a machine learning model. We demonstrate the performance of the proposed RI protocol by testing its predictions against the experimentally known RI values of 112 optical polymers. Our approach to combine first-principles and data modeling emerges as both a successful and a highly economical path to determining the RI values for a wide range of organic polymers.",2018,33,148 24,"[16009412.0, 3480094.0, 40471981.0]",16009412.0,Brussels,3,"[70310638.0, 145204762.0, 2111830587.0]",Y,"['Pros: Well written, thorough treatment of the approaches Improvements on top of Dual-AC with ablation study show improvement Cons: Empirical gains might not be very large', 'Unclear statements/notations: * end of page 3, notations are not entirely consist with previous notations * I do not understand which distribution is assumed on epsilon and gamma when taking the expectancy in equation (9).', 'Does each component is related to a certain topic?']",,,Journal of Chemical Physics,"['ri', 'polymers', 'values', 'data modeling', 'Data Modeling']"
Data Modeling,54a26f5ef4b0524c60249fe98d0fc3646b2791ad,Sampled-Data Modeling of Switched- Capacitor Voltage Regulator With Frequency-Modulation Control,https://www.semanticscholar.org/paper/54a26f5ef4b0524c60249fe98d0fc3646b2791ad,JournalArticle,The development of systems-on-chip requires embedded power management solutions due to the large number of power domains. The switched-capacitor voltage regulator is a suitable candidate as capacitors may be integrated whereas inductors still suffer limitations in that respect. Literature covers proposals of optimized power stages and several dedicated controllers for switched-capacitor DC-DC converters. Unfortunately the results do not cover systematic stability analyses. The paper proposes an original and systematic approach for the stability analysis of a switched-capacitor voltage regulator using sampled-data modeling. An application is given for a one-phase converter with a frequency-modulation based controller. The stability of the open-loop and closed-loop model is proposed and can be extended to multi-phase configurations.,2015,21,62,"[112878157.0, 145511872.0, 1399086860.0]",112878157.0,Berlin,3,"[50074956.0, 2182246691.0, 1500391633.0]",Y,"['Do the authors have any explanation/intuition for this behavior?', 'An interesting paper which is marginally above acceptance threshold The authors of the paper propose a framework to generate natural adversarial examples by searching adversaries in a latent space of dense and continuous data representation (instead of in the original input data space).', 'In conclusion, this paper does not have a major flaw.']",,,IEEE Transactions on Circuits and Systems Part 1: Regular Papers,"['power', 'switchedcapacitor', 'stability', 'data modeling', 'Data Modeling']"
Data Modeling,00bbc94806ca0821a9c82d8aedf16f0e6263b89f,Agreement between PRE2DUP register data modeling method and comprehensive drug use interview among older persons,https://www.semanticscholar.org/paper/00bbc94806ca0821a9c82d8aedf16f0e6263b89f,JournalArticle,"Background PRE2DUP is a modeling method that generates drug use periods (ie, when drug use started and ended) from drug purchases recorded in dispensing-based register data. It is based on the evaluation of personal drug purchasing patterns and considers hospital stays, possible stockpiling of drugs, and package information. Objective The objective of this study was to investigate person-level agreement between self-reported drug use in the interview and drug use modeled from dispensing data with PRE2DUP method for various drug classes used by older persons. Methods Self-reported drug use was assessed from the GeMS Study including a random sample of persons aged ≥75 years from the city of Kuopio, Finland, in 2006. Drug purchases recorded in the Prescription register data of these persons were modeled to determine drug use periods with PRE2DUP modeling method. Agreement between self-reported drug use on the interview date and drug use calculated from register-based data was compared in order to find the frequently used drugs and drug classes, which was evaluated by Cohen’s kappa. Kappa values 0.61–0.80 were considered to represent good and 0.81–1.00 as very good agreement. Results Among 569 participants with mean age of 82 years, the agreement between interview and register data was very good for 75% and very good or good for 93% of the studied drugs or drug classes. Good or very good agreement was observed for drugs that are typically used on regular bases, whereas “as needed” drugs represented poorer results. Conclusion PRE2DUP modeling method validly describes regular drug use among older persons. For most of drug classes investigated, PRE2DUP-modeled register data described drug use as well as interview-based data which are more time-consuming to collect. Further studies should be conducted by comparing it with other methods and in different drug user populations.",2016,58,8,"[2167301.0, 2638813.0, 2529812.0, 2628180.0, 7478280.0, 2119993.0]",2167301.0,Reykjavik,3,"[51985388.0, 1768360511.0, 1982950.0]",Y,"['Indeed, the authors have commented: ""AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures.""', '(This is totally consistent with Krishnan et al.’s finding that eliminating amortization error gave a bigger improvement for more complex datasets than for MNIST.)', 'So the PATH function helps and longer paths are better.']",,,Clinical Epidemiology,"['drug', 'use', 'data', 'data modeling', 'Data Modeling']"
Data Modeling,f0f1627db35b4942e0f83069f20dd0948fc35d28,A Novel Data-Driven Situation Awareness Approach for Future Grids—Using Large Random Matrices for Big Data Modeling,https://www.semanticscholar.org/paper/f0f1627db35b4942e0f83069f20dd0948fc35d28,JournalArticle,"Data-driven approaches, when tasked with situation awareness, are suitable for complex grids with <italic>massive datasets</italic>. It is a challenge, however, to efficiently turn these massive datasets into useful big data analytics. To address such a challenge, this paper, based on random matrix theory, proposes a data-driven approach. The approach models massive datasets as large random matrices; it is model-free and requires no knowledge about physical model parameters. In particular, the large data dimension <inline-formula> <tex-math notation=""LaTeX"">$N$ </tex-math></inline-formula> and the large time span <inline-formula> <tex-math notation=""LaTeX"">$T$ </tex-math></inline-formula>, from the spatial aspect and the temporal aspect, respectively, lead to favorable results. The beautiful thing lies in that these linear eigenvalue statistics (LESs) are built from data matrices to follow Gaussian distributions for very general conditions, due to the <italic>latest breakthroughs</italic> in probability on the central limit theorems of those LESs. Numerous case studies, with both simulated data and field data, are given to validate the proposed new algorithms.",2016,43,6,"[3026184.0, 144248374.0, 1709488.0, 1816765.0, 152355659.0]",3026184.0,Minsk,2,"[145599558.0, 153475895.0]",Y,"['The authors did a very good job presenting the problem, motivation and solution in a coherent fashion and easy to follow.', ""2) predicting a variable's name by consider its semantic context.""]",,,IEEE Access,"['data', 'challenge', 'datasets', 'data modeling', 'Data Modeling']"
Data Modeling,b3f7359c6d5780972c5ea8db016a01f0c705aa01,glmmTMB Balances Speed and Flexibility Among Packages for Zero-inflated Generalized Linear Mixed Modeling,https://www.semanticscholar.org/paper/b3f7359c6d5780972c5ea8db016a01f0c705aa01,JournalArticle,"Count data can be analyzed using generalized linear mixed models when observations are correlated in ways that require random effects. However, count data are often zero-inflated, containing more zeros than would be expected from the typical error distributions. We present a new package, glmmTMB, and compare it to other R packages that fit zero-inflated mixed models. The glmmTMB package fits many types of GLMMs and extensions, including models with continuously distributed responses, but here we focus on count responses. glmmTMB is faster than glmmADMB, MCMCglmm, and brms, and more flexible than INLA and mgcv for zero-inflated modeling. One unique feature of glmmTMB (among packages that fit zero-inflated mixed models) is its ability to estimate the Conway-Maxwell-Poisson distribution parameterized by the mean. Overall, its most appealing features for new users may be the combination of speed, flexibility, and its interface’s similarity to lme4.",2017,4831,9,"[32710347.0, 46980587.0, 5210567.0, 35493487.0, 10705679.0, 2079275650.0, 2128088.0, 144985567.0, 2254255.0]",32710347.0,Podgorica,3,"[2086973507.0, 2168285733.0, 1792647.0]",Y,"['So, while I cannot vouch for the correctness, I think it can and should go through a serious revision to make it succinct and that will likely considerably help in making it accessible to a wider readership and aligned to the expectations from a conference paper in the field.', 'The difference VGG - ResNets is also interesting, but it would have been interesting to see how this affects the current state of the art in understanding deep learning, something that was done for the ""flat vs sharp"" dilemma, but is lacking here.', 'The paper uses some interesting properties of the CPD model to derive an efficient optimization solver for the BCD subproblems.']",,,The R Journal,"['models', 'glmmtmb', 'count', 'data modeling', 'Data Modeling']"
Data Modeling,7ed665355ac78bf0c394602dd9d26075195ce2f2,A Novel Big Data Modeling Method for Improving Driving Range Estimation of EVs,https://www.semanticscholar.org/paper/7ed665355ac78bf0c394602dd9d26075195ce2f2,JournalArticle,"In this paper, we address a big-data analysis method for estimating the driving range of an electric vehicle (EV), allowing drivers to overcome range anxiety. First, we present an estimating approach to project the life of battery pack for 1600 cycles (i.e., 8 years/160 000 km) based on the data collected from a cycle-life test. This approach has the merit of simplicity. In addition, it considers several critical issues that occur inside battery packs, such as the dependence of internal resistance and the state-of-health. Subsequently, we describe our work on driving pattern analysis of an EV, using a machine-learning approach, namely growing hierarchical self-organizing maps, to cluster the collected EV big data. This paper contains the analysis of energy consumption and driving range estimation for EVs, including powertrain simulation and driving behavior analysis. The experimental results, including both simulating battery degradation and analysis of driving behaviors, demonstrate a feasible solution for improving driving range estimation by the EV big data.",2015,58,3,"[1783205.0, 2146335468.0]",1783205.0,Luxembourg,2,"[1410231361.0, 51125639.0]",Y,"['The experimental results show that Relation Networks outperform the other two baselines, but still ~30% behind human performance.', 'How is this actually done in practice?']",,,IEEE Access,"['analysis', 'range', 'battery', 'data modeling', 'Data Modeling']"
Data Modeling,c665003881c3c35589d1e48da1ee7234b48f2ac8,Fuzzy Spatiotemporal Data Modeling and Operations in XML,https://www.semanticscholar.org/paper/c665003881c3c35589d1e48da1ee7234b48f2ac8,JournalArticle,"Because increasing requirements of fuzzy spatiotemporal applications are attracting much attention from both academia and industry, it is challenging to model fuzzy spatiotemporal data and effectively operate them. However, various researches are studied in traditional databases that impose strict restrictions, and relatively little work has been carried out in modeling and operating fuzzy spatiotemporal data in XML. In this paper, we propose a novel approach to model fuzzy spatiotemporal data based on XML. On the basis of the model, we investigate how to represent fuzzy spatiotemporal data in XML documents and extend the XML schema so that it is possible to describe fuzzy spatiotemporal data and capture the structural information of fuzzy spatiotemporal XML documents. Furthermore, we give algorithms for fuzzy operations, containing node operations and topological relationship operations. Finally, we apply our model in meteorological events.",2015,15,29,"[1779967.0, 47002265.0, 2200379863.0]",1779967.0,London,3,"[144385184.0, 144882390.0, 3420212.0]",Y,"['Since any CoffeeScript programs can be compiled into the corresponding Javascript programs, we should assume that CoffeeScript is the only subset of Javascript (without physical difference of syntax), and this translation task may never capture the whole tendency of Javascript.', 'To be fair, one might make an argument as follows: ""the point of deep nets is to be expressive, expressiveness of a layer relates to the spetrum of the layer-Jacobian, a small increase in gradient scale implies the layer-Jacobian has many similar singular values, therefore a small increase in gradient scale implies low expressiveness of the layer, therefore the layer is pathological"".', 'How many times did you run node2vec on each graph?']",,,Applied Artificial Intelligence,"['data', 'xml', 'model', 'data modeling', 'Data Modeling']"
Data Modeling,633e2fbfc0b21e959a244100937c5853afca4853,Score-Based Generative Modeling through Stochastic Differential Equations,https://www.semanticscholar.org/paper/633e2fbfc0b21e959a244100937c5853afca4853,JournalArticle,"Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",2020,2851,abs/2011.13456,"[115504645.0, 1407546424.0, 1726807.0, 2109224633.0, 2490652.0, 16443937.0]",115504645.0,Andorra,2,"[1680579.0, 102445836.0]",Y,"['As there is no strong theory in the paper, this limited experimental evaluation is reason enough for rejection.', 'The core idea is to train the model on pairs of images corresponding to the same content but varying in views, using adversarial training to discriminate such examples from generated pairs.']",,,International Conference on Learning Representations,"['sde', 'distribution', 'data', 'data modeling', 'Data Modeling']"
Data Modeling,91b9d3ab7532ea24ae70cd726355f25235b1fe8b,Ten simple rules for the computational modeling of behavioral data,https://www.semanticscholar.org/paper/91b9d3ab7532ea24ae70cd726355f25235b1fe8b,JournalArticle,"Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.",2019,309,8,"[153160317.0, 50593332.0]",153160317.0,Dublin,2,"[2162779839.0, 8722618.0]",Y,"['This section could be improved by demonstrating the approach on more datasets.', 'The paper says Deep Voice 2 (Arik et al., 2017a) is only prior work for multi-speaker TTS.']",,,eLife,"['modeling', 'data', 'rules', 'data modeling', 'Data Modeling']"
Data Modeling,2e965b5d97c2d6fb4af284307735be39283792ba,Extracting Training Data from Diffusion Models,https://www.semanticscholar.org/paper/2e965b5d97c2d6fb4af284307735be39283792ba,JournalArticle,"Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.",2023,266,abs/2301.13188,"[2483738.0, 9200194.0, 3490923.0, 40844378.0, 3482535.0, 2444919.0, 1718064.0, 7975935.0, 145217343.0]",2483738.0,Chisinau,2,"[2746913.0, 2109481.0]",Y,"['In my first reading of the paper, I felt that the baselines in the experiments are too primitive.', 'Inclusion of Assumption 3 would in particular require better justification.']",,,USENIX Security Symposium,"['models', 'diffusion', 'training', 'data modeling', 'Data Modeling']"
Big Data,4b06c7e29280b1c6bc05c9df39023b48fef02c93,Escaping the Big Data Paradigm with Compact Transformers,https://www.semanticscholar.org/paper/4b06c7e29280b1c6bc05c9df39023b48fef02c93,JournalArticle,"With the rise of Transformers as the standard for language processing, and their advancements in computer vision, there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolutional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outperforms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers. Our code and pre-trained models are publicly available at https://github.com/SHI-Labs/Compact-Transformers.",2021,314,abs/2104.05704,"[2855934.0, 102471415.0, 2087066452.0, 133551208.0, 2125031571.0, 48667025.0]",2855934.0,Amsterdam,2,"[1693182792.0, 1710751.0]",Y,"['I could then identify the influential dimension(s) by taking the largest absolute values in the gradient vector.', 'Evaluation: Significance: Coresets give significant running time benefits when working with very big datasets.']",,,arXiv.org,"['transformers', 'size', 'data', 'big data', 'Big Data']"
Big Data,2a56bd89fd1a9457f0705142540ffc4396fad4f7,Variational LSTM Enhanced Anomaly Detection for Industrial Big Data,https://www.semanticscholar.org/paper/2a56bd89fd1a9457f0705142540ffc4396fad4f7,JournalArticle,"With the increasing population of Industry 4.0, industrial big data (IBD) has become a hotly discussed topic in digital and intelligent industry field. The security problem existing in the signal processing on large scale of data stream is still a challenge issue in industrial internet of things, especially when dealing with the high-dimensional anomaly detection for intelligent industrial application. In this article, to mitigate the inconsistency between dimensionality reduction and feature retention in imbalanced IBD, we propose a variational long short-term memory (VLSTM) learning model for intelligent anomaly detection based on reconstructed feature representation. An encoder–decoder neural network associated with a variational reparameterization scheme is designed to learn the low-dimensional feature representation from high-dimensional raw data. Three loss functions are defined and quantified to constrain the reconstructed hidden variable into a more explicit and meaningful form. A lightweight estimation network is then fed with the refined feature representation to identify anomalies in IBD. Experiments using a public IBD dataset named UNSW-NB15 demonstrate that the proposed VLSTM model can efficiently cope with imbalance and high-dimensional issues, and significantly improve the accuracy and reduce the false rate in anomaly detection for IBD according to F1, area under curve (AUC), and false alarm rate (FAR).",2021,202,17,"[2504776.0, 1752768069.0, 2088921907.0, 1695160.0, 46876083.0]",2504776.0,Vaduz,3,"[2047926221.0, 1689466093.0, 119869488.0]",Y,"['The paper is mostly clear and well-written (though with a few typos that should be fixed).', 'Presumably, there exists some hyperparameters where the model does not achieve 100% test accuracy, in which case, what are the failure modes?', 'Also, in Section 4.3, it is not clear what the point of this experiment is: whether to show that entropy regularization helps or the Stein gradient estimator outperforms other estimators.']",,,IEEE Transactions on Industrial Informatics,"['ibd', 'feature', 'data', 'big data', 'Big Data']"
Big Data,8894d431a768a35dc7ca4d762ebdba4f407b978c,The ProteomeXchange consortium in 2020: enabling ‘big data’ approaches in proteomics,https://www.semanticscholar.org/paper/8894d431a768a35dc7ca4d762ebdba4f407b978c,JournalArticle,"Abstract The ProteomeXchange (PX) consortium of proteomics resources (http://www.proteomexchange.org) has standardized data submission and dissemination of mass spectrometry proteomics data worldwide since 2012. In this paper, we describe the main developments since the previous update manuscript was published in Nucleic Acids Research in 2017. Since then, in addition to the four PX existing members at the time (PRIDE, PeptideAtlas including the PASSEL resource, MassIVE and jPOST), two new resources have joined PX: iProX (China) and Panorama Public (USA). We first describe the updated submission guidelines, now expanded to include six members. Next, with current data submission statistics, we demonstrate that the proteomics field is now actively embracing public open data policies. At the end of June 2019, more than 14 100 datasets had been submitted to PX resources since 2012, and from those, more than 9 500 in just the last three years. In parallel, an unprecedented increase of data re-use activities in the field, including ‘big data’ approaches, is enabling novel research and new data resources. At last, we also outline some of our future plans for the coming years.",2019,537,48,"[1763674.0, 2865038.0, 33582313.0, 1390051934.0, 31411209.0, 1605658912.0, 1402417041.0, 4007395.0, 13084208.0, 39268701.0, 151067942.0, 14948487.0, 46994990.0, 48576550.0, 49701119.0, 1785886.0, 144252544.0, 2167765.0, 2762241.0, 91596213.0, 3245770.0]",1763674.0,Oslo,3,"[1753013.0, 150238039.0, 1403239967.0]",Y,"['Afterward, the authors conduct experiments on MNIST, SVHN, CIFAR10, and ILSVRC12 datasets, where they show promising results compared to the errors provided by previous works.', 'The paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optimized.', '* Qualitative results on sampling from the model using the CIFAR dataset.']",,,Nucleic Acids Res.,"['data', 'px', 'resources', 'big data', 'Big Data']"
Big Data,b34fc78de28be598e21118d7cb9d84d63374addc,Analysis of Dimensionality Reduction Techniques on Big Data,https://www.semanticscholar.org/paper/b34fc78de28be598e21118d7cb9d84d63374addc,JournalArticle,"Due to digitization, a huge volume of data is being generated across several sectors such as healthcare, production, sales, IoT devices, Web, organizations. Machine learning algorithms are used to uncover patterns among the attributes of this data. Hence, they can be used to make predictions that can be used by medical practitioners and people at managerial level to make executive decisions. Not all the attributes in the datasets generated are important for training the machine learning algorithms. Some attributes might be irrelevant and some might not affect the outcome of the prediction. Ignoring or removing these irrelevant or less important attributes reduces the burden on machine learning algorithms. In this work two of the prominent dimensionality reduction techniques, Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are investigated on four popular Machine Learning (ML) algorithms, Decision Tree Induction, Support Vector Machine (SVM), Naive Bayes Classifier and Random Forest Classifier using publicly available Cardiotocography (CTG) dataset from University of California and Irvine Machine Learning Repository. The experimentation results prove that PCA outperforms LDA in all the measures. Also, the performance of the classifiers, Decision Tree, Random Forest examined is not affected much by using PCA and LDA.To further analyze the performance of PCA and LDA the eperimentation is carried out on Diabetic Retinopathy (DR) and Intrusion Detection System (IDS) datasets. Experimentation results prove that ML algorithms with PCA produce better results when dimensionality of the datasets is high. When dimensionality of datasets is low it is observed that the ML algorithms without dimensionality reduction yields better results.",2020,420,8,"[38608914.0, 49247239.0, 51017707.0, 46234199.0, 32336300.0, 2176030144.0, 2287603163.0, 2176030144.0]",38608914.0,Vaduz,2,"[1515428357.0, 2113951006.0]",Y,"['Many relevant papers could be referenced in the introduction as examples of successes in computer vision tasks,  identity mapping initialization, recent interpretations of ResNets/DensetNets, etc.', 'The manuscript mainly presents a cheap pruning algorithm for dense layers of DNNs.']",,,IEEE Access,"['machine', 'algorithms', 'datasets', 'big data', 'Big Data']"
Big Data,391a5f286f814d852dddcab1b2b68e5c1af6c79e,Data mining with big data,https://www.semanticscholar.org/paper/391a5f286f814d852dddcab1b2b68e5c1af6c79e,JournalArticle,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",2016,1674,26,"[2271668292.0, 2283098327.0, 2286440807.0, 2286232238.0]",2271668292.0,Skopje,3,"[2116956834.0, 144540018.0, 2082303711.0]",Y,"['Flexible muscle-based locomotion for bipedal creatures.', 'The use of the proposed gamma distribution, as a simple alternative, overcomes this problem.', '“Customers” randomly appear throughout the task and the taxi agents receive reward for moving to the same square as a customer.']",,,IEEE Transactions on Knowledge and Data Engineering,"['data', 'model', 'sources', 'big data', 'Big Data']"
Big Data,156609022dd6258c60238859622da0a1683bd062,Big Data Analysis Based Network Behavior Insight of Cellular Networks for Industry 4.0 Applications,https://www.semanticscholar.org/paper/156609022dd6258c60238859622da0a1683bd062,JournalArticle,"In this article, we propose a big data based analysis framework to analyze and extract network behaviors in cellular networks for Industry 4.0 applications from a big data perspective, using Hadoop, Hive, HBase, and so on. The data prehandling and traffic flow extraction approaches are presented to construct effective traffic matrices. Accordingly, we can capture network behaviors in cellular networks from a networkwide perspective. Although there have been a number of prior studies on cellular network usage, to the best of our knowledge, this article is a first study that characterizes network behaviors using the big data analytics to analyze a network big data of call detail records over a longer duration (five months), with more users (five million), more records (several hundred million lines) and nationwide coverage. The call pattern analysis and network behavior extraction approaches are designed to perform big data analysis and feature extractions. Then, the corresponding algorithms are proposed to characterize network behaviors, i.e., cellular call patterns and network resource usage. The detailed evaluation is proposed to validate our method. For example, we find that some unpopular calls can last longer time and thus consume more network resources.",2020,137,16,"[33367477.0, 2108035003.0, 1792647.0, 6353833.0, 143978827.0]",33367477.0,Kiev,2,"[1716179427.0, 2151264132.0]",Y,"['An interesting work on the characterization of critical points of neural networks This paper mainly focuses on the square loss function of linear networks.', '- I believe that different Monte Carlo (or Quasi-Monte Carlo) strategies can be applied in order to estimate the integral (expected value) in Eq.']",,,IEEE Transactions on Industrial Informatics,"['network', 'data', 'behaviors', 'big data', 'Big Data']"
Big Data,0e33833f5e2e2719edfba1d142eb4d27f96e799f,Big data analytics and enterprises: a bibliometric synthesis of the literature,https://www.semanticscholar.org/paper/0e33833f5e2e2719edfba1d142eb4d27f96e799f,JournalArticle,"ABSTRACT Understanding the developmental trajectories of big data analytics in the corporate context is highly relevant for information systems research and practice. This study presents a comprehensive bibliometric analysis of applications of big data analytics in enterprises. The sample for this study contained a total of 1727 articles from the Scopus database. The sample was analyzed with techniques such as bibliographic coupling, citation analysis, co-word analysis, and co-authorship analysis. Findings from the co-citation analysis identified four major thematic areas in the extant literature. The evolution of these thematic areas was documented with dynamic co-citation analysis.",2020,141,14,"[30757622.0, 1889986.0, 3062994.0]",30757622.0,Kiev,2,"[46555602.0, 143678972.0]",Y,"['It might be worthwhile to briefly describe the encoding/construction algorithm used in the paper.', '- The results on SQUAD seem pretty weak - 52-64%, compared to the SOTA of 81.']",,,Enterprise Information Systems,"['analysis', 'data', 'analytics', 'big data', 'Big Data']"
Big Data,18d87bff073687c025f9bd23ab2dfb20d5f72a66,BIM Big Data Storage in WebVRGIS,https://www.semanticscholar.org/paper/18d87bff073687c025f9bd23ab2dfb20d5f72a66,JournalArticle,"In the context of big data and the Internet of Things, with the advancement of geospatial data acquisition and retrieval, the volume of available geospatial data is increasing every minute. Thus, new data-management architecture is needed. We proposed a building information model (BIM) big data-storage-management solution with hybrid storage architecture based on web virtual reality geographical information system (WebVRGIS). BIM is associated with the integration of spatial and semantic information on the various stages of urban building. In this paper, based on the spatial distribution characteristics of BIM geospatial big data, a data storage and management model is proposed for BIM geospatial big data management. The architecture primarily includes Not only Structured Query Language (NoSQL) database and distributed peer-to-peer storage. The evaluation of the proposed storage method is conducted on the same software platform as our previous research about WebVR. The experimental results show that the hybrid storage architecture proposed in this research has a lower response time compared to the traditional relational database in geospatial big data searches. The integration and fusion of BIM big data in WebVRGIS realizes a revolutionary transformation of city information management during a full lifecycle. The system also has great promise for the storage of other geospatial big data, such as traffic data.",2020,129,16,"[145772858.0, 2116431916.0, 38929912.0, 9138305.0]",145772858.0,Skopje,3,"[40497400.0, 2006530617.0, 1699159.0]",Y,"['2. The proposed attention mechanism seems to be fairly domain (or task ) specific, and may not be beneficial for strong generalization (generalization over unseen category).', 'For better clarity figures 3 and 5 should be made bigger.', '[1] Semi-Supervised Learning with Generative Adversarial Networks (https://arxiv.org/abs/1606.01583) [2] Good Semi-supervised Learning that Requires a Bad GAN (https://arxiv.org/abs/1705.09783)']",,,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'bim', 'big data', 'Big Data']"
Big Data,18d87bff073687c025f9bd23ab2dfb20d5f72a66,BIM Big Data Storage in WebVRGIS,https://www.semanticscholar.org/paper/18d87bff073687c025f9bd23ab2dfb20d5f72a66,JournalArticle,"In the context of big data and the Internet of Things, with the advancement of geospatial data acquisition and retrieval, the volume of available geospatial data is increasing every minute. Thus, new data-management architecture is needed. We proposed a building information model (BIM) big data-storage-management solution with hybrid storage architecture based on web virtual reality geographical information system (WebVRGIS). BIM is associated with the integration of spatial and semantic information on the various stages of urban building. In this paper, based on the spatial distribution characteristics of BIM geospatial big data, a data storage and management model is proposed for BIM geospatial big data management. The architecture primarily includes Not only Structured Query Language (NoSQL) database and distributed peer-to-peer storage. The evaluation of the proposed storage method is conducted on the same software platform as our previous research about WebVR. The experimental results show that the hybrid storage architecture proposed in this research has a lower response time compared to the traditional relational database in geospatial big data searches. The integration and fusion of BIM big data in WebVRGIS realizes a revolutionary transformation of city information management during a full lifecycle. The system also has great promise for the storage of other geospatial big data, such as traffic data.",2020,129,16,"[145772858.0, 2116431916.0, 38929912.0, 9138305.0]",145772858.0,Skopje,3,"[40497400.0, 2006530617.0, 1699159.0]",Y,"['2. The proposed attention mechanism seems to be fairly domain (or task ) specific, and may not be beneficial for strong generalization (generalization over unseen category).', 'For better clarity figures 3 and 5 should be made bigger.', '[1] Semi-Supervised Learning with Generative Adversarial Networks (https://arxiv.org/abs/1606.01583) [2] Good Semi-supervised Learning that Requires a Bad GAN (https://arxiv.org/abs/1705.09783)']",,,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'bim', 'data storage', 'Big Data']"
Data Storage,18d87bff073687c025f9bd23ab2dfb20d5f72a66,BIM Big Data Storage in WebVRGIS,https://www.semanticscholar.org/paper/18d87bff073687c025f9bd23ab2dfb20d5f72a66,JournalArticle,"In the context of big data and the Internet of Things, with the advancement of geospatial data acquisition and retrieval, the volume of available geospatial data is increasing every minute. Thus, new data-management architecture is needed. We proposed a building information model (BIM) big data-storage-management solution with hybrid storage architecture based on web virtual reality geographical information system (WebVRGIS). BIM is associated with the integration of spatial and semantic information on the various stages of urban building. In this paper, based on the spatial distribution characteristics of BIM geospatial big data, a data storage and management model is proposed for BIM geospatial big data management. The architecture primarily includes Not only Structured Query Language (NoSQL) database and distributed peer-to-peer storage. The evaluation of the proposed storage method is conducted on the same software platform as our previous research about WebVR. The experimental results show that the hybrid storage architecture proposed in this research has a lower response time compared to the traditional relational database in geospatial big data searches. The integration and fusion of BIM big data in WebVRGIS realizes a revolutionary transformation of city information management during a full lifecycle. The system also has great promise for the storage of other geospatial big data, such as traffic data.",2020,129,16,"[145772858.0, 2116431916.0, 38929912.0, 9138305.0]",145772858.0,Reykjavik,2,"[2168285733.0, 145433223.0]",Y,"['Motivation In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle.', '2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing.']",,,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'bim', 'big data', 'Data Storage']"
Data Storage,18d87bff073687c025f9bd23ab2dfb20d5f72a66,BIM Big Data Storage in WebVRGIS,https://www.semanticscholar.org/paper/18d87bff073687c025f9bd23ab2dfb20d5f72a66,JournalArticle,"In the context of big data and the Internet of Things, with the advancement of geospatial data acquisition and retrieval, the volume of available geospatial data is increasing every minute. Thus, new data-management architecture is needed. We proposed a building information model (BIM) big data-storage-management solution with hybrid storage architecture based on web virtual reality geographical information system (WebVRGIS). BIM is associated with the integration of spatial and semantic information on the various stages of urban building. In this paper, based on the spatial distribution characteristics of BIM geospatial big data, a data storage and management model is proposed for BIM geospatial big data management. The architecture primarily includes Not only Structured Query Language (NoSQL) database and distributed peer-to-peer storage. The evaluation of the proposed storage method is conducted on the same software platform as our previous research about WebVR. The experimental results show that the hybrid storage architecture proposed in this research has a lower response time compared to the traditional relational database in geospatial big data searches. The integration and fusion of BIM big data in WebVRGIS realizes a revolutionary transformation of city information management during a full lifecycle. The system also has great promise for the storage of other geospatial big data, such as traffic data.",2020,129,16,"[145772858.0, 2116431916.0, 38929912.0, 9138305.0]",145772858.0,Reykjavik,2,"[2168285733.0, 145433223.0]",Y,"['Motivation In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle.', '2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing.']",,,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'bim', 'data storage', 'Data Storage']"
Big Data,cc1e82125f7f8636b25ccdcdb63e8f812add7f87,A Big Data Enabled Channel Model for 5G Wireless Communication Systems,https://www.semanticscholar.org/paper/cc1e82125f7f8636b25ccdcdb63e8f812add7f87,JournalArticle,"The standardization process of the fifth generation (5G) wireless communications has recently been accelerated and the first commercial 5G services would be provided as early as in 2018. The increasing of enormous smartphones, new complex scenarios, large frequency bands, massive antenna elements, and dense small cells will generate big datasets and bring 5G communications to the era of big data. This paper investigates various applications of big data analytics, especially machine learning algorithms in wireless communications and channel modeling. We propose a big data and machine learning enabled wireless channel model framework. The proposed channel model is based on artificial neural networks (ANNs), including feed-forward neural network (FNN) and radial basis function neural network (RBF-NN). The input parameters are transmitter (Tx) and receiver (Rx) coordinates, Tx–Rx distance, and carrier frequency, while the output parameters are channel statistical properties, including the received power, root mean square (RMS) delay spread (DS), and RMS angle spreads (ASs). Datasets used to train and test the ANNs are collected from both real channel measurements and a geometry based stochastic model (GBSM). Simulation results show good performance and indicate that machine learning algorithms can be powerful analytical tools for future measurement-based wireless channel modeling.",2020,82,6,"[2144794493.0, 50096877.0, 2075396941.0, 2152916916.0, 2149869431.0, 1735048.0, 2236529.0]",2144794493.0,Berlin,3,"[2061881331.0, 3029700.0, 2183294.0]",Y,"['The experiments were performed on relatively shallow networks (8 to 26 layers).', '5. The paper, in general, needs to be polished.', '* The divergences presented in this work are different from the above since the risk is minimised over a parametric class instead of over the whole set of integrable functions.']",,,IEEE Transactions on Big Data,"['channel', 'g', 'communications', 'big data', 'Big Data']"
Big Data,0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1,Big Data and Its Applications in Smart Real Estate and the Disaster Management Life Cycle: A Systematic Analysis,https://www.semanticscholar.org/paper/0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1,JournalArticle,"Big data is the concept of enormous amounts of data being generated daily in different fields due to the increased use of technology and internet sources. Despite the various advancements and the hopes of better understanding, big data management and analysis remain a challenge, calling for more rigorous and detailed research, as well as the identifications of methods and ways in which big data could be tackled and put to good use. The existing research lacks in discussing and evaluating the pertinent tools and technologies to analyze big data in an efficient manner which calls for a comprehensive and holistic analysis of the published articles to summarize the concept of big data and see field-specific applications. To address this gap and keep a recent focus, research articles published in last decade, belonging to top-tier and high-impact journals, were retrieved using the search engines of Google Scholar, Scopus, and Web of Science that were narrowed down to a set of 139 relevant research articles. Different analyses were conducted on the retrieved papers including bibliometric analysis, keywords analysis, big data search trends, and authors’ names, countries, and affiliated institutes contributing the most to the field of big data. The comparative analyses show that, conceptually, big data lies at the intersection of the storage, statistics, technology, and research fields and emerged as an amalgam of these four fields with interlinked aspects such as data hosting and computing, data management, data refining, data patterns, and machine learning. The results further show that major characteristics of big data can be summarized using the seven Vs, which include variety, volume, variability, value, visualization, veracity, and velocity. Furthermore, the existing methods for big data analysis, their shortcomings, and the possible directions were also explored that could be taken for harnessing technology to ensure data analysis tools could be upgraded to be fast and efficient. The major challenges in handling big data include efficient storage, retrieval, analysis, and visualization of the large heterogeneous data, which can be tackled through authentication such as Kerberos and encrypted files, logging of attacks, secure communication through Secure Sockets Layer (SSL) and Transport Layer Security (TLS), data imputation, building learning models, dividing computations into sub-tasks, checkpoint applications for recursive tasks, and using Solid State Drives (SDD) and Phase Change Material (PCM) for storage. In terms of frameworks for big data management, two frameworks exist including Hadoop and Apache Spark, which must be used simultaneously to capture the holistic essence of the data and make the analyses meaningful, swift, and speedy. Further field-specific applications of big data in two promising and integrated fields, i.e., smart real estate and disaster management, were investigated, and a framework for field-specific applications, as well as a merger of the two areas through big data, was highlighted. The proposed frameworks show that big data can tackle the ever-present issues of customer regrets related to poor quality of information or lack of information in smart real estate to increase the customer satisfaction using an intermediate organization that can process and keep a check on the data being provided to the customers by the sellers and real estate managers. Similarly, for disaster and its risk management, data from social media, drones, multimedia, and search engines can be used to tackle natural disasters such as floods, bushfires, and earthquakes, as well as plan emergency responses. In addition, a merger framework for smart real estate and disaster risk management show that big data generated from the smart real estate in the form of occupant data, facilities management, and building integration and maintenance can be shared with the disaster risk management and emergency response teams to help prevent, prepare, respond to, or recover from the disasters.",2020,84,4,"[71940750.0, 1657644010.0, 48633912.0, 90426770.0]",71940750.0,Zagreb,3,"[1741623.0, 2158502526.0, 1763086.0]",Y,"['The authors should consider exploring and discussing the effects of adding/moving/removing objects on the performance.', 'This can be used to simplify inference in cases were the gaussian process behaviour is desired, and opens questions on how to avoid this behaviour the rest of the time.', 'The paper notices through simulations that in a grid search over the initial parameters of generator optimal discriminator training always succeeds in recovering the true generator parameters, whereas the other two methods fail and exhibit mode collapse.']",,,Big Data and Cognitive Computing,"['data', 'management', 'analysis', 'big data', 'Big Data']"
Big Data,8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6,Applying big data based deep learning system to intrusion detection,https://www.semanticscholar.org/paper/8c4cb10c6d40b8f1d570944269a0a2e680dd5eb6,JournalArticle,"With vast amounts of data being generated daily and the ever increasing interconnectivity of the world's internet infrastructures, a machine learning based Intrusion Detection Systems (IDS) has become a vital component to protect our economic and national security. Previous shallow learning and deep learning strategies adopt the single learning model approach for intrusion detection. The single learning model approach may experience problems to understand increasingly complicated data distribution of intrusion patterns. Particularly, the single deep learning model may not be effective to capture unique patterns from intrusive attacks having a small number of samples. In order to further enhance the performance of machine learning based IDS, we propose the Big Data based Hierarchical Deep Learning System (BDHDLS). BDHDLS utilizes behavioral features and content features to understand both network traffic characteristics and information stored in the payload. Each deep learning model in the BDHDLS concentrates its efforts to learn the unique data distribution in one cluster. This strategy can increase the detection rate of intrusive attacks as compared to the previous single learning model approaches. Based on parallel training strategy and big data techniques, the model construction time of BDHDLS is reduced substantially when multiple machines are deployed.",2020,77,3,"[49408448.0, 2052212942.0, 144310754.0]",49408448.0,Chisinau,2,"[1602998334.0, 2327930.0]",Y,"['The text states that the model was trained where each observation in an episode comprised randomly sampled datapoints.', 'The theoretical results are clearly stated as lemmas a theorems that one can follow without looking at proofs.']",,,Big Data Mining and Analytics,"['learning', 'model', 'data', 'big data', 'Big Data']"
Big Data,ed9e7821b3e51c7e59183300d6c8cf90c8de0f26,COVID-19 is spatial: Ensuring that mobile Big Data is used for social good,https://www.semanticscholar.org/paper/ed9e7821b3e51c7e59183300d6c8cf90c8de0f26,JournalArticle,"The mobility restrictions related to COVID-19 pandemic have resulted in the biggest disruption to individual mobilities in modern times. The crisis is clearly spatial in nature, and examining the geographical aspect is important in understanding the broad implications of the pandemic. The avalanche of mobile Big Data makes it possible to study the spatial effects of the crisis with spatiotemporal detail at the national and global scales. However, the current crisis also highlights serious limitations in the readiness to take the advantage of mobile Big Data for social good, both within and beyond the interests of health sector. We propose two strategical pathways for the future use of mobile Big Data for societal impact assessment, addressing access to both raw mobile Big Data as well as aggregated data products. Both pathways require careful considerations of privacy issues, harmonized and transparent methodologies, and attention to the representativeness, reliability and continuity of data. The goal is to be better prepared to use mobile Big Data in future crises.",2020,64,7,"[101768561.0, 4902441.0, 32343498.0, 4478199.0]",101768561.0,Ljubljana,3,"[1409895301.0, 2061067079.0, 8050194.0]",Y,"['The paper uses some interesting properties of the CPD model to derive an efficient optimization solver for the BCD subproblems.', ""The authors' technique may let us do this data-generation easily."", 'The justification given is that it is ""to address the difficulty of training due to the complex nature of the problem"" but this is not really satisfying as the problems are not that hard.']",,,Big Data & Society,"['data', 'crisis', 'pathways', 'big data', 'Big Data']"
Big Data,7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca,The value of Big Data in government: The case of ‘smart cities’,https://www.semanticscholar.org/paper/7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca,JournalArticle,"The emergence of Big Data has added a new aspect to conceptualizing the use of digital technologies in the delivery of public services and for realizing digital governance. This article explores, via the ‘value-chain’ approach, the evolution of digital governance research, and aligns it with current developments associated with data analytics, often referred to as ‘Big Data’. In many ways, the current discourse around Big Data reiterates and repeats established commentaries within the eGovernment research community. This body of knowledge provides an opportunity to reflect on the ‘promise’ of Big Data, both in relation to service delivery and policy formulation. This includes, issues associated with the quality and reliability of data, from mixing public and private sector data, issues associated with the ownership of raw and manipulated data, and ethical issues concerning surveillance and privacy. These insights and the issues raised help assess the value of Big Data in government and smart city environments.",2020,58,7,"[2235459675.0, 2285816475.0, 2285813122.0]",2235459675.0,Monaco,3,"[2221446410.0, 2109215851.0, 2103454094.0]",Y,"['* I understood that the conclusion of part 3 was that the expectation of eq (9) was elegantly computable for certain non-linearity (including ReLU).', 'There are many typos (e.g., ""This advantage is also its difficulty"", ""Much previous work on language modeling has evaluated "", ""We focus in on the task"", and others) so the writing needs to be significantly improved for it to be a conference paper, preferably with some help from a native English speaker.', 'Since this could be a potential disadvantage, some discussions or empirical study on cross-category generalization seems to be interesting.']",,,Big Data & Society,"['data', 'issues', 'delivery', 'big data', 'Big Data']"
Data Processing,02b1607af35b48f0bd716367caf6a7428b969369,AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty,https://www.semanticscholar.org/paper/02b1607af35b48f0bd716367caf6a7428b969369,JournalArticle,"Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.",2019,988,abs/1912.02781,"[3422872.0, 52227748.0, 8132903.0, 2368067.0, 2058362.0, 40627523.0]",3422872.0,Luxembourg,2,"[150281558.0, 2054289955.0]",Y,"['The problem set-up of unpaired summarization is not particularly compelling, since summaries are typically found paired with their original documents.', '* A thorough discussion on mixing in feature space, as well as a baseline which mizes in feature space.']",,,International Conference on Learning Representations,"['accuracy', 'distribution', 'test', 'data processing', 'Data Processing']"
Data Processing,595101f13b961d69c553ce1ed24f60f3f1085e02,RecSSD: near data processing for solid state drive based recommendation inference,https://www.semanticscholar.org/paper/595101f13b961d69c553ce1ed24f60f3f1085e02,Conference,"Neural personalized recommendation models are used across a wide variety of datacenter applications including search, social media, and entertainment. State-of-the-art models comprise large embedding tables that have billions of parameters requiring large memory capacities. Unfortunately, large and fast DRAM-based memories levy high infrastructure costs. Conventional SSD-based storage solutions offer an order of magnitude larger capacity, but have worse read latency and bandwidth, degrading inference performance. RecSSD is a near data processing based SSD memory system customized for neural recommendation inference that reduces end-to-end model inference latency by 2× compared to using COTS SSDs across eight industry-representative models.",2021,80,,"[49212289.0, 2633839.0, 1481699378.0, 2061231.0, 2797270.0, 1896817.0, 2255803.0]",49212289.0,Stockholm,3,"[144588806.0, 2059271276.0, 32528506.0]",Y,"['On the negative side, the paper is only qualitative.', 'So, the main contribution of the paper is to do all the sensitivity calculations with respect to SVM problem and then use the importance sampling theory to obtain bounds on the coreset size.', 'If the text is from the user, a named entity recognizer is used.']",International Conference on Architectural Support for Programming Languages and Operating Systems,26.0,,"['models', 'inference', 'recommendation', 'data processing', 'Data Processing']"
Data Processing,41d04aa3c25dcfbf1b44ce666c48759e03c216c7,tf.data: A Machine Learning Data Processing Framework,https://www.semanticscholar.org/paper/41d04aa3c25dcfbf1b44ce666c48759e03c216c7,JournalArticle,"Training machine learning models requires feeding input data for models to ingest. Input pipelines for machine learning jobs are often challenging to implement efficiently as they require reading large volumes of data, applying complex transformations, and transferring data to hardware accelerators while overlapping computation and communication to achieve optimal performance. We present tf.data, a framework for building and executing efficient input pipelines for machine learning jobs. The tf.data API provides operators which can be parameterized with user-defined computation, composed, and reused across different machine learning domains. These abstractions allow users to focus on the application logic of data processing, while tf.data's runtime ensures that pipelines run efficiently. 
We demonstrate that input pipeline performance is critical to the end-to-end training time of state-of-the-art machine learning models. tf.data delivers the high performance required, while avoiding the need for manual tuning of performance knobs. We show that tf.data features, such as parallelism, caching, static optimizations, and non-deterministic execution are essential for high performance. Finally, we characterize machine learning input pipelines for millions of jobs that ran in Google's fleet, showing that input data processing is highly diverse and consumes a significant fraction of job resources. Our analysis motivates future research directions, such as sharing computation across jobs and pushing data projection to the storage layer.",2021,58,14,"[20154699.0, 38300863.0, 2285439911.0, 120658773.0]",20154699.0,Chisinau,2,"[2078528337.0, 2114147314.0]",Y,"['Could you explain how classes are predicted given a test problem?', ""I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships.""]",,,Proceedings of the VLDB Endowment,"['machine', 'input', 'data', 'data processing', 'Data Processing']"
Data Processing,ada0b87cd5c30d31186c38fb12e631d29426a3bf,Spark SQL: Relational Data Processing in Spark,https://www.semanticscholar.org/paper/ada0b87cd5c30d31186c38fb12e631d29426a3bf,Conference,"Spark SQL is a new module in Apache Spark that integrates relational processing with Spark's functional programming API. Built on our experience with Shark, Spark SQL lets Spark programmers leverage the benefits of relational processing (e.g. declarative queries and optimized storage), and lets SQL users call complex analytics libraries in Spark (e.g. machine learning). Compared to previous systems, Spark SQL makes two main additions. First, it offers much tighter integration between relational and procedural processing, through a declarative DataFrame API that integrates with procedural Spark code. Second, it includes a highly extensible optimizer, Catalyst, built using features of the Scala programming language, that makes it easy to add composable rules, control code generation, and define extension points. Using Catalyst, we have built a variety of features (e.g. schema inference for JSON, machine learning types, and query federation to external databases) tailored for the complex needs of modern data analysis. We see Spark SQL as an evolution of both SQL-on-Spark and of Spark itself, offering richer APIs and optimizations while keeping the benefits of the Spark programming model.",2015,1288,,"[144482217.0, 2066641.0, 1387529184.0, 40199213.0, 2536434.0, 2086593199.0, 39309572.0, 2403754.0, 143666627.0, 38565890.0, 143834867.0]",144482217.0,Vaduz,3,"[2082427140.0, 36347083.0, 2116579935.0]",Y,"['Detailed comments: The problem of unsupervised time series clustering is important and challenging.', 'This paper is clearly written, proposes a simple model and seems to outperform current methods.', 'Does each component is related to a certain topic?']",SIGMOD Conference,15.0,,"['spark', 'sql', 'processing', 'data processing', 'Data Processing']"
Data Processing,f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa,Cloud-Edge Orchestration for the Internet of Things: Architecture and AI-Powered Data Processing,https://www.semanticscholar.org/paper/f8056d942a1d8fe0ef73a6bd7758b3e12b0956fa,JournalArticle,"The Internet of Things (IoT) has been deeply penetrated into a wide range of important and critical sectors, including smart city, water, transportation, manufacturing, and smart factory. Massive data are being acquired from a fast growing number of IoT devices. Efficient data processing is a necessity to meet diversified and stringent requirements of many emerging IoT applications. Due to the constrained computation and storage resources, IoT devices have resorted to the powerful cloud computing to process their data. However, centralized and remote cloud computing may introduce unacceptable communication delay since its physical location is far away from IoT devices. Edge cloud has been introduced to overcome this issue by moving the cloud in closer proximity to IoT devices. The orchestration and cooperation between the cloud and the edge provides a crucial computing architecture for IoT applications. Artificial intelligence (AI) is a powerful tool to enable the intelligent orchestration in this architecture. This article first introduces such a kind of computing architecture from the perspective of IoT applications. It then investigates the state-of-the-art proposals on AI-powered cloud-edge orchestration for the IoT. Finally, a list of potential research challenges and open issues is provided and discussed, which can provide useful resources for carrying out future research in this area.",2020,89,8,[2947357.0],2947357.0,Warsaw,3,"[2124473614.0, 1947239202.0, 3045593.0]",Y,"[""The theory is not strong and the experiments don't necessary support the intuitive claims made in the paper."", 'I myself am very curious about what would happen and would love to see this exchange catalyzed.', '“Customers” randomly appear throughout the task and the taxi agents receive reward for moving to the same square as a customer.']",,,IEEE Internet of Things Journal,"['iot', 'cloud', 'data', 'data processing', 'Data Processing']"
Data Processing,febe776e285dc5e72c7e3ee697a87a794e1c00ff,Privacy-Preserving Data Processing with Flexible Access Control,https://www.semanticscholar.org/paper/febe776e285dc5e72c7e3ee697a87a794e1c00ff,JournalArticle,"Cloud computing provides an efficient and convenient platform for cloud users to store, process and control their data. Cloud overcomes the bottlenecks of resource-constrained user devices and greatly releases their storage and computing burdens. However, due to the lack of full trust in cloud service providers, the cloud users generally prefer to outsource their sensitive data in an encrypted form, which, however, seriously complicates data processing, analysis, as well as access control. Homomorphic encryption (HE) as a single key system cannot flexibly control data sharing and access after encrypted data processing. How to realize various computations over encrypted data in an efficient way and at the same time flexibly control the access to data processing results has been an important challenging issue. In this paper, we propose a privacy-preserving data processing scheme with flexible access control. With the cooperation of a data service provider (DSP) and a computation party (CP), our scheme, based on Paillier's partial homomorphic encryption (PHE), realizes seven basic operations, i.e., Addition, Subtraction, Multiplication, Sign Acquisition, Absolute, Comparison, and Equality Test, over outsourced encrypted data. In addition, our scheme, based on the homomorphism of attribute-based encryption (ABE), is also designed to support flexible access control over processing results of encrypted data. We further prove the security of our scheme and demonstrate its efficiency and advantages through simulations and comparisons with existing work.",2020,65,17,"[2234493.0, 145843577.0, 1713669.0]",2234493.0,Tallinn,2,"[1706922.0, 2108129412.0]",Y,"['Flexible muscle-based locomotion for bipedal creatures.', 'Linear CCA naturally would not be sufficient for generative modeling and its non-linear variants (e.g. Wang et al. ""Deep variational canonical correlation analysis"", arXiv:1610.03454, 2016; Damianou et al. ""Manifold relevance determination"", ICML, 2012) would not produce visually pleasing generative samples either, but the relationship is so close that these models have even been used for analysing setups identical to yours (e.g. Li et al. ""Cross-pose face recognition by canonical correlation analysis"", arXiv:1507.08076, 2015) but with goals other than generation.']",,,IEEE Transactions on Dependable and Secure Computing,"['data', 'cloud', 'processing', 'data processing', 'Data Processing']"
Data Processing,22c141b489e6e189f5996537b0a908fc10f90de7,Adaptive and Fault-Tolerant Data Processing in Healthcare IoT Based on Fog Computing,https://www.semanticscholar.org/paper/22c141b489e6e189f5996537b0a908fc10f90de7,JournalArticle,"In recent years, healthcare IoT have been helpful in mitigating pressures of hospital and medical resources caused by aging population to a large extent. As a safety-critical system, the rapid response from the health care system is extremely important. To fulfill the low latency requirement, fog computing is a competitive solution by deploying healthcare IoT devices on the edge of clouds. However, these fog devices generate huge amount of sensor data. Designing a specific framework for fog devices to ensure reliable data transmission and rapid data processing becomes a topic of utmost significance. In this paper, a Reduced Variable Neighborhood Search (RVNS)-based sEnsor Data Processing Framework (REDPF) is proposed to enhance reliability of data transmission and processing speed. Functionalities of REDPF include fault-tolerant data transmission, self-adaptive filtering and data-load-reduction processing. Specifically, a reliable transmission mechanism, managed by a self-adaptive filter, will recollect lost or inaccurate data automatically. Then, a new scheme is designed to evaluate the health status of the elderly people. Through extensive simulations, we show that our proposed scheme improves network reliability, and provides a faster processing speed.",2020,68,7,"[28014924.0, 2112573464.0, 144206960.0, 39006765.0, 144123438.0]",28014924.0,Dublin,2,"[2115905112.0, 2173602.0]",Y,"['The authors rely on two prior works in multi-task learning  that explore parameter sharing (Lee et al, 2016) and subspace learning (Kumar & Daume III 2012) for multi-task learning.', 'The authors propose a new gradient compression method for efficient distributed training of neural networks.']",,,IEEE Transactions on Network Science and Engineering,"['data', 'transmission', 'fog', 'data processing', 'Data Processing']"
Data Processing,ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee,Learning scheduling algorithms for data processing clusters,https://www.semanticscholar.org/paper/ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee,Conference,"Efficiently scheduling data processing jobs on distributed compute clusters requires complex algorithms. Current systems use simple, generalized heuristics and ignore workload characteristics, since developing and tuning a scheduling policy for each workload is infeasible. In this paper, we show that modern machine learning techniques can generate highly-efficient policies automatically. Decima uses reinforcement learning (RL) and neural networks to learn workload-specific scheduling algorithms without any human instruction beyond a high-level objective, such as minimizing average job completion time. However, off-the-shelf RL techniques cannot handle the complexity and scale of the scheduling problem. To build Decima, we had to develop new representations for jobs' dependency graphs, design scalable RL models, and invent RL training methods for dealing with continuous stochastic job arrivals. Our prototype integration with Spark on a 25-node cluster shows that Decima improves average job completion time by at least 21% over hand-tuned scheduling heuristics, achieving up to 2x improvement during periods of high cluster load.",2018,509,,"[2512621.0, 1962485.0, 2043402.0, 40071013.0, 79404966.0]",2512621.0,Podgorica,3,"[3024698.0, 1473151134.0, 151213231.0]",Y,"['The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss.', 'This paper studies the problem of multi-label learning for text copora.', 'So, I wish to see a section on testing with Resnet and GoogleNet.']","Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",18.0,,"['scheduling', 'rl', 'decima', 'data processing', 'Data Processing']"
Data Processing,f90f526b101cb8a0260f5165a3875928c58ae48a,NATSA: A Near-Data Processing Accelerator for Time Series Analysis,https://www.semanticscholar.org/paper/f90f526b101cb8a0260f5165a3875928c58ae48a,Conference,"Time series analysis is a key technique for extracting and predicting events in domains as diverse as epidemiology, genomics, neuroscience, environmental sciences, economics, and more. Matrix profile, the state-of-the-art algorithm to perform time series analysis, computes the most similar subsequence for a given query subsequence within a sliced time series. Matrix profile has low arithmetic intensity, but it typically operates on large amounts of time series data. In current computing systems, this data needs to be moved between the off-chip memory units and the on-chip computation units for performing matrix profile. This causes a major performance bottleneck as data movement is extremely costly in terms of both execution time and energy. In this work, we present NATSA, the first Near-Data Processing accelerator for time series analysis. The key idea is to exploit modern 3D-stacked High Bandwidth Memory (HBM) to enable efficient and fast specialized matrix profile computation near memory, where time series data resides. NATSA provides three key benefits: 1) quickly computing the matrix profile for a wide range of applications by building specialized energy-efficient floating-point arithmetic processing units close to HBM, 2) improving the energy efficiency and execution time by reducing the need for data movement over slow and energy-hungry buses between the computation units and the memory units, and 3) analyzing time series data at scale by exploiting low-latency, high-bandwidth, and energy-efficient memory access provided by HBM. Our experimental evaluation shows that NATSA improves performance by up to 14.2× (9.9× on average) and reduces energy by up to 27.2 × (19.4 × on average), over the state-of-the-art multi-core implementation. NATSA also improves performance by 6.3 × and reduces energy by 10.2 × over a general-purpose NDP platform with 64 in-order cores.",2020,62,,"[2061067079.0, 1939292.0, 46175739.0, 3387282.0, 1474355913.0, 2059547812.0, 3190187.0, 145929920.0]",2061067079.0,Sofia,3,"[1738190.0, 37722032.0, 47781311.0]",Y,"['The encoder network from NEM can be seen as a recurrent network which takes one latent variable theta_k at time t and some input x to produce the next latent variable theta_k at time t+1.', 'How does this compare to the near-identity constraints in resnets in Shaham et al.', 'I assume that it involves \\hat{M}, but it would be good to formally define this notation.']",ICCD,38.0,,"['time', 'series', 'data', 'data processing', 'Data Processing']"
Data Processing,d6bc29a897fd85e7187dc33c3c974b8879462237,BloCkEd: Blockchain-Based Secure Data Processing Framework in Edge Envisioned V2X Environment,https://www.semanticscholar.org/paper/d6bc29a897fd85e7187dc33c3c974b8879462237,JournalArticle,"There has been an increasing trend of moving computing activities closer to the edge of the network, particularly in smart city applications (e.g., vehicle-to-everything – V2X). Such a paradigm allows the end user’s requests to be handled/processed by nodes at the edge of the network; thus, reducing latency, and preserving privacy of user data/activities. However, there are a number of challenges in such an edge computing ecosystem. Examples include (1) potential inappropriate utilization of resources at the edge nodes, (2) operational challenges in cache management and data integrity due to data migration between edge nodes, particularly when dealing with vehicular mobility in a V2X application, and (3) high energy consumption due to continuous link breakage and subsequent reestablishment of link(s). Therefore in this paper, we design a blockchain-based secure data processing framework for an edge envisioned V2X environment (hereafter referred to as BloCkEd). Specifically, a multi-layered edge-enabled V2X system model for BloCkEd is presented, which includes the formulation of a multi-objective optimization problem. In addition, BloCkEd comprises an optimal container-based data processing scheme, and a blockchain-based data integrity management scheme, designed to minimize link breakage and reducing latency. Using Chandigarh City, India, as the scenario, we implement and evaluate the proposed approach in terms of its latency, energy consumption, and service level agreement compliance.",2020,58,69,"[8880101.0, 48775545.0, 2110675251.0, 144996075.0, 116022805.0]",8880101.0,Dublin,2,"[51235411.0, 100653750.0]",Y,"['Otherwise, the perfect latent representation is z=x.', '- In Section 4.3, why did you consider the entropy regularizer?']",,,IEEE Transactions on Vehicular Technology,"['edge', 'data', 'latency', 'data processing', 'Data Processing']"
Data Processing,8d942a3b52e2ad16ff8e5970be59591970d89fae,Big Data Processing Workflows Oriented Real-Time Scheduling Algorithm using Task-Duplication in Geo-Distributed Clouds,https://www.semanticscholar.org/paper/8d942a3b52e2ad16ff8e5970be59591970d89fae,JournalArticle,"Scheduling big data processing workflows involves both large-scale tasks and transmission of massive intermediate data among tasks, thus optimizing their completion time and monetary cost becomes a challenging issue. Besides, data streams are continuously generated, and dynamically submitted to clouds for real-time or near real-time processing. Naturally, responsive schedules are required to keep pace with such dynamic environments and this further aggravates the difficulty of the workflow scheduling problem. To address these issues, we first derive two theorems to minimize the completion time of a set of parallel workflow tasks and the start time of each workflow task, and then define the latest finish time for workflow tasks, which is also proved its advantage in reducing costs without delaying the completion of workflows. On the basis of these theorems, we propose a novel real-time scheduling algorithm using task-duplication, RTSATD, such that minimizing both the completion time and monetary cost of processing big data workflows in clouds. The performance of RTSATD is analyzed by using both synthesized and real-world workflows. The experimental results demonstrate the superiority of the proposed algorithm with respect to completion time (up to 28.73 percent) and resource utilization (up to 46.31 percent) over two existing approaches.",2020,43,6,"[2382347.0, 2090538077.0, 1731634.0, 46532001.0]",2382347.0,Ljubljana,3,"[2186578511.0, 2287603163.0, 145494588.0]",Y,"['The dataset consists of over 100,000 images and the questions are from 15 templates.', 'Why the error rate reported here is higher than that in the original paper?', 'How much can change between the goal images and the environment before the system fails?']",,,IEEE Transactions on Big Data,"['time', 'completion', 'data', 'data processing', 'Data Processing']"
Data Processing,d9a7fa7616a327367696e19b1846519745cd43ff,The Devil Is in the Details: Delving Into Unbiased Data Processing for Human Pose Estimation,https://www.semanticscholar.org/paper/d9a7fa7616a327367696e19b1846519745cd43ff,Conference,"Recently, the leading performance of human pose estimation is dominated by top-down methods. Being a fundamental component in training and inference, data processing has not been systematically considered in pose estimation community, to the best of our knowledge. In this paper, we focus on this problem and find that the devil of top-down pose estimator is in the biased data processing. Specifically, by investigating the standard data processing in state-of-the-art approaches mainly including data transformation and encoding-decoding, we find that the results obtained by common flipping strategy are unaligned with the original ones in inference. Moreover, there is statistical error in standard encoding-decoding during both training and inference. Two problems couple together and significantly degrade the pose estimation performance. Based on quantitative analyses, we then formulate a principled way to tackle this dilemma. Data is processed in continuous space based on unit length (the intervals between pixels) instead of in discrete space with pixel, and a combined classification and regression approach is adopted to perform encoding-decoding. The Unbiased Data Processing (UDP) for human pose estimation can be achieved by combining the two together. UDP not only boosts the performance of existing methods by a large margin but also plays a important role in result reproducing and future exploration. As a model-agnostic approach, UDP promotes SimpleBaseline-ResNet50-256x192 by 1.5 AP (70.2 to 71.7) and HRNet-W32-256x192 by 1.7 AP (73.5 to 75.2) on COCO test-dev set. The HRNet-W48-384x288 equipped with UDP achieves 76.5 AP and sets a new state-of-the-art for human pose estimation. The source code is publicly available for further research.",2019,134,,"[47513708.0, 2118932732.0, 2065456437.0, 143986385.0]",47513708.0,Oslo,3,"[2070951368.0, 2112197162.0, 6667699.0]",Y,"['This paper also would probably be more suitable for a chemoinformatics journal, where the rationale learning would be highly appreciated.', ""But beyond this, and despite fairly reasonable familiarity with the subject, I simply don't understand other elements that the paper is talking about."", 'Some empirical observations are made, but it is not discussed whether what is observed is surprising in any way, or just as expected?']",Computer Vision and Pattern Recognition,19.0,,"['data', 'estimation', 'udp', 'data processing', 'Data Processing']"
Data Processing,4e746359afd6f81705b875d71cc499b904a320df,ScienceEarth: A Big Data Platform for Remote Sensing Data Processing,https://www.semanticscholar.org/paper/4e746359afd6f81705b875d71cc499b904a320df,JournalArticle,"Mass remote sensing data management and processing is currently one of the most important topics. In this study, we introduce ScienceEarth, a cluster-based data processing framework. The aim of ScienceEarth is to store, manage, and process large-scale remote sensing data in a cloud-based cluster-computing environment. The platform consists of the following three main parts: ScienceGeoData, ScienceGeoIndex, and ScienceGeoSpark. ScienceGeoData stores and manages remote sensing data. ScienceGeoIndex is an index and query system, a spatial index based on quad-tree and Hilbert curve which is combined for heterogeneous tiled remote sensing data that makes efficient data retrieval in ScienceGeoData. ScienceGeoSpark is an easy-to-use computing framework in which we use Apache Spark as the analytics engine for big remote sensing data processing. The result of tests proves that ScienceEarth can efficiently store, retrieve, and process remote sensing data. The results reveal ScienceEarth has the potential and capabilities of efficient big remote sensing data processing.",2020,34,12,"[2153077994.0, 121682029.0, 49356798.0, 1697984.0]",2153077994.0,Madrid,3,"[2279712392.0, 2108930648.0, 2224017005.0]",Y,"['This result would be instructive to see for each of the tasks.', 'Another experiment missing is seeing whether joint supervised-GAN-reconstruction training can outperform purely supervised training.', 'The paper is well written: clear and easy to understand.']",,,Remote Sensing,"['data', 'processing', 'scienceearth', 'data processing', 'Data Processing']"
Data Processing,5a32ebacd5c32d52734f9d2a2cfb5d0cdbe469e2,Toward Big Data Processing in IoT: Path Planning and Resource Management of UAV Base Stations in Mobile-Edge Computing System,https://www.semanticscholar.org/paper/5a32ebacd5c32d52734f9d2a2cfb5d0cdbe469e2,JournalArticle,"Heavy data load and wide cover range have always been crucial problems for big data processing in Internet of Things (IoT). Recently, mobile-edge computing (MEC) and unmanned aerial vehicle base stations (UAV-BSs) have emerged as promising techniques in IoT. In this article, we propose a three-layer online data processing network based on the MEC technique. On the bottom layer, raw data are generated by distributed sensors with local information. Upon them, UAV-BSs are deployed as moving MEC servers, which collect data and conduct initial steps of data processing. On top of them, a center cloud receives processed results and conducts further evaluation. For online processing requirements, the edge nodes should stabilize delay to ensure data freshness. Furthermore, limited onboard energy poses constraints to edge processing capability. In this article, we propose an online edge processing scheduling algorithm based on Lyapunov optimization. In cases of low data rate, it tends to reduce edge processor frequency for saving energy. In the presence of a high data rate, it will smartly allocate bandwidth for edge data offloading. Meanwhile, hovering UAV-BSs bring a large and flexible service coverage, which results in a path planning issue. In this article, we also consider this problem and apply deep reinforcement learning to develop an online path planning algorithm. Taking observations of around environment as an input, a CNN network is trained to predict action rewards. By simulations, we validate its effectiveness in enhancing service coverage. The result will contribute to big data processing in future IoT.",2019,70,7,"[49725081.0, 2605688.0, 7469995.0, 145142172.0]",49725081.0,Paris,3,"[1856878.0, 2108025636.0, 2114140713.0]",Y,"['I understand that you are trying to improve the existing results with their optimizer, but this paper also introduces new algorithm.', 'But I also feel that in this case, it borders on being an issue with the paper itself, as I think this idea needs to be described much more clearly if it is central to the underlying paper.', 'It would also help clarify under what situations one should or should not use this.']",,,IEEE Internet of Things Journal,"['data', 'processing', 'edge', 'data processing', 'Data Processing']"
Data Processing,a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b,Panthera: holistic memory management for big data processing over hybrid memories,https://www.semanticscholar.org/paper/a8ee35c445c627bce7cb1b192a1ad7db2a98ea5b,Conference,"Modern data-parallel systems such as Spark rely increasingly on in-memory computing that can significantly improve the efficiency of iterative algorithms. To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy-inefficient. Emerging non-volatile memory (NVM) technologies offers high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages (e.g., Scala and Java) and executed on top of a managed runtime (e.g., the Java Virtual Machine) that already performs various dimensions of memory management. Supporting hybrid physical memories adds in a new dimension, creating unique challenges in data replacement and migration. This paper proposes Panthera, a semantics-aware, fully automated memory management technique for Big Data processing over hybrid memories. Panthera analyzes user programs on a Big Data system to infer their coarse-grained access patterns, which are then passed down to the Panthera runtime for efficient data placement and migration. For Big Data applications, the coarse-grained data division is accurate enough to guide GC for data layout, which hardly incurs data monitoring and moving overhead. We have implemented Panthera in OpenJDK and Apache Spark. An extensive evaluation with various datasets and applications demonstrates that Panthera reduces energy by 32 – 52% at only a 1 – 9% execution time overhead.",2019,49,,"[2121328398.0, 1734175.0, 145153097.0, 3260130.0, 144041053.0, 145929920.0, 46496975.0, 32215073.0, 38394648.0]",2121328398.0,Brussels,3,"[2086632521.0, 2274111800.0, 1682773.0]",Y,"['Overall it feels as if this is an interesting project but that it is not yet ready for publication.', '* A thorough discussion on mixing in feature space, as well as a baseline which mizes in feature space.', 'I would strongly encourage the authors to try harder datasets such as COCO, VOC etc.']",ACM-SIGPLAN Symposium on Programming Language Design and Implementation,40.0,,"['data', 'panthera', 'memory', 'data processing', 'Data Processing']"
Data Processing,59c2968fb9672a7152c52127255d8f0784bc2368,Using the ESP32 Microcontroller for Data Processing,https://www.semanticscholar.org/paper/59c2968fb9672a7152c52127255d8f0784bc2368,Conference,This article deals with experiences with the development of applications of the ESP32 microcontrollers and provides a comprehensive review of the possibilities of applications development on this platform in the area of data measurement and processing. Microcontrollers usually connect with IoT modules and other smart sensors and provide data to the superior system. This paper also describes implementation of application with the version of connected OLED display and with ESP32 Wrover development board with integrated display.,2019,107,,"[2354124.0, 151167821.0, 31277482.0]",2354124.0,San Marino,3,"[29629766.0, 2119044211.0, 5430731.0]",Y,"['Also, the detailed specification of the VAE should be detailed.', ') The dataset classifier (sec 4.3.4) could be learnt end-to-end by using a softmax output of the dataset classifier as the alpha weighting.', 'This second part was confusing - although the maze can be viewed as a graph, many other works apply ConvNets to maze environments [1, 2, 3], and their work has little relation to other work on graph CNNs.']",International Conference on Innovative Computing and Cloud Computing,20.0,,"['development', 'applications', 'esp32', 'data processing', 'Data Processing']"
Data Processing,02a1e8e77f501675945890df45fbdc11726cb0ba,An Open-Source Monitor-Independent Movement Summary for Accelerometer Data Processing.,https://www.semanticscholar.org/paper/02a1e8e77f501675945890df45fbdc11726cb0ba,JournalArticle,"Background
Physical behavior researchers using motion sensors often use acceleration summaries to visualize, clean, and interpret data. Such output is dependent on device specifications (e.g., dynamic range, sampling rate) and/or are proprietary, which invalidate cross-study comparison of findings when using different devices. This limits flexibility in selecting devices to measure physical activity, sedentary behavior, and sleep.


Purpose
Develop an open-source, universal acceleration summary metric that accounts for discrepancies in raw data among research and consumer devices.


Methods
We used signal processing techniques to generate a Monitor-Independent Movement Summary unit (MIMS-unit) optimized to capture normal human motion. Methodological steps included raw signal harmonization to eliminate inter-device variability (e.g., dynamic g-range, sampling rate), bandpass filtering (0.2-5.0 Hz) to eliminate non-human movement, and signal aggregation to reduce data to simplify visualization and summarization. We examined the consistency of MIMS-units using orbital shaker testing on eight accelerometers with varying dynamic range (±2 to ±8 g) and sampling rates (20-100 Hz), and human data (N = 60) from an ActiGraph GT9X.


Results
During shaker testing, MIMS-units yielded lower between-device coefficient of variations than proprietary ActiGraph and ENMO acceleration summaries. Unlike the widely used ActiGraph activity counts, MIMS-units were sensitive in detecting subtle wrist movements during sedentary behaviors.


Conclusions
Open-source MIMS-units may provide a means to summarize high-resolution raw data in a device-independent manner, thereby increasing standardization of data cleaning and analytical procedures to estimate selected attributes of physical behavior across studies.",2019,81,2 4,"[144403874.0, 2869719.0, 3072621.0, 1705903.0]",144403874.0,Vaduz,2,"[2064711347.0, 2083106.0]",Y,"['In my opinion both contributions suffer from some significant limitations.', '- It seems weird that the smoothed logical AND/OR functions do not depend on the number of inputs; that is unless there are always 3 inputs (but it is not explained why; logical functions are usually formalised as functions of 2 inputs) as suggested by Fig 3.']",,,Journal for the Measurement of Physical Behaviour,"['data', 'mimsunits', 'behavior', 'data processing', 'Data Processing']"
Data Processing,08764019e9762da527253b37b0ff39c46a4206b7,The Seven Sins of Personal-Data Processing Systems under GDPR,https://www.semanticscholar.org/paper/08764019e9762da527253b37b0ff39c46a4206b7,Workshop,"In recent years, our society is being plagued by unprecedented levels of privacy and security breaches. To rein in this trend, the European Union, in 2018, introduced a comprehensive legislation called the General Data Protection Regulation (GDPR). In this paper, we review GDPR from a system design perspective, and identify how its regulations conflict with the design, architecture, and operation of modern systems. We illustrate these conflicts via the seven GDPR sins: storing data forever; reusing data indiscriminately; walled gardens and black markets; risk-agnostic data processing; hiding data breaches; making unexplainable decisions; treating security as a secondary goal. Our findings reveal a deep-rooted tussle between GDPR requirements and how modern systems have evolved. We believe that achieving compliance requires comprehensive, grounds up solutions, and anything short would amount to fixing a leaky faucet in a sinking ship.",2019,51,,"[7456408.0, 35752445.0, 2002462.0]",7456408.0,Lisbon,2,"[6339256.0, 1727784.0]",Y,"['An additional heatmap generator component can be further included in the clustering model.', 'They say that ""almost always"" the synthesizer does not overfit, but I would have liked them to be clear about whether their reported results include any cases of overfitting (i.e. did they ensure they the final generate program always generalized)?']",USENIX Workshop on Hot Topics in Cloud Computing,19.0,,"['data', 'gdpr', 'security', 'data processing', 'Data Processing']"
Data Processing,15370f51d666ab8ef17185679553c6a8647b2a15,Interunit Reliability and Effect of Data-Processing Methods of Global Positioning Systems.,https://www.semanticscholar.org/paper/15370f51d666ab8ef17185679553c6a8647b2a15,JournalArticle,"PURPOSE
To establish the interunit reliability of a range of global positioning system (GPS)-derived movement indicators, to determine the variation between manufacturers, and to investigate the difference between software-derived and raw data.


METHODS
A range of movement variables were obtained from 27 GPS units from 3 manufacturers (GPSports EVO, 10 Hz, n = 10; STATSports Apex, 10 Hz, n = 10; and Catapult S5, 10 Hz, n = 7) that measured the same team-sport simulation session while positioned on a sled. The interunit reliability was determined using the coefficient of variation (%) and 90% confidence limits, whereas between-manufacturers comparisons and comparisons of software versus raw processed data were established using standardized effect sizes and 90% confidence limits.


RESULTS
The interunit reliability for both software and raw processed data ranged from good to poor (coefficient of variation = 0.2%; ±1.5% to 78.2%; ±1.5%), with distance, speed, and maximal speed exhibiting the best reliability. There were substantial differences between manufacturers, particularly for threshold-based acceleration and deceleration variables (effect sizes; ±90% confidence limits: -2.0; ±0.1 to 1.9; ±0.1), and there were substantial differences between data-processing methods for a range of movement indicators.


CONCLUSIONS
The interunit reliability of most movement indicators was deemed as good regardless of processing method, suggesting that practitioners can have confidence within systems. Standardized data-processing methods are recommended, due to the large differences between data outputs from various manufacturer-derived software.",2019,75,14 4,"[40068409.0, 29886701.0, 8579299.0, 5521724.0, 145356043.0]",40068409.0,Warsaw,3,"[7992672.0, 40125294.0, 144123438.0]",Y,"['[1] Semi-Supervised Learning with Generative Adversarial Networks (https://arxiv.org/abs/1606.01583) [2] Good Semi-supervised Learning that Requires a Bad GAN (https://arxiv.org/abs/1705.09783)', 'This suggests a fairly significant sensitivity to this hyperparameter if so.', 'Just to elaborate: in equation (2), the probability of a document is related to the set of distinct words, so it does not distinguish between documents where a word appear multiple times or only once.']",,,International Journal of Sports Physiology and Performance,"['reliability', 'interunit', 'movement', 'data processing', 'Data Processing']"
Data Processing,665b0c776ff7507c32793f10ce9edf90bc2f674a,Specifics of the data processing of precession electron diffraction tomography data and their implementation in the program PETS2.0.,https://www.semanticscholar.org/paper/665b0c776ff7507c32793f10ce9edf90bc2f674a,JournalArticle,"Electron diffraction tomography (EDT) data are in many ways similar to X-ray diffraction data. However, they also present certain specifics. One of the most noteworthy is the specific rocking curve observed for EDT data collected using the precession electron diffraction method. This double-peaked curve (dubbed `the camel') may be described with an approximation based on a circular integral of a pseudo-Voigt function and used for intensity extraction by profile fitting. Another specific aspect of electron diffraction data is the high likelihood of errors in the estimation of the crystal orientation, which may arise from the inaccuracies of the goniometer reading, crystal deformations or crystal movement during the data collection. A method for the refinement of crystal orientation for each frame individually is proposed based on the least-squares optimization of simulated diffraction patterns. This method provides typical angular accuracy of the frame orientations of less than 0.05°. These features were implemented in the computer program PETS 2.0. The implementation of the complete data processing workflow in the program PETS and the incorporation of the features specific for electron diffraction data is also described.",2019,80,75 Pt 4,"[6667699.0, 34279810.0, 32574880.0, 13587113.0, 12098632.0, 8303781.0]",6667699.0,Tirana,2,"[2069981213.0, 1614034792.0]",Y,"['Given the simplicity of the algorithmic choices, the potential novelty of the paper lies more in the problem formulation itself, which considers the question of separating two sets of latent variables from each other in setups where one of them (the ""view"") can vary from pair to pair in arbitrary manner and no attributes characterising the view are provided.', 'In this work, the authors suggest the use of control variate schemes for estimating gradient values, within a reinforcement learning  framework.']",,,"Acta Crystallographica. Section B: Structural Science, Crystal Engineering and Materials","['data', 'diffraction', 'electron', 'data processing', 'Data Processing']"
Data Processing,dcc8d6a87c69ca44cb6636d343347ddd6c8c3860,DPI: The Data Processing Interface for Modern Networks,https://www.semanticscholar.org/paper/dcc8d6a87c69ca44cb6636d343347ddd6c8c3860,Conference,"As data processing evolves towards large scale, distributed platforms, the network will necessarily play a substantial role in achieving efficiency and performance. Increasingly, switches, network cards, and protocols are becoming more flexible while programmability at all levels (aka, software defined networks) opens up many possibilities to tailor the network to data processing applications and to push processing down to the network elements. 
 
In this paper, we propose DPI, an interface providing a set of simple yet powerful abstractions flexible enough to exploit features of modern networks (e.g., RDMA or in-network processing) suitable for data processing. Mirroring the concept behind the Message Passing Interface (MPI) used extensively in high-performance computing, DPI is an interface definition rather than an implementation so as to be able to bridge different networking technologies and to evolve with them. In the paper we motivate and discuss key primitives of the interface and present a number of use cases that show the potential of DPI for data-intensive applications, such as analytic engines and distributed database systems.",2019,31,,"[144641551.0, 2691974.0, 2754078.0, 145527641.0, 66030040.0, 3087426.0, 1413903838.0, 48469973.0, 2108724347.0, 46533048.0]",144641551.0,Skopje,3,"[8847603.0, 2260830380.0, 1453724884.0]",Y,"['The partitioning of each task must currently be designed by hand.', 'The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse.', 'In the experiment, the proposed method shows no or slight degradation of accuracy with big savings in communication cost.']",Conference on Innovative Data Systems Research,19.0,,"['processing', 'network', 'data', 'data processing', 'Data Processing']"
Data Processing,2b061f7f108fdd4e90452aaaead574c7b4b5b780,"IoT Data Processing in the Fog: Functions, Streams, or Batch Processing?",https://www.semanticscholar.org/paper/2b061f7f108fdd4e90452aaaead574c7b4b5b780,Conference,"When processing IoT data on a large scale, the cloud is no longer sufficient and it has been proposed to move parts of the computation closer to the IoT devices – the so-called fog computing. There are also three basic processing paradigms today that lend themselves to IoT data processing: stream and batch processing as well as serverless functions. Where to place which part of the data processing and which processing paradigm to choose, however, is often unclear. In this paper, we give an overview of all three paradigms as well as different data processing use-cases. We use these to derive a decision framework which provides general guidelines for placement of processing and the respectively suitable paradigm when designing a large-scale IoT data processing architecture.",2019,33,,"[52198091.0, 3077067.0]",52198091.0,Madrid,2,"[7557913.0, 1666260553.0]",Y,"['Interestingly, DQN + heuristic reward approaches expert performance while behavioral cloning never achieves expert performance level even though it has actions.', 'Moreover, the authors proposed updating the parameter \\theta of the generator g_\\theta.']",International Conference on Fog Computing,19.0,,"['processing', 'data', 'iot', 'data processing', 'Data Processing']"
Data Processing,bb5d26da72bfe7030dbc6650b686b210ae661f2c,A New Data Processing Architecture for Multi-Scenario Applications in Aviation Manufacturing,https://www.semanticscholar.org/paper/bb5d26da72bfe7030dbc6650b686b210ae661f2c,JournalArticle,"The development of industry 4.0 has spurred the transformation of traditional manufacturing into modern industrial Internet-of-Things. The most notable feature during this transition is the improvement of digitization and intelligence based on the massive data drives. In such a data-driven environment, the processing, storage, and utilization of the industry data get more and more important. Usually, the traditional data processing architecture runs as a one-way streamline, which cannot adapt to the different requirements of the multi-scenario application. This paper proposed a new industrial big data processing architecture called Phi architecture, which can realize many functions such as batch data processing and stream data processing, distributed storage and access, and real-time control. Compared with other data processing architecture, the Phi architecture combined with edge computing and feedback control has the ability to deal with the different demands in aviation manufacturing. Next, the new architecture is designed for microservices pattern, which improves the flexibility and stability of the architecture, and makes it independent operated in multi-scenarios, such as state monitoring of workshop, adaptive data acquisition, feedback control, and user-oriented information classification. As a proof of concept, the architecture has been tested in a simulation digital manufacturing workshop. The results verify the improved effectiveness of the Phi architecture on the data feedback control and real-time processing. And, the development of microservices architecture greatly improves the efficiency, adaptability, and extensibility of the manufacturing process.",2019,23,7,"[2158624007.0, 2147261221.0, 2119407396.0, 103474705.0]",2158624007.0,Luxembourg,2,"[9959840.0, 1727784.0]",Y,"['Because of the plethora of VAE models used in video prediction [1] (albeit, used with pre-structured latent spaces), there has to be atleast one VAE baseline.', 'The authors later describes how to improve this technique by further equating certain observations and exploring only one in each equivalence class.']",,,IEEE Access,"['architecture', 'data', 'processing', 'data processing', 'Data Processing']"
Data Processing,c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b,"Performance Analysis of IoT-Based Sensor, Big Data Processing, and Machine Learning Model for Real-Time Monitoring System in Automotive Manufacturing",https://www.semanticscholar.org/paper/c1fdfcd4470c65aa1238d3a1719c60672c8a4f5b,JournalArticle,"With the increase in the amount of data captured during the manufacturing process, monitoring systems are becoming important factors in decision making for management. Current technologies such as Internet of Things (IoT)-based sensors can be considered a solution to provide efficient monitoring of the manufacturing process. In this study, a real-time monitoring system that utilizes IoT-based sensors, big data processing, and a hybrid prediction model is proposed. Firstly, an IoT-based sensor that collects temperature, humidity, accelerometer, and gyroscope data was developed. The characteristics of IoT-generated sensor data from the manufacturing process are: real-time, large amounts, and unstructured type. The proposed big data processing platform utilizes Apache Kafka as a message queue, Apache Storm as a real-time processing engine and MongoDB to store the sensor data from the manufacturing process. Secondly, for the proposed hybrid prediction model, Density-Based Spatial Clustering of Applications with Noise (DBSCAN)-based outlier detection and Random Forest classification were used to remove outlier sensor data and provide fault detection during the manufacturing process, respectively. The proposed model was evaluated and tested at an automotive manufacturing assembly line in Korea. The results showed that IoT-based sensors and the proposed big data processing system are sufficiently efficient to monitor the manufacturing process. Furthermore, the proposed hybrid prediction model has better fault prediction accuracy than other models given the sensor data as input. The proposed system is expected to support management by improving decision-making and will help prevent unexpected losses caused by faults during the manufacturing process.",2018,224,18,"[2913270.0, 3212733.0, 31380410.0, 3353468.0]",2913270.0,Stockholm,3,"[9319875.0, 1691549.0, 144403874.0]",Y,"['It would also be great to integrate a direct regularization mechanism in the cost  in order to do so.', '3. The section on curriculum learning does not mention relevant work on “starting small”  and the “less is more"" hypothesis in language development by Jeff Elman and Elissa Newport: https://pdfs.semanticscholar.org/371b/240bebcaa68921aa87db4cd3a5d4e2a3a36b.pdf http://www.sciencedirect.com/science/article/pii/0388000188900101', 'Pros: Well written, thorough treatment of the approaches Improvements on top of Dual-AC with ablation study show improvement Cons: Empirical gains might not be very large']",,,Italian National Conference on Sensors,"['data', 'manufacturing', 'process', 'data processing', 'Data Processing']"
Data Processing,2f44ab0e52eb98fdfc0086638887f175b6f2fe4b,Benchmarking Distributed Stream Data Processing Systems,https://www.semanticscholar.org/paper/2f44ab0e52eb98fdfc0086638887f175b6f2fe4b,Conference,"The need for scalable and efficient stream analysis has led to the development of many open-source streaming data processing systems (SDPSs) with highly diverging capabilities and performance characteristics. While first initiatives try to compare the systems for simple workloads, there is a clear gap of detailed analyses of the systems' performance characteristics. In this paper, we propose a framework for benchmarking distributed stream processing engines. We use our suite to evaluate the performance of three widely used SDPSs in detail, namely Apache Storm, Apache Spark, and Apache Flink. Our evaluation focuses in particular on measuring the throughput and latency of windowed operations, which are the basic type of operations in stream analytics. For this benchmark, we design workloads based on real-life, industrial use-cases inspired by the online gaming industry. The contribution of our work is threefold. First, we give a definition of latency and throughput for stateful operators. Second, we carefully separate the system under test and driver, in order to correctly represent the open world model of typical stream processing deployments and can, therefore, measure system performance under realistic conditions. Third, we build the first benchmarking framework to define and test the sustainable performance of streaming systems. Our detailed evaluation highlights the individual characteristics and use-cases of each system.",2018,145,,"[3317889.0, 1731210.0, 1680579.0, 7482477.0, 2868254.0, 1733290.0]",3317889.0,Sarajevo,3,"[2082426870.0, 1967677.0, 2772470.0]",Y,"['The authors indicate that they do not need to compare to variational methods because Gal and Ghahramani 2015 compare already to those methods.', 'This section could be improved by demonstrating the approach on more datasets.', 'There are many multi-agent techniques that can be applied to the problem that would have served as a better baseline.']",IEEE International Conference on Data Engineering,34.0,,"['performance', 'stream', 'systems', 'data processing', 'Data Processing']"
Data Processing,c2a448bb511ebae41a87e69891da8bbf17ddba3d,KAMO: towards automated data processing for microcrystals,https://www.semanticscholar.org/paper/c2a448bb511ebae41a87e69891da8bbf17ddba3d,JournalArticle,An automated data-processing pipeline for protein microcrystals is presented. The processing of multiple small-wedge data sets was made dramatically easier by this pipeline.,2018,150,74,"[13736628.0, 47781577.0, 2243646160.0]",13736628.0,Budapest,3,"[2118902636.0, 144248374.0, 2056913832.0]",Y,"['There are a few unanswered questions in the paper: - What are the performance of the sentiment unit on other datasets (e.g., SST, MR, CR)?', '6.\tThe paper should clarify which CNN is used for CNN + LSTM and Relation Networks models?', ""As such it's not even clear if this is proper for a conference.""]",,,Acta Crystallographica Section D: Structural Biology,"['pipeline', 'dataprocessing', 'protein', 'data processing', 'Data Processing']"
Data Processing,1051abf1e3dae90241ad15b3f98f2e41197ee611,A Personalized Healthcare Monitoring System for Diabetic Patients by Utilizing BLE-Based Sensors and Real-Time Data Processing,https://www.semanticscholar.org/paper/1051abf1e3dae90241ad15b3f98f2e41197ee611,JournalArticle,"Current technology provides an efficient way of monitoring the personal health of individuals. Bluetooth Low Energy (BLE)-based sensors can be considered as a solution for monitoring personal vital signs data. In this study, we propose a personalized healthcare monitoring system by utilizing a BLE-based sensor device, real-time data processing, and machine learning-based algorithms to help diabetic patients to better self-manage their chronic condition. BLEs were used to gather users’ vital signs data such as blood pressure, heart rate, weight, and blood glucose (BG) from sensor nodes to smartphones, while real-time data processing was utilized to manage the large amount of continuously generated sensor data. The proposed real-time data processing utilized Apache Kafka as a streaming platform and MongoDB to store the sensor data from the patient. The results show that commercial versions of the BLE-based sensors and the proposed real-time data processing are sufficiently efficient to monitor the vital signs data of diabetic patients. Furthermore, machine learning–based classification methods were tested on a diabetes dataset and showed that a Multilayer Perceptron can provide early prediction of diabetes given the user’s sensor data as input. The results also reveal that Long Short-Term Memory can accurately predict the future BG level based on the current sensor data. In addition, the proposed diabetes classification and BG prediction could be combined with personalized diet and physical activity suggestions in order to improve the health quality of patients and to avoid critical conditions in the future.",2018,149,18,"[3212733.0, 2913270.0, 48761825.0, 2060154.0, 31380410.0, 3353468.0]",3212733.0,Minsk,3,"[1786389.0, 144588806.0, 48510386.0]",Y,"['Qualitative results show that the attentional mechanism attends to objects which are close to the context object together, acting like the heuristic neighborhood mechanism from previous work.', 'So in contrast to kernelized kmeans, what is the advantage of the proposed framework?', '3. In presenting autoencoders it is crucial to note that they are all built around the idea of compression.']",,,Italian National Conference on Sensors,"['data', 'sensor', 'signs', 'data processing', 'Data Processing']"
Data Processing,1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7,When Computation Hugs Intelligence: Content-Aware Data Processing for Industrial IoT,https://www.semanticscholar.org/paper/1ee1d353d309eef7a74cfaebd4304c09d0f2b0d7,JournalArticle,"Data service has been considered as one the most prominent characteristics for Industrial Internet of Things (IIoT). This paper studies how to design an optimal computing manner for a general IIoT system. On the theory end, we analyze the relationship between the data processing and the energy consumption through investigating the content correlation of the captured data. Importantly, we derive an exact expression for the performance of IIoT by combining computation with intelligence. On the application end, we design an efficient way to obtain a threshold by approximating the performances of different computing manners, and show how to apply it to practical IIoT applications. We believe that the proposed computation rules hold great significance for the IIoT designer, that is, it is better to use distributed computing manner when the content correlation is high, otherwise, centralized computing manner is better.",2018,112,5,"[2865824.0, 40259011.0, 2115526972.0, 34665139.0]",2865824.0,Lisbon,3,"[2174235132.0, 1452678825.0, 2118798587.0]",Y,"['1. The authors claimed that this is the first end-to-end trainable hierarchical attention model, but there is a previous work that also addressed the similar task: Seo et al, Progressive Attention Networks for Visual Attribute Prediction, in Arxiv preprint:1606.02393, 2016', 'Because the tasks are not widely explored in the literature, it could be difficult to know how crucial exploiting graphically structured information is, so the authors performed several ablation studies to analyze  this out.', 'The main result is that such methods can yield smaller, efficient and sometimes more accurate models.']",,,IEEE Internet of Things Journal,"['iiot', 'data', 'manner', 'data processing', 'Data Processing']"
Data Processing,1b3675fc0f2b16743b1e1f0c2f84829cfdb3d34f,DeepSense: A Unified Deep Learning Framework for Time-Series Mobile Sensing Data Processing,https://www.semanticscholar.org/paper/1b3675fc0f2b16743b1e1f0c2f84829cfdb3d34f,Conference,"Mobile sensing and computing applications usually require time-series inputs from sensors, such as accelerometers, gyroscopes, and magnetometers. Some applications, such as tracking, can use sensed acceleration and rate of rotation to calculate displacement based on physical system models. Other applications, such as activity recognition, extract manually designed features from sensor inputs for classification. Such applications face two challenges. On one hand, on-device sensor measurements are noisy. For many mobile applications, it is hard to find a distribution that exactly describes the noise in practice. Unfortunately, calculating target quantities based on physical system and noise models is only as accurate as the noise assumptions. Similarly, in classification applications, although manually designed features have proven to be effective, it is not always straightforward to find the most robust features to accommodate diverse sensor noise patterns and heterogeneous user behaviors. To this end, we propose DeepSense, a deep learning framework that directly addresses the aforementioned noise and feature customization challenges in a unified manner. DeepSense integrates convolutional and recurrent neural networks to exploit local interactions among similar mobile sensors, merge local interactions of different sensory modalities into global interactions, and extract temporal relationships to model signal dynamics. DeepSense thus provides a general signal estimation and classification framework that accommodates a wide range of applications. We demonstrate the effectiveness of DeepSense using three representative and challenging tasks: car tracking with motion sensors, heterogeneous human activity recognition, and user identification with biometric motion analysis. DeepSense significantly outperforms the state-of-the-art methods for all three tasks. In addition, we show that DeepSense is feasible to implement on smartphones and embedded devices thanks to its moderate energy consumption and low latency.",2016,529,,"[2325031.0, 3225210.0, 2580112.0, 2085709.0, 1730531.0]",2325031.0,Dublin,2,"[143695559.0, 1402912902.0]",Y,"['I found it would have also been interesting and helpful to define and show a new metric that incorporates both accuracy and compression rate into a single metric, e.g., how much accuracy is lost (or gained) per compression rate relatively to the baseline of no compression.', 'A full implementation of binary CNN with code This paper builds on Binary-NET [Hubara et al. 2016] and expands it to CNN architectures.']",The Web Conference,26.0,,"['applications', 'deepsense', 'noise', 'data processing', 'Data Processing']"
Data Processing,14fe35149aed6a47b6ebfd207deb7681b9446bb6,Urban Planning and Smart City Decision Management Empowered by Real-Time Data Processing Using Big Data Analytics,https://www.semanticscholar.org/paper/14fe35149aed6a47b6ebfd207deb7681b9446bb6,JournalArticle,"The Internet of Things (IoT), inspired by the tremendous growth of connected heterogeneous devices, has pioneered the notion of smart city. Various components, i.e., smart transportation, smart community, smart healthcare, smart grid, etc. which are integrated within smart city architecture aims to enrich the quality of life (QoL) of urban citizens. However, real-time processing requirements and exponential data growth withhold smart city realization. Therefore, herein we propose a Big Data analytics (BDA)-embedded experimental architecture for smart cities. Two major aspects are served by the BDA-embedded smart city. Firstly, it facilitates exploitation of urban Big Data (UBD) in planning, designing, and maintaining smart cities. Secondly, it occupies BDA to manage and process voluminous UBD to enhance the quality of urban services. Three tiers of the proposed architecture are liable for data aggregation, real-time data management, and service provisioning. Moreover, offline and online data processing tasks are further expedited by integrating data normalizing and data filtering techniques to the proposed work. By analyzing authenticated datasets, we obtained the threshold values required for urban planning and city operation management. Performance metrics in terms of online and offline data processing for the proposed dual-node Hadoop cluster is obtained using aforementioned authentic datasets. Throughput and processing time analysis performed with regard to existing works guarantee the performance superiority of the proposed work. Hence, we can claim the applicability and reliability of implementing proposed BDA-embedded smart city architecture in the real world.",2018,83,18,"[7992672.0, 2109215851.0, 2703536.0, 2111508316.0, 51455282.0, 1845785166.0, 3310138.0, 152742012.0]",7992672.0,Reykjavik,3,"[1822288.0, 73509301.0, 15574937.0]",Y,"['This second part was confusing - although the maze can be viewed as a graph, many other works apply ConvNets to maze environments [1, 2, 3], and their work has little relation to other work on graph CNNs.', 'Do you normalize the features (the output of the dimension reduction and the representation error are quite different)?', 'Consequently, there is not a great degree of novelty in terms of the proposed method, and the results are only slightly better than those of previous methods.']",,,Italian National Conference on Sensors,"['data', 'city', 'architecture', 'data processing', 'Data Processing']"
Data Processing,780c725848aac1118d00c8bb306719ec803369cd,Model-free Control for Distributed Stream Data Processing using Deep Reinforcement Learning,https://www.semanticscholar.org/paper/780c725848aac1118d00c8bb306719ec803369cd,JournalArticle,"In this paper, we focus on general-purpose Distributed Stream Data Processing Systems (DSDPSs), which deal with processing of unbounded streams of continuous data at scale distributedly in real or near-real time. A fundamental problem in a DSDPS is the scheduling problem with the objective of minimizing average end-to-end tuple processing time. A widely-used solution is to distribute workload evenly over machines in the cluster in a round-robin manner, which is obviously not efficient due to lack of consideration for communication delay. Model-based approaches do not work well either due to the high complexity of the system environment. We aim to develop a novel model-free approach that can learn to well control a DSDPS from its experience rather than accurate and mathematically solvable system models, just as a human learns a skill (such as cooking, driving, swimming, etc). Specifically, we, for the first time, propose to leverage emerging Deep Reinforcement Learning (DRL) for enabling model-free control in DSDPSs; and present design, implementation and evaluation of a novel and highly effective DRL-based control framework, which minimizes average end-to-end tuple processing time by jointly learning the system environment via collecting very limited runtime statistics data and making decisions under the guidance of powerful Deep Neural Networks. To validate and evaluate the proposed framework, we implemented it based on a widely-used DSDPS, Apache Storm, and tested it with three representative applications. Extensive experimental results show 1) Compared to Storm's default scheduler and the state-of-the-art model-based method, the proposed framework reduces average tuple processing by 33.5% and 14.0% respectively on average. 2) The proposed framework can quickly reach a good scheduling solution during online learning, which justifies its practicability for online control in DSDPSs.",2018,75,11,"[2115646383.0, 48559420.0, 2115854503.0, 46393431.0]",2115646383.0,Tallinn,3,"[77790220.0, 2145954003.0, 2108467971.0]",Y,"['I found confusing how to use the proposed method to obtain estimates of uncertainty for a particular test data point x_star.', '- In Section 4.3, why did you consider the entropy regularizer?', 'I also found difficult to measure the degree of novelty of the approach considering the recent works and  the related work section should have been much more precise in terms of comparison.']",,,Proceedings of the VLDB Endowment,"['time', 'control', 'framework', 'data processing', 'Data Processing']"
Data Processing,5371896313ac227eb819038dd55f213cb42b99e2,RHEEM: Enabling Cross-Platform Data Processing - May The Big Data Be With You! -,https://www.semanticscholar.org/paper/5371896313ac227eb819038dd55f213cb42b99e2,JournalArticle,"
 Solving business problems increasingly requires going beyond the limits of a single data processing platform (platform for short), such as Hadoop or a DBMS. As a result, organizations typically perform tedious and costly tasks to juggle their code and data across different platforms. Addressing this pain and achieving automatic cross-platform data processing is quite challenging: finding the most efficient platform for a given task requires quite good expertise for all the available platforms. We present R
 heem
 , a general-purpose cross-platform data processing system that decouples applications from the underlying platforms. It not only determines the best platform to run an incoming task, but also splits the task into subtasks and assigns each subtask to a specific platform to minimize the overall cost (e.g., runtime or monetary cost). It features (i) an interface to easily compose data analytic tasks; (ii) a novel cost-based optimizer able to find the most efficient platform in almost all cases; and (iii) an executor to efficiently orchestrate tasks over different platforms. As a result, it allows users to focus on the business logic of their applications rather than on the mechanics of how to compose and execute them. Using different real-world applications with R
 heem
 , we demonstrate how cross-platform data processing can accelerate performance by more than one order of magnitude compared to single-platform data processing.
",2018,62,11,"[143970078.0, 50793091.0, 1404359012.0, 145188857.0, 3416143.0, 2827559.0, 143924672.0, 2081346.0, 1801187.0, 2168047.0, 1802817.0, 1399355221.0, 8669763.0, 2934941.0, 1410159156.0]",143970078.0,Budapest,3,"[2152202354.0, 51917504.0, 3257323.0]",Y,"['Given the simplicity of the algorithmic choices, the potential novelty of the paper lies more in the problem formulation itself, which considers the question of separating two sets of latent variables from each other in setups where one of them (the ""view"") can vary from pair to pair in arbitrary manner and no attributes characterising the view are provided.', 'Second, the experimental set-up on the Cora and Citeseer data sets should be properly randomized.', '2. Even if this was correct, the main point is that this is ""only"" d times worse - see eq (11).']",,,Proceedings of the VLDB Endowment,"['data', 'platform', 'processing', 'data processing', 'Data Processing']"
Data Processing,b4c9c134ad5bd4a037115df65411b4c49abe1322,Framework for Mobile Internet of Things Security Monitoring Based on Big Data Processing and Machine Learning,https://www.semanticscholar.org/paper/b4c9c134ad5bd4a037115df65411b4c49abe1322,JournalArticle,"The paper discusses a new framework combining the possibilities of Big Data processing and machine leaning developed for security monitoring of mobile Internet of Things. The mathematical foundations and the problem statement are considered. The description of the used data set and the architecture of proposed security monitoring framework are provided. The framework specifies several machine learning mechanisms intended for solving classification tasks. The classifier operation results are exposed to plurality voting, weighted voting, and soft voting. The framework performance and accuracy is assessed experimentally.",2018,53,6,"[16777650.0, 145683384.0, 2636647.0]",16777650.0,Oslo,2,"[2064711347.0, 2038786.0]",Y,"['To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER.', 'The proposed algorithm is essentially an ensemble algorithm, there exist several works on deep model ensemble (e.g., Boosted convolutional neural networks, and Snapshot Ensemble) should be compared against.']",,,IEEE Access,"['framework', 'data', 'machine', 'data processing', 'Data Processing']"
Data Processing,6eb8caebbffc7e5b301b66dc36acad46b4dca5c9,3-D Data Processing to Extract Vehicle Trajectories from Roadside LiDAR Data,https://www.semanticscholar.org/paper/6eb8caebbffc7e5b301b66dc36acad46b4dca5c9,JournalArticle,"High-resolution vehicle data including location, speed, and direction is significant for new transportation systems, such as connected-vehicle applications, micro-level traffic performance evaluation, and adaptive traffic control. This research developed a data processing procedure for detection and tracking of multi-lane multi-vehicle trajectories with a roadside light detection and ranging (LiDAR) sensor. Different from existing methods for vehicle onboard sensing systems, this procedure was developed specifically to extract high-resolution vehicle trajectories from roadside LiDAR sensors. This procedure includes preprocessing of the raw data, statistical outlier removal, a Least Median of Squares based ground estimation method to accurately remove the ground points, vehicle data clouds clustering, a principle component-based oriented bounding box method to estimate the location of the vehicle, and a geometrically-based tracking algorithm. The developed procedure has been applied to a two-way-stop-sign intersection and an arterial road in Reno, Nevada. The data extraction procedure has been validated by comparing tracking results and speeds logged from a testing vehicle through the on-board diagnostics interface. This data processing procedure could be applied to extract high-resolution trajectories of connected and unconnected vehicles for connected-vehicle applications, and the data will be valuable to practices in traffic safety, traffic mobility, and fuel efficiency estimation.",2018,52,2672,"[2108706355.0, 39911449.0, 2109184366.0, 48510386.0, 2071260355.0]",2108706355.0,Copenhagen,2,"[2261454711.0, 2266712798.0]",Y,"['It would be interesting to see how well the model performs in the limiting case of T=1.', 'This analysis is conducted by drawing on results from the field of critical percolation in physics.']",,,Transportation Research Record,"['data', 'vehicle', 'procedure', 'data processing', 'Data Processing']"
Data Processing,2dafea864f74a477414c3b71b742f7997e216102,Energy-Aware Mobile Edge Computing and Routing for Low-Latency Visual Data Processing,https://www.semanticscholar.org/paper/2dafea864f74a477414c3b71b742f7997e216102,JournalArticle,"New paradigms such as Mobile Edge Computing (MEC) are becoming feasible for use in, e.g., real-time decision-making during disaster incident response to handle the data deluge occurring in the network edge. However, MEC deployments today lack flexible IoT device data handling such as handling user preferences for real-time versus energy-efficient processing. Moreover, MEC can also benefit from a policy-based edge routing to handle sustained performance levels with efficient energy consumption. In this paper, we study the potential of MEC to address application issues related to energy management on constrained IoT devices with limited power sources, while also providing low-latency processing of visual data being generated at high resolutions. Using a facial recognition application that is important in disaster incident response scenarios, we propose a novel “offload decision-making” algorithm that analyzes the tradeoffs in computing policies to offload visual data processing (i.e., to an edge cloud or a core cloud) at low-to-high workloads. This algorithm also analyzes the impact on energy consumption in the decision-making under different visual data consumption requirements (i.e., users with thick clients or thin clients). To address the processing-throughput versus energy-efficiency tradeoffs, we propose a “Sustainable Policy-based Intelligence-Driven Edge Routing” algorithm that uses machine learning within Mobile Ad hoc Networks. This algorithm is energy aware and improves the geographic routing baseline performance (i.e., minimizes impact of local minima) for throughput performance sustainability, while also enabling flexible policy specification. We evaluate our proposed algorithms by conducting experiments on a realistic edge and core cloud testbed in the GENI Cloud infrastructure, and recreate disaster scenes of tornado damages within simulations. Our empirical results show how MEC can provide flexibility to users who desire energy conservation over low latency or vice versa in the visual data processing with a facial recognition application. In addition, our simulation results show that our routing approach outperforms existing solutions under diverse user preferences, node mobility, and severe node failure conditions.",2018,54,20,"[2064711347.0, 3174714.0, 2729273.0, 134771108.0, 2066206022.0, 2112228538.0, 1921638.0]",2064711347.0,Bucharest,3,"[2043402.0, 1396184193.0, 36347083.0]",Y,"['Then the addition of the VAE had no measurable effect on the PixelCNN++’s performance, i.e., it seems like a bad idea due to the added complexity and loss of tractability.', 'I encourage the authors to have the work proofread by a native speaker; clearer writing will ultimately increase the reach and impact of the paper.', ""My main concern with this work is that I don't see any mechanism in the framework that prevents an expert  (or few of them) to win all examples except its own learning capacities.""]",,,IEEE transactions on multimedia,"['edge', 'data', 'energy', 'data processing', 'Data Processing']"
Data Storage,86dbd884043eb5807c61d2c65b813e673b4a04fa,Blockchain-Based Secure Data Storage Protocol for Sensors in the Industrial Internet of Things,https://www.semanticscholar.org/paper/86dbd884043eb5807c61d2c65b813e673b4a04fa,JournalArticle,"The Industrial Internet of Things (IIoT) that introduces Internet of Things (IoT) technology into industrial environments is beneficial to construct smart factories. It utilizes various sensors to collect the data of industrial devices. These data are analyzed to improve the manufacturing efficiency and product quality. Cloud storage provides a solution for storing data outsourced, especially for sensors that have limited local storage and computational capacity. To ensure the privacy preserving of devices, the collected data should be stored in the formal ciphertext. Therefore, encrypted data sharing should be implemented to analyze the devices’ data. In this article, the cloud storage solution for sensors is considered. To achieve a secure and efficient data storage and sharing, a novel group signature scheme, which has less computation overhead and communication overhead, is designed to realize anonymous authentication first. And then, a novel blockchain-based cloud storage protocol for sensors in IIoT is constructed on basis of the proposed group signature scheme. Smart contract and proxy re-encryption are utilized in this protocol to realize secure data sharing with a less computational overhead. Furthermore, security proofs and performance evaluations demonstrate that this protocol is secure, privacy-preserving, and has at least 40% and 20% performance improvement in data storage and sharing phase, respectively.",2022,34,18,"[2049071558.0, 143808691.0, 46342348.0, 2082427140.0]",2049071558.0,Brussels,3,"[2490652.0, 1388013676.0, 1754091.0]",Y,"['Nor the model neither the optimized criterion is detailled: the authors present some curve mentioning ""bits per character"" but we do not know what is measured.', '2 kind is ""attacks are generated"" for this purpose, and the ReLU network is simplified to a single layer network with quadratic nonlinearity.', 'The authors provide the code with permissive licensing.']",,,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'sensors', 'data storage', 'Data Storage']"
Data Storage,24ab4e99e582c9770281eee0a39cbeb70ddd891a,Information-Theoretic Foundations of DNA Data Storage,https://www.semanticscholar.org/paper/24ab4e99e582c9770281eee0a39cbeb70ddd891a,JournalArticle,"Due to its longevity and enormous information density, DNA is an attractive medium for archival data storage. Natural DNA more than 700.000 years old has been recovered, and about 5 grams of DNA can in principle hold a Zetabyte of digital information, orders of magnitude more than what is achieved on conventional storage media. Thanks to rapid technological advances, DNA storage is becoming practically feasible, as demonstrated by a number of experimental storage systems, making it a promising solution for our society’s increasing need of data storage. While in living things, DNA molecules can consist of millions of nucleotides, due to technological constraints, in practice, data is stored on many short DNA molecules, which are preserved in a DNA pool and cannot be spatially ordered. Moreover, imperfections in sequencing, synthesis, and handling, as well as DNA decay during storage, introduce random noise into the system, making the task of reliably storing and retrieving information in DNA challenging. This unique setup raises a natural information-theoretic question: how much information can be reliably stored on and reconstructed from millions of short noisy sequences? The goal of this monograph is to address this question by discussing the fundamental limits of storing information on DNA. Motivated by current technological constraints on DNA synthesis and sequencing, we propose a probabilistic channel model that captures three key distinctive aspects of the DNA storage systems: (1) the data is written onto many short DNA molecules that are stored in an unordered fashion; (2) the molecules are corrupted by noise and (3) the data is read by randomly sampling from the DNA pool. Our goal is to investigate the impact of each of these key aspects on the capacity of the DNA storage system. Rather than focusing on coding-theoretic considerations and computationally efficient encoding and decoding, we aim to build an information-theoretic foundation for the analysis of these channels, developing tools for achievability and converse arguments. This is a preprint of the following publication: Ilan Shomorony and Reinhard Heckel (2022),“Information-Theoretic Foundations of DNA Data Storage”, Foundations and Trends in Communications and Information Theory: Vol. 19, No. 1, pp 1-106. DOI: 10.1561/0100000117. ar X iv :2 21 1. 05 55 2v 1 [ cs .I T ] 1 0 N ov 2 02 2",2022,22,abs/2211.05552,"[1800004.0, 145639495.0]",1800004.0,Prague,2,"[35374367.0, 145356043.0]",Y,"['ACM Transactions on Graphics (TOG), 32(6), 206.', 'Much of the authors’ analysis is based on a qualitative evaluation of samples.']",,,Foundations and Trends in Communications and Information Theory,"['dna', 'storage', 'information', 'data storage', 'Data Storage']"
Data Storage,b904dcdbd7c7b33938583f2f57d05ca70e121ea9,An Efficient and Secure Big Data Storage in Cloud Environment by Using Triple Data Encryption Standard,https://www.semanticscholar.org/paper/b904dcdbd7c7b33938583f2f57d05ca70e121ea9,JournalArticle,"In recent decades, big data analysis has become the most important research topic. Hence, big data security offers Cloud application security and monitoring to host highly sensitive data to support Cloud platforms. However, the privacy and security of big data has become an emerging issue that restricts the organization to utilize Cloud services. The existing privacy preserving approaches showed several drawbacks such as a lack of data privacy and accurate data analysis, a lack of efficiency of performance, and completely rely on third party. In order to overcome such an issue, the Triple Data Encryption Standard (TDES) methodology is proposed to provide security for big data in the Cloud environment. The proposed TDES methodology provides a relatively simpler technique by increasing the sizes of keys in Data Encryption Standard (DES) to protect against attacks and defend the privacy of data. The experimental results showed that the proposed TDES method is effective in providing security and privacy to big healthcare data in the Cloud environment. The proposed TDES methodology showed less encryption and decryption time compared to the existing Intelligent Framework for Healthcare Data Security (IFHDS) method.",2022,24,6,"[2186480691.0, 2200758545.0, 2150706913.0, 65965701.0, 2200528122.0, 2143272549.0]",2186480691.0,Ljubljana,3,"[3045042.0, 52161096.0, 46199596.0]",Y,"['3) and extracting 2D slices from the compressed space and computing 2D histograms per slice.', 'Overall the work is important, original, well-executed, and should open new directions for deep learning in program analysis.', 'I found the discussion about rank to be very intuitive, however this intuition is not fully tested.']",,,Big Data and Cognitive Computing,"['data', 'security', 'privacy', 'data storage', 'Data Storage']"
Data Storage,287a7da1801a07cf7fd85ffcc23c79504876ecc0,An artificial chromosome for data storage,https://www.semanticscholar.org/paper/287a7da1801a07cf7fd85ffcc23c79504876ecc0,JournalArticle,"Abstract DNA digital storage provides an alternative for information storage with high density and long-term stability. Here, we report the de novo design and synthesis of an artificial chromosome that encodes two pictures and a video clip. The encoding paradigm utilizing the superposition of sparsified error correction codewords and pseudo-random sequences tolerates base insertions/deletions and is well suited to error-prone nanopore sequencing for data retrieval. The entire 254 kb sequence was 95.27% occupied by encoded data. The Transformation-Associated Recombination method was used in the construction of this chromosome from DNA fragments and necessary autonomous replication sequences. The stability was demonstrated by transmitting the data-carrying chromosome to the 100th generation. This study demonstrates a data storage method using encoded artificial chromosomes via in vivo assembly for write-once and stable replication for multiple retrievals, similar to a compact disc, with potential in economically massive data distribution.",2021,58,8,"[2154742781.0, 1635504630.0, 48129278.0, 2089054636.0, 2108355738.0, 2108098765.0, 2110018952.0, 2040448821.0, 2158141874.0]",2154742781.0,Belgrade,3,"[3018223.0, 35805107.0, 2143359114.0]",Y,"['See e.g. the discussion in ""Variational Lossy Autoencoder"" (Chen et al. 2016) - Experiments are not complete (e.g. for AR, as noted in the paper).', '2. Employ “obverter” technique, showing that it can be an alternative approach comparing to RL', 'The paper frequently refers to ""embedding"" ""imaginary trajectories"" into the dynamics model, and I still have no idea what this is actually referring to (the definition at the start of section 4 is completely opaque to me).']",,,National Science Review,"['data', 'storage', 'chromosome', 'data storage', 'Data Storage']"
Data Storage,1e1cf81a1113482be3f0c280db994a832cb9426a,Secure Data Storage and Recovery in Industrial Blockchain Network Environments,https://www.semanticscholar.org/paper/1e1cf81a1113482be3f0c280db994a832cb9426a,JournalArticle,"The massive redundant data storage and communication in network 4.0 environments have issues of low integrity, high cost, and easy tampering. To address these issues, in this article, a secure data storage and recovery scheme in the blockchain-based network is proposed by improving the decentration, tampering-proof, real-time monitoring, and management of storage systems, as such design supports the dynamic storage, fast repair, and update of distributed data in the data storage system of industrial nodes. A local regenerative code technology is used to repair and store data between failed nodes while ensuring the privacy of user data. That is, as the data stored are found to be damaged, multiple local repair groups constructed by vector code can simultaneously yet efficiently repair multiple distributed data storage nodes. Based on the unique chain storage structure, such as data consensus mechanism and smart contract, the storage structure of blockchain distributed coding not only quickly repair the nearby local regenerative codes in the blockchain but also reduce the resource overhead in the data storage process of industrial nodes. Experimental results show that the proposed scheme improves the repair rate of multinode data by 9% and data storage rate increased by 8.6%, indicating to be promising with good security and real-time performance.",2020,165,16,"[97233415.0, 34548938.0, 69486668.0, 40452934.0, 144086037.0]",97233415.0,Valletta,3,"[144375552.0, 145235149.0, 49408448.0]",Y,"['A paper with interesting ideas but lacking convincing evidence Summary: The authors proposed an unsupervised time series clustering methods built with deep neural networks.', 'This is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial NLL loss and a reconstruction loss at the last hidden layer.', 'Particularly, I found it interesting to re-evaluate the variance with (virtually) increasing larger batch size.']",,,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'repair', 'data storage', 'Data Storage']"
Data Storage,6544259ff6b335b1dcec75e031b6d57e5b9509f4,Scaling DNA data storage with nanoscale electrode wells,https://www.semanticscholar.org/paper/6544259ff6b335b1dcec75e031b6d57e5b9509f4,JournalArticle,A demonstration of DNA synthesis control at over 1000x higher density shows the path to large scale DNA data storage.,2021,31,7,"[38697713.0, 35479753.0, 2061294792.0, 2119125110.0, 2142147919.0, 2142142532.0, 1729804.0, 2056913832.0, 29865576.0, 2387195.0, 119935593.0, 2058303089.0, 52129431.0, 1717411.0, 145033446.0]",38697713.0,Kiev,2,"[1576996670.0, 153583218.0]",Y,"['2) The second prior work is Kumar & Daume III 2012 (and also an early work of Argyrio et al 2008) that is based on learning a common feature representation.', 'The authors later describes how to improve this technique by further equating certain observations and exploring only one in each equivalence class.']",,,Science Advances,"['dna', 'demonstration', 'synthesis', 'data storage', 'Data Storage']"
Data Storage,fc61c7221350806c25379f385c27b2102ff8eb57,Uncertainties in synthetic DNA-based data storage,https://www.semanticscholar.org/paper/fc61c7221350806c25379f385c27b2102ff8eb57,JournalArticle,"Abstract Deoxyribonucleic acid (DNA) has evolved to be a naturally selected, robust biomacromolecule for gene information storage, and biological evolution and various diseases can find their origin in uncertainties in DNA-related processes (e.g. replication and expression). Recently, synthetic DNA has emerged as a compelling molecular media for digital data storage, and it is superior to the conventional electronic memory devices in theoretical retention time, power consumption, storage density, and so forth. However, uncertainties in the in vitro DNA synthesis and sequencing, along with its conjugation chemistry and preservation conditions can lead to severe errors and data loss, which limit its practical application. To maintain data integrity, complicated error correction algorithms and substantial data redundancy are usually required, which can significantly limit the efficiency and scale-up of the technology. Herein, we summarize the general procedures of the state-of-the-art DNA-based digital data storage methods (e.g. write, read, and preservation), highlighting the uncertainties involved in each step as well as potential approaches to correct them. We also discuss challenges yet to overcome and research trends in the promising field of DNA-based data storage.",2021,25,49,"[1515428357.0, 151480423.0, 2084599260.0, 2118902636.0]",1515428357.0,Skopje,2,"[2990847.0, 144593763.0]",Y,"['For longer programs, I can imagine there can be thousands of bad traces as it only needs one small mistake to propagate to full traces.', '- Presumably the policy learned in Phase 1 is a decent model by itself, since it can reliably find candidate traces.']",,,Nucleic Acids Research,"['data', 'storage', 'dna', 'data storage', 'Data Storage']"
Data Storage,82663577cf1d08235bb56ad648c9dad36343ccfb,Electrochemical DNA synthesis and sequencing on a single electrode with scalability for integrated data storage,https://www.semanticscholar.org/paper/82663577cf1d08235bb56ad648c9dad36343ccfb,JournalArticle,DNA synthesis and sequencing on a single electrode enables integrated data storage using a sliding microarray chip.,2021,27,7,"[1515428357.0, 2084599260.0, 97709033.0, 93610956.0, 2112728651.0, 2118902636.0]",1515428357.0,Minsk,2,"[2181606.0, 1996173264.0]",Y,"['The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss.', 'Is it better to add attention to lower layers or higher layers?']",,,Science Advances,"['dna', 'synthesis', 'electrode', 'data storage', 'Data Storage']"
Data Storage,3b230f14c46e7e177e9bebb2ebc9f46b346b646d,Secure and efficient data storage and sharing scheme for blockchain‐based mobile‐edge computing,https://www.semanticscholar.org/paper/3b230f14c46e7e177e9bebb2ebc9f46b346b646d,JournalArticle,"With the rapid development of Internet of Things (IoT) technology, IoT devices have been widely used to collect physiological health data and provide diversified services to the terminal users. However, traditional data storage and sharing scheme cloud computing based in IoT face many challenges. For example, IoT devices are usually resource‐constrained (storage, computing power, battery capacity, etc.), data signed by IoT devices to ensure data integrity and authenticity will consume a lot of computing resources of IoT devices. At the same time, there is the challenge of high latency and unsafe data storage and sharing. To overcome these challenges, we propose a secure and efficient data storage and sharing scheme for blockchain‐based mobile‐edge computing. In our scheme, we construct the unique signature private key in a region into multiple key shares. IoT devices only need to submit the data and the random key shares allocated to the edge node. Edge node uses the recovered signature private key to realize data signature and homomorphic encryption. At the same time, the edge node will process timely data and return to the user. For data that need to be uploaded to the cloud for analysis, we use backup uploads to avoid data floods. Through experiments, it was found that our scheme can not only realize low‐latency message response for the terminal users, but also realize anonymous identity verification while ensuring data integrity and authenticity. The key shares of the signature private key are stored in different blocks of the blockchain to improve fault tolerance. The content extraction signature algorithm ensures that the key shares stored in different blocks are publicly verifiable. Safety analysis and performance analysis verify the feasibility and effectiveness of our scheme.",2021,27,32,"[2108781632.0, 2055642882.0, 2108438707.0, 2481509.0, 2269644.0, 2155550922.0]",2108781632.0,San Marino,3,"[2258552414.0, 150116413.0, 2261361394.0]",Y,"['The authors state that "". . . the effectiveness of an architecture to learn visual-relation problems should be measured in terms of generalization over multiple variants of the same problem, not over multiple splits of the same dataset.""  Taken literally, this would rule out a lot of modern machine learning, even obviously very good work.', ""If so, doesn't that correspond to evaluating the model under a different generative assumption?"", ""The authors' technique may let us do this data-generation easily.""]",,,Transactions on Emerging Telecommunications Technologies,"['data', 'iot', 'scheme', 'data storage', 'Data Storage']"
Data Storage,bb1118fb9fd86da6a2f65770353d8fb4362d9883,An Empirical Comparison of Preservation Methods for Synthetic DNA Data Storage.,https://www.semanticscholar.org/paper/bb1118fb9fd86da6a2f65770353d8fb4362d9883,JournalArticle,"Synthetic DNA has recently risen as a viable alternative for long-term digital data storage. To ensure that information is safely recovered after storage, it is essential to appropriately preserve the physical DNA molecules encoding the data. While preservation of biological DNA has been studied previously, synthetic DNA differs in that it is typically much shorter in length, it has different sequence profiles with fewer, if any, repeats (or homopolymers), and it has different contaminants. In this paper, nine different methods used to preserve data files encoded in synthetic DNA are evaluated by accelerated aging of nearly 29 000 DNA sequences. In addition to a molecular count comparison, the DNA is also sequenced and analyzed after aging. These findings show that errors and erasures are stochastic and show no practical distribution difference between preservation methods. Finally, the physical density of these methods is compared and a stability versus density trade-offs discussion provided.",2021,24,5 5,"[23757022.0, 38697713.0, 1957273475.0, 2111830587.0, 13163128.0, 29865576.0, 48560944.0, 1717411.0, 145033446.0]",23757022.0,Zagreb,2,"[49746834.0, 2125469004.0]",Y,"['Given the simplicity of the algorithmic choices, the potential novelty of the paper lies more in the problem formulation itself, which considers the question of separating two sets of latent variables from each other in setups where one of them (the ""view"") can vary from pair to pair in arbitrary manner and no attributes characterising the view are provided.', 'This paper studies the problem of multi-label learning for text copora.']",,,Small Methods,"['dna', 'data', 'methods', 'data storage', 'Data Storage']"
Data Storage,44786a2c2a8ba8cf5c74a8fb10098c220e924c56,A Blockchain-Based Incremental Update Supported Data Storage System for Intelligent Vehicles,https://www.semanticscholar.org/paper/44786a2c2a8ba8cf5c74a8fb10098c220e924c56,JournalArticle,"With the development of autonomous driving and the Internet of Vehicles, vehicle data communication and data security become more and more important. Blockchain which has transparency, decentralization and immutability nature is treated as a promising approach to support intelligent vehicle systems. However, due to the high data update overhead, vulnerable raw data storage policy and inflexible consensus algorithm, traditional blockchain technologies are not suitable in modern vehicular systems. Hence, we propose, a blockchain data storage system that supports incremental data updating. Specifically, the system reduces the re-uploaded data size through smart contract and data partition to decrease the overhead. Besides, data replica and multi-data source addressing of index on the chain enhance the data reliability. In addition, an adaptive proof-of-work algorithm is developed, whose execution cost is dynamically adjusted based on nodes’ behavior. It greatly improves the data record and updating efficiency. Comprehensive experimental results show that BUS can effectively improve the data updating efficiency with low overhead and fewer resources in intelligent vehicle scenarios.",2021,21,70,"[1785971.0, 49034115.0, 2109971162.0, 34905515.0, 2116611035.0]",1785971.0,Nicosia,2,"[1950842.0, 1750017.0]",Y,"['See e.g. the discussion in ""Variational Lossy Autoencoder"" (Chen et al. 2016) - Experiments are not complete (e.g. for AR, as noted in the paper).', '— Can the authors provide training time comparison of their model and other/baseline models?']",,,IEEE Transactions on Vehicular Technology,"['data', 'vehicle', 'systems', 'data storage', 'Data Storage']"
Data Storage,639bfab64e2f35917d450013e136cb24c7755fad,Portable and Error-Free DNA-Based Data Storage,https://www.semanticscholar.org/paper/639bfab64e2f35917d450013e136cb24c7755fad,Conference,"The paper involves critical evaluation of all the significant encryption, decryption, and cryptography techniques that are being used for DNA (deoxyribonucleic acid) data Storage. This paper covers the basics of Data storage in DNA and how it can be used to stockpile data and how it is highly promising in changing the data storage methods of the world in the foreseeable future. All the vital methods which are being used for DNA data storage have been discussed. These methods are also be applicable for data storage density and graphs are plotted. This paper also examines how DNA is being used as a tool for cryptography along with the fundamental limitations of DNA storage. Towards the end, the future scope of DNA data storage is questioned, and with the help of a density graph the research predicts whether ""DNA data storage has a future scope or not""ƒ",2021,16,,"[2144633223.0, 2146970349.0, 2118511088.0]",2144633223.0,Ljubljana,2,"[150127950.0, 1736651.0]",Y,"['A number of different fine-tuning regimes are explored.', '[1] Wang, Weiran, Honglak Lee, and Karen Livescu.']","2021 4th International Conference on Recent Developments in Control, Automation & Power Engineering (RDCAPE)",4.0,,"['data', 'storage', 'dna', 'data storage', 'Data Storage']"
Data Storage,649c3497e3b34b15a5011259fcb837cf6c1ac04a,Lensless phase retrieval based on deep learning used in holographic data storage.,https://www.semanticscholar.org/paper/649c3497e3b34b15a5011259fcb837cf6c1ac04a,JournalArticle,"This paper proposes a lensless phase retrieval method based on deep learning (DL) used in holographic data storage. By training an end-to-end convolutional neural network between the phase-encoded data pages and the corresponding near-field diffraction intensity images, the new unknown phase data page can be predicted directly from the intensity image by the network model without any iterations. The DL-based phase retrieval method has a higher storage density, lower bit-error-rate (BER), and higher data transfer rate compared to traditional iterative methods. The retrieval optical system is simple, stable, and robust to environment fluctuations which is suitable for holographic data storage. Besides, we studied and demonstrated that the DL method has a good suppression effect on the dynamic noise of the holographic data storage system.",2021,14,46 17,"[1752784087.0, 2117689925.0, 2108139284.0, 92401367.0, 2037478325.0, 2128089506.0, 2119043585.0, 48391667.0]",1752784087.0,Dublin,2,"[38394648.0, 1412902270.0]",Y,"['Is the implicit assumption that for real, high-dimensional data the generator and data manifolds will *never* overlap?', 'The proposed CPD model also is essentially learning a linear transformation of the kernelized feature space.']",,,Optics Letters,"['data', 'storage', 'phase', 'data storage', 'Data Storage']"
Data Storage,f7b69b8babfa607f2e8b0371f24cb720a7a827d6,Blockchain for Large-Scale Internet of Things Data Storage and Protection,https://www.semanticscholar.org/paper/f7b69b8babfa607f2e8b0371f24cb720a7a827d6,JournalArticle,"With the dramatically increasing deployment of IoT devices, storing and protecting the large volume of IoT data has become a significant issue. Traditional cloud-based IoT structures impose extremely high computation and storage demands on the cloud servers. Meanwhile, the strong dependencies on the centralized servers bring significant trust issues. To mitigate these problems, we propose a distributed data storage scheme employing blockchain and cetrificateless cryptography. Our scheme eliminates the traditional centralized servers by leveraging the blockchain miners who perform “transaction” verifications and records audit with the help of certificateless cryptography. We present a clear definition of the transactions in a non-cryptocurrency system and illustrate how the transactions are processed. To the best of our knowledge, this is the first work designing a secure and accountable IoT storage system using blockchain. Additionally, we extend our scheme to enable data trading and elaborate how data trading can be efficiently and effectively achieved.",2019,210,12,"[2336789.0, 2746913.0, 34312176.0, 46382448.0, 143651788.0, 2110651504.0]",2336789.0,Bern,2,"[1699363.0, 2256996328.0]",Y,"['Moreover, the authors proposed updating the parameter \\theta of the generator g_\\theta.', 'The novelty of this work consists of an approach based on score matching and Stein’s identity to estimate the gradient directly and the empirical results of the proposed method on meta-learning for approximate inference and entropy regularized GANs.']",,,IEEE Transactions on Services Computing,"['data', 'iot', 'storage', 'data storage', 'Data Storage']"
Data Storage,03aeb4520e760a906393aaf9c1bf4e526483d081,Capacity-Approaching Constrained Codes With Error Correction for DNA-Based Data Storage,https://www.semanticscholar.org/paper/03aeb4520e760a906393aaf9c1bf4e526483d081,JournalArticle,"We propose coding techniques that simultaneously limit the length of homopolymers runs, ensure the <inline-formula> <tex-math notation=""LaTeX"">${\tt G}{\tt C}$ </tex-math></inline-formula>-content constraint, and are capable of correcting a single edit error in strands of nucleotides in DNA-based data storage systems. In particular, for given <inline-formula> <tex-math notation=""LaTeX"">$\ell, \epsilon >0$ </tex-math></inline-formula>, we propose simple and efficient encoders/decoders that transform binary sequences into DNA base sequences (codewords), namely sequences of the symbols <inline-formula> <tex-math notation=""LaTeX"">${\tt A}, {\tt T}, {\tt C}$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=""LaTeX"">${\tt G}$ </tex-math></inline-formula>, that satisfy all of the following properties: 1) runlength constraint: the maximum homopolymer run in each codeword is at most <inline-formula> <tex-math notation=""LaTeX"">$\ell $ </tex-math></inline-formula>; 2) <inline-formula> <tex-math notation=""LaTeX"">${\tt G}{\tt C}$ </tex-math></inline-formula>-content constraint: the <inline-formula> <tex-math notation=""LaTeX"">${\tt G}{\tt C}$ </tex-math></inline-formula>-content of each codeword is within <inline-formula> <tex-math notation=""LaTeX"">$[0.5-\epsilon,0.5+\epsilon]$ </tex-math></inline-formula>; 3) error-correction: each codeword is capable of correcting a single deletion, or single insertion, or single substitution error. While various combinations of these properties have been considered in the literature, this work provides generalizations of codes constructions that satisfy all the properties with arbitrary parameters of <inline-formula> <tex-math notation=""LaTeX"">$\ell $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=""LaTeX"">$\epsilon $ </tex-math></inline-formula>. Furthermore, for practical values of <inline-formula> <tex-math notation=""LaTeX"">$\ell $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=""LaTeX"">$\epsilon $ </tex-math></inline-formula>, we show that our encoders achieve higher rates than existing results in the literature and approach capacity. Our methods have low encoding/decoding complexity and limited error propagation.",2020,44,67,"[27445282.0, 3680574.0, 1741737.0, 1977220.0]",27445282.0,Paris,3,"[2119125110.0, 3375249.0, 1402417041.0]",Y,"['The authors also analyze the generalization error bound of DNN after pruning based on the work of (Sokolic et al., 2017).', '3. Some of the architectural choices (the one derived from ""shortcut problem"") are barely explained or looked into.', 'Currently, I don’t understand from the manuscript, how DQN is actually trained.']",,,IEEE Transactions on Information Theory,"['texmath', 'inlineformula', 'notationlatextt', 'data storage', 'Data Storage']"
Data Storage,147c868b721c8d29df7c61db7f2360114c760614,Digital Data Storage Using DNA Nanostructures and Solid-State Nanopores.,https://www.semanticscholar.org/paper/147c868b721c8d29df7c61db7f2360114c760614,JournalArticle,"Solid-state nanopores are powerful tools for reading the three-dimensional shape of molecules, allowing for the translation of molecular structure information into electric signals. Here, we show a high-resolution integrated nanopore system for identifying DNA nanostructures that has the capability of distinguishing attached short DNA hairpins with only a stem length difference of 8 bp along a DNA double strand named the DNA carrier. Using our platform, we can read up to 112 DNA hairpins with a separating distance of 114 bp attached on a DNA carrier that carries digital information. Our encoding strategy allows for the creation of a library of molecules with a size of up to 5 × 1033 (2112) that is only built from a few hundred types of base molecules for data storage and has the potential to be extended by linking multiple DNA carriers. Our platform provides a nanopore- and DNA nanostructure-based data storage method with convenient access and the potential for miniature-scale integration.",2019,105,19 2,"[8336569.0, 50596731.0, 5499803.0, 14869064.0, 48627323.0, 3356500.0]",8336569.0,Reykjavik,3,"[8847603.0, 32528506.0, 152742012.0]",Y,"['Doing so will help better understand what is gained from using retaining a probabilistic form of memory versus a determinstic memory indexed with attention as in [Li et. al].', '1. Most of the ideas presented in the paper rely on works by Arjovsky et al. (2017), Gulrajani et al. (2017), and Gulrajani et al. (2017).', 'The authors mislabel the value function V as the  action value, or Q function.']",,,Nano letters (Print),"['dna', 'molecules', 'information', 'data storage', 'Data Storage']"
Data Storage,8fcbd1cd1ee2211bd183b986900042470ee7f440,Secure data storage based on blockchain and coding in edge computing.,https://www.semanticscholar.org/paper/8fcbd1cd1ee2211bd183b986900042470ee7f440,JournalArticle,"Edge computing is an important tool for smart computing, which brings convenience to data processing as well as security problems. In particular, the security of data storage under edge computing has become an obstacle to its widespread use. To solve the problem, the mechanism combing blockchain with regeneration coding is proposed to improve the security and reliability of stored data under edge computing. Our contribution is as follows. 1) According to the three-tier edge computing architecture and data security storage requirements, we proposed hybrid storage architecture and model specifically adapted to edge computing. 2) Making full use of the data storage advantages of edge network devices and cloud storage servers, we build a global blockchain in the cloud service layer and local blockchain is built on the terminals of the Internet of things. Moreover, the regeneration coding is utilized to further improve the reliability of data storage in blockchains. 3) Our scheme provides a mechanism for periodically validating hash values of data to ensure the integrity of data stored in global blockchain.",2019,103,16 4,"[2955771.0, 1491631727.0, 8259409.0, 2143717442.0]",2955771.0,Tirana,3,"[49556437.0, 1725656.0, 1524732527.0]",Y,"['Good paper, pushing the limits of RL to harder tasks.', 'For CNN, they a speedup of 5x is obtained from the GPU to binary-optimizimed-GPU.', 'A well written, clear paper presenting a novel representation of graphs   as multi-channel image-like structures from their node embeddings.']",,,Mathematical biosciences and engineering : MBE,"['data', 'storage', 'edge', 'data storage', 'Data Storage']"
Data Storage,c84389369720dcd2f004c48e58fbac2c45c8f092,XIndex: a scalable learned index for multicore data storage,https://www.semanticscholar.org/paper/c84389369720dcd2f004c48e58fbac2c45c8f092,Conference,"We present XIndex, a concurrent ordered index designed for fast queries. Similar to a recent proposal of the learned index, XIndex uses learned models to optimize index efficiency. Comparing with the learned index, XIndex is able to effectively handle concurrent writes without affecting the query performance by leveraging fine-grained synchronization and a new compaction scheme, Two-Phase Compaction. Furthermore, XIndex adapts its structure according to run-time workload characteristics to support dynamic workload. We demonstrate the advantages of XIndex with both YCSB and TPC-C (KV), a TPC-C variant for key-value stores. XIndex achieves up to 3.2X and 4.4X performance improvement comparing with Masstree and Wormhole, respectively, on a 24-core machine, and it is open-sourced1.",2020,62,,"[41211459.0, 2115725948.0, 1508399092.0, 1508456598.0, 8491577.0, 2136369359.0, 2118438836.0]",41211459.0,Copenhagen,2,"[2551387.0, 2108451006.0]",Y,"['- The claim of Theorem 2 in appendix B does not follow from its proof: what is proven is that the value of S(w) lies in an interval [1-e..1+e] with a certain probability for all w.', 'Only simple baselines (eg autoencoder, kmeans) implemented by this paper are included.']",ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming,25.0,,"['xindex', 'index', 'performance', 'data storage', 'Data Storage']"
Data Storage,3adb779bb37d22e3aa299364c2a337603801ca5c,DNA Micro‐Disks for the Management of DNA‐Based Data Storage with Index and Write‐Once–Read‐Many (WORM) Memory Features,https://www.semanticscholar.org/paper/3adb779bb37d22e3aa299364c2a337603801ca5c,JournalArticle,"DNA‐based data storage has attracted attention because of its higher physical density of the data and longer retention time than those of conventional digital data storage. However, previous DNA‐based data storage lacked index features and the data quality of storage after a single access was not preserved, obstructing its industrial use. Here, DNA micro‐disks, QR‐coded micro‐sized disks that harbor data‐encoded DNA molecules for the efficient management of DNA‐based data storage, are proposed. The two major features that previous DNA‐based data‐storage studies could not achieve are demonstrated. One feature is accessing data items efficiently by indexing the data‐encoded DNA library. Another is achieving write‐once–read‐many (WORM) memory through the immobilization of DNA molecules on the disk and their enrichment through in situ DNA production. Through these features, the reliability of DNA‐based data storage is increased by allowing selective and multiple accession of data‐encoded DNA with lower data loss than previous DNA‐based data storage methods.",2020,33,32,"[73509301.0, 15994978.0, 31399475.0, 2111389723.0, 50403503.0, 5430731.0, 2058870027.0, 2109559620.0, 114449847.0, 51163989.0, 2110007025.0, 145720154.0, 88396204.0]",73509301.0,Vilnius,2,"[51484149.0, 2336789.0]",Y,"['This result would be instructive to see for each of the tasks.', '— Is the “fork” module the main contribution of the paper?']",,,Advances in Materials,"['data', 'storage', 'dna', 'data storage', 'Data Storage']"
Data Storage,fac67bf55456b52ac6e4f280ad953d0250c74ebc,2D MXene–TiO2 Core–Shell Nanosheets as a Data‐Storage Medium in Memory Devices,https://www.semanticscholar.org/paper/fac67bf55456b52ac6e4f280ad953d0250c74ebc,JournalArticle,"MXenes, an emerging class of 2D transition metal carbides and nitrides with the general formula Mn+1XnTx (n = 1–4), have potential for application as floating gates in memory devices because of their intrinsic properties of a 2D structure, high density‐of‐states, and high work function. In this study, a series of MXene–TiO2 core–shell nanosheets are synthesized by deterministic control of the surface oxidation of MXene. The floating gate (multilayer MXene) and tunneling layer (TiO2) in a nano‐floating‐gate transistor memory (NFGTM) device are prepared simultaneously by a facile, low‐cost, and water‐based process. The memory performance is optimized via adjustment of the thickness of the oxidation layer formed on the MXene surface. The fabricated MXene NFGTMs exhibit excellent nonvolatile memory characteristics, including a large memory window (>35.2 V), high programming/erasing current ratio (≈106), low off‐current (<1 pA), long retention (>104 s), and cyclic endurance (300 cycles). Furthermore, synaptic functions, including the excitatory postsynaptic current/inhibitory postsynaptic current, paired‐pulse facilitation, and synaptic plasticity (long‐term potentiation/depression), are successfully emulated using the MXene NFGTMs. The successful control of MXene oxidation and its application to NFGTMs are expected to inspire the application of MXene as a data‐storage medium in future memory devices.",2020,73,32,"[1380082069.0, 49146822.0, 94314332.0, 47703819.0, 2115457464.0, 153311051.0, 5559947.0]",1380082069.0,Paris,3,"[2156252582.0, 32058742.0, 1720381.0]",Y,"['The experiments performed show that the Bayesian view of batch normalization performs similarly as MC dropout in terms of the estimates of uncertainty that it produces.', 'It is well-known that these are sensitive to initialization and often require running the optimizer with multiple random seeds and picking the best one.', 'Discriminator is union of two intervals.']",,,Advances in Materials,"['mxene', 'memory', 'application', 'data storage', 'Data Storage']"
Data Storage,4bd3c9e1bb1ca2df62b66201616b8740300efd0a,Integrating encryption techniques for secure data storage in the cloud,https://www.semanticscholar.org/paper/4bd3c9e1bb1ca2df62b66201616b8740300efd0a,JournalArticle,"Cloud computing has emerged as one of the most groundbreaking technologies to have redefined the bounds of conventional computing techniques. It has ushered in a paradigm shift and pushed the frontiers of how computing assets, inclusive of infrastructure resources, software, and applications can be used, adopted, and purchased. The economic benefits or rather the fundamental economic shift offered by cloud computing in reducing capital expenditure and converting it to operational expenditure has been a primary motivating factor for early adopters. However, despite its inherent advantages that include better access and control, there exist several reservations around cloud computing that have impeded its growth. The control, elasticity, and ease of use that cloud computing is associated with also engender many security issues. Security is considered to be the topmost hurdle out of the nine identified challenges of cloud computing as underlined by the study conducted by the International Data Corporation. It therefore follows that an exceedingly secure system is essential for the safeguarding of an organizational entity, its resources, and assets. In this article, it is our endeavor to offer insights into the implementation of a novel architecture that can deliver an enhanced degree of security for outsourcing information in a cloud computing environment while involving numerous independent cloud providers. The framework comprises of dual encryption and data fragmentation techniques that envision the secure distribution of information in a multicloud environment. The various concerns surrounding this area, specifically, the challenges of integrity, security, confidentiality, and authentication have been addressed. All simulations and scrutiny have been accomplished on an Oracle virtual machine Virtual‐Box and a Fog environment on an Ubuntu 16.04 platform. Extensive safety measures and performance analysis that take into account diverse parameters, especially execution time, integrity, throughput, entropy, transfer rate, and delay demonstrate that our projected proposal is vastly proficient and satisfies the security prerequisites of secure data sharing and can efficiently withstand security attacks.",2020,56,33,"[84156264.0, 38298800.0, 66519890.0, 34956744.0, 150302778.0, 144369609.0]",84156264.0,Nicosia,3,"[2375710.0, 1577665701.0, 51413028.0]",Y,"['The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse.', 'If the former, what was sigma and how was it chosen?', 'he resulting multi-channel image-like structures are then feed into vanilla 2D CNN.']",,,Transactions on Emerging Telecommunications Technologies,"['cloud', 'computing', 'security', 'data storage', 'Data Storage']"
Data Storage,9a60d3c9d3a5b7f165325fca45bd418d651682d3,Boafft: Distributed Deduplication for Big Data Storage in the Cloud,https://www.semanticscholar.org/paper/9a60d3c9d3a5b7f165325fca45bd418d651682d3,JournalArticle,"As data progressively grows within data centers, the cloud storage systems continuously facechallenges in saving storage capacity and providing capabilities necessary to move big data within an acceptable time frame. In this paper, we present the Boafft, a cloud storage system with distributed deduplication. The Boafft achieves scalable throughput and capacity usingmultiple data servers to deduplicate data in parallel, with a minimal loss of deduplication ratio. Firstly, the Boafft uses an efficient data routing algorithm based on data similarity that reduces the network overhead by quickly identifying the storage location. Secondly, the Boafft maintains an in-memory similarity indexing in each data server that helps avoid a large number of random disk reads and writes, which in turn accelerates local data deduplication. Thirdly, the Boafft constructs hot fingerprint cache in each data server based on access frequency, so as to improve the data deduplication ratio. Our comparative analysis with EMC's stateful routing algorithm reveals that the Boafft can provide a comparatively high deduplication ratio with a low network bandwidth overhead. Moreover, the Boafft makes better usage of the storage space, with higher read/write bandwidth and good load balance.",2020,44,8,"[35365766.0, 8274453.0, 1980434.0, 1740261.0, 2181606.0]",35365766.0,Stockholm,3,"[49889860.0, 1398683279.0, 2093582149.0]",Y,"['The authors mention several philosophical arguments in favor of their approach, but is there a concrete example of an model which is cumbersome to write in an existing framework but easy here?', 'To achieve this goal, the approach relies on the approximation of differential operators by convolution of filters of appropriate order.', 'If Neumann optimizer can surpass Adam on ImageNet, I think your algorithm will be widely used after being published.']",,,IEEE Transactions on Cloud Computing,"['data', 'boafft', 'storage', 'data storage', 'Data Storage']"
Data Storage,08be4e23951a0def1c5d235befbb39c8d8d373a3,A Thorough Trust and Reputation Based RBAC Model for Secure Data Storage in the Cloud,https://www.semanticscholar.org/paper/08be4e23951a0def1c5d235befbb39c8d8d373a3,JournalArticle,"Cloud computing is a widespread technology, which has attracted much attention nowadays. Among the many criteria that must be considered for data storage in the cloud, access control plays a vital role. Role-based access control (RBAC) is a well-known technique for secure data storage in the cloud. Since the traditional RBAC models are improper for open and decentralized environments, recently, some works have integrated the trust concept into the RBAC model. Nevertheless, they have not fully addressed the required security metrics of a trust-based system. Therefore, in this paper, we first introduce the security goals that should be considered in an efficient trust-based system. Second, we propose a novel trust and reputation based RBAC model that not only can properly withstand the security threats of trust-based RBAC models, but also is scalable as it has reasonable execution time. Third, we evaluate the proposed model using the famous trust network of advogato dataset. Eventually, we compare the proposed model with recently-published ones in terms of mean absolute error, execution time of indirect trust computation, and provided features. The achieved results are indicative of the priority of the proposed model to be employed in real cloud environments.",2019,47,30,"[1411256149.0, 1410127739.0, 11531589.0]",1411256149.0,Ljubljana,2,"[2150098899.0, 2119993.0]",Y,"['In what we call the Pong Player’s Dilemma (PPD) when an agent scores they gain a reward of 1 but the partner receives a reward of −2.', 'This is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial NLL loss and a reconstruction loss at the last hidden layer.']",,,IEEE Transactions on Parallel and Distributed Systems,"['rbac', 'model', 'cloud', 'data storage', 'Data Storage']"
Data Storage,29279e52008848ee494f5af1b836313ab99c25ed,Efficient Decentralized Data Storage Based on Public Blockchain and IPFS,https://www.semanticscholar.org/paper/29279e52008848ee494f5af1b836313ab99c25ed,Conference,"Blockchain technology has enabled the keeping of a decentralized, tamper-proof, immutable, and ordered ledger of transactional events. Efforts to leverage such a ledger may be challenging when data storage requirements exceed most blockchain protocols’ current capacities. Storing large amounts of decentralized data while maintaining system efficiency is the challenge that we target. This paper proposes using the IPFS distributed hash table (DHT) technology to store information immutably and in a decentralized manner to mitigate the high cost of storage. A storage system involving blockchain and other storage systems in concert should be based on immutable data and allow removal of data from malicious users in the DHT. Efficiency is improved by decreasing the overall processing time in the blockchain with the help of DHT technology and introducing an agreement service that communicate with the blockchain via a RESTful API. We demonstrate the applicability of the proposed method and conclude that the combination of IPFS and blockchain provides efficient cryptographic storage, immutable history and overall better efficiency in a decentralized manner.",2020,14,,"[36877027.0, 145667775.0, 2509535.0]",36877027.0,Vaduz,3,"[3410500.0, 36347083.0, 1752784087.0]",Y,"['The paper frequently refers to ""embedding"" ""imaginary trajectories"" into the dynamics model, and I still have no idea what this is actually referring to (the definition at the start of section 4 is completely opaque to me).', 'One could also argue that the manuscript would profit from a better theoretical analysis of the IRL problem, say: C. A. Rothkopf, C. Dimitrakakis.', 'The encoder network from NEM can be seen as a recurrent network which takes one latent variable theta_k at time t and some input x to produce the next latent variable theta_k at time t+1.']",2020 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE),20.0,,"['storage', 'data', 'blockchain', 'data storage', 'Data Storage']"
Data Storage,492c389d560d9db39c758d07e635408d2e0eaf7d,COMPARATIVE STUDY OF NOSQL DATABASES FOR BIG DATA STORAGE,https://www.semanticscholar.org/paper/492c389d560d9db39c758d07e635408d2e0eaf7d,Conference,"Nowadays the amount of data that is required to be stored in databases relates to big data and that data is required to be stored in real time, but knowing that DBMSs cannot support such a thing, we use the NoSQL databases. NoSQL databases support dynamic scheme design, offering the potential for increased flexibility, scalability and adaptability over relational software. This makes them suitable for Web Applications, content management systems, and other uses that involve large amounts of non-uniform data that require more frequent updates and formats in different domains. There are already a large number of NoSQL databases, with a number of over 255. Their performance depends on various factors, so it is important to compare them based on the requirements of an application. So, during this research there will be made a comparison of some of the most popular NoSQL databases for big data storage.",2020,14,,"[14833620.0, 2285582737.0, 20802526.0, 40992700.0]",14833620.0,Brussels,2,"[41037252.0, 2061706386.0]",Y,"['“We can quantitatively determine how close the posterior is to a FFG distribution by comparing the Optimal FFG bound and the Optimal Flow bound.”: Why not just compare the optimal with the AIS evaluation?', 'simple, effective method, some discussion/understanding missing This paper proposes a new method of detecting in vs. out of distribution samples.']","14th International Conference on Computer Graphics, Visualization, Computer Vision and Image Processing, 5th International Conference on Big Data Analytics, Data Mining and Computational Intelligence and 9th International Conference on Theory and Practice in Modern Computing",14.0,,"['data', 'nosql', 'number', 'data storage', 'Data Storage']"
Data Storage,82a09f72d7b9587e3b519b1cd9640a5a611f3480,Data Storage Mechanism Based on Blockchain with Privacy Protection in Wireless Body Area Network,https://www.semanticscholar.org/paper/82a09f72d7b9587e3b519b1cd9640a5a611f3480,JournalArticle,"Wireless body area networks (WBANs) are expected to play a vital role in the field of patient-health monitoring shortly. They provide a convenient way to collect patient data, but they also bring serious problems which are mainly reflected in the safe storage of the collected data. The privacy and security of data storage in WBAN devices cannot meet the needs of WBAN users. Therefore, this paper adopts blockchain technology to store data, which improves the security of the collected data. Moreover, a storage model based on blockchain in WBAN is proposed in our solution. However, blockchain storage brings new problems, for example, that the storage space of blockchain is small, and the stored content is open to unauthorized attackers. To solve the problems above, this paper proposed a sequential aggregate signature scheme with a designated verifier (DVSSA) to ensure that the user’s data can only be viewed by the designated person and to protect the privacy of the users of WBAN. In addition, the new signature scheme can also compress the size of the blockchain storage space.",2019,61,19,"[2955771.0, 1491631727.0, 108621123.0, 2143717442.0, 79987966.0]",2955771.0,Podgorica,3,"[144260125.0, 2285196231.0, 2152775219.0]",Y,"['Comments: The model proposes to learn a conditional stochastic deep model by training an output noise model on the input x_i and the residual y_i - g(x_i).', 'If I understand what is being evaluated correctly (i.e., best random guess) then I am not surprised the EEN can perform better with enough random samples.', 'Standard idea, great results This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives.']",,,Italian National Conference on Sensors,"['data', 'storage', 'blockchain', 'data storage', 'Data Storage']"
Data Storage,1986318d8a565fbff8fde545b8d0c2012c6462d8,Construction of Bio-Constrained Code for DNA Data Storage,https://www.semanticscholar.org/paper/1986318d8a565fbff8fde545b8d0c2012c6462d8,JournalArticle,"With extremely high density and durable preservation, DNA data storage has become one of the most cutting-edge techniques for long-term data storage. Similar to traditional storage which impose restrictions on the form of encoded data, data stored in DNA storage systems are also subject to two biochemical constraints, i.e., maximum homopolymer run limit and balanced GC content limit. Previous studies used successive process to satisfy these two constraints. As a result, the process suffers low efficiency and high complexity. In this letter, we propose a novel content-balanced run-length limited code with an efficient code construction method, which generates short DNA sequences that satisfy both constraints at one time. Besides, we develop an encoding method to map binary data into long DNA sequences for DNA data storage, which ensures both local and global stability in terms of satisfying the biochemical constraints. The proposed encoding method has high effective code rate of 1.917 bits per nucleotide and low coding complexity.",2019,47,23,"[2108734743.0, 1398730731.0, 143884284.0, 1737045.0, 10310238.0]",2108734743.0,Sofia,2,"[1705260.0, 1405319717.0]",Y,"['Either human training data showing very effective generalization (if one could somehow make ""novel"" relationships unfamiliar to humans), or a different network architecture that was obviously superior in generalization to CNN+RN.', '(9) Lemma 3.2: Is \\hat{D} defined in the paper?']",,,IEEE Communications Letters,"['data', 'dna', 'storage', 'data storage', 'Data Storage']"
Data Storage,4895c430c7810b45840b58cc9182f12143013a43,Magnetic Nanofiber Mats for Data Storage and Transfer,https://www.semanticscholar.org/paper/4895c430c7810b45840b58cc9182f12143013a43,JournalArticle,"Electrospun nanofiber mats may serve as new hardware for neuromorphic computing. To enable data storage and transfer in them, they should be magnetic, possibly electrically conductive and able to respond to further external impulses. Here we report on creating magnetic nanofiber mats, consisting of magnetically doped polymer nanofibers for data transfer and polymer beads containing larger amounts of magnetic nanoparticles for storage purposes. Using magnetite and iron nickel oxide nanoparticles, a broad range of doping ratios could be electrospun with a needleless technique, resulting in magnetic nanofiber mats with varying morphologies and different amounts of magnetically doped beads.",2019,45,9,"[66770881.0, 50353334.0, 65827603.0, 66552339.0, 51312398.0, 66760366.0, 49746834.0, 32559865.0]",66770881.0,Madrid,3,"[34699434.0, 2138053020.0, 2118890103.0]",Y,"['For the latter question, the authors propose using a ""query network"" that based on the current state, pulls out one state from the memory according to certain probability distribution.', '1. The authors claimed that this is the first end-to-end trainable hierarchical attention model, but there is a previous work that also addressed the similar task: Seo et al, Progressive Attention Networks for Visual Attribute Prediction, in Arxiv preprint:1606.02393, 2016', 'The motivation is clear and the idea is simple and effective.']",,,Nanomaterials,"['nanofiber', 'mats', 'data', 'data storage', 'Data Storage']"
Data Storage,0599f45e03ac2016321df0dd653ba4c0034c79d5,Fuzzy-Folded Bloom Filter-as-a-Service for Big Data Storage in the Cloud,https://www.semanticscholar.org/paper/0599f45e03ac2016321df0dd653ba4c0034c79d5,JournalArticle,"With the ongoing trend of smart and Internet-connected objects being deployed across a broad range of applications, there is also a corresponding increase in the amount of data movement across different geographical regions. This, in turn, poses a number of challenges with respect to big data storage across multiple locations, including cloud computing platform. For example, the underlying distributed file system has a large number of directories and files in the form of gigantic trees, which are difficult to parse in polynomial time. Moreover, with the exponential increase of big data streams (i.e., unbounded sets of continuous data flows), challenges associated with indexing and membership queries are compounded. The capability to process such significant amount of data with high accuracy can have significant impact on decision-making and formulation of business and risk-related strategies, particularly in our current Industrial Internet of Things environment (IIoT). However, existing storage solutions are deterministic in nature. In other words, they tend to consume considerable memory and CPU time to yield accurate results. This necessitates the design of efficient quality of service-aware IIoT applications that are able to deal with the challenges of data storage and retrieval in the cloud computing environment. In this paper, we present an effective space-effective strategy for massive data storage using bloom filter (BF). Specifically, in the proposed scheme, the standard BF is extended to incorporate fuzzy-enabled folding approach, hereafter referred to as fuzzy folded BF (FFBF). In FFBF, fuzzy operations are used to accommodate the hashed data of one BF into another to reduce storage requirements. Evaluations on UCI ML AReM and Facebook datasets demonstrate the efficacy of FFBF, in terms of dealing with approximately 1.9 times more data as compared to using the standard BF. This is also achieved without affecting the false positive rate and query time.",2019,40,15,"[48775545.0, 144529280.0, 46555602.0, 2818166.0, 144996075.0, 2840539.0]",48775545.0,Oslo,3,"[134047389.0, 143813668.0, 1943097969.0]",Y,"['The papers is well written and clear, and proposes an interesting idea of representing graphs as multi-channel image-like structures.', '* On the convexity vs non-convexity (Sec. 6), it is interesting to see how pushing the Id through the net changes the look of the loss for deep nets.', 'I do not think class labels are usually assumed to be given in the standard definition of NN, and it is not clear to me how the proposed setup can accommodate NN without class labels.']",,,IEEE Transactions on Industrial Informatics,"['data', 'storage', 'bf', 'data storage', 'Data Storage']"
Data Storage,742747c7a453b293352b772d0d99541c96a351c3,Heterogeneous Data Storage Management with Deduplication in Cloud Computing,https://www.semanticscholar.org/paper/742747c7a453b293352b772d0d99541c96a351c3,JournalArticle,"Cloud storage as one of the most important services of cloud computing helps cloud users break the bottleneck of restricted resources and expand their storage without upgrading their devices. In order to guarantee the security and privacy of cloud users, data are always outsourced in an encrypted form. However, encrypted data could incur much waste of cloud storage and complicate data sharing among authorized users. We are still facing challenges on encrypted data storage and management with deduplication. Traditional deduplication schemes always focus on specific application scenarios, in which the deduplication is completely controlled by either data owners or cloud servers. They cannot flexibly satisfy various demands of data owners according to the level of data sensitivity. In this paper, we propose a heterogeneous data storage management scheme, which flexibly offers both deduplication management and access control at the same time across multiple Cloud Service Providers (CSPs). We evaluate its performance with security analysis, comparison and implementation. The results show its security, effectiveness and efficiency towards potential practical usage.",2019,40,5,"[145843577.0, 2108025636.0, 2234493.0, 50320297.0]",145843577.0,Bern,3,"[8937909.0, 2143359114.0, 7557913.0]",Y,"['In fact, I would go so far as to say that the English to French scenario described in that paragraph is a notable outlier, in that it is the other language pair where you beat the oracle re-ordering baseline in both Multi30k and WMT.', 'In Section 3.1, the attack methods #2 and #3 should be detailed more.', 'Considering representative synthetic problems is a good idea, but it is not clear to me why this particular choice is useful for the purpose.']",,,IEEE Transactions on Big Data,"['data', 'cloud', 'storage', 'data storage', 'Data Storage']"
Data Storage,624b2f14be4287d6a400cdf88a6f911b434b182e,Enabling Ternary Hash Tree Based Integrity Verification for Secure Cloud Data Storage,https://www.semanticscholar.org/paper/624b2f14be4287d6a400cdf88a6f911b434b182e,JournalArticle,"Cloud Computing enables the remote users to access data, services, and applications in on-demand from the shared pool of configurable computing resources, without the consideration of storage, hardware, and software management. On the other hand, it is not easy for cloud users to identify whether Cloud Service Provider's (CSP) tag along with the data security legal expectations. So, cloud users could not rely on CSP's in terms of trust. So, it is significant to build a secure and efficient data auditing framework for increasing and maintaining cloud users trust with CSP. Researchers suggested introducing Third Party Auditor (TPA) on behalf of cloud user for verifying the outsourced data integrity, which may reduce the computation overhead of cloud users. In this work, we proposed a novel integrity verification framework for securing cloud storage based on Ternary Hash Tree (THT) and Replica based Ternary Hash Tree (R-THT), which will be used by TPA to perform data auditing. Differing from existing work, the proposed framework performs Block-level, File-level and Replica-level auditing with tree block ordering, storage block ordering for verifying the data integrity and ensuring data availability in the cloud. We further extend our framework to support error localization with data correctness, dynamic updates with block update, insert, and delete operations in the cloud. The structure of THT and R-THT will reduce the computation cost and provide efficiency in data updates compared to the existing schemes. The security analysis of the proposed public auditing framework indicates the achievement of desired properties and performance has been evaluated with the detailed experiment set. The results show that the proposed secure cloud auditing framework is highly secure and efficient in storage, communication, and computation costs.",2020,21,32,"[2288759936.0, 48919600.0]",2288759936.0,Minsk,2,"[1766680.0, 2249759901.0]",Y,"['The paper compares the Coreset construction with simple uniform sampling.', '* On the convexity vs non-convexity (Sec. 6), it is interesting to see how pushing the Id through the net changes the look of the loss for deep nets.']",,,IEEE Transactions on Knowledge and Data Engineering,"['cloud', 'data', 'framework', 'data storage', 'Data Storage']"
Data Storage,54814b0669f20f07ec8d8c7e4fabddfadfe66b1a,DNA Data Storage and Hybrid Molecular–Electronic Computing,https://www.semanticscholar.org/paper/54814b0669f20f07ec8d8c7e4fabddfadfe66b1a,JournalArticle,"Moore’s law may be slowing, but our ability to manipulate molecules is improving faster than ever. DNA could provide alternative substrates for computing and storage as existing ones approach physical limits. In this paper, we explore the implications of this trend in computer architecture. We present a computer systems perspective on molecular processing and storage, positing a hybrid molecular–electronic architecture that plays to the strengths of both domains. We cover the design and implementation of all stages of the pipeline: encoding, DNA synthesis, system integration with digital microfluidics, DNA sequencing (including emerging technologies such as nanopores), and decoding. We first draw on our experience designing a DNA-based archival storage system, which includes the largest demonstration to date of DNA digital data storage of over three billion nucleotides encoding over 400 MB of data. We then propose a more ambitious hybrid–electronic design that uses a molecular form of near-data processing for massive parallelism. We present a model that demonstrates the feasibility of these systems in the near future. We think the time is ripe to consider molecular storage seriously and explore system designs and architectural implications.",2019,39,107,"[52129431.0, 1717411.0, 49997612.0, 145966834.0, 145033446.0, 8810543.0]",52129431.0,Reykjavik,3,"[2706729.0, 49248672.0, 2605688.0]",Y,"['Practical contributions: The paper introduces a new technique for training DNNs by forming a convex combination between two training data instances, as well as changing the associated label to the corresponding convex combination of the original 2 labels.', 'The motivation is clear and the idea is simple and effective.', 'The main con lies in this work being very closely related to t-sne, i.e. compare the the temporal clustering loss based on kl-div (eq 6) to t-sne.']",,,Proceedings of the IEEE,"['storage', 'dna', 'system', 'data storage', 'Data Storage']"
Data Storage,11c3154d709c74dbbe702e7f7c46a37224f9cc36,Secure Data Storage and Searching for Industrial IoT by Integrating Fog Computing and Cloud Computing,https://www.semanticscholar.org/paper/11c3154d709c74dbbe702e7f7c46a37224f9cc36,JournalArticle,"With the fast development of industrial Internet of things (IIoT), a large amount of data is being generated continuously by different sources. Storing all the raw data in the IIoT devices locally is unwise considering that the end devices’ energy and storage spaces are strictly limited. In addition, the devices are unreliable and vulnerable to many threats because the networks may be deployed in remote and unattended areas. In this paper, we discuss the emerging challenges in the aspects of data processing, secure data storage, efficient data retrieval and dynamic data collection in IIoT. Then, we design a flexible and economical framework to solve the problems above by integrating the fog computing and cloud computing. Based on the time latency requirements, the collected data are processed and stored by the edge server or the cloud server. Specifically, all the raw data are first preprocessed by the edge server and then the time-sensitive data (e.g., control information) are used and stored locally. The non-time-sensitive data (e.g., monitored data) are transmitted to the cloud server to support data retrieval and mining in the future. A series of experiments and simulation are conducted to evaluate the performance of our scheme. The results illustrate that the proposed framework can greatly improve the efficiency and security of data storage and retrieval in IIoT.",2018,211,14,"[1843181.0, 2144443688.0, 144861189.0, 1712275.0, 1831104.0]",1843181.0,Tirana,2,"[48455738.0, 1396822962.0]",Y,"['I do not think the method is theoretically well-motivated as presented, but the empirical results seem solid.', 'If such a representation z could be found, x is deemed to be healthy, and anomalous otherwise.']",,,IEEE Transactions on Industrial Informatics,"['data', 'server', 'iiot', 'data storage', 'Data Storage']"
Data Storage,94de6bab96ad533169dbc3bf9e6557581e59cb6f,Data Storage in the Decentralized World: Blockchain and Derivatives,https://www.semanticscholar.org/paper/94de6bab96ad533169dbc3bf9e6557581e59cb6f,JournalArticle,"We have entered an era where the importance of decentralized solutions has become more obvious. Blockchain technology and its derivatives are distributed ledger technologies that keep the registry of data between peers of a network. This ledger is secured within a successive over looping cryptographic chain. The accomplishment of the Bitcoin cryptocurrency proved that blockchain technology and its derivatives could be used to eliminate intermediaries and provide security for cyberspace. However, there are some challenges in the implementation of blockchain technology. This chapter first explains the concept of blockchain technology and the data that we can store therein. The main advantage of blockchain is the security services that it provides. This section continues by describing these services.. The challenges of blockchain; blockchain anomalies, energy consumption, speed, scalability, interoperability, privacy and cryptology in the age of quantum computing are described. Selected solutions for these challenges are given. Remarkable derivatives of blockchain, which use different solutions (directed acyclic graph, distributed hash table, gossip consensus protocol) to solve some of these challenges are described. Then the data storage in blockchain and evolving data solutions are explained. The comparison of decentralized solutions with the lcentralized database systems is given. A multi-platform interoperable scalable architecture (MPISA) is proposed. In the conclusion we include the evolution assumptions of data storage in a decentralized world.",2020,19,abs/2012.10253,"[40631192.0, 1573532302.0]",40631192.0,Dublin,2,"[40238834.0, 2231175914.0]",Y,"['The proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.', 'Presents an interesting idea fairly clearly, but overall very similar to expectation-linear dropout.']",,,arXiv.org,"['solutions', 'blockchain', 'data', 'data storage', 'Data Storage']"
Data Storage,afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84,IoT-Based Big Data Storage Systems in Cloud Computing: Perspectives and Challenges,https://www.semanticscholar.org/paper/afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84,JournalArticle,"Internet of Things (IoT) related applications have emerged as an important field for both engineers and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations especially in cloud computing. This paper first provides a functional framework that identifies the acquisition, management, processing and mining areas of IoT big data, and several associated technical modules are defined and described in terms of their key characteristics and capabilities. Then current research in IoT application is analyzed, moreover, the challenges and opportunities associated with IoT big data research are identified. We also report a study of critical IoT application publications and research topics based on related academic and industry publications. Finally, some open issues and some typical examples are given under the proposed IoT-related research framework.",2017,441,4,"[1720869.0, 2219053320.0, 30000987.0, 1747034.0]",1720869.0,Madrid,3,"[50768264.0, 1492047220.0, 2107856766.0]",Y,"['Indeed, the authors have succeed in showing that this is not necessarily the case.', 'Although, a more detailed discussion and a clearer explanation is needed to clarify what SSF is actually doing, based on the provided formulation.', 'Following the design of the tandem blocks proposed in the paper, I wonder why the tandem block B3x3(2,w) was not included.']",,,IEEE Internet of Things Journal,"['research', 'iot', 'framework', 'data storage', 'Data Storage']"
Data Storage,cb12fe714dd4fa1d14b6a8023d494c14c89e90ea,ElfStore: A Resilient Data Storage Service for Federated Edge and Fog Resources,https://www.semanticscholar.org/paper/cb12fe714dd4fa1d14b6a8023d494c14c89e90ea,Conference,"Edge and fog computing have grown popular as IoT deployments become wide-spread. While application composition and scheduling on such resources are being explored, there exists a gap in a distributed data storage service on the edge and fog layer, instead depending solely on the cloud for data persistence. Such a service should reliably store and manage data on fog and edge devices, even in the presence of failures, and offer transparent discovery and access to data for use by edge computing applications. Here, we present ElfStore, a first-of-its-kind edge-local federated store for streams of data blocks. It uses reliable fog devices as a super-peer overlay to monitor the edge resources, offers federated metadata indexing using Bloom filters, locates data within 2-hops, and maintains approximate global statistics about the reliability and storage capacity of edges. Edges host the actual data blocks, and we use a unique differential replication scheme to select edges on which to replicate blocks, to guarantee a minimum reliability and to balance storage utilization. Our experiments on two IoT virtual deployments with 20 and 272 devices show that ElfStore has low overheads, is bound only by the network bandwidth, has scalable performance, and offers tunable resilience.",2019,23,,"[48439542.0, 2229703159.0, 1761220.0]",48439542.0,Belgrade,2,"[2112455515.0, 48576745.0]",Y,"['My concern is that one-bit system is already complicated to implement.', '* Figure 2 seems like a test made to work for this method and does not add much to the paper.']",2019 IEEE International Conference on Web Services (ICWS),19.0,,"['data', 'edge', 'fog', 'data storage', 'Data Storage']"
Data Storage,9d788cfe4a0991d3b1a266c8329f6e903840b82f,Deep Learning-Based Data Storage for Low Latency in Data Center Networks,https://www.semanticscholar.org/paper/9d788cfe4a0991d3b1a266c8329f6e903840b82f,JournalArticle,"Low-latency data access is becoming an upcoming and increasingly important challenge. The proper placement of data blocks can reduce data travel among distributed storage systems, which contributes significantly to the latency reduction. However, the dominant data placement optimization has primarily relied on prior known data requests or static initial data distribution, which ignores the dynamics of clients’ data access requests and networks. The learning technology can help the data center networks (DCNs) learn from historical access information and make optimal data storage decision. Consider a more practical DCNs with fat-tree topology, we utilize a deep-learning technology <inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula>-means to help store data blocks and then improve the read and write latency of the DCN, where <inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> is the number of cores in the fat-tree. The evaluation results demonstrate that the average write and read latency of the whole system can be lowered by 33% and 45%, respectively. And the best set of parameter <inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> is analyzed and recommended to provide guidance to the real application, which is equal to the number of cores in the DCNs.",2019,33,7,"[2431843.0, 2109960523.0, 32434251.0, 1796706.0, 2143716755.0, 79987966.0]",2431843.0,Tallinn,3,"[2072010.0, 1761667.0, 46234199.0]",Y,"['Suggestion: After capturing the motivation of the task, I suspect that the traditional tree-to-tree (also X-to-tree) ""statistical"" machine translation methods still can also work correctly in this task.', 'Significance ========== Having an RL approach that can benefit from truly off-policy samples is highly relevant.', 'Although, the observations are interesting, especially the one on MNIST where the network performs well even with correct labels slightly above chance, the overall contributions are incremental.']",,,IEEE Access,"['data', 'access', 'latency', 'data storage', 'Data Storage']"
Data Storage,90e4330fed2da147dd41490e8ad638b618112b3d,Fast Erasure Coding for Data Storage,https://www.semanticscholar.org/paper/90e4330fed2da147dd41490e8ad638b618112b3d,Conference,"Various techniques have been proposed in the literature to improve erasure code computation efficiency, including optimizing bitmatrix design and computation schedule, common XOR (exclusive-OR) operation reduction, caching management techniques, and vectorization techniques. These techniques were largely proposed individually, and, in this work, we seek to use them jointly. To accomplish this task, these techniques need to be thoroughly evaluated individually and their relation better understood. Building on extensive testing, we develop methods to systematically optimize the computation chain together with the underlying bitmatrix. This led to a simple design approach of optimizing the bitmatrix by minimizing a weighted computation cost function, and also a straightforward coding procedure—follow a computation schedule produced from the optimized bitmatrix to apply XOR-level vectorization. This procedure provides better performances than most existing techniques (e.g., those used in ISA-L and Jerasure libraries), and sometimes can even compete against well-known but less general codes such as EVENODD, RDP, and STAR codes. One particularly important observation is that vectorizing the XOR operations is a better choice than directly vectorizing finite field operations, not only because of the flexibility in choosing finite field size and the better encoding throughput, but also its minimal migration efforts onto newer CPUs.",2020,28,,"[1500391633.0, 143804003.0]",1500391633.0,Podgorica,2,"[2039003.0, 2158141874.0]",Y,"['I feel that the authors should give a more prominent disclaimer to potential users of the test.', 'Flexible muscle-based locomotion for bipedal creatures.']",USENIX Conference on File and Storage Technologies,20.0,,"['techniques', 'computation', 'bitmatrix', 'data storage', 'Data Storage']"
Data Querying,695bdc6e24608364491b9418a220c65a7cd17413,Semantic Data Querying over NoSQL Databases with Apache Spark,https://www.semanticscholar.org/paper/695bdc6e24608364491b9418a220c65a7cd17413,Conference,"The rapid growth of semantic data in the form of RDF triples demands a scalable distributed storage and efficient query processing engine for its management and reuse. To overcome the limitation of native RDF stores and traditional relational database management systems and scale adequately with the exponential increase in the size of RDF datasets, Big Data processing infrastructure like Hadoop with MapReduce have been used. NoSQL databases such as HBase and Cassandra for storing large-scale RDF data and in-memory data processing to execute SPARQL query as SQL query using Apache Spark is proposed in this paper. This paper presents techniques for distributed RDF data storage and querying schemes for HBase and Cassandra clusters. We also present a compiler that translates SPARQL queries into their Spark SQL equivalent for execution. An empirical comparison of HBase and Cassandra systems using datasets and queries from Berlin SPARQL Benchmark (BSBM) and SPARQL Performance Benchmark (SP2Bench) on Microsoft Azure cloud is presented.",2018,8,,"[2042697689.0, 1720381.0]",2042697689.0,Stockholm,2,"[49573525.0, 2220547623.0]",Y,"['It seems like the proposed method is quite generic, so why not apply it to a stronger baseline?', 'The experiments are very clearly presented and solidly designed.']",IEEE International Conference on Information Reuse and Integration,18.0,,"['data', 'rdf', 'sparql', 'data querying', 'Data Querying']"
Data Querying,92912dd895c360f01a6be9c9f6d207642139525e,An ontology-based approach to improve data querying and organization of Alzheimer’s Disease data,https://www.semanticscholar.org/paper/92912dd895c360f01a6be9c9f6d207642139525e,Conference,"The recent advances in biotechnology and IT have led to an ever-increasing availability of public biomedical data distributed in large databases. Analyzing this huge volume of data is a challenging task because of its complexity, high heterogeneity and its multiple and numerous correlated factors. In the framework of neurodegenerative diseases, the last years have witnessed the creation of specialized databases such as the international projects ADNI (Alzheimer’s Disease Neuroimaging Initiative). The main problems to fully exploit this database are related to the querying, integration, and analysis of data themselves. Here, we aim to develop a detailed ontology for clinical multidimensional datasets from ADNI repository in order to simplify the data access and to obtain new diagnostic knowledge about Alzheimer’s Disease.",2018,5,,"[2756615.0, 3341829.0, 50064714.0, 6922855.0, 6339256.0, 2866596.0, 2484334.0, 2794706.0]",2756615.0,Brussels,3,"[2104195713.0, 2162779709.0, 39765564.0]",Y,"[""Just because such a baseline wasn't previously proposed in literature (in the narrow scope of this problem) doesn't mean it's not an obvious baseline to try."", 'Deterministic latent models seem to work better than stochastic ones.', 'Moreover, a basic system called delta t block implements one level of full approximation and is stoked several times.']",IEEE International Conference on Bioinformatics and Biomedicine,18.0,,"['data', 'adni', 'alzheimer', 'data querying', 'Data Querying']"
Data Querying,224c11bc51b4959bc787d6681c2b152468294b11,Multi-Touch Querying on Data Physicalizations in Immersive AR,https://www.semanticscholar.org/paper/224c11bc51b4959bc787d6681c2b152468294b11,JournalArticle,"Data physicalizations (3D printed terrain models, anatomical scans, or even abstract data) can naturally engage both the visual and haptic senses in ways that are difficult or impossible to do with traditional planar touch screens and even immersive digital displays. Yet, the rigid 3D physicalizations produced with today's most common 3D printers are fundamentally limited for data exploration and querying tasks that require dynamic input (e.g., touch sensing) and output (e.g., animation), functions that are easily handled with digital displays. We introduce a novel style of hybrid virtual + physical visualization designed specifically to support interactive data exploration tasks. Working toward a ""best of both worlds"" solution, our approach fuses immersive AR, physical 3D data printouts, and touch sensing through the physicalization. We demonstrate that this solution can support three of the most common spatial data querying interactions used in scientific visualization (streamline seeding, dynamic cutting places, and world-in-miniature visualization). Finally, we present quantitative performance data and describe a first application to exploratory visualization of an actively studied supercomputer climate simulation data with feedback from domain scientists.",2021,7,5,"[153475895.0, 2089478126.0, 1825752990.0, 2141634264.0, 3307286.0, 144027436.0, 145735309.0]",153475895.0,Berlin,2,"[48667025.0, 50521235.0]",Y,"['This paper shows an observation of “super-convergence” when training resnet with cyclical learning rates but does not provide conclusive analysis or experiment results.', 'How did you generate these stories with so many sentences?']",,,Proc. ACM Hum. Comput. Interact.,"['data', '3d', 'visualization', 'data querying', 'Data Querying']"
Data Querying,0ecf3f089e6dc7944b440227069b9d0143e18d78,SpeakQL: Towards Speech-driven Multimodal Querying of Structured Data,https://www.semanticscholar.org/paper/0ecf3f089e6dc7944b440227069b9d0143e18d78,Conference,"Speech-driven querying is becoming popular in new device environments such as smartphones, tablets, and even conversational assistants. However, such querying is largely restricted to natural language. Typed SQL remains the gold standard for sophisticated structured querying although it is painful in many environments, which restricts when and how users consume their data. In this work, we propose to bridge this gap by designing a speech-driven querying system and interface for structured data we call SpeakQL. We support a practically useful subset of regular SQL and allow users to query in any domain with novel touch/speech based human-in-the-loop correction mechanisms. Automatic speech recognition (ASR) introduces myriad forms of errors in transcriptions, presenting us with a technical challenge. We exploit our observations of SQL's properties, its grammar, and the queried database to build a modular architecture. We present the first dataset of spoken SQL queries and a generic approach to generate them for any arbitrary schema. Our experiments show that SpeakQL can automatically correct a large fraction of errors in ASR transcriptions. User studies show that SpeakQL can help users specify SQL queries significantly faster with a speedup of average 2.7x and up to 6.7x compared to typing on a tablet device. SpeakQL also reduces the user effort in specifying queries by a factor of average 10x and up to 60x compared to raw typing effort.",2020,12,,"[144759563.0, 47319783.0, 2119303314.0, 1796044.0]",144759563.0,Moscow,2,"[2691974.0, 2273645.0]",Y,"['The proposed method achieves perfect accuracy in every condition.', 'What is the per-pixel likelihood obtained on the CIFAR dataset and what is the likelihood on a model where T=1 (for omniglot/cifar)?']",SIGMOD Conference,20.0,,"['speakql', 'sql', 'users', 'data querying', 'Data Querying']"
Data Querying,557ee73ff4e87ca0cb1b6c78af0730a2d94b710f,Panorama: A Data System for Unbounded Vocabulary Querying over Video,https://www.semanticscholar.org/paper/557ee73ff4e87ca0cb1b6c78af0730a2d94b710f,JournalArticle,"Deep convolutional neural networks (CNNs) achieve state-of-the-art accuracy for many computer vision tasks. But using them for video monitoring applications incurs high computational cost and inference latency. Thus, recent works have studied how to improve system efficiency. But they largely focus on small ""closed world"" prediction vocabularies even though many applications in surveillance security, traffic analytics, etc. have an ever-growing set of target entities. We call this the ""unbounded vocabulary"" issue, and it is a key bottleneck for emerging video monitoring applications. We present the first data system for tacking this issue for video querying, Panorama. Our design philosophy is to build a unified and domain-agnostic system that lets application users generalize to unbounded vocabularies in an out-of-the-box manner without tedious manual re-training. To this end, we synthesize and innovate upon an array of techniques from the ML, vision, databases, and multimedia systems literature to devise a new system architecture. We also present techniques to ensure Panorama has high inference efficiency. Experiments with multiple real-world datasets show that Panorama can achieve between 2x to 20x higher efficiency than baseline approaches on in-vocabulary queries, while still yielding comparable accuracy and also generalizing well to unbounded vocabularies.",2019,29,13,"[2108545581.0, 2119303314.0]",2108545581.0,Prague,2,"[2173602.0, 1784862.0]",Y,"['Overall, this works seems somewhat too preliminary at this stage.', 'It would be interesting to see the results of the new activation function on LSTM.']",,,Proceedings of the VLDB Endowment,"['system', 'video', 'applications', 'data querying', 'Data Querying']"
Data Querying,1fa4936fb06319c3f4536c26a447d5507c92bd48,A Privacy-Preserving and Verifiable Querying Scheme in Vehicular Fog Data Dissemination,https://www.semanticscholar.org/paper/1fa4936fb06319c3f4536c26a447d5507c92bd48,JournalArticle,"Vehicular fog has attracted considerable attention recently, as the densely deployed fog devices are in proximity to vehicular end-users, and they are particularly suitable for the latency-sensitive and location-aware vehicular services. In this paper, we propose a secure querying scheme in vehicular fog data dissemination, in which the roadside units (RSUs) act as fog storage devices to cache data at network edge and disseminate data upon querying. To disrupt the association between a specific data request and its origin vehicle, the proposed scheme exploits an invertible matrix to structure multiple data requests from different vehicles, and aggregates the ciphertexts of data requests at the RSU side with the homomorphic Paillier cryptosystem. Meanwhile, given the invertible matrix and decryption result, the RSU can recover each individual data request without identifying its origin vehicle. In addition, the RSU can verify the correctness of the recovered data requests with an identity-based batch verification scheme. Through security analysis, we demonstrate that the proposed scheme can achieve the security goals of unlinkability, confidentiality, and verifiability. Performance evaluations are also conducted, in which the obtained results show that the proposed scheme can be adaptive to the fluctuating number of the data querying vehicles, and significantly reduce the computation complexity and communication overhead.",2019,30,68,"[2984224.0, 103073143.0, 144637915.0, 3326354.0]",2984224.0,Valletta,2,"[50521235.0, 2253561592.0]",Y,"['I do not think the method is theoretically well-motivated as presented, but the empirical results seem solid.', 'It is nice to know the bounds given in the paper and to understand the theoretical conditions under which we can obtain running time benefits using corsets.']",,,IEEE Transactions on Vehicular Technology,"['data', 'scheme', 'fog', 'data querying', 'Data Querying']"
Data Querying,8799ec4bdf98bc313aa8c41a706a385e026d1f88,Enabling Automatic Discovery and Querying of Web APIs at Web Scale using Linked Data Standards,https://www.semanticscholar.org/paper/8799ec4bdf98bc313aa8c41a706a385e026d1f88,Conference,"To help in making sense of the ever-increasing number of data sources available on the Web, in this article we tackle the problem of enabling automatic discovery and querying of data sources at Web scale. To pursue this goal, we suggest to (1) provision rich descriptions of data sources and query services thereof, (2) leverage the power of Web search engines to discover data sources, and (3) rely on simple, well-adopted standards that come with extensive tooling. We apply these principles to the concrete case of SPARQL micro-services that aim at querying Web APIs using SPARQL. The proposed solution leverages SPARQL Service Description, SHACL, DCAT, VoID, Schema.org and Hydra to express a rich functional description that allows a software agent to decide whether a micro-service can help in carrying out a certain task. This description can be dynamically transformed into a Web page embedding rich markup data. This Web page is both a human-friendly documentation and a machine-readable description that makes it possible for humans and machines alike to discover and invoke SPARQL micro-services at Web scale, as if they were just another data source. We report on a prototype implementation that is available on-line for test purposes, and that can be effectively discovered using Google’s Dataset Search engine.",2019,24,,"[35202970.0, 1390195074.0, 1773317.0, 1753013.0]",35202970.0,Zagreb,2,"[49997612.0, 2060154.0]",Y,"['(This is hinted at by the mention of fully convolutional networks.)  The method could just as easily be applied to learn a task-specific rotation of the fully-connected layer weights.', 'Stronger ankles are more generally correlated with a heavier body rather than heavy feet, given that a key role of the ankle is to be able to provide a ""push"" to the body at the end of a stride, and perhaps less for ""lifting the foot"".']",The Web Conference,19.0,,"['web', 'data', 'sources', 'data querying', 'Data Querying']"
Data Querying,99ae62cca0b15275d9ed1528f345f9dcefe50dd7,Temporal Data Representation and Querying Based on RDF,https://www.semanticscholar.org/paper/99ae62cca0b15275d9ed1528f345f9dcefe50dd7,JournalArticle,"With the explosive growth of temporal data, how to query and manage temporal data has become an important research issue. Resource description framework (RDF), as the standard data and knowledge description language of the semantic web, has been widely used to represent various domain data. Aiming at the representation and querying of temporal data, this paper proposes a temporal data representation model based on RDF and its corresponding querying method. First, a representation model called RDFt is proposed, which can represent temporal data with both the time information and the update count information, and the syntax and semantics of the RDFt model are given. Then, we propose a query language called SPARQL[t] for RDFt, and we give the query syntax and operations of SPARQL[t] in detail. In addition, a querying transformation algorithm from SPARQL[t] to SPARQL is proposed, in order to achieve compatibility with the existing RDF query engines. Finally, we implemented a prototype system that can support RDFt temporal data representation and querying, and the case studies and experimental results verify the feasibility of the proposed approach.",2019,15,7,"[2686365.0, 2133843731.0, 9248001.0, 4671421.0]",2686365.0,Bratislava,2,"[40068904.0, 26391899.0]",Y,"['While I can now clearly see the contributions of the paper, the minimal revisions in the paper do not make the contributions clear yet (in my opinion that should already be clear after having read the introduction).', 'An interesting paper, but not the clearest presentation.']",,,IEEE Access,"['data', 'query', 'representation', 'data querying', 'Data Querying']"
Data Querying,01bf0e83159712fbbbd12171a7e268547a4cfbc5,Querying Data Lakes using Spark and Presto,https://www.semanticscholar.org/paper/01bf0e83159712fbbbd12171a7e268547a4cfbc5,Conference,"Squerall is a tool that allows the querying of heterogeneous, large-scale data sources by leveraging state-of-the-art Big Data processing engines: Spark and Presto. Queries are posed on-demand against a Data Lake, i.e., directly on the original data sources without requiring prior data transformation. We showcase Squerall's ability to query five different data sources, including inter alia the popular Cassandra and MongoDB. In particular, we demonstrate how it can jointly query heterogeneous data sources, and how interested developers can easily extend it to support additional data sources. Graphical user interfaces (GUIs) are offered to support users in (1) building intra-source queries, and (2) creating required input files.",2019,14,,"[3059455.0, 2235966.0, 1705658.0, 2362078.0, 145044578.0]",3059455.0,Copenhagen,2,"[3353468.0, 2254255.0]",Y,"['For evaluating whether the data point x is anomalous or not, we search for a latent representation z such that x \\approx g_\\theta(z).', '2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing.']",The Web Conference,19.0,,"['data', 'sources', 'support', 'data querying', 'Data Querying']"
Data Querying,8babcaf89f8537dc628a029ebf932100f57289fd,Lightweight Indexing and Querying Services for Big Spatial Data,https://www.semanticscholar.org/paper/8babcaf89f8537dc628a029ebf932100f57289fd,JournalArticle,"With the widespread use of GPS-equipped smartphones and Internet of Things devices, a huge amount of data with location information is being generated at an unprecedented rate. To gain a deeper insight into such a plethora of spatial data, scientists and engineers are widely using spatial queries for their big data applications. However, because of not only the massive spatial data size but also the complexity of spatial query processing, they are struggling to efficiently process the spatial queries. In this paper, we propose lightweight and scalable indexing and querying services for big spatial data stored in distributed storage systems or graph-based systems. Our spatial services have several advantages over existing approaches. First, our services can be easily applied to existing storage systems or graph-based models without modifying the internal implementation of existing systems/models. Second, our services achieve high pruning power by efficiently selecting only relevant spatial objects based on a simple yet effective filter. Third, our services support a customizable and easy-to-use control of index data size by adjusting the precision of indexed geometries. Lastly, our services support efficient updates of spatial data. Our experimental results using real-world datasets validate the effectiveness and efficiency of our spatial services.",2019,14,12,"[1899648.0, 46458150.0, 143642366.0, 1718467.0, 2145907057.0, 144586053.0, 2115979102.0]",1899648.0,Belgrade,2,"[1744598.0, 2256558402.0]",Y,"['I found the discussion about rank to be very intuitive, however this intuition is not fully tested.', 'This second part was confusing - although the maze can be viewed as a graph, many other works apply ConvNets to maze environments [1, 2, 3], and their work has little relation to other work on graph CNNs.']",,,IEEE Transactions on Services Computing,"['data', 'services', 'systems', 'data querying', 'Data Querying']"
Data Querying,d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b,BimSPARQL: Domain-specific functional SPARQL extensions for querying RDF building data,https://www.semanticscholar.org/paper/d9c6b474fb2f303391b50c9fd59a5f2ce4becc0b,JournalArticle,"In this paper, we propose to extend SPARQL functions for querying Industry Foundation Classes (IFC) building data. The official IFC documentation and BIM requirement checking use cases are used to drive the development of the proposed functionality. By extending these functions, we aim to (1) simplify writing queries and (2) retrieve useful information implied in 3D geometry data according to requirement checking use cases. Extended functions are modelled as RDF vocabularies and classified into groups for further extensions. We combine declarative rules with procedural programming to implement extended functions. Realistic requirement checking scenarios are used to evaluate and demonstrate the effectiveness of this approach and indicate query performance. Compared with query techniques developed in the conventional Building Information Modeling domain, we show the added value of such approach by providing an application example of querying building and regulatory data, where spatial and logic reasoning can be applied and data from multiple sources are required. Based on the implementation and evaluation work, we discuss the advantages and applicability of this approach, current issues and future challenges.",2018,66,9,"[2115811945.0, 144204299.0, 35195456.0]",2115811945.0,London,2,"[2217936172.0, 144330671.0]",Y,"['After reading the paper, it’s not clear to me what the components of the model are, what each of them take as input and produce as output, what these modules do and how they are combined.', 'It is not clear to me why h_t should depend on \\tilde{b}_t.']",,,Semantic Web,"['functions', 'data', 'requirement', 'data querying', 'Data Querying']"
Data Querying,b9f190339ce0b4cdad3464c45ec3266a7369fb7c,Querying Log Data with Metric Temporal Logic,https://www.semanticscholar.org/paper/b9f190339ce0b4cdad3464c45ec3266a7369fb7c,JournalArticle,"
 
 
We propose a novel framework for ontology-based access to temporal log data using a datalog extension datalogMTL of the Horn fragment of the metric temporal logic MTL. We show that datalogMTL is EXPSPACE-complete even with punctual intervals, in which case full MTL is known to be undecidable. We also prove that nonrecursive datalogMTL is PSPACE-complete for combined complexity and in AC0 for data complexity. We demonstrate by two real-world use cases that nonrecursive datalogMTL programs can express complex temporal concepts from typical user queries and thereby facilitate access to temporal log data. Our experiments with Siemens turbine data and MesoWest weather data show that datalogMTL ontology-mediated queries are efficient and scale on large datasets. 
 
 
",2017,60,62,"[144105942.0, 3449760.0, 145500489.0, 47490276.0, 1749505.0]",144105942.0,Riga,3,"[2147184066.0, 2238219937.0, 2258785611.0]",Y,"['In the no reconstruction loss experiments do you still sample \\tilde{b}_t in the generative part?', 'The model is further extended for conditional generation and demonstrated on a range of image benchmark data sets.', 'Standard idea, great results This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives.']",,,Journal of Artificial Intelligence Research,"['data', 'datalogmtl', 'access', 'data querying', 'Data Querying']"
Data Querying,ac67d5f9c89d8d72fbd074f94079608220348f3f,ATHENA: An Ontology-Driven System for Natural Language Querying over Relational Data Stores,https://www.semanticscholar.org/paper/ac67d5f9c89d8d72fbd074f94079608220348f3f,JournalArticle,"In this paper, we present ATHENA, an ontology-driven system for natural language querying of complex relational databases. Natural language interfaces to databases enable users easy access to data, without the need to learn a complex query language, such as SQL. ATHENA uses domain specific ontologies, which describe the semantic entities, and their relationships in a domain. We propose a unique two-stage approach, where the input natural language query (NLQ) is first translated into an intermediate query language over the ontology, called OQL, and subsequently translated into SQL. Our two-stage approach allows us to decouple the physical layout of the data in the relational store from the semantics of the query, providing physical independence. Moreover, ontologies provide richer semantic information, such as inheritance and membership relations, that are lost in a relational schema. By reasoning over the ontologies, our NLQ engine is able to accurately capture the user intent. We study the effectiveness of our approach using three different workloads on top of geographical (GEO), academic (MAS) and financial (FIN) data. ATHENA achieves 100% precision on the GEO and MAS workloads, and 99% precision on the FIN workload which operates on a complex financial ontology. Moreover, ATHENA attains 87.2%, 88.3%, and 88.9% recall on the GEO, MAS, and FIN workloads, respectively.",2016,153,9,"[39376048.0, 2327080.0, 145590185.0, 1856878.0, 3441349.0, 143972996.0]",39376048.0,Berlin,2,"[34725175.0, 49030422.0]",Y,"['As a result, it seems that at least part of the effect of explicitly reducing the variance is just stronger weight penalty.', 'Lack of convincing motivation and results not particularly unimpressive This paper proposes to train a classifier neural network not just to classifier, but also to reconstruct a representation of its input, in order to factorize the class information from the appearance (or ""style"" as used in this paper).']",,,Proceedings of the VLDB Endowment,"['language', 'athena', 'query', 'data querying', 'Data Querying']"
Data Querying,538fbdb8013ab43a9b5d725461b294ad29fcced7,Sub-millisecond Stateful Stream Querying over Fast-evolving Linked Data,https://www.semanticscholar.org/paper/538fbdb8013ab43a9b5d725461b294ad29fcced7,Conference,"Applications like social networking, urban monitoring and market feed processing require stateful stream query: a query consults not only streaming data but also stored data to extract timely information; useful information from streaming data also needs to be continuously and consistently integrated into stored data to serve inflight and future queries. However, prior streaming systems either focus on stream computation, or are not stateful, or cannot provide low latency and high throughput to handle the fast-evolving linked data and increasing concurrency of queries. This paper presents Wukong+S, a distributed stream querying engine that provides sub-millisecond stateful query at millions of queries per-second over fast-evolving linked data. Wukong+S uses an integrated design that combines the stream processor and the persistent store with efficient state sharing, which avoids the cross-system cost and sub-optimal query plan in conventional composite designs (e.g., Storm/Heron+Wukong). Wukong+S uses a hybrid store to differentially manage timeless data and timing data accordingly and provides an efficient stream index with locality-aware partitioning to facilitate fast access to streaming data. Wukong+S further provides decentralized vector timestamps with bounded snapshot scalarization to scale with nodes and massive queries at efficient memory usage. We have designed Wukong+S conforming to the RDF data model and Continuous SPARQL (C-SPARQL) query interface and have implemented Wukong+S by extending a state-of-the-art static RDF store (namely Wukong). Evaluation on an 8-node RDMA-capable cluster using LSBench and CityBench shows that Wukong+S significantly outperforms existing system designs (e.g., CSPARQL-engine, Storm/Heron+Wukong, and Spark Streaming/Structured Streaming) for both latency and throughput, usually at the scale of orders of magnitude.",2017,48,,"[2108123758.0, 144593763.0, 2118438836.0]",2108123758.0,Monaco,3,"[49889487.0, 35202970.0, 47947548.0]",Y,"['If this is not the case for some reason, more detailed explanation is needed.', '3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling.', 'As is often the case with such comparisons, it is hard to disentangle from where exactly come the speedups.']",Symposium on Operating Systems Principles,26.0,,"['data', 'wukongs', 'stream', 'data querying', 'Data Querying']"
Data Querying,736ef8a32d6c5f76a21d61299300cf796480d507,Toward sustainable publishing and querying of distributed Linked Data archives,https://www.semanticscholar.org/paper/736ef8a32d6c5f76a21d61299300cf796480d507,JournalArticle,"Purpose 
 
 
 
 
The purpose of this paper is to detail a low-cost, low-maintenance publishing strategy aimed at unlocking the value of Linked Data collections held by libraries, archives and museums (LAMs). 
 
 
 
 
Design/methodology/approach 
 
 
 
 
The shortcomings of commonly used Linked Data publishing approaches are identified, and the current lack of substantial collections of Linked Data exposed by LAMs is considered. To improve on the discussed status quo, a novel approach for publishing Linked Data is proposed and demonstrated by means of an archive of DBpedia versions, which is queried in combination with other Linked Data sources. 
 
 
 
 
Findings 
 
 
 
 
The authors show that the approach makes publishing Linked Data archives easy and affordable, and supports distributed querying without causing untenable load on the Linked Data sources. 
 
 
 
 
Research limitations/implications 
 
 
 
 
The proposed approach significantly lowers the barrier for publishing, maintaining, and making Linked Data collections queryable. As such, it offers the potential to substantially grow the distributed network of queryable Linked Data sources. Because the approach supports querying without causing unacceptable load on the sources, the queryable interfaces are expected to be more reliable, allowing them to become integral building blocks of robust applications that leverage distributed Linked Data sources. 
 
 
 
 
Originality/value 
 
 
 
 
The novel publishing strategy significantly lowers the technical and financial barriers that LAMs face when attempting to publish Linked Data collections. The proposed approach yields Linked Data sources that can reliably be queried, paving the way for applications that leverage distributed Linked Data sources through federated querying.",2018,23,74,"[2701137.0, 1723397.0, 2911425.0, 8050194.0]",2701137.0,London,3,"[1453724884.0, 144031464.0, 34650964.0]",Y,"['However, the presentation of the method is not clear, the experiment is not sufficient, and the paper is not polished.', 'The method leads to better performance than using no external resources, but not as high performance as using Glove embeddings.', '(e.g. recent libraries pytorch, TF eager can express conditional logic much more simply than previous approaches, its easy to communicate why you might use them).']",,,J. Documentation,"['data', 'sources', 'publishing', 'data querying', 'Data Querying']"
Data Querying,fa77a44f3f1857361a50c3137d623c35ef8a5739,Querying Graphs with Data,https://www.semanticscholar.org/paper/fa77a44f3f1857361a50c3137d623c35ef8a5739,JournalArticle,"Graph databases have received much attention as of late due to numerous applications in which data is naturally viewed as a graph; these include social networks, RDF and the Semantic Web, biological databases, and many others. There are many proposals for query languages for graph databases that mainly fall into two categories. One views graphs as a particular kind of relational data and uses traditional relational mechanisms for querying. The other concentrates on querying the topology of the graph. These approaches, however, lack the ability to combine data and topology, which would allow queries asking how data changes along paths and patterns enveloping it. In this article, we present a comprehensive study of languages that enable such combination of data and topology querying. These languages come in two flavors. The first follows the standard approach of path queries, which specify how labels of edges change along a path, but now we extend them with ways of specifying how both labels and data change. From the complexity point of view, the right type of formalisms are subclasses of register automata. These, however, are not well suited for querying. To overcome this, we develop several types of extended regular expressions to specify paths with data and study their querying power and complexity. The second approach adopts the popular XML language XPath and extends it from XML documents to graphs. Depending on the exact set of allowed features, we have a family of languages, and our study shows that it includes efficient and highly expressive formalisms for querying both the structure of the data and the data itself.",2016,105,63,"[2285196231.0, 2243335778.0, 2434366.0]",2285196231.0,Oslo,2,"[50403503.0, 1990265392.0]",Y,"['See more detailed comments in the paragraph below.', '* The experiments are few and too specific, specially given that the paper presents a very general framework.']",,,Journal of the ACM,"['data', 'graph', 'languages', 'data querying', 'Data Querying']"
Data Querying,7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2,"Storing, Tracking, and Querying Provenance in Linked Data",https://www.semanticscholar.org/paper/7fe709ecc1e9d91ff80c5e8fa354bb1a773fa5f2,JournalArticle,"The proliferation of heterogeneous Linked Data on the Web poses new challenges to database systems. In particular, the capacity to store, track, and query provenance data is becoming a pivotal feature of modern triplestores. We present methods extending a native RDF store to efficiently handle the storage, tracking, and querying of provenance in RDF data. We describe a reliable and understandable specification of the way results were derived from the data and how particular pieces of data were combined to answer a query. Subsequently, we present techniques to tailor queries with provenance data. We empirically evaluate the presented methods and show that the overhead of storing and tracking provenance is acceptable. Finally, we show that tailoring a query with provenance information can also significantly improve the performance of query execution.",2017,28,29,"[2108533.0, 1393644275.0, 1733076.0, 1727784.0]",2108533.0,Stockholm,3,"[1774311.0, 3393711.0, 2141735101.0]",Y,"['The problem dimension is varied along with the class separation in order to control the difficulty of the problem.', ""2) predicting a variable's name by consider its semantic context."", '3. The proposed attention mechanism is mainly demonstrated for single-class classification task, but it would be interesting to see if it can also help the multi-class classification (e.g. image classification on MS-COCO or PASCAL VOC datasets)']",,,IEEE Transactions on Knowledge and Data Engineering,"['data', 'provenance', 'query', 'data querying', 'Data Querying']"
Data Querying,b3dbe1b460a3b66df1653508c9eed7dd51dee4d2,Lusail: A System for Querying Linked Data at Scale,https://www.semanticscholar.org/paper/b3dbe1b460a3b66df1653508c9eed7dd51dee4d2,JournalArticle,"The RDF data model allows publishing interlinked RDF datasets, where each dataset is independently maintained and is queryable via a SPARQL endpoint. Many applications would benefit from querying the resulting large, decentralized, geo-distributed graph through a federated SPARQL query processor. A crucial factor for good performance in federated query processing is pushing as much computation as possible to the local endpoints. Surprisingly, existing federated SPARQL engines are not effective at this task since they rely only on schema information. Consequently, they cause unnecessary data retrieval and communication, leading to poor scalability and response time. This paper addresses these limitations and presents Lusail, a scalable and efficient federated SPARQL system for querying large RDF graphs that are geo-distributed on different endpoints. Lusail uses a novel query rewriting algorithm to push computation to the local endpoints by relying on information about the RDF instances and not only the schema. The query rewriting algorithm has the additional advantage of exposing parallelism in query processing, which Lusail exploits through advanced scheduling at query run time. Our experiments on billions of triples of real and synthetic data show that Lusail outperforms state-of-the-art systems by orders of magnitude in terms of scalability and response time.",2017,24,11,"[145749443.0, 1801187.0, 2168047.0, 1704622.0, 2000187.0]",145749443.0,Chisinau,3,"[1691549.0, 2113618679.0, 9106757.0]",Y,"['9, the definition of Y_c is incorrect and unclear.', 'For example, when showing that the head direction cells generalize in the new mazes how can we be sure that it is not using a common lighting scheme common to both train and test mazes to orient itself?', 'Claiming much of common intuition around tricks for avoiding gradient issues are incorrect.']",,,Proceedings of the VLDB Endowment,"['query', 'rdf', 'sparql', 'data querying', 'Data Querying']"
Data Querying,2374106a32169c07703599ff3f6f4b31e8067b89,Querying biomedical Linked Data with natural language questions,https://www.semanticscholar.org/paper/2374106a32169c07703599ff3f6f4b31e8067b89,JournalArticle,"Recent and intensive research in the biomedical area enabled to accumulate and disseminate biomedical knowledge through various knowledge bases increasingly available on the Web. The exploitation of this knowledge requires to create links between these bases and to use them jointly. Linked Data, the SPARQL language and interfaces in natural language question answering provide interesting solutions for querying such knowledge bases. However, while using biomedical Linked Data is crucial, life-science researchers may have difficulties using the SPARQL language. Interfaces based on natural language question answering are recognized to be suitable for querying knowledge bases. In this paper, we propose a method for translating natural language questions into SPARQL queries. We use Natural Language Processing tools, semantic resources and RDF triple descriptions. We designed a four-step method which allows to linguistically and semantically annotate questions, to perform an abstraction of these questions, then to build a representation of the SPARQL queries, and finally to generate the queries. The method is designed on 50 questions over three biomedical knowledge bases used in the task 2 of the QALD-4 challenge framework and evaluated on 27 new questions. It achieves good performance with 0.78 F-measure on the test set. The method for translating questions into SPARQL queries is implemented as a Perl module and is available at http://search.cpan.org/~thhamon/ RDF-NLP-SPARQLQuery/.",2017,24,8,"[1690504.0, 1732198.0, 1791253.0]",1690504.0,San Marino,3,"[2078196952.0, 151500725.0, 93811144.0]",Y,"['A novel speaker can be adapted just by fitting it with SGD while fixing all other components.', 'A new method for weight quantization.', 'The paper follows with some fun experiments implementing these new game theory notions.']",,,Semantic Web,"['knowledge', 'language', 'questions', 'data querying', 'Data Querying']"
Data Querying,da3f33d858586d24cb265e79eb54f3746e998f57,Querying industrial stream-temporal data: An ontology-based visual approach,https://www.semanticscholar.org/paper/da3f33d858586d24cb265e79eb54f3746e998f57,JournalArticle,"An increasing number of sensors are being deployed in business-critical environments, systems, and equipment; and stream a vast amount of data. The operational efficiency and effectiveness of business processes rely on domain experts’ agility in interpreting data into actionable business information. A domain expert has extensive domain knowledge but not necessarily skills and knowledge on databases and formal query languages. Therefore, centralised approaches are often preferred. These require IT experts to translate the information needs of domain experts into extract-transform-load (ETL) processes in order to extract and integrate data and then let domain experts apply predefined analytics. Since such a workflow is too time intensive, heavy-weight and inflexible given the high volume and velocity of data, domain experts need to extract and analyse the data of interest directly. Ontologies, i.e., semantically rich conceptual domain models, present an intelligible solution by describing the domain of interest on a higher level of abstraction closer to the reality. Moreover, recent ontology-based data access (OBDA) technologies enable end users to formulate their information needs into queries using a set of terms defined in an ontology. Ontological queries could then be translated into SQL or some other database query languages, and executed over the data in its original place and format automatically. To this end, this article reports an ontology-based visual query system (VQS), namely OptiqueVQS, how it is extended for a stream-temporal query language called STARQL, a user experiment with the domain experts at Siemens AG, and STARQL’s query answering performance over a proof of concept implementation for PostgreSQL.",2017,30,9,"[153750193.0, 144911780.0, 1783242.0, 1402158435.0, 1697928.0, 3342377.0, 2028218.0, 144105942.0]",153750193.0,Sarajevo,2,"[2052840328.0, 1453724884.0]",Y,"['The authors evaluate their method based on a mechanical turk survey, where humans are asked to judge the realism of the generated images; additionally, they propose to measure prediction quality by the distance between the manually annotated positions of objects within ground truth and predicted frames.', 'More generally, could the authors comment on how this approach is related to these semi-supervised approaches?']",,,Journal of Ambient Intelligence and Smart Environments,"['domain', 'data', 'experts', 'data querying', 'Data Querying']"
Data Querying,19b93280f17696a4ddfa2c75490a50ab107addf2,SHAHED: A MapReduce-based system for querying and visualizing spatio-temporal satellite data,https://www.semanticscholar.org/paper/19b93280f17696a4ddfa2c75490a50ab107addf2,Conference,"Remote sensing data collected by satellites are now made publicly available by several space agencies. This data is very useful for scientists pursuing research in several applications including climate change, desertification, and land use change. The benefit of this data comes from its richness as it provides an archived history for over 15 years of satellite observations for natural phenomena such as temperature and vegetation. Unfortunately, the use of such data is very limited due to the huge size of archives (> 500TB) and the limited capabilities of traditional applications. This paper introduces SHAHED; a MapReduce-based system for querying, visualizing, and mining large scale satellite data. SHAHED considers both the spatial and temporal aspects of the data to provide efficient query processing at large scale. The core of SHAHED is composed of four main components. The uncertainty component recovers missing data in the input which comes from cloud coverage and satellite mis-alignment. The indexing component provides a novel multi-resolution quad-tree-based spatio-temporal index structure, which indexes satellite data efficiently with minimal space overhead. The querying component answers selection and aggregate queries in real-time using the constructed index. Finally, the visualization component uses MapReduce programs to generate heat map images and videos for user queries. A set of experiments running on a live system deployed on a cluster of machines show the efficiency of the proposed design. All the features supported by SHAHED are made accessible through an easy to use Web interface that hides the complexity of the system and provides a nice user experience.",2015,76,,"[1876123.0, 1756679.0, 1388622435.0, 1404602362.0, 32653213.0, 2635216.0]",1876123.0,Amsterdam,2,"[80842917.0, 2035210.0]",Y,"['For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers.', 'To me, the comparison makes sense but it also shows that the ideas presented here are less novel than they might initially seem.']",IEEE International Conference on Data Engineering,31.0,,"['data', 'satellite', 'component', 'data querying', 'Data Querying']"
Data Querying,5b34752817bc0d6aa96466dabcbc24a83dd071fe,SPARQLByE: Querying RDF data by example,https://www.semanticscholar.org/paper/5b34752817bc0d6aa96466dabcbc24a83dd071fe,JournalArticle,"Semantic Web technologies such as RDF and its query language, SPARQL, offer the possibility of opening up the use of public datasets to a great variety of ordinary users. But a key obstacle to the use of open data is the unfamiliarity of users with the structure of data or with SPARQL. To deal with these issues, we introduce a system for querying RDF data by example. At its core is a technique for reverse-engineering SPARQL queries by example. We demonstrate how reverse engineering along with other techniques, such as query relaxation, enables our system, SPARQLByE, to guide users who are unfamiliar with both the dataset and with SPARQL to the desired query and result set.",2016,48,9,"[2899295.0, 144658846.0, 1750856.0]",2899295.0,Bern,3,"[31380410.0, 2267649469.0, 3149531.0]",Y,"['* On optimisation paths, the choice of PCA directions is wise compared to random projections, and results are nice as plotted.', 'Just to elaborate: in equation (2), the probability of a document is related to the set of distinct words, so it does not distinguish between documents where a word appear multiple times or only once.', 'If it is from the bot response, then it is known which words are named entities therefore embedding can be constructed directly.']",,,Proceedings of the VLDB Endowment,"['sparql', 'query', 'users', 'data querying', 'Data Querying']"
Data Querying,fd0496ab020acf366375615ab40235e6dd3c5897,Querying Geo-Textual Data: Spatial Keyword Queries and Beyond,https://www.semanticscholar.org/paper/fd0496ab020acf366375615ab40235e6dd3c5897,Conference,"Over the past decade, we have moved from a predominantly desktop based web to a predominantly mobile web, where users most often access the web from mobile devices such as smartphones. In addition, we are witnessing a proliferation of geo-located, textual web content. Motivated in part by these developments, the research community has been hard at work enabling the efficient computation of a variety of query functionality on geo-textual data, yielding a sizable body of literature on the querying of geo-textual data. With a focus on different types of keyword-based queries on geo-textual data, the tutorial also explores topics such as continuous queries on streaming geo-textual data, queries that retrieve attractive regions of geo-textual objects, and queries that extract properties, e.g., topics and top-$k$ frequent words, of the objects in regions. The tutorial is designed to offer an overview of the problems addressed in this body of literature and offers an overview of pertinent concepts and techniques. In addition, the tutorial suggests open problems and new research direction.",2016,43,,"[144145211.0, 145917501.0]",144145211.0,Monaco,3,"[46174961.0, 3268360.0, 2273645419.0]",Y,"['- What is the feedback in the CIFAR-10 experiments?', 'This is a solid paper about model evaluation in the chemical domain.', ""If the loss for the task is proper, then it's well known how to construct a divergence which coincides with the optimal risk.""]",SIGMOD Conference,16.0,,"['web', 'data', 'queries', 'data querying', 'Data Querying']"
Data Querying,eb76aabdb294814d36b11f1d961f3a0fa2d04e57,Integrated Querying of SQL database data and S3 data in Amazon Redshift,https://www.semanticscholar.org/paper/eb76aabdb294814d36b11f1d961f3a0fa2d04e57,JournalArticle,"Amazon Redshift features integrated, in-place access to data residing in a relational database and to data residing in the Amazon S3 object storage. In this paper we discuss associated query planning and processing aspects. Redshift plays the role of the integration query processor, in addition to the usual processor of queries over Redshift tables. In particular, during query execution, every compute node of a Redshift cluster issues (sub)queries over S3 objects, employing a novel multi-tenant (sub)query execution layer, called Amazon Redshift Spectrum, and merges/joins the results in an streaming and parallel fashion. The Spectrum layer offers massive scalability, with independent scaling of storage and computation. Redshifts optimizer determines how to minimize the amount of data scanned by Spectrum, the amount of data communicated to Redshift and the number of Spectrum nodes to be used. In particular, Redshifts query processor dynamically prunes partitions and pushes subqueries to Spectrum, recognizing which objects are relevant and restricting the subqueries to a subset of SQL that is amenable to Spectrums massively scalable processing. Furthermore, Redshift employs novel dynamic optimization techniques in order to formulate the subqueries. One such technique is a variant of semijoin reduction, which is combined in Redshift with join-aggregation query rewritings. Other optimizations and rewrit-ings relate to the memory footprint of query processing in Spectrum, and the SQL functionality that it is being supported by the Spectrum layer. The users of Redshift use the same SQL syntax to access scalar Redshift and external tables.",2018,14,41,"[39527078.0, 2062848.0, 2110760082.0, 12421844.0, 2754078.0, 1786049.0, 1835963.0]",39527078.0,Valletta,3,"[2257310922.0, 2266467782.0, 2162042348.0]",Y,"['In the videos, it seems that the people and chairs are always in the same place.', 'A key baseline that is missing is pretraining the generator as a language model over summaries.', 'Previous formulation of GAIL uses a stochastic behavior policy and the RIENFORCE-like algorithms.']",,,IEEE Data Engineering Bulletin,"['redshift', 'query', 'spectrum', 'data querying', 'Data Querying']"
Data Querying,9312e5efa0dcef1445d45a41771f12e2a8dc6715,Towards Schema-independent Querying on Document Data Stores,https://www.semanticscholar.org/paper/9312e5efa0dcef1445d45a41771f12e2a8dc6715,Workshop,"Documents are a pervasive semi-structured data model in today's web and internet of things applications where the data structure is rapidly evolving over time. NoSQL documents stores are well tailored to efficiently load and manage large amounts of heterogeneous documents without any prior structure validations or constraints. However, this flexibility becomes a serious challenge while querying data from a heterogeneous collection of documents. Hence, it is mandatory to modify existing queries or add new ones whenever new structures are introduced in the collection. In this paper, we propose a novel approach to enable transparent querying over a heterogeneous collection of documents. We offer an automatic query enrichment mechanism that benefits from a pre-materialized dictionary gathering different possible underlying document structures. The query enrichment is automated via query operators rewriting algorithms. Also, we refer to a set of experiments to evaluate the performances of our approach over a synthetic datas",2018,9,,"[35318166.0, 1702822.0, 1761667.0, 2862341.0]",35318166.0,Bucharest,3,"[1990265392.0, 40231586.0, 2115725948.0]",Y,"['Quite a few natural questions left unanswered, limiting what readers can learn from this paper, e.g. - How quickly does the model learn?', ""I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships."", 'It would be good if they could expand and analyze how well does their framework generalizes across other non-binary tasks, tasks in other domains and different NNs.']",International Workshop on Data Warehousing and OLAP,18.0,,"['documents', 'data', 'collection', 'data querying', 'Data Querying']"
Data Querying,e4788ee4f5e90c6f42cedc5116acd2d6475c3180,QUIS: InSitu Heterogeneous Data Source Querying,https://www.semanticscholar.org/paper/e4788ee4f5e90c6f42cedc5116acd2d6475c3180,JournalArticle,"Existing data integration frameworks are poorly suited for the special requirements of scientists. To answer a specific research question, often, excerpts of data from different sources need to be integrated. The relevant parts and the set of underlying sources may differ from query to query. The analyses also oftentimes involve frequently changing data and exploratory querying. Additionally, The data sources not only store data in different formats, but also provide inconsistent data access functionality. The classic Extract-Transform-Load (ETL) approach seems too complex and time-consuming and does not fit well with interest and expertise of the scientists. 
 
With QUIS (QUery In-Situ), we provide a solution for this problem. QUIS is an open source heterogeneous in-situ data querying system. It utilizes a federated query virtualization approach that is built upon plugged-in adapters. QUIS takes a user query and transforms appropriate portions of it into the corresponding computation model on individual data sources and executes it. It complements the segments of the query that the target data sources can not execute. Hence, it guarantees full syntax and semantic support for its language on all data sources. QUIS's in-situ querying facility almost eliminates the time to prepare the data while maintaining a competitive performance and steady scalability. 
 
The present demonstration illustrates interesting features of the system: virtual Schemas, heterogeneous joins, and visual query results. We provide a realistic data processing scenario to examine the system's features. Users can interact with QUIS using its desktop workbench, command line interface, or from any R client including RStudio Server.",2017,9,10,"[2840853.0, 1389035902.0, 145531067.0]",2840853.0,Zagreb,2,"[52198091.0, 2024783.0]",Y,"['— Is the “fork” module the main contribution of the paper?', 'Interestingly, the assisted method starts off much higher in the “reacher” task.']",,,Proceedings of the VLDB Endowment,"['data', 'query', 'sources', 'data querying', 'Data Querying']"
Data Querying,52e510271b172d098ec9b107a4159216ec08527e,Querying Big Data by Accessing Small Data,https://www.semanticscholar.org/paper/52e510271b172d098ec9b107a4159216ec08527e,Conference,"This paper investigates the feasibility of querying big data by accessing a bounded amount of the data. We study boundedly evaluable queries under a form of access constraints, when their evaluation cost is determined by the queries and constraints only. While it is undecidable to determine whether FO queries are boundedly evaluable, we show that for several classes of FO queries, the bounded evaluability problem is decidable. We also provide characterization and effective syntax for their boundedly evaluable queries. When a query Q is not boundedly evaluable, we study two approaches to approximately answering Q under access constraints. (1) We search for upper and lower envelopes of Q that are boundedly evaluable and warrant a constant accuracy bound. (2) We instantiate a minimum set of variables (parameters) in Q such that the specialized query is boundedly evaluable. We study problems for deciding the existence of envelopes and bounded specialized queries, and establish their complexity for various classes of FO queries.",2015,42,,"[144502903.0, 1729031.0, 2125469004.0, 2063047670.0, 2069299218.0]",144502903.0,Vilnius,3,"[1791253.0, 35202970.0, 2145907057.0]",Y,"['3. This paper lack original technical contribution from themselves.', 'The authors show through several experiments that the divide and conquer (DnC) technique can solve more complex tasks than can be solved with conventional policy gradient methods (TRPO is used as the baseline).', 'Wasserstein distances for eliminating batch effect, not enough novelty and no thorough comparisons to other methods.']",ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,34.0,,"['queries', 'study', 'constraints', 'data querying', 'Data Querying']"
Data Querying,5e4141d4be74fe673f092036822f1312103b9294,Mixed-instance querying: a lightweight integration architecture for data journalism,https://www.semanticscholar.org/paper/5e4141d4be74fe673f092036822f1312103b9294,JournalArticle,"As the world's affairs get increasingly more digital, timely production and consumption of news require to efficiently and quickly exploit heterogeneous data sources. Discussions with journalists revealed that content management tools currently at their disposal fall very short of expectations. We demonstrate Tatooine, a lightweight data integration prototype, which allows to quickly set up integration queries across (very) heterogeneous data sources, capitalizing on the many data links (joins) available in this application domain. Our demonstration is based on scenarios we study in collaboration with Le Monde, France's major newspaper.",2016,38,9,"[3361826.0, 152612731.0, 2772242.0, 2273645.0, 2086632521.0, 1739309.0, 1412353532.0, 3099593.0, 2404850.0, 2812731.0]",3361826.0,Bucharest,2,"[1734332.0, 2254047333.0]",Y,"['Additional regularization terms are also added to encourage the model to encode longer term dependencies in its latent distributions.', 'Preliminary experimental results are reported which demonstrate that ISRLU can perform similar to ELU.']",,,Proceedings of the VLDB Endowment,"['data', 'sources', 'integration', 'data querying', 'Data Querying']"
Data Querying,d57f11ed40c3cdcbb36cb758191db4f2c9372965,An extension of SPARQL with fuzzy navigational capabilities for querying fuzzy RDF data,https://www.semanticscholar.org/paper/d57f11ed40c3cdcbb36cb758191db4f2c9372965,Conference,"The Resource Description Framework (RDF) is the graph-based standard data model for representing semantic web information, and SPARQL is the standard query language for querying RDF data. Because of the huge volume of linked open data published on the web, these standards have aroused a large interest in the last years. This paper proposes a fuzzy extension of the SPARQL language that improves its expressiveness and usability. This extension allows (1) to query a fuzzy RDF data model, and (2) to express fuzzy preferences on data and on the structure of the data graph, which has not been proposed in any previous fuzzy extensions of SPARQL.",2016,24,,"[1741623.0, 50458739.0, 2673479.0]",1741623.0,Athens,3,"[49889487.0, 40228633.0, 8347899.0]",Y,"['An additional GAN loss is used on the generator output to encourage the output to look like summaries -- this procedure only requires unpaired summaries.', 'In the paper, the authors proposed using GAN for anomaly detection.', 'The authors propose a centralized solution to the problem by adapting the Deep Q-learning Network model.']",IEEE International Conference on Fuzzy Systems,16.0,,"['data', 'rdf', 'sparql', 'data querying', 'Data Querying']"
Data Querying,a620ea8654f86b282e0d2c1fbc6616b90ca45bd1,"A Data Analytic Algorithm for Managing, Querying, and Processing Uncertain Big Data in Cloud Environments",https://www.semanticscholar.org/paper/a620ea8654f86b282e0d2c1fbc6616b90ca45bd1,JournalArticle,"Big data are everywhere as high volumes of varieties of valuable precise and uncertain data can be easily collected or generated at high velocity in various real-life applications. Embedded in these big data are rich sets of useful information and knowledge. To mine these big data and to discover useful information and knowledge, we present a data analytic algorithm in this article. Our algorithm manages, queries, and processes uncertain big data in cloud environments. More specifically, it manages transactions of uncertain big data, allows users to query these big data by specifying constraints expressing their interests, and processes the user-specified constraints to discover useful information and knowledge from the uncertain big data. As each item in every transaction in these uncertain big data is associated with an existential probability value expressing the likelihood of that item to be present in a particular transaction, computation could be intensive. Our algorithm uses the MapReduce model on a cloud environment for effective data analytics on these uncertain big data. Experimental results show the effectiveness of our data analytic algorithm for managing, querying, and processing uncertain big data in cloud environments.",2015,45,8,"[2069981213.0, 1726081.0]",2069981213.0,Minsk,2,"[5521724.0, 51413028.0]",Y,"['Secondly, there are no other state-of-the-art baselines are used.', 'The real value of the paper is in providing an alternative way of thinking about LSTMs that is theoretically sound and intuitive.']",,,Algorithms,"['data', 'algorithm', 'information', 'data querying', 'Data Querying']"
Data Querying,d280cf82a6144b3e168840802b1a8a14d4eaccb9,"A Hierarchical Tensor-Based Approach to Compressing, Updating and Querying Geospatial Data",https://www.semanticscholar.org/paper/d280cf82a6144b3e168840802b1a8a14d4eaccb9,JournalArticle,"With the rapid development of data observation and model simulation in geoscience, spatial-temporal data have become increasingly multidimensional, massive and are consistently being updated. As a result, the integrated maintenance of these data is becoming a challenge. This paper presents a blocked hierarchical tensor representation within the split-and-merge paradigm for the compressed storage, continuously updating and data querying of multidimensional geospatial field data. The original multidimensional geospatial field data are split into small blocks according to their spatial-temporal references. These blocks are represented and compressed hierarchically, and then combined into a single hierarchical tree as the representation of original data. With a buffered binary tree data structure and corresponding optimized operation algorithms, the original multidimensional geospatial field data can be continuously compressed, appended, and queried. Data from the 20th Century Reanalysis Monthly Mean Composites are used to evaluate the performance of this approach. Compared to traditional methods, the new approach is shown to retain the quality of the original data with much lower storage costs and faster computational performance. The result suggests that the blocked hierarchical tensor representation provides an effective structure for integrated storage, presentation and computation of multidimensional geospatial field data.",2015,19,27,"[2100621.0, 2660835.0, 145773524.0, 98150490.0, 2931093.0, 145555027.0]",2100621.0,Oslo,2,"[2066165404.0, 2151226838.0]",Y,"['A step in the right direction, with interesting results, but not a huge level of novelty.', 'Comments: The model proposes to learn a conditional stochastic deep model by training an output noise model on the input x_i and the residual y_i - g(x_i).']",,,IEEE Transactions on Knowledge and Data Engineering,"['data', 'field', 'representation', 'data querying', 'Data Querying']"
Data Querying,cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5,Querying RDF Data Using A Multigraph-based Approach,https://www.semanticscholar.org/paper/cad72a86c97e1b9ae6afb0b3ba45b966cd42e7e5,Conference,"RDF is a standard for the conceptual description of knowledge , and SPARQL is the query language conceived to query RDF data. The RDF data is cherished and exploited by various domains such as life sciences, Semantic Web, social network, etc. Further, its integration at Web-scale compels RDF management engines to deal with complex queries in terms of both size and structure. In this paper, we propose AMbER (Attributed Multigraph Based Engine for RDF querying), a novel RDF query engine specifically designed to optimize the computation of complex queries. AMbER leverages subgraph matching techniques and extends them to tackle the SPARQL query problem. First of all RDF data is represented as a multigraph, and then novel indexing structures are established to efficiently access the information from the multigraph. Finally a SPARQL query is represented as a multigraph, and the SPARQL querying problem is reduced to the subgraph homomorphism problem. AMbER exploits structural properties of the query multigraph as well as the proposed indexes, in order to tackle the problem of subgraph homomorphism. The performance of AMbER, in comparison with state-of-the-art systems, has been extensively evaluated over several RDF benchmarks. The advantages of employing AMbER for complex SPARQL queries have been experimentally validated.",2016,14,,"[2810491.0, 1789397.0, 1744598.0, 1725656.0]",2810491.0,Kiev,2,"[2845020.0, 2065048323.0]",Y,"[""My main concern with this work is that I don't see any mechanism in the framework that prevents an expert  (or few of them) to win all examples except its own learning capacities."", 'The proposed method is new and technically sound.']",International Conference on Extending Database Technology,16.0,,"['rdf', 'query', 'amber', 'data querying', 'Data Querying']"
Data Querying,f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f,Price-Optimal Querying with Data APIs,https://www.semanticscholar.org/paper/f5a843ccd7e4a74ada6f046fa1bbee6f5ba9213f,JournalArticle,"Data is increasingly being purchased online in data markets and REST APIs have emerged as a favored method to acquire such data. Typically, sellers charge buyers based on how much data they purchase. In many scenarios, buyers need to make repeated calls to the seller's API. The challenge is then for buyers to keep track of the data they purchase and avoid purchasing the same data twice. In this paper, we propose lightweight modifications to data APIs to achieve optimal history-aware pricing so that buyers are only charged once for data that they have purchased and that has not been updated. The key idea behind our approach is the notion of refunds: buyers buy data as needed but have the ability to ask for refunds of data that they had already purchased before. We show that our techniques can provide significant data cost savings while reducing overheads by two orders of magnitude as compared to the state-of-the-art competing approaches.",2016,23,9,"[1758301.0, 1718134.0, 144823759.0]",1758301.0,Berlin,2,"[1416319999.0, 3253856.0]",Y,"[""This may not be relevant for the practical results, but to be accurate, it can't be simply stated that the algorithm converges, without a more careful analysis."", 'Algorithm 1 does not make clear the relationship between the model learned in step 2 and the algorithms in steps 4 to 6.']",,,Proceedings of the VLDB Endowment,"['data', 'buyers', 'apis', 'data querying', 'Data Querying']"
Data Querying,0235c8697bf5ab8aef776d822746ce26fac0f7ba,LODeX: A Tool for Visual Querying Linked Open Data,https://www.semanticscholar.org/paper/0235c8697bf5ab8aef776d822746ce26fac0f7ba,Workshop,"Formulating a query on a Linked Open Data (LOD) source is not an easy task; a technical knowledge of the query language, and, the awareness of the structure of the dataset are essential to create a query. We present a revised version of LODeX that provides the user an easy way for building queries in a fast and interactive manner. When a user decides to explore a LOD source, he/she can take advantage of the Schema Summary produced by LODeX (i.e. a synthetic view of the dataset’s structure) and he/she can pick graphical elements from it to create a visual query. The tool also supports the user in browsing the results and, eventually, in refining the query. The prototype has been evaluated on hundreds of public SPARQL endpoints (listed in Data Hub) and it is available online at http://dbgroup.unimo.it/lodex2. A survey conducted on 27 users has demonstrated that our tool can effectively support both unskilled and skilled users in exploring and querying LOD datasets.",2015,20,,"[2064746531.0, 2724566.0, 3081813.0]",2064746531.0,Vaduz,3,"[1720763480.0, 1786049.0, 3098251.0]",Y,"['Given representations from the language model, a logistic regression classifier is trained with supervised data from the task of interest to produce the final model.', 'Finally, I’ve found the paper very interesting and promising but regarding the standard of scientific publication, it requires additional attention to provide a better description the model and discuss the learning scheme to get a strongest and reproducible approach.', 'They then evaluate several existing network architectures on these tasks, and show that results are not as impressive as others might have assumed they would be.']",International Workshop on the Semantic Web,15.0,,"['query', 'lod', 'user', 'data querying', 'Data Querying']"
Data Querying,77a59de2e2b832321875cadcf9619dc313f02384,SPARQL-LD: a SPARQL Extension for Fetching and Querying Linked Data,https://www.semanticscholar.org/paper/77a59de2e2b832321875cadcf9619dc313f02384,Workshop,"SPARQL is a standard query language for retrieving and manipulating RDF data. However, the majority of SPARQL implementations require the data to be available in advance, i.e., to exist in main memory or in a RDF repository (accessible through a SPARQL endpoint). Nonetheless, Linked Data exists in the Web in various forms; even an HTML Web page can contain RDF data through RDFa, or RDF data may be dynamically created by a Web Service. In this paper, we propose and demonstrate an extension of SPARQL 1.1, called SPARQL-LD, that allows to directly and flexibly exploit this wealth of data. SPARQL-LD allows to fetch, query and integrate in the same SPARQL query: i) data coming from online RDF or JSON-LD files, ii) data coming from dereferenceable URIs, iii) data embedded in Web pages as RDFa, iv) data that is dynamically created by Web Services, and v) data coming by querying other endpoints. A distinctive characteristic of this extension is that it enables to fetch and query even data in datasets returned by a portion of the query, i.e. discovered at query-execution time.",2015,17,,"[2393008.0, 1801959.0]",2393008.0,Bern,3,"[144482217.0, 37574242.0, 79470079.0]",Y,"['- Presumably the policy learned in Phase 1 is a decent model by itself, since it can reliably find candidate traces.', 'The paper is well-written, clearly illustrating the goal of this work and the corresponding approach.', 'I could then identify the influential dimension(s) by taking the largest absolute values in the gradient vector.']",International Workshop on the Semantic Web,15.0,,"['data', 'sparql', 'query', 'data querying', 'Data Querying']"
Data Querying,a5710a7a54b50c26f3c40c37a19b2ec63330e90b,Querying Semantic Web Data Cubes,https://www.semanticscholar.org/paper/a5710a7a54b50c26f3c40c37a19b2ec63330e90b,Workshop,"We address the problem of querying data cubes for Online Analytical Processing (OLAP) analysis, directly on the Semantic Web (SW). We rst introduce CQL, a simple algebra for querying data cubes at a conceptual level. Taking advantage of QB4OLAP metadata, we automatically translate CQL queries into SPARQL ones, and propose query optimization strategies that adapt, to the particular OLAP setting, general-purpose techniques. A web application allows exploring and querying OLAP cubes on the SW, using the machinery presented here.",2016,12,,"[2586969.0, 2070105034.0]",2586969.0,Kiev,3,"[51479145.0, 41037252.0, 2065277797.0]",Y,"['This paper introduces a method for regularizing the REINFORCE algorithm by keeping around a small set of known high-quality samples as part of the sample set when performing stochastic gradient estimation.', 'There is a p(z_1:T) term that should appear in the integrand.', 'The manuscript mainly presents a cheap pruning algorithm for dense layers of DNNs.']",Alberto Mendelzon Workshop on Foundations of Data Management,16.0,,"['cubes', 'olap', 'data', 'data querying', 'Data Querying']"
Data Querying,5cdb31c5e3e0aa4cc2d10229793b4911f69fd78f,Ontology for Querying Heterogeneous Data Sources in Freight Transportation,https://www.semanticscholar.org/paper/5cdb31c5e3e0aa4cc2d10229793b4911f69fd78f,JournalArticle,"AbstractNavigating through multiple heterogeneous freight data sources to find the ones that are relevant to answering a question can be a challenging task when performed manually. It is highly dependent on the individual’s knowledge of all available data sources and the information contained in each one. Multiple factors contribute to freight data heterogeneity, including differences in data element definitions, level of disaggregation, and classification systems. This paper proposes a standardized knowledge representation of freight data sources using an ontology that both computer systems and domain experts can understand. The ontology is developed from a formal representation of data elements found to exist in freight data sources. The paper also presents a querying algorithm for searching through the ontology and determining relevant freight data sources that can be used for answering user queries. The proposed ontology and querying algorithm facilitate the querying and identification of relevant fre...",2016,10,30,"[51413028.0, 72448716.0, 1483964179.0]",51413028.0,Bucharest,3,"[2064509404.0, 52221692.0, 50355692.0]",Y,"['However, no empirical comparison between the two methods is performed.', 'In my opinion both contributions suffer from some significant limitations.', 'Perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the E-step output in EM for a GMM.']",,,Journal of computing in civil engineering,"['data', 'freight', 'sources', 'data querying', 'Data Querying']"
Data Querying,b69a35662a2cac38eab22f4481285116bdf8c30e,A fuzzy extension of SPARQL for querying gradual RDF data,https://www.semanticscholar.org/paper/b69a35662a2cac38eab22f4481285116bdf8c30e,Conference,"In this work, the first stones of a flexible approach to linked data querying based on fuzzy set theory are laid. Flexibility refers to the capability of expressing flexible queries over a RDF model containing gradual information.",2016,4,,"[1741623.0, 50458739.0, 145433223.0, 2673479.0]",1741623.0,Warsaw,3,"[1693182792.0, 5973699.0, 2108025636.0]",Y,"['A step in the right direction, with interesting results, but not a huge level of novelty.', 'How D will handle an example far from fake or real ones ?', 'Both datasets should be compared to LASSO as well.']",Research Challenges in Information Science,16.0,,"['stones', 'flexible', 'approach', 'data querying', 'Data Querying']"
Data Querying,343500e0052eb1b683f32b00efbbd1331c94184a,Sapphire: Querying RDF Data Made Simple,https://www.semanticscholar.org/paper/343500e0052eb1b683f32b00efbbd1331c94184a,JournalArticle,"There is currently a large amount of publicly accessible structured data available as RDF data sets. For example, the Linked Open Data (LOD) cloud now consists of thousands of RDF data sets with over 30 billion triples, and the number and size of the data sets is continuously growing. Many of the data sets in the LOD cloud provide public SPARQL endpoints to allow issuing queries over them. These end-points enable users to retrieve data using precise and highly expressive SPARQL queries. However, in order to do so, the user must have sufficient knowledge about the data sets that she wishes to query, that is, the structure of data, the vocabulary used within the data set, the exact values of literals, their data types, etc. Thus, while SPARQL is powerful, it is not easy to use. An alternative to SPARQL that does not require as much prior knowledge of the data is some form of keyword search over the structured data. Keyword search queries are easy to use, but inherently ambiguous in describing structured queries. 
 
This demonstration introduces Sapphire, a system for querying RDF data that strikes a middle ground between ambiguous keyword search and difficult-to-use SPARQL. Our system does not replace either, but utilizes both where they are most effective. Sapphire helps the user construct expressive SPARQL queries that represent her information needs without requiring detailed knowledge about the queried data sets. These queries are then executed over public SPARQL endpoints from the LOD cloud. Sapphire guides the user in the query writing process by showing suggestions of query terms based on the queried data, and by recommending changes to the query based on a predictive user model.",2016,14,9,"[90981528.0, 2073283133.0, 1704622.0, 145580839.0]",90981528.0,Skopje,2,"[2114147314.0, 3192572.0]",Y,"['How much can change between the goal images and the environment before the system fails?', 'Qualitative results show that the attentional mechanism attends to objects which are close to the context object together, acting like the heuristic neighborhood mechanism from previous work.']",,,Proceedings of the VLDB Endowment,"['data', 'sparql', 'queries', 'data querying', 'Data Querying']"
Data Querying,1661d0d8d47cac41e01c59c60aac3675b4396698,A demonstration of Shahed: A MapReduce-based system for querying and visualizing satellite data,https://www.semanticscholar.org/paper/1661d0d8d47cac41e01c59c60aac3675b4396698,Conference,"Several space agencies such as NASA are continuously collecting datasets of earth dynamics-e.g., temperature, vegetation, and cloud coverage-through satellites. This data is stored in a publicly available archive for scientists and researchers and is very useful for studying climate, desertification, and land use change. The benefit of this data comes from its richness as it provides an archived history for over 15 years of satellite observations. Unfortunately, the use of such data is very limited due to the huge size of archives (> 500TB) and the limited capabilities of traditional applications. In this demo, we present Shahed, an interactive system which provides an efficient way to index, query, and visualize satellite datasets available in NASA archive. Shahed is composed of four main modules. The uncertainty module resolves data uncertainty imposed by the satellites. The indexing module organizes the data in a novel multi-resolution spatio-temporal index designed for satellite data. The querying module uses the indexes to answer both spatiotemporal selection and aggregate queries provided by the user. The visualization module generates images, videos, and multi-level images which gives an insight of data distribution and dynamics over time. This demo gives users a hands-on experience with Shahed through a map-based web interface in which users can browse the available datasets using the map, issue spatiotemporal queries, and visualize the results as images or videos.",2015,9,,"[1876123.0, 1388622435.0, 1404602362.0, 2279240.0, 2635216.0, 3087877.0, 1756679.0]",1876123.0,Andorra,3,"[1394550182.0, 40443723.0, 2083106.0]",Y,"['Is this on bAbi as well?', 'The “obverter” technique is quite interesting since it incorporates the concept from the theory of mind which is similar to human alignment or AGI approach.', 'For example, when showing that the head direction cells generalize in the new mazes how can we be sure that it is not using a common lighting scheme common to both train and test mazes to orient itself?']",IEEE International Conference on Data Engineering,31.0,,"['data', 'module', 'datasets', 'data querying', 'Data Querying']"
Data Querying,dd2deed2ce6e110236a1280db765fa02c7488eb1,Indexing Fuzzy Spatiotemporal Data for Efficient Querying: A Meteorological Application,https://www.semanticscholar.org/paper/dd2deed2ce6e110236a1280db765fa02c7488eb1,JournalArticle,"Spatiotemporal data, in particular fuzzy and complex spatial objects representing geographic entities and relations, is a topic of great importance in geographic information systems and environmental data management systems. For database researchers, modeling and designing a database of fuzzy spatiotemporal data and querying such a database efficiently have been challenging issues due to complex spatial features and uncertainty involved. This paper presents an integrated approach to modeling, indexing, and efficiently querying spatiotemporal data related to fuzzy spatial and complex objects and spatial relations. As our case study, we design and implement a meteorological database application that involves fuzzy spatial and complex objects, and a spatiotemporal index structure, and supports various types of spatial queries including fuzzy spatiotemporal queries. Our implementation is based on an intelligent database system architecture that combines a fuzzy object-oriented database with a fuzzy knowledge base.",2015,10,23,"[3262907.0, 144502489.0, 1712420.0]",3262907.0,Sofia,2,"[98034859.0, 1733290.0]",Y,"['We have to, however, evaluate the approach on what it is able to do at the moment.', 'The manuscript mainly presents a cheap pruning algorithm for dense layers of DNNs.']",,,IEEE transactions on fuzzy systems,"['database', 'data', 'relations', 'data querying', 'Data Querying']"
Data Querying,8ab93bee04cd5bdbd96002b2e325d02f61ba695a,TUSQ: Targeted High-Utility Sequence Querying,https://www.semanticscholar.org/paper/8ab93bee04cd5bdbd96002b2e325d02f61ba695a,JournalArticle,"Significant efforts have been expended in the research and development of a database management system (DBMS) that has a wide range of applications for managing an enormous collection of multisource, heterogeneous, complex, or growing data. Besides the primary function (i.e., create, delete, and update), a practical and impeccable DBMS can interact with users through information selection, that is, querying with their targets. Previous querying algorithms, such as frequent itemset querying and sequential pattern querying (SPQ) have focused on the measurement of frequency, which does not involve the concept of utility, which is helpful for users to discover more informative patterns. To apply the querying technology for wider applications, we incorporate utility into target-oriented SPQ and formulate the task of targeted utility-oriented sequence querying. To address the proposed problem, we develop a novel algorithm, namely targeted high-utility sequence querying (TUSQ), based on two novel upper bounds (suffix remain utility and terminated descendants utility) as well as a vertical last instance table. For further efficiency, TUSQ relies on a projection technology utilizing a compact data structure called the targeted chain. An extensive experimental study conducted on several real and synthetic datasets shows that the proposed algorithm outperformed the designed baseline algorithm in terms of runtime, memory consumption, and candidate filtering.",2021,16,9,"[3114478.0, 35061398.0, 2061514417.0, 3045042.0, 2111880710.0, 144019071.0]",3114478.0,Podgorica,3,"[2074432.0, 98182097.0, 2156252582.0]",Y,"['As a demonstration of the usefulness of their approach, the authors ran their model on an unnamed open-source project and claimed to find several bugs, at least one of which potentially reduced memory performance.', 'This insight is simple, elegant and valuable in my opinion.', 'However no intuition is given as to why the mode collapse happens or why the single discriminator updates fail (see for ex. https://arxiv.org/abs/1705.10461)?']",,,IEEE Transactions on Big Data,"['utility', 'algorithm', 'applications', 'data querying', 'Data Querying']"
Data Querying,1a37223175138bc1aa53b425ea2fdd0b382405a5,Massively Distributed Time Series Indexing and Querying,https://www.semanticscholar.org/paper/1a37223175138bc1aa53b425ea2fdd0b382405a5,JournalArticle,"Indexing is crucial for many data mining tasks that rely on efficient and effective similarity query processing. Consequently, indexing large volumes of time series, along with high performance similarity query processing, have became topics of high interest. For many applications across diverse domains though, the amount of data to be processed might be intractable for a single machine, making existing centralized indexing solutions inefficient. We propose a parallel indexing solution that gracefully scales to billions of time series, and a parallel query processing strategy that, given a batch of queries, efficiently exploits the index. Our experiments, on both synthetic and real world data, illustrate that our index creation algorithm works on four billion time series in less than five hours, while the state of the art centralized algorithms do not scale and have their limit on 1 billion time series, where they need more than five days. Also, our distributed querying algorithm is able to efficiently process millions of queries over collections of billions of time series, thanks to an effective load balancing mechanism.",2020,40,32,"[32212787.0, 1709023.0, 2028901.0, 1725167.0]",32212787.0,Tallinn,2,"[1799822.0, 3262907.0]",Y,"['What is new compared to [Finn et al. 2016] is that the authors managed to train the network in combination with an adversarial loss, which allows for the generation of more realistic images.', 'Please, please discuss and cite some papers if required.']",,,IEEE Transactions on Knowledge and Data Engineering,"['time', 'series', 'data', 'data querying', 'Data Querying']"
Data Querying,fb0aeed456bff8607fab2bf443e9d86d51c3dff6,SMART-KG: Hybrid Shipping for SPARQL Querying on the Web,https://www.semanticscholar.org/paper/fb0aeed456bff8607fab2bf443e9d86d51c3dff6,Conference,"While Linked Data (LD) provides standards for publishing (RDF) and (SPARQL) querying Knowledge Graphs (KGs) on the Web, serving, accessing and processing such open, decentralized KGs is often practically impossible, as query timeouts on publicly available SPARQL endpoints show. Alternative solutions such as Triple Pattern Fragments (TPF) attempt to tackle the problem of availability by pushing query processing workload to the client side, but suffer from unnecessary transfer of irrelevant data on complex queries with large intermediate results. In this paper we present smart-KG, a novel approach to share the load between servers and clients, while significantly reducing data transfer volume, by combining TPF with shipping compressed KG partitions. Our evaluations show that smart-KG outperforms state-of-the-art client-side solutions and increases server-side availability towards more cost-effective and balanced hosting of open and decentralized KGs.",2020,28,,"[20829758.0, 144341429.0, 144841747.0, 2065791442.0, 1708607.0]",20829758.0,Belgrade,3,"[2273679997.0, 49298465.0, 1614034792.0]",Y,"['However, that is moved completely to the Appendix.', '- Top of page 5, when you describe the checkerboard BFS: please include a visualization somewhere, it could be in the Appendix.', 'This seems to prove that an NER is useful for tasks where DB queries are needed, rather than that the dynamic NE-Table construction is useful.']",The Web Conference,20.0,,"['data', 'kgs', 'sparql', 'data querying', 'Data Querying']"
Data Querying,a18f02e5c24e1f924aea268dd343bbdea234f2bb,"ChIP-Atlas 2021 update: a data-mining suite for exploring epigenomic landscapes by fully integrating ChIP-seq, ATAC-seq and Bisulfite-seq data",https://www.semanticscholar.org/paper/a18f02e5c24e1f924aea268dd343bbdea234f2bb,JournalArticle,"Abstract ChIP-Atlas (https://chip-atlas.org) is a web service providing both GUI- and API-based data-mining tools to reveal the architecture of the transcription regulatory landscape. ChIP-Atlas is powered by comprehensively integrating all data sets from high-throughput ChIP-seq and DNase-seq, a method for profiling chromatin regions accessible to DNase. In this update, we further collected all the ATAC-seq and whole-genome bisulfite-seq data for six model organisms (human, mouse, rat, fruit fly, nematode, and budding yeast) with the latest genome assemblies. These together with ChIP-seq data can be visualized with the Peak Browser tool and a genome browser to explore the epigenomic landscape of a query genomic locus, such as its chromatin accessibility, DNA methylation status, and protein–genome interactions. This epigenomic landscape can also be characterized for multiple genes and genomic loci by querying with the Enrichment Analysis tool, which, for example, revealed that inflammatory bowel disease-associated SNPs are the most significantly hypo-methylated in neutrophils. Therefore, ChIP-Atlas provides a panoramic view of the whole epigenomic landscape. All datasets are free to download via either a simple button on the web page or an API.",2022,91,50,"[2151226838.0, 5548191.0, 49030422.0, 3960565.0]",2151226838.0,Vaduz,2,"[40052301.0, 2283098327.0]",Y,"[""Those results show that as structural information is removed, the GGNN's performance diminishes, as expected."", 'Final Evaluation --- This paper clearly advances the body of work on neural intuitive physics by incorporating NEM entity representation to allow for less supervision.']",,,Nucleic Acids Res.,"['landscape', 'chipatlas', 'data', 'data querying', 'Data Querying']"
Data Querying,4b9184937da308914b9e13c43bfd75845eaf910b,Decentralizing Privacy: Using Blockchain to Protect Personal Data,https://www.semanticscholar.org/paper/4b9184937da308914b9e13c43bfd75845eaf910b,Workshop,"The recent increase in reported incidents of surveillance and security breaches compromising users' privacy call into question the current model, in which third-parties collect and control massive amounts of personal data. Bit coin has demonstrated in the financial space that trusted, auditable computing is possible using a decentralized network of peers accompanied by a public ledger. In this paper, we describe a decentralized personal data management system that ensures users own and control their data. We implement a protocol that turns a block chain into an automated access-control manager that does not require trust in a third party. Unlike Bit coin, transactions in our system are not strictly financial -- they are used to carry instructions, such as storing, querying and sharing data. Finally, we discuss possible future extensions to block chains that could harness them into a well-rounded solution for trusted computing problems in society.",2015,1895,,"[2653464.0, 31945113.0, 1682773.0]",2653464.0,Sarajevo,3,"[3018223.0, 145520115.0, 1753223.0]",Y,"['Overall, while I find there are some interesting theoretical bits in this paper, it lacks focus, the experiments do not offer any surprises, and there are no comparisons with prior literature.', 'The final sentence probably relates back to the CI approach.', 'I agree with the authors that this line of work, that is not very well known in the current machine learning community, includes a number of ideas that should be able to shed light on some of the currently open theoretical questions.']",2015 IEEE Security and Privacy Workshops,15.0,,"['data', 'users', 'control', 'data querying', 'Data Querying']"
Data Querying,6db0f8d396371078590faa7b34ae2e0e1b154a60,Building and Querying an Enterprise Knowledge Graph,https://www.semanticscholar.org/paper/6db0f8d396371078590faa7b34ae2e0e1b154a60,JournalArticle,"Information providers are faced with a critical challenge to process, retrieve and present information to their users in order to satisfy their complex information needs, because data has been increasing in an unprecedented manner, coming from diverse sources, and covering a variety of domains in heterogeneous formats. In this paper, we present Thomson Reuters’ effort in developing a family of services for building and querying an enterprise knowledge graph in order to address this challenge. We first acquire data from various sources via different approaches. Furthermore, we mine useful information from the data by adopting a variety of techniques, including Named Entity Recognition and Relation Extraction; such mined information is further integrated with existing structured data (e.g., via Entity Linking techniques) in order to obtain relatively comprehensive descriptions of the entities. By modeling the data as an RDF graph model, we enable easy data management and the embedding of rich semantics in our data. Finally, in order to facilitate the querying of this mined and integrated data, i.e., the knowledge graph, we propose TR Discover, a natural language interface that allows users to ask questions of our knowledge graph in their own words; such natural language questions are then translated into executable queries for answer retrieval. We evaluate our services, i.e., named entity recognition, relation extraction, entity linking and natural language interface, on real-world datasets, and demonstrate and discuss their practicability and limitations.",2019,36,12,"[34695819.0, 52222067.0, 3159223.0, 1412465368.0, 37722032.0, 1414195584.0, 1414195614.0, 32244429.0, 1410506555.0, 2135403.0, 1403903208.0, 5486617.0, 2061706386.0]",34695819.0,Luxembourg,3,"[2153627626.0, 2185953295.0, 3001926.0]",Y,"['The table 2 does not seem reliable result, and should use more folds and more randomizations, etc.', 'One might argue that visually, striped hyena is as informative as white tigers.', '1.\tThere are many syntax errors, e.g., “Closer to our approach recently in Aghasi et al. (2016) the authors”, “an cheap pruning algorithm”, etc.']",,,IEEE Transactions on Services Computing,"['data', 'information', 'graph', 'data querying', 'Data Querying']"
Data Querying,8b7b2e88207fc2dd849f5d83c9b6e890f0174abc,Expressive Languages for Querying the Semantic Web,https://www.semanticscholar.org/paper/8b7b2e88207fc2dd849f5d83c9b6e890f0174abc,JournalArticle,"The problem of querying RDF data is a central issue for the development of the Semantic Web. The query language SPARQL has become the standard language for querying RDF since its W3C standardization in 2008. However, the 2008 version of this language missed some important functionalities: reasoning capabilities to deal with RDFS and OWL vocabularies, navigational capabilities to exploit the graph structure of RDF data, and a general form of recursion much needed to express some natural queries. To overcome these limitations, a new version of SPARQL, called SPARQL 1.1, was released in 2013, which includes entailment regimes for RDFS and OWL vocabularies, and a mechanism to express navigation patterns through regular expressions. Unfortunately, there are a number of useful navigation patterns that cannot be expressed in SPARQL 1.1, and the language lacks a general mechanism to express recursive queries. To the best of our knowledge, no efficient RDF query language that combines the above functionalities is known. It is the aim of this work to fill this gap. To this end, we focus on a core fragment of the OWL 2 QL profile of OWL 2 and show that every SPARQL query enriched with the above features can be naturally translated into a query expressed in a language that is based on an extension of Datalog, which allows for value invention and stratified negation. However, the query evaluation problem for this language is highly intractable, which is not surprising since it is expressive enough to encode some inherently hard queries. We identify a natural fragment of it, and we show it to be tractable and powerful enough to define SPARQL queries enhanced with the desired functionalities.",2018,68,43,"[144658846.0, 1684745.0, 1771740.0]",144658846.0,Luxembourg,2,"[1904199594.0, 2145500840.0]",Y,"['The paper notices through simulations that in a grid search over the initial parameters of generator optimal discriminator training always succeeds in recovering the true generator parameters, whereas the other two methods fail and exhibit mode collapse.', 'In the meantime, they learn to generate reviews.']",,,TODS,"['language', 'sparql', 'query', 'data querying', 'Data Querying']"
Data Querying,88a724083b2cfcc096448c28e6973c8f761ee463,S2RDF: RDF Querying with SPARQL on Spark,https://www.semanticscholar.org/paper/88a724083b2cfcc096448c28e6973c8f761ee463,JournalArticle,"RDF has become very popular for semantic data publishing due to its flexible and universal graph-like data model. Thus, the ever-increasing size of RDF data collections raises the need for scalable distributed approaches. We endorse the usage of existing infrastructures for Big Data processing like Hadoop for this purpose. Yet, SPARQL query performance is a major challenge as Hadoop is not intentionally designed for RDF processing. Existing approaches often favor certain query pattern shapes while performance drops significantly for other shapes. In this paper, we introduce a novel relational partitioning schema for RDF data called ExtVP that uses a semi-join based preprocessing, akin to the concept of Join Indices in relational databases, to efficiently minimize query input size regardless of its pattern shape and diameter. Our prototype system S2RDF is built on top of Spark and uses SQL to execute SPARQL queries over ExtVP. We demonstrate its superior performance in comparison to state of the art SPARQL-on-Hadoop approaches.",2015,194,abs/1512.07021,"[2131714.0, 1403683697.0, 2772470.0, 1809410.0]",2131714.0,Vienna,2,"[1918781.0, 151480423.0]",Y,"['19 benchmarks are proposed, small data sets are expanded to a large, standardized data set and it is explored how to apply new RL techniques effectively for molecular design.', 'The paper makes some bold claims.']",,,Proceedings of the VLDB Endowment,"['data', 'rdf', 'approaches', 'data querying', 'Data Querying']"
Data Querying,f3eb8b6b836c0ef419e1fa565476e6892c8717ff,CoV-Spectrum: analysis of globally shared SARS-CoV-2 data to identify and characterize new variants,https://www.semanticscholar.org/paper/f3eb8b6b836c0ef419e1fa565476e6892c8717ff,JournalArticle,"Abstract Summary The CoV-Spectrum website supports the identification of new SARS-CoV-2 variants of concern and the tracking of known variants. Its flexible amino acid and nucleotide mutation search allows querying of variants before they are designated by a lineage nomenclature system. The platform brings together SARS-CoV-2 data from different sources and applies analyses. Results include the proportion of different variants over time, their demographic and geographic distributions, common mutations, hospitalization and mortality probabilities, estimates for transmission fitness advantage and insights obtained from wastewater samples. Availability and implementation CoV-Spectrum is available at https://cov-spectrum.org. The code is released under the GPL-3.0 license at https://github.com/cevo-public/cov-spectrum-website.",2021,186,38,"[4385113.0, 11684279.0, 2112425054.0, 2020052361.0, 2057542266.0, 2142494085.0, 2580290.0]",4385113.0,Berlin,2,"[39268701.0, 2163313042.0]",Y,"['The submitted paper shows that this principle is not a necessary condition large-scale classification.', 'The method is tested on a number of datasets (each used as source and target) and shows good transfer learning performance on each one.']",,,Bioinform.,"['variants', 'covspectrum', 'sarscov2', 'data querying', 'Data Querying']"
Data Querying,519ffd9744de5638d8c950090f065923e0793a93,Reachability Querying: Can It Be Even Faster?,https://www.semanticscholar.org/paper/519ffd9744de5638d8c950090f065923e0793a93,JournalArticle,"As an important graph operator, reachability query has been extensively studied over decades, which is to check whether a vertex can reach another vertex over a large directed graph <inline-formula><tex-math notation=""LaTeX"">$G$ </tex-math><alternatives><inline-graphic xlink:href=""zhu-ieq1-2631160.gif""/></alternatives></inline-formula> with <inline-formula><tex-math notation=""LaTeX"">$n$</tex-math><alternatives> <inline-graphic xlink:href=""zhu-ieq2-2631160.gif""/></alternatives></inline-formula> vertices and <inline-formula> <tex-math notation=""LaTeX"">$m$</tex-math><alternatives><inline-graphic xlink:href=""zhu-ieq3-2631160.gif""/> </alternatives></inline-formula> edges. The efforts made in the reported studies have greatly improved the query time of answering reachability queries online, while reducing the offline index construction time to construct an index with a reasonable size given the approach taken, where an entry in an index for a vertex is called a label of the vertex. Among all the work, the recent development of <italic>IP</italic> (Independent Permutation) employs randomness using <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives> <inline-graphic xlink:href=""zhu-ieq4-2631160.gif""/></alternatives></inline-formula>-min-wise independent permutations to process reachability queries, and shows the advantages for both query time and index construction time. In this paper, we propose a new Bloom filter Labeling, denoted as <italic>BFL</italic>. We show that the probability to answer reachability queries by <italic>BFL</italic> can be bounded, and <italic>BFL</italic> has high pruning power to answer more reachability queries directly. We give algorithms and analyze the pruning power of <italic>BFL</italic>. We conduct extensive studies using 19 large datasets. We show that <italic>BFL</italic> with an interval label performs best in the index construction time for all 19 cases, and performs best in query time for 16 out of 19 cases.",2017,74,29,"[2116956834.0, 2152202354.0, 2109393608.0, 144666776.0]",2116956834.0,Valletta,3,"[2673479.0, 46559852.0, 2108355738.0]",Y,"['Particularly, I found it interesting to re-evaluate the variance with (virtually) increasing larger batch size.', 'The experiments performed show that the Bayesian view of batch normalization performs similarly as MC dropout in terms of the estimates of uncertainty that it produces.', ""- Section 6: there is lots of math here, but the main results don't obviously stand out.""]",,,IEEE Transactions on Knowledge and Data Engineering,"['time', 'reachability', 'index', 'data querying', 'Data Querying']"
Data Querying,af13a92977d4f4dc5b28b13746d86111d42939e8,F1 Query: Declarative Querying at Scale,https://www.semanticscholar.org/paper/af13a92977d4f4dc5b28b13746d86111d42939e8,JournalArticle,"F1 Query is a stand-alone, federated query processing platform that executes SQL queries against data stored in different file-based formats as well as different storage systems at Google (e.g., Bigtable, Spanner, Google Spreadsheets, etc.). F1 Query eliminates the need to maintain the traditional distinction between different types of data processing workloads by simultaneously supporting: (i) OLTP-style point queries that affect only a few records; (ii) low-latency OLAP querying of large amounts of data; and (iii) large ETL pipelines. F1 Query has also significantly reduced the need for developing hard-coded data processing pipelines by enabling declarative queries integrated with custom business logic. F1 Query satisfies key requirements that are highly desirable within Google: (i) it provides a unified view over data that is fragmented and distributed over multiple data sources; (ii) it leverages datacenter resources for performant query processing with high throughput and low latency; (iii) it provides high scalability for large data sizes by increasing computational parallelism; and (iv) it is extensible and uses innovative approaches to integrate complex business logic in declarative query processing. This paper presents the end-to-end design of F1 Query. Evolved out of F1, the distributed database originally built to manage Google's advertising data, F1 Query has been in production for multiple years at Google and serves the querying needs of a large number of users and systems.",2018,36,11,"[2922886.0, 1958664.0, 2762838.0, 3001926.0, 1734332.0, 1410051210.0, 1726031123.0, 2083106.0, 2254406.0, 2709144.0, 1774311.0, 144317729.0, 2144495208.0, 2152986683.0, 153154328.0, 2112339870.0, 33857494.0, 2114309525.0, 89387275.0, 2090205657.0, 1414760032.0, 2111133570.0, 1772311.0, 2140961.0, 2324910.0, 2328103.0, 34842481.0, 2112558225.0, 145182862.0, 1410635059.0, 1799376.0, 1918781.0, 145204762.0, 143970078.0, 50179113.0, 1710223.0]",2922886.0,Zagreb,3,"[2069981213.0, 3382568.0, 49673164.0]",Y,"['The model is further extended for conditional generation and demonstrated on a range of image benchmark data sets.', 'By proper restructuring of paper and adding more details, the paper can be converted to a solid submission.', 'Since this could be a potential disadvantage, some discussions or empirical study on cross-category generalization seems to be interesting.']",,,Proceedings of the VLDB Endowment,"['query', 'data', 'f1', 'data querying', 'Data Querying']"
Data Querying,b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8,Expressive Time Series Querying with Hand-Drawn Scale-Free Sketches,https://www.semanticscholar.org/paper/b145f72d9abc4b2c48a46fd1bdc4476a4b5fa4f8,Conference,"We present Qetch, a tool where users freely sketch patterns on a scale-less canvas to query time series data without specifying query length or amplitude. We study how humans sketch time series patterns --- humans preserve visually salient perceptual features but often non-uniformly scale and locally distort a pattern --- and we develop a novel matching algorithm that accounts for human sketching errors. Qetch enables the easy construction of complex and expressive queries with two key features: regular expressions over sketches and relative positioning of sketches to query multiple time-aligned series. Through user studies, we demonstrate the effectiveness of Qetch's different interaction features. We also demonstrate the effectiveness of Qetch's matching algorithm compared to popular algorithms on targeted, and exploratory query-by-sketch search tasks on a variety of data sets.",2018,43,,"[40964207.0, 1698925.0]",40964207.0,Vilnius,3,"[144132699.0, 47319783.0, 2317297.0]",Y,"[""There's clear value in having good inductive biases (e.g. expressed in the form of the discriminator architecture) when defining divergences for practical applications."", 'The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy?', 'A good approach with some open questions on related work, scalability, and robustness The authors propose an approach for zero-shot visual learning.']",International Conference on Human Factors in Computing Systems,18.0,,"['query', 'series', 'features', 'data querying', 'Data Querying']"
Data Querying,10cf0045bc0f58aa3699e4451f65b12a08019c5c,"Demonstration of Taghreed: A system for querying, analyzing, and visualizing geotagged microblogs",https://www.semanticscholar.org/paper/10cf0045bc0f58aa3699e4451f65b12a08019c5c,Conference,"This paper demonstrates Taghreed; a full-fledged system for efficient and scalable querying, analyzing, and visualizing geotagged microblogs, such as tweets. Taghreed supports a wide variety of queries on all microblogs attributes. In addition, it is able to manage a large number (billions) of microblogs for relatively long periods, e.g., months. Taghreed consists of four main components: (1) indexer, (2) query engine, (3) recovery manager, and (4) visualizer. Taghreed indexer efficiently digests incoming microblogs with high arrival rates in light main-memory indexes. When the memory becomes full, the memory contents are flushed to disk indexes which are managing billions of microblogs efficiently. On memory failure, the recovery manager restores the memory contents from backup copies. Taghreed query engine consists of two modules: a query optimizer and a query processor. The query optimizer generates an optimized query plan to be executed by the query processor to provide low query responses. Taghreed visualizer features to its users a wide variety of spatiotemporal queries and presents the answers on a map-based user interface that allows an interactive exploration. Taghreed is the first system that addresses all these challenges collectively for geotagged microblogs data. The system is demonstrated based on real system implementation through different scenarios that show system functionality and internals.",2015,71,,"[143811079.0, 2877140.0, 1388622435.0, 2186764.0, 35205093.0, 2635216.0, 3087877.0, 1756679.0]",143811079.0,Tirana,2,"[2166312768.0, 2285813122.0]",Y,"['So, the main contribution of the paper is to do all the sensitivity calculations with respect to SVM problem and then use the importance sampling theory to obtain bounds on the coreset size.', 'The proposed algorithm in 3 out of 5 datasets, two of theme with statistical significant.']",IEEE International Conference on Data Engineering,31.0,,"['query', 'microblogs', 'system', 'data querying', 'Data Querying']"
Data Querying,0fa554d981809c5eb78956c779f75092c4f6c16b,psrqpy: a python interface for querying the ATNF pulsar catalogue,https://www.semanticscholar.org/paper/0fa554d981809c5eb78956c779f75092c4f6c16b,JournalArticle,"This Python module provides an interface for querying the Australia Telescope National Facility (ATNF) pulsar catalogue (Manchester et al. 2005). The intended users are astronomers wanting to extract data from the catalogue through a script rather than having to download and parse text tables output using the standard web interface. It allows users to access information, such as pulsar frequencies and sky locations, on all pulsars in the catalogue. Querying of the catalogue can easily be incorporated into Python scripts.",2018,20,3,[119136574.0],119136574.0,Vienna,2,"[115300694.0, 2229372676.0]",Y,"['(muscle routing parameters, including insertion and attachment points) are optimized along with the control).', 'The paper is very clearly written, and the proposal is very well placed in the context of previous methods for the same purpose.']",,,Journal of Open Source Software,"['catalogue', 'interface', 'pulsar', 'data querying', 'Data Querying']"
Data Querying,b5270b32a17fcc2e3dc209add6f4e4ba709b7358,Recent Advances in Querying Probabilistic Knowledge Bases,https://www.semanticscholar.org/paper/b5270b32a17fcc2e3dc209add6f4e4ba709b7358,Conference,"We give a survey on recent advances at the forefront of research on probabilistic knowledge bases for representing and querying large-scale automatically extracted data. We concentrate especially on increasing the semantic expressivity of formalisms for representing and querying probabilistic knowledge (i) by giving up the closed-world assumption, (ii) by allowing for commonsense knowledge (and in parallel giving up the tuple-independence assumption), and (iii) by giving up the closed-domain assumption, while preserving some computational properties of query answering in such formalisms.",2018,19,,"[1784772.0, 49633004.0, 1690572.0]",1784772.0,Budapest,3,"[2129460589.0, 10708829.0, 3382568.0]",Y,"['Moreover, I like the qualitative experiment (Figure 2) in which the tree is used to vary a latent dimension to change the digit’s class.', 'Standard idea, great results This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives.', 'All the experiments use large tasks - it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task.']",International Joint Conference on Artificial Intelligence,18.0,,"['knowledge', 'assumption', 'formalisms', 'data querying', 'Data Querying']"
Data Querying,0bca61986b8edeaf33018d0203b44110f2480110,haploR: an R package for querying web-based annotation tools,https://www.semanticscholar.org/paper/0bca61986b8edeaf33018d0203b44110f2480110,JournalArticle,"We developed haploR, an R package for querying web based genome annotation tools HaploReg and RegulomeDB. haploR gathers information in a data frame which is suitable for downstream bioinformatic analyses. This will facilitate post-genome wide association studies streamline analysis for rapid discovery and interpretation of genetic associations.",2017,33,6,"[29629766.0, 2183217.0, 2040119.0]",29629766.0,Andorra,2,"[50096877.0, 1919541.0]",Y,"['For example, the accuracy drops from 95.2% to 91% compared with Net-Trim in the LeNet-5 model and from 80.5% to 74.6% compared with LOBS in the CifarNet model.', 'As is often the case with such comparisons, it is hard to disentangle from where exactly come the speedups.']",,,F1000Research,"['haplor', 'r', 'package', 'data querying', 'Data Querying']"
Data Querying,2e7f532796eed2847d4c19e3cff03756049e81b4,Diversified Top-k Subgraph Querying in a Large Graph,https://www.semanticscholar.org/paper/2e7f532796eed2847d4c19e3cff03756049e81b4,Conference,"Subgraph querying in a large data graph is interesting for different applications. A recent study shows that top-k diversified results are useful since the number of matching subgraphs can be very large. In this work, we study the problem of top-k diversified subgraph querying that asks for a set of up to k subgraphs isomorphic to a given query graph, and that covers the largest number of vertices. We propose a novel level-based algorithm for this problem which supports early termination and has a theoretical approximation guarantee. From experiments, most of our results on real datasets used in previous works are near optimal with a query time within 10ms on a commodity machine.",2016,44,,"[2149234208.0, 1699363.0, 2186806073.0]",2149234208.0,Ljubljana,2,"[1394550182.0, 31131132.0]",Y,"['In general, this is an interesting direction to explore, the idea is interesting, however, I would like to see more experiments', 'But the aspect I like most about this paper is the experimental analysis.']",SIGMOD Conference,16.0,,"['subgraph', 'graph', 'study', 'data querying', 'Data Querying']"
Data Querying,834fdec542153aae5fe725df801aac87ba5e8f56,Graph Querying Meets HCI: State of the Art and Future Directions,https://www.semanticscholar.org/paper/834fdec542153aae5fe725df801aac87ba5e8f56,Conference,"Querying graph databases has emerged as an important research problem for real-world applications that center on large graph data. Given the syntactic complexity of graph query languages (e.g., SPARQL, Cypher), visual graph query interfaces make it easy for non-expert users to query such graph data repositories. In this tutorial, we survey recent developments in the emerging area of visual graph querying paradigm that bridges traditional graph querying with human computer interaction (HCI). We discuss manual and data-driven visual graph query interfaces, various strategies and guidance for constructing graph queries visually, interleaving processing of graph queries and visual actions, and visual exploration of graph query results. In addition, the tutorial suggests open problems and new research directions. In summary, in this tutorial we review and summarize the research thus far into HCI and graph querying in the database community, giving researchers a snapshot of the current state of the art in this topic, and future research directions.",2017,20,,"[1730344.0, 1891554.0, 2128664093.0]",1730344.0,Skopje,3,"[1764236.0, 1723636206.0, 3471557.0]",Y,"['2) compressing the embedding space using pca', 'It is investigated how several RL strategies perform on a large, standardized data set.', '2. The proposed attention mechanism seems to be fairly domain (or task ) specific, and may not be beneficial for strong generalization (generalization over unseen category).']",SIGMOD Conference,17.0,,"['graph', 'research', 'query', 'data querying', 'Data Querying']"
Data Querying,819f2778eba0d4c9eea86307bedaaeed94dc751d,"Pathway Commons 2019 Update: integration, analysis and exploration of pathway data",https://www.semanticscholar.org/paper/819f2778eba0d4c9eea86307bedaaeed94dc751d,JournalArticle,"Abstract Pathway Commons (https://www.pathwaycommons.org) is an integrated resource of publicly available information about biological pathways including biochemical reactions, assembly of biomolecular complexes, transport and catalysis events and physical interactions involving proteins, DNA, RNA, and small molecules (e.g. metabolites and drug compounds). Data is collected from multiple providers in standard formats, including the Biological Pathway Exchange (BioPAX) language and the Proteomics Standards Initiative Molecular Interactions format, and then integrated. Pathway Commons provides biologists with (i) tools to search this comprehensive resource, (ii) a download site offering integrated bulk sets of pathway data (e.g. tables of interactions and gene sets), (iii) reusable software libraries for working with pathway information in several programming languages (Java, R, Python and Javascript) and (iv) a web service for programmatically querying the entire dataset. Visualization of pathways is supported using the Systems Biological Graphical Notation (SBGN). Pathway Commons currently contains data from 22 databases with 4794 detailed human biochemical processes (i.e. pathways) and ∼2.3 million interactions. To enhance the usability of this large resource for end-users, we develop and maintain interactive web applications and training materials that enable pathway exploration and advanced analysis.",2019,190,48,"[2485039.0, 2039003.0, 2804763.0, 6562624.0, 48455738.0, 73460467.0, 1724282.0, 8847603.0, 1384224631.0, 1384221174.0, 1382043577.0, 1384221131.0, 1384221034.0, 1381370960.0, 2061150843.0, 1725976794.0, 1381997561.0, 48293468.0, 35805107.0, 49248672.0, 34762869.0, 2276851.0, 2807929.0, 144937305.0, 144882390.0]",2485039.0,Copenhagen,2,"[1414195584.0, 2146335468.0]",Y,"['The authors propose a new gradient compression method for efficient distributed training of neural networks.', 'So the PATH function helps and longer paths are better.']",,,Nucleic Acids Res.,"['pathway', 'interactions', 'commons', 'data querying', 'Data Querying']"
Data Querying,87048df918c34b662bc0d28894efa430d70a9206,Crowdsourced Clustering: Querying Edges vs Triangles,https://www.semanticscholar.org/paper/87048df918c34b662bc0d28894efa430d70a9206,Conference,"We consider the task of clustering items using answers from non-expert crowd workers. In such cases, the workers are often not able to label the items directly, however, it is reasonable to assume that they can compare items and judge whether they are similar or not. An important question is what queries to make, and we compare two types: random edge queries, where a pair of items is revealed, and random triangles, where a triple is. Since it is far too expensive to query all possible edges and/or triangles, we need to work with partial observations subject to a fixed query budget constraint. When a generative model for the data is available (and we consider a few of these) we determine the cost of a query by its entropy; when such models do not exist we use the average response time per query of the workers as a surrogate for the cost. In addition to theoretical justification, through several simulations and experiments on two real data sets on Amazon Mechanical Turk, we empirically demonstrate that, for a fixed budget, triangle queries uniformly outperform edge queries. Even though, in contrast to edge queries, triangle queries reveal dependent edges, they provide more reliable edges and, for a fixed budget, many more of them. We also provide a sufficient condition on the number of observations, edge densities inside and outside the clusters and the minimum cluster size required for the exact recovery of the true adjacency matrix via triangle queries using a convex optimization-based clustering algorithm.",2016,45,,"[40275640.0, 1736279.0]",40275640.0,Berlin,2,"[2115338656.0, 2086632521.0]",Y,"['It is also simple and easy to understand.', 'This section could be improved by demonstrating the approach on more datasets.']",Neural Information Processing Systems,16.0,,"['items', 'queries', 'query', 'data querying', 'Data Querying']"
Data Querying,7eaac9847257c32afd450017d1348ecda4dcaade,NOUS: Construction and Querying of Dynamic Knowledge Graphs,https://www.semanticscholar.org/paper/7eaac9847257c32afd450017d1348ecda4dcaade,Conference,"The ability to construct domain specific knowledge graphs (KG) and perform question-answering or hypothesis generation is a transformative capability. Despite their value, automated construction of knowledge graphs remains an expensive technical challenge that is beyond the reach for most enterprises and academic institutions. We propose an end-toend framework for developing custom knowledge graph driven analytics for arbitrary application domains. The uniqueness of our system lies A) in its combination of curated KGs along with knowledge extracted from unstructured text, B) support for advanced trending and explanatory questions on a dynamic KG, and C) the ability to answer queries where the answer is embedded across multiple data sources.",2016,50,,"[7617146.0, 39073194.0, 1950842.0, 1786639.0, 1882948.0, 121917790.0, 143840196.0]",7617146.0,Valletta,2,"[2064711347.0, 46255467.0]",Y,"['Concerning the lack of mathematical rigour in the statistical physics literature on which the authors comment, they might want to relate to a very recent work https://arxiv.org/pdf/1708.03395.pdf work that sets all the past statistical physics results on optimal generalization in single layer neural networks on fully rigorous basis by proving that the corresponding formulas stemming from the replica method are indeed correct.', 'Also, the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies, i.e., a priori information.']",IEEE International Conference on Data Engineering,33.0,,"['knowledge', 'ability', 'graphs', 'data querying', 'Data Querying']"
